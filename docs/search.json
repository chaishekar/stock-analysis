[
  {
    "objectID": "about.html",
    "href": "about.html",
    "title": "",
    "section": "",
    "text": "Name: Chaitanya Shekar\nNet ID: cs2046\nHello everyone! My name is Chaitanya Shekar pursuing my masters degree in Data Science and Analytics at Georgetown University. I graduated with a bachelor’s degree with a major in statistics, mathematics, and economics. During the intensive programming component of my master’s degree, I’m learning skills and techniques to deal with unique challenges in complex algorithms and scientific problems. My DSAN goals involve strengthening my machine learning and deep learning understanding and programming skills.\nAs I take a quick look about, I see that the utilization of big data for business applications, establishing marketing trends, and forecasting consumer behavior has skyrocketed, and as a direct consequence of this, the demand for both business and data analysts has also considerably increased. I want to deepen my understanding of data science and statistics as well as have a better handle on the intricacies that are present in both areas of study. My high school placed a significant emphasis on mathematics and science, which kindled my interest in data and motivated me to pursue a degree in statistics while I was enrolled in my university studies. As a consequence of this, I aimed my qualifications in the right direction and worked on developing my analytical skills in order to acquire a deeper comprehension of the influence that data can wield.\nI consider myself to be an ambitious person who has the long-term goal of working in the field of data science at some point in the not-too-distant future. I would like to continue developing my talents in this area. In addition to completing the requirements for a conventional graduate degree, I also earned certificates in data science and analysis, machine learning, neural networks, deep learning, programming in Python and R, and all these subjects. Because I have such a great interest in data sciences, I decided to carry this out. To estimate who would emerge triumphant from a game of roulette based on statistical probability and game theory as the underlying concepts, I constructed a model-based project for the inter-collegiate Statistics Festival. During the time that I was an undergraduate student, I took part in a project in the field of data science that concentrated primarily on statistical principles. The study focused on cardiovascular health and the factors that are related with increased risk of cardiovascular disease.\nFollowing the completion of my undergraduate degree, I obtained an internship with Feedback Business Consulting Services Pvt, Bangalore, in the role of research analyst. As part of my responsibilities, I was responsible for managing the data gathering for the Patent Operations B2B benchmark research as well as performing analysis.\nAfter giving my current skill set and the demand for innovative analystâ€™s considerable consideration, I’ve come to the conclusion that I want to initiate the process of converting myself into a global citizen who is capable of thriving in high-pressure work environments. This course will provide me with the right steppingstone to enable me to enter the field of sophisticated technology and play a leadership role in the investigation and development of breakthroughs of this kind. This class will also prepare me to enter the field of sophisticated technology. Because of the meticulous study, analysis, and presentation of needs that the program provides, I am in a position where I can provide ideas and provide professional advice. I am extremely confident that it will assist me in applying the skills by undertaking projects and paving the path for ground-breaking research and development; furthermore, the emphasis on real-world skills, such as problem-solving, legal, ethical, and professional framework throughout the course that would give a holistic view. I am extremely confident that it will assist me in applying the skills by undertaking projects and paving the path for ground-breaking research and development. My head is spinning at the amazing amount of data and breadth that is necessary because I am detail-oriented, enjoys working with a variety of data, and appreciates performing analysis. The development of the capabilities and skills essential to prosper in the industry while also being able to adjust to the continuous changes that are taking place in that sector is the purpose of the program. In addition to my academic interests now, at a time when the global information technology industry is on the edge of a major technological transition, the data sciences are crucial to our attempts to explore the next frontier.\nI am quite proud of my academic achievements, but as a person, I am also aware of the advantages I have enjoyed in life, and I want to help close the gaps by making positive contributions toward creating a more equitable world. It motivated me to get involved in actions for social causes, such as becoming an active member of the core committee of Amnesty International. was an event that left me feeling really fulfilled and got me thinking about how I could put my skills to use in the service of social improvement. When I think back on my path, I realize that it was at that point that I first became aware of the applications of data sciences in a variety of fields, including but not limited to business, artificial intelligence, healthcare, sustainability, and climate change. During that time, I participated in several different events and campaigns in which I pushed for human rights, LGBTQ rights, and the rights of other underrepresented groups in our society. As part of my efforts to further the cause, I went to Home of Hope, a shelter for the disadvantaged, and while there, I became aware of socioeconomic and structural disparities. Not only that, but I also took part in efforts to raise general awareness about cancer and the silent march for AIDS. It was something I desired to investigate, and I dove headfirst into the data.\nIn addition to pursuing a career in academia, one of my passions is traveling and learning about new countries and ways of life. In addition, a significant reason for me to get involved in the fight against climate change and global warming is to promote sustainable living practices in all aspects of human endeavor."
  },
  {
    "objectID": "arch_Consumer_Discretionary_Sector_Fund.html",
    "href": "arch_Consumer_Discretionary_Sector_Fund.html",
    "title": "",
    "section": "",
    "text": "Impact of Macroeconomic Factors on Amazon Stock Price\n\n\n\nThe Consumer Discretionary sector comprises companies that produce non-essential goods and services, such as apparel, entertainment, and luxury items. One of the top companies in this sector is Amazon.com, Inc. (AMZN), which has revolutionized the retail industry by providing customers with an online platform to purchase a wide range of products.\nBy using an ARIMAX+ARCH/GARCH model to analyze the stock price behavior of Amazon, we can gain insights into how macroeconomic factors impact the company’s performance. For example, an increase in GDP growth rate or a decrease in unemployment rate may lead to increased consumer spending and a rise in Amazon’s stock price. Conversely, an increase in inflation or interest rates may lead to a decrease in consumer spending and a decline in Amazon’s stock price.\n\n\nTime series Plot\n\nAmazonDifferentitaed Chart SeriesGDP GrowthInflationInterestUnemployment\n\n\n\n\nCode\n# get data\noptions(\"getSymbols.warning4.0\"=FALSE)\noptions(\"getSymbols.yahoo.warning\"=FALSE)\n\n\ndata.info = getSymbols(\"AMZN\",src='yahoo', from = '2010-01-01',to = \"2023-03-01\",auto.assign = FALSE)\ndata = getSymbols(\"AMZN\",src='yahoo', from = '2010-01-01',to = \"2023-03-01\")\ndf <- data.frame(Date=index(AMZN),coredata(AMZN))\n\n# create Bollinger Bands\nbbands <- BBands(AMZN[,c(\"AMZN.High\",\"AMZN.Low\",\"AMZN.Close\")])\n\n# join and subset data\ndf <- subset(cbind(df, data.frame(bbands[,1:3])), Date >= \"2010-01-01\")\n\n\n# colors column for increasing and decreasing\nfor (i in 1:length(df[,1])) {\n  if (df$AMZN.Close[i] >= df$AMZN.Open[i]) {\n      df$direction[i] = 'Increasing'\n  } else {\n      df$direction[i] = 'Decreasing'\n  }\n}\n\ni <- list(line = list(color = '#7FB3D5'))\nd <- list(line = list(color = '#7F7F7F'))\n\n# plot candlestick chart\n\nfig <- df %>% plot_ly(x = ~Date, type=\"candlestick\",\n          open = ~AMZN.Open, close = ~AMZN.Close,\n          high = ~AMZN.High, low = ~AMZN.Low, name = \"AMZN\",\n          increasing = i, decreasing = d) \nfig <- fig %>% add_lines(x = ~Date, y = ~up , name = \"B Bands\",\n            line = list(color = '#ccc', width = 0.5),\n            legendgroup = \"Bollinger Bands\",\n            hoverinfo = \"none\", inherit = F) \nfig <- fig %>% add_lines(x = ~Date, y = ~dn, name = \"B Bands\",\n            line = list(color = '#ccc', width = 0.5),\n            legendgroup = \"Bollinger Bands\", inherit = F,\n            showlegend = FALSE, hoverinfo = \"none\") \nfig <- fig %>% add_lines(x = ~Date, y = ~mavg, name = \"Mv Avg\",\n            line = list(color = '#C052B3', width = 0.5),\n            hoverinfo = \"none\", inherit = F) \nfig <- fig %>% layout(yaxis = list(title = \"Price\"))\n\n# plot volume bar chart\nfig2 <- df \nfig2 <- fig2 %>% plot_ly(x=~Date, y=~AMZN.Volume, type='bar', name = \"AMZN Volume\",\n          color = ~direction, colors = c('#7FB3D5','#7F7F7F')) \nfig2 <- fig2 %>% layout(yaxis = list(title = \"Volume\"))\n\n# create rangeselector buttons\nrs <- list(visible = TRUE, x = 0.5, y = -0.055,\n           xanchor = 'center', yref = 'paper',\n           font = list(size = 9),\n           buttons = list(\n             list(count=1,\n                  label='RESET',\n                  step='all'),\n             list(count=3,\n                  label='3 YR',\n                  step='year',\n                  stepmode='backward'),\n             list(count=1,\n                  label='1 YR',\n                  step='year',\n                  stepmode='backward'),\n             list(count=1,\n                  label='1 MO',\n                  step='month',\n                  stepmode='backward')\n           ))\n\n# subplot with shared x axis\nfig <- subplot(fig, fig2, heights = c(0.7,0.2), nrows=2,\n             shareX = TRUE, titleY = TRUE)\nfig <- fig %>% layout(title = paste(\"Amazon Stock Price: January 2010 - March 2023\"),\n         xaxis = list(rangeselector = rs),\n         legend = list(orientation = 'h', x = 0.5, y = 1,\n                       xanchor = 'center', yref = 'paper',\n                       font = list(size = 10),\n                       bgcolor = 'transparent'))\n\nfig\n\n\n\n\n\n\n\n\n\n\nCode\nlog(data.info$`AMZN.Adjusted`) %>% diff() %>% chartSeries(theme=chartTheme('white'),up.col='#7FB3D5')\n\n\n\n\n\n\n\n\n\nCode\n#import the data\ngdp <- read.csv(\"DATA/RAW DATA/gdp-growth.csv\")\n\n#change date format\ngdp$Date <- as.Date(gdp$DATE , \"%m/%d/%Y\")\n\n#drop DATE column\ngdp <- subset(gdp, select = -c(1))\n\n#export the cleaned data\ngdp_clean <- gdp\nwrite.csv(gdp_clean, \"DATA/CLEANED DATA/gdp_clean_data.csv\", row.names=FALSE)\n\n#plot gdp growth rate \nfig <- plot_ly(gdp, x = ~Date, y = ~value, type = 'scatter', mode = 'lines',line = list(color = 'rgb(240, 128, 128)'))\nfig <- fig %>% layout(title = \"U.S GPD Growth Rate: 2010 - 2022\",xaxis = list(title = \"Time\"),yaxis = list(title =\"GDP Growth Rate\"))\nfig\n\n\n\n\n\n\n\n\n\n\nCode\n#import the data\ninflation_rate <- read.csv(\"DATA/RAW DATA/inflation-rate.csv\")\n\n#cleaning the data\n#remove unwanted columns\ninflation_rate_clean <- subset(inflation_rate, select = -c(1,HALF1,HALF2))\n\n#convert the data to time series data\ninflation_data_ts <- ts(as.vector(t(as.matrix(inflation_rate_clean))), start=c(2010,1), end=c(2023,2), frequency=12)\n\n#export the data\nwrite.csv(inflation_rate_clean, \"DATA/CLEANED DATA/inflation_rate_clean_data.csv\", row.names=FALSE)\n\n\n#plot inflation rate \nfig <- autoplot(inflation_data_ts, ylab = \"Inflation Rate\", color=\"#FFA07A\")+ggtitle(\"U.S Inflation Rate: January 2010 - February 2023\")+theme_bw()\nggplotly(fig)\n\n\n\n\n\n\n\n\n\n\nCode\n#import the data\ninterest_data <- read.csv(\"DATA/RAW DATA/interest-rate.csv\")\n\n#change date format\ninterest_data$Date <- as.Date(interest_data$Date , \"%m/%d/%Y\")\n\n#export the cleaned data\ninterest_clean_data <- interest_data\nwrite.csv(interest_clean_data, \"DATA/CLEANED DATA/interest_rate_clean_data.csv\", row.names=FALSE)\n\n#plot interest rate \nfig <- plot_ly(interest_data, x = ~Date, y = ~value, type = 'scatter', mode = 'lines',line = list(color='rgb(219, 112, 147)'))\nfig <- fig %>% layout(title = \"U.S Interest Rate: January 2010 - March 2023\",xaxis = list(title = \"Time\"),yaxis = list(title =\"Interest Rate\"))\nfig\n\n\n\n\n\n\n\n\n\n\nCode\n#import the data\nunemployment_rate <- read.csv(\"DATA/RAW DATA/unemployment-rate.csv\")\n\n#change date format\nunemployment_rate$Date <- as.Date(unemployment_rate$Date , \"%m/%d/%Y\")\n\n# export the data\nwrite.csv(unemployment_rate, \"DATA/CLEANED DATA/unemployment_rate_clean_data.csv\", row.names=FALSE)\n\n#plot unemployment rate \nfig <- plot_ly(unemployment_rate, x = ~Date, y = ~Value, type = 'scatter', mode = 'lines',line = list(color = 'rgb(189, 183, 107)'))\nfig <- fig %>% layout(title = \"U.S Unemployment Rate: January 2010 - March 2023\",xaxis = list(title = \"Time\"),yaxis = list(title =\"Unemployment Rate\"))\nfig\n\n\n\n\n\n\n\n\n\nFrom 2010 to 2023, Amazon’s stock experienced significant growth as the company expanded its operations and diversified its offerings. In 2010, Amazon was primarily known as an online retailer of books, electronics, and other consumer goods. However, over the next several years, the company expanded into new markets, including streaming video and music, cloud computing services, and even physical retail stores.\nThroughout this period, Amazon’s stock price saw consistent growth, although there were occasional dips in response to macroeconomic events or company-specific news. For example, the company’s stock price declined in 2014 amid concerns about its profitability and increased competition in the retail industry. However, Amazon’s stock quickly rebounded as the company continued to innovate and expand into new markets.\nIn recent years, Amazon’s stock price has been impacted by a variety of factors, including the COVID-19 pandemic and increased scrutiny from regulators. Nevertheless, the company’s strong position in the e-commerce and cloud computing markets, as well as its continued investment in new technologies and initiatives, have helped to maintain investor confidence in Amazon’s long-term growth potential.\nSince early 2021, Amazon’s stock price has experienced some volatility, likely due to a combination of factors such as global economic uncertainty and fluctuations in consumer demand for the company’s products.\nAs discussed before, the macroeconomic factors of GDP growth, inflation, interest rates, and unemployment rate are closely interrelated and play a crucial role in the overall health and stability of an economy. From 2010 to 2023, the global economy experienced a mix of ups and downs, with periods of strong GDP growth followed by slowdowns and recessions.\nThe second plot shows the first difference of the logarithm of the adjusted Amazon stock price. Taking the first difference removes any long-term trends and transforms the time series into a stationary process. From the plot, we can observe that the first difference of the logarithm of the Amazon stock price appears to be stationary, as the mean and variance are roughly constant over time.\n\n\nEnodogenous and Exogenous Variables\n\nPlotCorrelation HeatmapCCF GDPCCF InterestCCF InflationCCF Unemployment\n\n\n\n\nCode\nnumeric_data <- c(\"AMZN.Adjusted\",\"gdp\", \"interest\", \"inflation\", \"unemployment\")\nnumeric_data <- final[, numeric_data]\nnormalized_data_numeric <- scale(numeric_data)\nnormalized_data <- ts(normalized_data_numeric, start = c(2010, 1), end = c(2021,10),frequency = 4)\nts_plot(normalized_data,\n        title = \"Normalized Time Series Data for AMZN Stock and Macroeconomic Variables\",\n        Ytitle = \"Normalized Values\",\n        Xtitle = \"Year\")\n\n\n\n\n\n\n\n\n\n\nCode\n# Get upper triangle of the correlation matrix\nget_upper_tri <- function(cormat){\n    cormat[lower.tri(cormat)]<- NA\n    return(cormat)\n}\ncormat <- round(cor(normalized_data_numeric),2)\n\nupper_tri <- get_upper_tri(cormat)\n\nmelted_cormat <- melt(upper_tri, na.rm = TRUE)\n# Create a ggheatmap\nggheatmap <- ggplot(melted_cormat, aes(Var2, Var1, fill = value))+\n geom_tile(color = \"white\")+\n scale_fill_gradient2(low = \"blue\", high = \"red\", mid = \"white\", \n   midpoint = 0, limit = c(-1,1), space = \"Lab\", \n    name=\"Pearson\\nCorrelation\") +\n  theme_minimal()+ # minimal theme\n theme(axis.text.x = element_text(angle = 45, vjust = 1, \n    size = 12, hjust = 1))+\n coord_fixed()\n\nggheatmap + \ngeom_text(aes(Var2, Var1, label = value), color = \"black\", size = 4) +\ntheme(\n  axis.title.x = element_blank(),\n  axis.title.y = element_blank(),\n  panel.grid.major = element_blank(),\n  panel.border = element_blank(),\n  panel.background = element_blank(),\n  axis.ticks = element_blank(),\n  legend.justification = c(1, 0),\n  legend.position = c(0.6, 0.7),\n  legend.direction = \"horizontal\")+\n  guides(fill = guide_colorbar(barwidth = 7, barheight = 1,\n                title.position = \"top\", title.hjust = 0.5))\n\n\n\n\n\n\n\n\n\nCode\npar(mfrow=c(1,1))\nccf_result <- ccf(normalized_data[, c(\"AMZN.Adjusted\")], normalized_data[, c(\"gdp\")], \n    lag.max = 300,\n    main = \"Cros-Correlation Plot for AMZN Stock Price and GDP Growth Rate \",\n    ylab = \"CCF\")\n\n\n\n\n\nCode\ncat(\"The sum of cross correlation function is\", sum(abs(ccf_result$acf)))\n\n\nThe sum of cross correlation function is 4.585922\n\n\n\n\n\n\nCode\npar(mfrow=c(1,1))\nccf_result <- ccf(normalized_data[, c(\"AMZN.Adjusted\")], normalized_data[, c(\"interest\")], \n    lag.max = 300,\n    main = \"Cros-Correlation Plot for AMZN Stock Price and Interest Rate\",\n    ylab = \"CCF\")\n\n\n\n\n\nCode\ncat(\"The sum of cross correlation function is\", sum(abs(ccf_result$acf)))\n\n\nThe sum of cross correlation function is 11.29588\n\n\n\n\n\n\nCode\npar(mfrow=c(1,1))\nccf_result <- ccf(normalized_data[, c(\"AMZN.Adjusted\")], normalized_data[, c(\"inflation\")], \n    lag.max = 300,\n    main = \"Cros-Correlation Plot for AMZN Stock Price and Inflation Rate\",\n    ylab = \"CCF\")\n\n\n\n\n\nCode\ncat(\"The sum of cross correlation function is\", sum(abs(ccf_result$acf)))\n\n\nThe sum of cross correlation function is 16.68466\n\n\n\n\n\n\nCode\npar(mfrow=c(1,1))\nccf_result <- ccf(normalized_data[, c(\"AMZN.Adjusted\")], normalized_data[, c(\"unemployment\")], \n    lag.max = 300,\n    main = \"Cros-Correlation Plot for AMZN Stock Priceand Unemployment Rate\",\n    ylab = \"CCF\")\n\n\n\n\n\nCode\ncat(\"The sum of cross correlation function is\", sum(abs(ccf_result$acf)))\n\n\nThe sum of cross correlation function is 20.53876\n\n\n\n\n\nThe Normalized Time Series Data for Stock Price and Macroeconomic Variables plot shows the same variables as the first plot but has been normalized to a common range of 0 to 1 using the scale() function in R, which standardizes the variables to have a mean of 0 and a standard deviation of 1. The heatmap analysis of the normalized data reveals that inflation and unemployment rate exhibit strong positive correlations with the stock price indices, indicating that these variables may significantly influence stock price movements. On the other hand, weaker correlations were observed between the stock price indices and GDP and interest rates, suggesting that these variables may have less impact on stock price fluctuations. The cross-correlation feature plots confirm these findings, indicating that inflation and unemployment rate are more suitable feature variables for the ARIMAX model when predicting Amazon movements.\nFinal Exogenous variables: Macroeconomic indicators: Inflation rate and unemployment rate.\n\n\nEnodogenous and Exogenous Variables Plot\n\nPlotCheck the stationarity\n\n\n\n\nCode\nfinal_data <- final %>%dplyr::select( Date,AMZN.Adjusted, inflation,unemployment)\nnumeric_data <- c(\"AMZN.Adjusted\", \"inflation\",\"unemployment\")\nnumeric_data <- final_data[, numeric_data]\nnormalized_data_numeric <- scale(numeric_data)\nnormalized_numeric_df <- data.frame(normalized_data_numeric)\nnormalized_data_ts <- ts(normalized_data_numeric, start = c(2010, 1), frequency = 4)\n\nautoplot(normalized_data_ts, facets=TRUE) +\n  xlab(\"Year\") + ylab(\"\") +\n  ggtitle(\"Amazon Stock Price, Inflation Rate and Unemployment Rate in USA 2010-2023\")\n\n\n\n\n\n\n\n\n\nCode\n# Convert your multivariate time series data to a matrix\nfinal_data_ts_multivariate <- as.matrix(normalized_data_ts)\n\n# Check for stationarity using Phillips-Perron test\nphillips_perron_test <- ur.pp(final_data_ts_multivariate)  \nsummary(phillips_perron_test)\n\n\n\n################################## \n# Phillips-Perron Unit Root Test # \n################################## \n\nTest regression with intercept \n\n\nCall:\nlm(formula = y ~ y.l1)\n\nResiduals:\n    Min      1Q  Median      3Q     Max \n-1.4360 -0.1440 -0.0602  0.0993  4.5029 \n\nCoefficients:\n             Estimate Std. Error t value Pr(>|t|)    \n(Intercept) 0.0001847  0.0403454   0.005    0.996    \ny.l1        0.8645022  0.0407542  21.213   <2e-16 ***\n---\nSignif. codes:  0 '***' 0.001 '**' 0.01 '*' 0.05 '.' 0.1 ' ' 1\n\nResidual standard error: 0.5023 on 153 degrees of freedom\nMultiple R-squared:  0.7463,    Adjusted R-squared:  0.7446 \nF-statistic:   450 on 1 and 153 DF,  p-value: < 2.2e-16\n\n\nValue of test-statistic, type: Z-alpha  is: -22.0542 \n\n         aux. Z statistics\nZ-tau-mu            0.0055\n\n\n\n\n\nThe results of the Phillips-Perron unit root test indicate strong evidence against the null hypothesis of a unit root, as the p-value for the coefficient of the lagged variable is less than the significance level of 0.05. This suggests that the variable y, which is being tested for stationarity, is likely stationary. Furthermore, the test statistic Z-tau-mu is 0.0055, which is smaller than the critical value of Z-alpha (-22.0542), providing further evidence of stationarity.\nTo determine whether the linear model requires an ARCH model, an ARCH test is conducted. The ACF and PACF plots are also used to identify suitable model values.\n\n\nModel Fitting\n\nPlotARCH TestACF PlotPACF Plot\n\n\n\n\nCode\nnormalized_numeric_df$AMZN.Adjusted<-ts(normalized_numeric_df$AMZN.Adjusted,star=decimal_date(as.Date(\"2010-01-01\",format = \"%Y-%m-%d\")),frequency = 4)\nnormalized_numeric_df$inflation<-ts(normalized_numeric_df$inflation,star=decimal_date(as.Date(\"2010-01-01\",format = \"%Y-%m-%d\")),frequency = 4)\nnormalized_numeric_df$unemployment<-ts(normalized_numeric_df$unemployment,star=decimal_date(as.Date(\"2010-01-01\",format = \"%Y-%m-%d\")),frequency = 4)\n\nfit <- lm(AMZN.Adjusted ~ inflation+unemployment, data=normalized_numeric_df)\nfit.res<-ts(residuals(fit),star=decimal_date(as.Date(\"2010-01-01\",format = \"%Y-%m-%d\")),frequency = 4)\n############## Then look at the residuals ############\nreturns <- fit.res  %>% diff()\nautoplot(returns)+ggtitle(\"Linear Model Returns\")\n\n\n\n\n\n\n\n\n\nCode\nbyd.archTest <- ArchTest(fit.res, lags = 1, demean = TRUE)\nbyd.archTest\n\n\n\n    ARCH LM-test; Null hypothesis: no ARCH effects\n\ndata:  fit.res\nChi-squared = 36.896, df = 1, p-value = 1.246e-09\n\n\n\n\n\n\nCode\nggAcf(returns) +ggtitle(\"ACF for returns\")\n\n\n\n\n\n\n\n\n\nCode\nggPacf(returns) +ggtitle(\"PACF for returns\")\n\n\n\n\n\n\n\n\nThe ARCH LM-test was conducted with the null hypothesis of no ARCH effects. The test resulted in a chi-squared value of 36.896 with one degree of freedom, and a very low p-value of 1.246e-09. This suggests strong evidence against the null hypothesis, indicating the presence of ARCH effects in the data.\nBased on the ACF and PACF plots, it appears that there is some significant autocorrelation and partial autocorrelation at multiple lags, which suggests that an ARIMA model may not be sufficient to capture the time series behavior. Additionally, the values for p and q appear to be relatively high, with p = 1 and q = 1 being suggested by the plots.\n\n\nARIMAX Model\n\nAuto Arima ModelAuto Arima ResidualsARIMA (1,1,1) PlotARIMA (1,1,1) ModelCross Validation\n\n\n\n\nCode\nxreg <- cbind(Inflation = normalized_data_ts[, \"inflation\"],\n              Unemployment = normalized_data_ts[, \"unemployment\"])\nfit.auto <- auto.arima(normalized_data_ts[, \"AMZN.Adjusted\"], xreg = xreg)\nsummary(fit.auto)\n\n\nSeries: normalized_data_ts[, \"AMZN.Adjusted\"] \nRegression with ARIMA(0,1,0) errors \n\nCoefficients:\n      Inflation  Unemployment\n         0.0625       -0.0687\ns.e.     0.0966        0.0438\n\nsigma^2 = 0.04933:  log likelihood = 5.39\nAIC=-4.78   AICc=-4.27   BIC=1.01\n\nTraining set error measures:\n                     ME     RMSE       MAE      MPE     MAPE     MASE      ACF1\nTraining set 0.03057732 0.215598 0.1225672 11.69566 31.46174 0.408043 0.1470963\n\n\n\n\n\n\nCode\ncheckresiduals(fit.auto)\n\n\n\n\n\n\n    Ljung-Box test\n\ndata:  Residuals from Regression with ARIMA(0,1,0) errors\nQ* = 4.4858, df = 8, p-value = 0.8109\n\nModel df: 0.   Total lags used: 8\n\n\n\n\n\n\nCode\nset.seed(1234)\n\nmodel_output <- capture.output(sarima(fit.res, 1,1,1)) \n\n\n\n\n\n\n\n\n\nCode\ncat(model_output[30:61], model_output[length(model_output)], sep = \"\\n\")\n\n\nCall:\narima(x = xdata, order = c(p, d, q), seasonal = list(order = c(P, D, Q), period = S), \n    xreg = constant, transform.pars = trans, fixed = fixed, optim.control = list(trace = trc, \n        REPORT = 1, reltol = tol))\n\nCoefficients:\n         ar1      ma1  constant\n      0.6076  -0.2899   -0.0064\ns.e.  0.2341   0.2628    0.0647\n\nsigma^2 estimated as 0.06782:  log likelihood = -3.83,  aic = 15.66\n\n$degrees_of_freedom\n[1] 48\n\n$ttable\n         Estimate     SE t.value p.value\nar1        0.6076 0.2341  2.5960  0.0125\nma1       -0.2899 0.2628 -1.1031  0.2755\nconstant  -0.0064 0.0647 -0.0984  0.9220\n\n$AIC\n[1] 0.3070239\n\n$AICc\n[1] 0.3170364\n\n$BIC\n[1] 0.4585396\n\nNA\nNA\n\n\n\n\n\n\nCode\nn=length(fit.res)\nk= 51\n \n \nrmse1 <- matrix(NA, (n-k),4)\nrmse2 <- matrix(NA, (n-k),4)\nrmse3 <- matrix(NA, (n-k),4)\n\n\nst <- tsp(fit.res)[1]+(k-5)/4 \n\nfor(i in 1:(n-k))\n{\n  xtrain <- window(fit.res, end=st + i/4)\n  xtest <- window(fit.res, start=st + (i+1)/4, end=st + (i+4)/4)\n  \n  #ARIMA(0,1,0) ARIMA(1,1,1)\n  \n  fit <- Arima(xtrain, order=c(0,1,0),\n                include.drift=TRUE, method=\"ML\")\n  fcast <- forecast(fit, h=4)\n  \n  fit2 <- Arima(xtrain, order=c(1,1,1),\n                include.drift=TRUE, method=\"ML\")\n  fcast2 <- forecast(fit2, h=4)\n\n\n  rmse1[i,1:length(xtest)]   <- sqrt((fcast$mean-xtest)^2)\n  rmse2[i,1:length(xtest)] <- sqrt((fcast2$mean-xtest)^2)\n}\n\nplot(1:4,colMeans(rmse1,na.rm=TRUE), type=\"l\",col=2, xlab=\"horizon\", ylab=\"RMSE\")\nlines(1:4, colMeans(rmse2,na.rm=TRUE), type=\"l\",col=3)\nlegend(\"topleft\",legend=c(\"fit1\",\"fit2\"),col=2:4,lty=1)\n\n\n\n\n\n\n\n\nBased on the results of the auto.arima function, the suggested best model is ARIMA(0,1,0), but the acf and pacf plots suggest a simpler ARIMA(1,1,1) model. To determine the best model, we conduct cross-validation and compare the RMSE values of both models. The results show that ARIMA(1,1,1) has lower RMSE values than ARIMA(0,1,0), indicating that it is the better model.\nWe can then proceed to choose the best GARCH model using ARIMA(1,1,1) as the base model.\n\n\nSquared Residuals\n\nPlotACF PlotPACF PlotGRACH(1,1)\n\n\n\n\nCode\nfit <- lm(AMZN.Adjusted ~ inflation+unemployment, data=normalized_numeric_df)\nfit.res<-ts(residuals(fit),star=decimal_date(as.Date(\"2010-01-01\",format = \"%Y-%m-%d\")),frequency = 4)\nfit <- Arima(fit.res,order=c(1,1,1))\nres=fit$res\nplot(res^2,main='Squared Residuals')\n\n\n\n\n\n\n\n\n\nCode\nacf(res^2,24, main = \"ACF Residuals Square\")\n\n\n\n\n\n\n\n\n\nCode\npacf(res^2,24, main = \"PACF Residuals Square\")\n\n\n\n\n\n\n\n\n\nCode\nsummary(garchFit(~garch(1,1),res, trace=F))\n\n\n\nTitle:\n GARCH Modelling \n\nCall:\n garchFit(formula = ~garch(1, 1), data = res, trace = F) \n\nMean and Variance Equation:\n data ~ garch(1, 1)\n<environment: 0x143e2d1d8>\n [data = res]\n\nConditional Distribution:\n norm \n\nCoefficient(s):\n         mu        omega       alpha1        beta1  \n-0.00530411   0.00068953   0.48627960   0.65780719  \n\nStd. Errors:\n based on Hessian \n\nError Analysis:\n         Estimate  Std. Error  t value Pr(>|t|)    \nmu     -0.0053041   0.0155220   -0.342   0.7326    \nomega   0.0006895   0.0011861    0.581   0.5610    \nalpha1  0.4862796   0.2392140    2.033   0.0421 *  \nbeta1   0.6578072   0.1208067    5.445 5.18e-08 ***\n---\nSignif. codes:  0 '***' 0.001 '**' 0.01 '*' 0.05 '.' 0.1 ' ' 1\n\nLog Likelihood:\n 9.169268    normalized:  0.1763321 \n\nDescription:\n Sat Jan  6 19:52:01 2024 by user:  \n\n\nStandardised Residuals Tests:\n                                Statistic p-Value   \n Jarque-Bera Test   R    Chi^2  1.08557   0.5811275 \n Shapiro-Wilk Test  R    W      0.9616717 0.09248575\n Ljung-Box Test     R    Q(10)  5.843556  0.8282312 \n Ljung-Box Test     R    Q(15)  8.260042  0.9129345 \n Ljung-Box Test     R    Q(20)  15.71232  0.7343031 \n Ljung-Box Test     R^2  Q(10)  9.933015  0.4463896 \n Ljung-Box Test     R^2  Q(15)  11.22686  0.7363534 \n Ljung-Box Test     R^2  Q(20)  13.8952   0.8357696 \n LM Arch Test       R    TR^2   13.33945  0.3448583 \n\nInformation Criterion Statistics:\n        AIC         BIC         SIC        HQIC \n-0.19881801 -0.04872234 -0.20956333 -0.14127488 \n\n\n\n\n\nFrom the squared residuals of the best ARIMA model, it can be observed that the ACF plot and PACF plot indicate that the residuals are not autocorrelated and are white noise, indicating a good fit of the model. Based on the squared residuals of the best ARIMA model, we can see that the ACF and PACF plots indicate that most of the values lie between the blue lines. Additionally, the p-value is 1 and q-value is 1. This suggests that the model has a good fit and that there is no significant autocorrelation or partial autocorrelation in the residuals.\nThe bst model is ARIMA(1,1,1) and GARCH(1,1). #### Best Model\n\nARIMA ModelGARCH ModelVolatility\n\n\n\n\nCode\n#fiting an ARIMA model to the Inflation variable\ninflation_fit<-auto.arima(normalized_numeric_df$inflation) \nfinflation<-forecast(inflation_fit)\n\n#fitting an ARIMA model to the Unemployment variable\nunemployment_fit<-auto.arima(normalized_numeric_df$unemployment) \nfunemployment<-forecast(unemployment_fit)\n\n# best model fit for forcasting\nxreg <- cbind(Inflation = normalized_data_ts[, \"inflation\"],\n              Unemployment = normalized_data_ts[, \"unemployment\"])\n\n\n\nsummary(arima.fit<-Arima(normalized_data_ts[, \"AMZN.Adjusted\"],order=c(1,1,1),xreg=xreg),include.drift = TRUE)\n\n\nSeries: normalized_data_ts[, \"AMZN.Adjusted\"] \nRegression with ARIMA(1,1,1) errors \n\nCoefficients:\n          ar1     ma1  Inflation  Unemployment\n      -0.5637  1.0000    -0.0025       -0.0790\ns.e.   0.1390  0.0573     0.0949        0.0336\n\nsigma^2 = 0.0412:  log likelihood = 9.68\nAIC=-9.36   AICc=-8.03   BIC=0.3\n\nTraining set error measures:\n                     ME      RMSE      MAE      MPE     MAPE      MASE\nTraining set 0.02911543 0.1929808 0.114944 11.94347 27.98997 0.3826642\n                    ACF1\nTraining set -0.06750725\n\n\n\n\n\n\nCode\nsummary(final.fit <- garchFit(~garch(1,1), res,trace = F))\n\n\n\nTitle:\n GARCH Modelling \n\nCall:\n garchFit(formula = ~garch(1, 1), data = res, trace = F) \n\nMean and Variance Equation:\n data ~ garch(1, 1)\n<environment: 0x16579cb60>\n [data = res]\n\nConditional Distribution:\n norm \n\nCoefficient(s):\n         mu        omega       alpha1        beta1  \n-0.00530411   0.00068953   0.48627960   0.65780719  \n\nStd. Errors:\n based on Hessian \n\nError Analysis:\n         Estimate  Std. Error  t value Pr(>|t|)    \nmu     -0.0053041   0.0155220   -0.342   0.7326    \nomega   0.0006895   0.0011861    0.581   0.5610    \nalpha1  0.4862796   0.2392140    2.033   0.0421 *  \nbeta1   0.6578072   0.1208067    5.445 5.18e-08 ***\n---\nSignif. codes:  0 '***' 0.001 '**' 0.01 '*' 0.05 '.' 0.1 ' ' 1\n\nLog Likelihood:\n 9.169268    normalized:  0.1763321 \n\nDescription:\n Sat Jan  6 19:52:02 2024 by user:  \n\n\nStandardised Residuals Tests:\n                                Statistic p-Value   \n Jarque-Bera Test   R    Chi^2  1.08557   0.5811275 \n Shapiro-Wilk Test  R    W      0.9616717 0.09248575\n Ljung-Box Test     R    Q(10)  5.843556  0.8282312 \n Ljung-Box Test     R    Q(15)  8.260042  0.9129345 \n Ljung-Box Test     R    Q(20)  15.71232  0.7343031 \n Ljung-Box Test     R^2  Q(10)  9.933015  0.4463896 \n Ljung-Box Test     R^2  Q(15)  11.22686  0.7363534 \n Ljung-Box Test     R^2  Q(20)  13.8952   0.8357696 \n LM Arch Test       R    TR^2   13.33945  0.3448583 \n\nInformation Criterion Statistics:\n        AIC         BIC         SIC        HQIC \n-0.19881801 -0.04872234 -0.20956333 -0.14127488 \n\n\n\n\n\n\nCode\nht <- final.fit@h.t #a numeric vector with the conditional variances (h.t = sigma.t^delta)\n\n#############################\ndata=data.frame(final)\ndata$Date<-as.Date(data$Date,\"%Y-%m-%d\")\n\n\ndata2= data.frame(ht,data$Date)\nggplot(data2, aes(y = ht, x = data.Date)) + geom_line(col = '#7FB3D5') + ylab('Conditional Variance') + xlab('Date')\n\n\n\n\n\n\n\n\nFrom the ARIMA(1,1,1), we see that the training set error measures also suggest a good fit, with low mean absolute error, root mean squared error, and autocorrelation of the residuals. GATCH(1,1) model model is used to estimate the volatility of the standardized residuals of the previous regression model. The model includes a mean equation that estimates the mean of the residuals and a variance equation that models the conditional variance of the residuals. The coefficients of the mean equation suggest that the mean of the residuals is close to zero. The variance equation coefficients suggest that the conditional variance of the residuals is dependent on the past conditional variances and the past squared standardized residuals. The model’s the AIC, BIC, SIC, and HQIC values are all relatively low, indicating a good fit of the model. The standardized residuals tests indicate that the residuals are approximately normally distributed and that there is no significant autocorrelation in the residuals.\nThe volatility of the model seems high in 2020 but has decreased gradually in the past few months. This could indicate that the asset’s price was experiencing a lot of fluctuations in 2020, but the market has stabilized recently.\n\n\nModel Diagnostics\n\nResidualsQQ PlotBox Test\n\n\n\n\nCode\nfit2<-garch(res,order=c(1,1),trace=F)\ncheckresiduals(fit2) \n\n\n\n\n\n\n\n\n\nCode\nqqnorm(fit2$residuals, pch = 1)\nqqline(fit2$residuals, col = \"blue\", lwd = 2)\n\n\n\n\n\n\n\n\n\nCode\nBox.test (fit2$residuals, type = \"Ljung\")\n\n\n\n    Box-Ljung test\n\ndata:  fit2$residuals\nX-squared = 2.0187, df = 1, p-value = 0.1554\n\n\nThe ACF plot of the residuals shows all the values between the blue lines, which indicates that the residuals are not significantly autocorrelated. The range of values for the residual plot between -2 and 2 is considered acceptable. Additionally, the QQ plot of the residuals shows a linear plot on the line, which is another good indication that the residuals are normally distributed. The QQ plot is a valuable tool to assess if the residuals follow a normal distribution, and in this case, the plot suggests that the residuals do indeed follow a normal distribution.\nThe Box-Ljung test, a p-value of 0.1554 indicates that the model’s residuals are not significantly autocorrelated, meaning that the model has captured most of the information in the data. This result is good because it suggests that the model is a good fit for the data and has accounted for most of the underlying patterns in the data. Therefore, we can rely on the model’s predictions and use them to make informed decisions.\n\n\n\n\n\nForecast\n\n\nCode\npredict(final.fit, n.ahead = 5, plot=TRUE)\n\n\n\n\n\n  meanForecast meanError standardDeviation lowerInterval upperInterval\n1 -0.005304111 0.5028875         0.5028875    -0.9909454     0.9803372\n2 -0.005304111 0.5385390         0.5385390    -1.0608212     1.0502130\n3 -0.005304111 0.5766303         0.5766303    -1.1354787     1.1248705\n4 -0.005304111 0.6173340         0.6173340    -1.2152565     1.2046483\n5 -0.005304111 0.6608347         0.6608347    -1.3005162     1.2899080\n\n\nThe forecasted plot is based on the best model ARIMAX(1,1,1)+GARCH(1,1). This model takes into account the autoregressive and moving average components of the data, as well as the impact off exogenous variables on the time series. Additionally, the GARCH component of the model accounts for the volatility clustering in the data. Overall, this model is well-suited to make accurate predictions about future values of the time series.\n\n\nEquation of the Model\nThe equation of the ARIMAX(1,1,1) model is:\n\\(Y(t) = c + \\phi_1(Y{(t-1)} - X{(t-1)}) + \\theta_1\\epsilon{(t-1)} + \\epsilon(t)\\)\nwhere, \\(Y(t)\\) is the time series variable, \\(X(t-1)\\) is the exogenous variable, \\(c\\) is a constant, \\(\\phi_1\\) and \\(\\theta_1\\) are the parameters, and \\(\\epsilon(t)\\) is the error term.\nThe equation of the GARCH(1,1) model is:\n\\(\\sigma^2(t) = \\alpha_0+\\alpha_1\\epsilon_t^2(t-1)+ \\beta_1\\sigma^2(t-1)\\)\nwhere \\(\\sigma^2_t\\) is the conditional variance at time \\(t\\), \\(\\alpha_0\\) is a constant, \\(\\alpha_1\\) and \\(\\beta_1\\) are the parameters, and \\(\\epsilon_t\\) is the error term.\nThe combined equation of the ARIMAX(1,1,1)+GARCH(1,1) model is:\n\\(Y(t) = c + \\phi_1(Y(t-1) - X(t-1)) + \\theta_1\\epsilon(t-1) + \\epsilon(t)\\)\n\\(\\epsilon(t) = \\sigma(t) * \\epsilon~(t)\\)\n\\(\\sigma^2(t) = \\alpha_0+\\alpha_1\\epsilon_t^2(t-1)+ \\beta_1\\sigma^2(t-1)\\)"
  },
  {
    "objectID": "arch_Consumer_Staples_Sector_Fund.html",
    "href": "arch_Consumer_Staples_Sector_Fund.html",
    "title": "",
    "section": "",
    "text": "Impact of Macroeconomic Factors on P&G Stock Price\n\n\n\nConsumer staple funds are mutual funds or exchange-traded funds (ETFs) that invest in companies that produce essential goods and services, such as food, beverages, household products, and personal care items. These funds are designed to provide investors with stable and consistent returns, even in uncertain market conditions.\nOne of the top companies in the consumer staple sector is Procter & Gamble Co. (PG), which produces a wide range of products including personal care, cleaning, and food and beverage items. To analyze the stock price behavior of Procter & Gamble, an ARIMAX+ARCH/GARCH model can be employed.\nBy using an ARIMAX+ARCH/GARCH model to analyze the stock price behavior of Procter & Gamble, we can gain insights into how macroeconomic factors impact the company’s performance. For example, an increase in GDP growth rate or a decrease in unemployment rate may lead to increased consumer spending and a rise in Procter & Gamble’s stock price. Conversely, an increase in inflation or interest rates may lead to a decrease in consumer spending and a decline in Procter & Gamble’s stock price.\n\n\nTime series Plot\n\nP&GDifferentitaed Chart SeriesGDP GrowthInflationInterestUnemployment\n\n\n\n\nCode\n# get data\noptions(\"getSymbols.warning4.0\"=FALSE)\noptions(\"getSymbols.yahoo.warning\"=FALSE)\n\n\ndata.info = getSymbols(\"PG\",src='yahoo', from = '2010-01-01',to = \"2023-03-01\",auto.assign = FALSE)\ndata = getSymbols(\"PG\",src='yahoo', from = '2010-01-01',to = \"2023-03-01\")\ndf <- data.frame(Date=index(PG),coredata(PG))\n\n# create Bollinger Bands\nbbands <- BBands(PG[,c(\"PG.High\",\"PG.Low\",\"PG.Close\")])\n\n# join and subset data\ndf <- subset(cbind(df, data.frame(bbands[,1:3])), Date >= \"2010-01-01\")\n\n\n# colors column for increasing and decreasing\nfor (i in 1:length(df[,1])) {\n  if (df$PG.Close[i] >= df$PG.Open[i]) {\n      df$direction[i] = 'Increasing'\n  } else {\n      df$direction[i] = 'Decreasing'\n  }\n}\n\ni <- list(line = list(color = '#43A098'))\nd <- list(line = list(color = '#7F7F7F'))\n\n# plot candlestick chart\n\nfig <- df %>% plot_ly(x = ~Date, type=\"candlestick\",\n          open = ~PG.Open, close = ~PG.Close,\n          high = ~PG.High, low = ~PG.Low, name = \"PG\",\n          increasing = i, decreasing = d) \nfig <- fig %>% add_lines(x = ~Date, y = ~up , name = \"B Bands\",\n            line = list(color = '#ccc', width = 0.5),\n            legendgroup = \"Bollinger Bands\",\n            hoverinfo = \"none\", inherit = F) \nfig <- fig %>% add_lines(x = ~Date, y = ~dn, name = \"B Bands\",\n            line = list(color = '#ccc', width = 0.5),\n            legendgroup = \"Bollinger Bands\", inherit = F,\n            showlegend = FALSE, hoverinfo = \"none\") \nfig <- fig %>% add_lines(x = ~Date, y = ~mavg, name = \"Mv Avg\",\n            line = list(color = '#C052B3', width = 0.5),\n            hoverinfo = \"none\", inherit = F) \nfig <- fig %>% layout(yaxis = list(title = \"Price\"))\n\n# plot volume bar chart\nfig2 <- df \nfig2 <- fig2 %>% plot_ly(x=~Date, y=~PG.Volume, type='bar', name = \"PG Volume\",\n          color = ~direction, colors = c('#43A098','#7F7F7F')) \nfig2 <- fig2 %>% layout(yaxis = list(title = \"Volume\"))\n\n# create rangeselector buttons\nrs <- list(visible = TRUE, x = 0.5, y = -0.055,\n           xanchor = 'center', yref = 'paper',\n           font = list(size = 9),\n           buttons = list(\n             list(count=1,\n                  label='RESET',\n                  step='all'),\n             list(count=3,\n                  label='3 YR',\n                  step='year',\n                  stepmode='backward'),\n             list(count=1,\n                  label='1 YR',\n                  step='year',\n                  stepmode='backward'),\n             list(count=1,\n                  label='1 MO',\n                  step='month',\n                  stepmode='backward')\n           ))\n\n# subplot with shared x axis\nfig <- subplot(fig, fig2, heights = c(0.7,0.2), nrows=2,\n             shareX = TRUE, titleY = TRUE)\nfig <- fig %>% layout(title = paste(\"P&G Stock Price: January 2010 - March 2023\"),\n         xaxis = list(rangeselector = rs),\n         legend = list(orientation = 'h', x = 0.5, y = 1,\n                       xanchor = 'center', yref = 'paper',\n                       font = list(size = 10),\n                       bgcolor = 'transparent'))\n\nfig\n\n\n\n\n\n\n\n\n\n\nCode\nlog(data.info$`PG.Adjusted`) %>% diff() %>% chartSeries(theme=chartTheme('white'),up.col='#43A098')\n\n\n\n\n\n\n\n\n\nCode\n#import the data\ngdp <- read.csv(\"DATA/RAW DATA/gdp-growth.csv\")\n\n#change date format\ngdp$Date <- as.Date(gdp$DATE , \"%m/%d/%Y\")\n\n#drop DATE column\ngdp <- subset(gdp, select = -c(1))\n\n#export the cleaned data\ngdp_clean <- gdp\nwrite.csv(gdp_clean, \"DATA/CLEANED DATA/gdp_clean_data.csv\", row.names=FALSE)\n\n#plot gdp growth rate \nfig <- plot_ly(gdp, x = ~Date, y = ~value, type = 'scatter', mode = 'lines',line = list(color = 'rgb(240, 128, 128)'))\nfig <- fig %>% layout(title = \"U.S GPD Growth Rate: 2010 - 2022\",xaxis = list(title = \"Time\"),yaxis = list(title =\"GDP Growth Rate\"))\nfig\n\n\n\n\n\n\n\n\n\n\nCode\n#import the data\ninflation_rate <- read.csv(\"DATA/RAW DATA/inflation-rate.csv\")\n\n#cleaning the data\n#remove unwanted columns\ninflation_rate_clean <- subset(inflation_rate, select = -c(1,HALF1,HALF2))\n\n#convert the data to time series data\ninflation_data_ts <- ts(as.vector(t(as.matrix(inflation_rate_clean))), start=c(2010,1), end=c(2023,2), frequency=12)\n\n#export the data\nwrite.csv(inflation_rate_clean, \"DATA/CLEANED DATA/inflation_rate_clean_data.csv\", row.names=FALSE)\n\n\n#plot inflation rate \nfig <- autoplot(inflation_data_ts, ylab = \"Inflation Rate\", color=\"#FFA07A\")+ggtitle(\"U.S Inflation Rate: January 2010 - February 2023\")+theme_bw()\nggplotly(fig)\n\n\n\n\n\n\n\n\n\n\nCode\n#import the data\ninterest_data <- read.csv(\"DATA/RAW DATA/interest-rate.csv\")\n\n#change date format\ninterest_data$Date <- as.Date(interest_data$Date , \"%m/%d/%Y\")\n\n#export the cleaned data\ninterest_clean_data <- interest_data\nwrite.csv(interest_clean_data, \"DATA/CLEANED DATA/interest_rate_clean_data.csv\", row.names=FALSE)\n\n#plot interest rate \nfig <- plot_ly(interest_data, x = ~Date, y = ~value, type = 'scatter', mode = 'lines',line = list(color='rgb(219, 112, 147)'))\nfig <- fig %>% layout(title = \"U.S Interest Rate: January 2010 - March 2023\",xaxis = list(title = \"Time\"),yaxis = list(title =\"Interest Rate\"))\nfig\n\n\n\n\n\n\n\n\n\n\nCode\n#import the data\nunemployment_rate <- read.csv(\"DATA/RAW DATA/unemployment-rate.csv\")\n\n#change date format\nunemployment_rate$Date <- as.Date(unemployment_rate$Date , \"%m/%d/%Y\")\n\n# export the data\nwrite.csv(unemployment_rate, \"DATA/CLEANED DATA/unemployment_rate_clean_data.csv\", row.names=FALSE)\n\n#plot unemployment rate \nfig <- plot_ly(unemployment_rate, x = ~Date, y = ~Value, type = 'scatter', mode = 'lines',line = list(color = 'rgb(189, 183, 107)'))\nfig <- fig %>% layout(title = \"U.S Unemployment Rate: January 2010 - March 2023\",xaxis = list(title = \"Time\"),yaxis = list(title =\"Unemployment Rate\"))\nfig\n\n\n\n\n\n\n\n\n\nThe stock price of P&G experienced a steady increase from 2010 to 2014, likely due to the company’s strong financial performance and consistent dividend payouts. However, from 2014 to 2016, P&G’s stock price experienced a decline, which could be attributed to a combination of factors such as slowing sales growth and increased competition in the consumer goods industry.\nFollowing this period of steadiness and decline, P&G’s stock price began to recover from 2016 to 2018, likely due to the company’s efforts to streamline its operations and focus on core brands. This trend continued into 2019, with P&G’s stock price reaching an all-time high in mid-2019.\nHowever, the outbreak of the COVID-19 pandemic in early 2020 caused a brief dip in P&G’s stock price, as investors were uncertain about the impact of the pandemic on the company’s operations and financial performance. Nevertheless, P&G’s strong position in the consumer goods industry and its ability to adapt to changing market conditions helped it to quickly rebound and continue its growth trend throughout 2020 and into early 2021.\nSince early 2021, P&G’s stock price has experienced some volatility, likely due to a combination of factors such as global economic uncertainty and fluctuations in consumer demand for the company’s products.\nAs discussed before, the macroeconomic factors of GDP growth, inflation, interest rates, and unemployment rate are closely interrelated and play a crucial role in the overall health and stability of an economy. From 2010 to 2023, the global economy experienced a mix of ups and downs, with periods of strong GDP growth followed by slowdowns and recessions.\nThe second plot shows the first difference of the logarithm of the adjusted P&G stock price. Taking the first difference removes any long-term trends and transforms the time series into a stationary process. From the plot, we can observe that the first difference of the logarithm of the P&G stock price appears to be stationary, as the mean and variance are roughly constant over time.\n\n\nEnodogenous and Exogenous Variables\n\nPlotCorrelation HeatmapCCF GDPCCF InterestCCF InflationCCF Unemployment\n\n\n\n\nCode\nnumeric_data <- c(\"PG.Adjusted\",\"gdp\", \"interest\", \"inflation\", \"unemployment\")\nnumeric_data <- final[, numeric_data]\nnormalized_data_numeric <- scale(numeric_data)\nnormalized_data <- ts(normalized_data_numeric, start = c(2010, 1), end = c(2021,10),frequency = 4)\nts_plot(normalized_data,\n        title = \"Normalized Time Series Data for PG Stock and Macroeconomic Variables\",\n        Ytitle = \"Normalized Values\",\n        Xtitle = \"Year\")\n\n\n\n\n\n\n\n\n\n\nCode\n# Get upper triangle of the correlation matrix\nget_upper_tri <- function(cormat){\n    cormat[lower.tri(cormat)]<- NA\n    return(cormat)\n}\ncormat <- round(cor(normalized_data_numeric),2)\n\nupper_tri <- get_upper_tri(cormat)\n\nmelted_cormat <- melt(upper_tri, na.rm = TRUE)\n# Create a ggheatmap\nggheatmap <- ggplot(melted_cormat, aes(Var2, Var1, fill = value))+\n geom_tile(color = \"white\")+\n scale_fill_gradient2(low = \"blue\", high = \"red\", mid = \"white\", \n   midpoint = 0, limit = c(-1,1), space = \"Lab\", \n    name=\"Pearson\\nCorrelation\") +\n  theme_minimal()+ # minimal theme\n theme(axis.text.x = element_text(angle = 45, vjust = 1, \n    size = 12, hjust = 1))+\n coord_fixed()\n\nggheatmap + \ngeom_text(aes(Var2, Var1, label = value), color = \"black\", size = 4) +\ntheme(\n  axis.title.x = element_blank(),\n  axis.title.y = element_blank(),\n  panel.grid.major = element_blank(),\n  panel.border = element_blank(),\n  panel.background = element_blank(),\n  axis.ticks = element_blank(),\n  legend.justification = c(1, 0),\n  legend.position = c(0.6, 0.7),\n  legend.direction = \"horizontal\")+\n  guides(fill = guide_colorbar(barwidth = 7, barheight = 1,\n                title.position = \"top\", title.hjust = 0.5))\n\n\n\n\n\n\n\n\n\nCode\npar(mfrow=c(1,1))\nccf_result <- ccf(normalized_data[, c(\"PG.Adjusted\")], normalized_data[, c(\"gdp\")], \n    lag.max = 300,\n    main = \"Cros-Correlation Plot for PG Stock Price and GDP Growth Rate \",\n    ylab = \"CCF\")\n\n\n\n\n\nCode\ncat(\"The sum of cross correlation function is\", sum(abs(ccf_result$acf)))\n\n\nThe sum of cross correlation function is 4.8926\n\n\n\n\n\n\nCode\npar(mfrow=c(1,1))\nccf_result <- ccf(normalized_data[, c(\"PG.Adjusted\")], normalized_data[, c(\"interest\")], \n    lag.max = 300,\n    main = \"Cros-Correlation Plot for PG Stock Price and Interest Rate\",\n    ylab = \"CCF\")\n\n\n\n\n\nCode\ncat(\"The sum of cross correlation function is\", sum(abs(ccf_result$acf)))\n\n\nThe sum of cross correlation function is 12.00986\n\n\n\n\n\n\nCode\npar(mfrow=c(1,1))\nccf_result <- ccf(normalized_data[, c(\"PG.Adjusted\")], normalized_data[, c(\"inflation\")], \n    lag.max = 300,\n    main = \"Cros-Correlation Plot for PG Stock Price and Inflation Rate\",\n    ylab = \"CCF\")\n\n\n\n\n\nCode\ncat(\"The sum of cross correlation function is\", sum(abs(ccf_result$acf)))\n\n\nThe sum of cross correlation function is 15.81772\n\n\n\n\n\n\nCode\npar(mfrow=c(1,1))\nccf_result <- ccf(normalized_data[, c(\"PG.Adjusted\")], normalized_data[, c(\"unemployment\")], \n    lag.max = 300,\n    main = \"Cros-Correlation Plot for PG Stock Priceand Unemployment Rate\",\n    ylab = \"CCF\")\n\n\n\n\n\nCode\ncat(\"The sum of cross correlation function is\", sum(abs(ccf_result$acf)))\n\n\nThe sum of cross correlation function is 19.34548\n\n\n\n\n\nThe Normalized Time Series Data for Stock Price and Macroeconomic Variables plot shows the same variables as the first plot but has been normalized to a common range of 0 to 1 using the scale() function in R, which standardizes the variables to have a mean of 0 and a standard deviation of 1. The heatmap analysis of the normalized data reveals that inflation and unemployment rate exhibit strong positive correlations with the stock price indices, indicating that these variables may significantly influence stock price movements. On the other hand, weaker correlations were observed between the stock price indices and GDP and interest rates, suggesting that these variables may have less impact on stock price fluctuations. The cross-correlation feature plots confirm these findings, indicating that inflation and unemployment rate are more suitable feature variables for the ARIMAX model when predicting P&G movements.\nFinal Exogenous variables: Macroeconomic indicators: Inflation rate and unemployment rate.\n\n\nEnodogenous and Exogenous Variables Plot\n\nPlotCheck the stationarity\n\n\n\n\nCode\nfinal_data <- final %>%dplyr::select( Date,PG.Adjusted, inflation,unemployment)\nnumeric_data <- c(\"PG.Adjusted\", \"inflation\",\"unemployment\")\nnumeric_data <- final_data[, numeric_data]\nnormalized_data_numeric <- scale(numeric_data)\nnormalized_numeric_df <- data.frame(normalized_data_numeric)\nnormalized_data_ts <- ts(normalized_data_numeric, start = c(2010, 1), frequency = 4)\n\nautoplot(normalized_data_ts, facets=TRUE) +\n  xlab(\"Year\") + ylab(\"\") +\n  ggtitle(\"P&G Stock Price, Inflation Rate and Unemployment Rate in USA 2010-2023\")\n\n\n\n\n\n\n\n\n\nCode\n# Convert your multivariate time series data to a matrix\nfinal_data_ts_multivariate <- as.matrix(normalized_data_ts)\n\n# Check for stationarity using Phillips-Perron test\nphillips_perron_test <- ur.pp(final_data_ts_multivariate)  \nsummary(phillips_perron_test)\n\n\n\n################################## \n# Phillips-Perron Unit Root Test # \n################################## \n\nTest regression with intercept \n\n\nCall:\nlm(formula = y ~ y.l1)\n\nResiduals:\n    Min      1Q  Median      3Q     Max \n-1.7824 -0.1562 -0.0531  0.0989  4.4953 \n\nCoefficients:\n            Estimate Std. Error t value Pr(>|t|)    \n(Intercept) 0.001564   0.040861   0.038     0.97    \ny.l1        0.859052   0.041275  20.813   <2e-16 ***\n---\nSignif. codes:  0 '***' 0.001 '**' 0.01 '*' 0.05 '.' 0.1 ' ' 1\n\nResidual standard error: 0.5087 on 153 degrees of freedom\nMultiple R-squared:  0.739, Adjusted R-squared:  0.7373 \nF-statistic: 433.2 on 1 and 153 DF,  p-value: < 2.2e-16\n\n\nValue of test-statistic, type: Z-alpha  is: -23.2156 \n\n         aux. Z statistics\nZ-tau-mu            0.0386\n\n\n\n\n\nThe results of the Phillips-Perron unit root test indicate strong evidence against the null hypothesis of a unit root, as the p-value for the coefficient of the lagged variable is less than the significance level of 0.05. This suggests that the variable y, which is being tested for stationarity, is likely stationary. Furthermore, the test statistic Z-tau-mu is 0.0386, which is smaller than the critical value of Z-alpha (-23.2156), providing further evidence of stationarity.\nTo determine whether the linear model requires an ARCH model, an ARCH test is conducted. The ACF and PACF plots are also used to identify suitable model values.\n\n\nModel Fitting\n\nPlotARCH TestACF PlotPACF Plot\n\n\n\n\nCode\nnormalized_numeric_df$PG.Adjusted<-ts(normalized_numeric_df$PG.Adjusted,star=decimal_date(as.Date(\"2010-01-01\",format = \"%Y-%m-%d\")),frequency = 4)\nnormalized_numeric_df$inflation<-ts(normalized_numeric_df$inflation,star=decimal_date(as.Date(\"2010-01-01\",format = \"%Y-%m-%d\")),frequency = 4)\nnormalized_numeric_df$unemployment<-ts(normalized_numeric_df$unemployment,star=decimal_date(as.Date(\"2010-01-01\",format = \"%Y-%m-%d\")),frequency = 4)\n\nfit <- lm(PG.Adjusted ~ inflation+unemployment, data=normalized_numeric_df)\nfit.res<-ts(residuals(fit),star=decimal_date(as.Date(\"2010-01-01\",format = \"%Y-%m-%d\")),frequency = 4)\n############## Then look at the residuals ############\nreturns <- fit.res  %>% diff()\nautoplot(returns)+ggtitle(\"Linear Model Returns\")\n\n\n\n\n\n\n\n\n\nCode\nbyd.archTest <- ArchTest(fit.res, lags = 1, demean = TRUE)\nbyd.archTest\n\n\n\n    ARCH LM-test; Null hypothesis: no ARCH effects\n\ndata:  fit.res\nChi-squared = 33.303, df = 1, p-value = 7.885e-09\n\n\n\n\n\n\nCode\nggAcf(returns) +ggtitle(\"ACF for returns\")\n\n\n\n\n\n\n\n\n\nCode\nggPacf(returns) +ggtitle(\"PACF for returns\")\n\n\n\n\n\n\n\n\nThe ARCH LM-test was conducted with the null hypothesis of no ARCH effects. The test resulted in a chi-squared value of 33.303 with one degree of freedom, and a very low p-value of 7.885e-09. This suggests strong evidence against the null hypothesis, indicating the presence of ARCH effects in the data.\nBased on the ACF and PACF plots, it appears that there is some significant autocorrelation and partial autocorrelation at multiple lags, which suggests that an ARIMA model may not be sufficient to capture the time series behavior. Additionally, the values for p and q appear to be relatively high, with p = 8 and q = 8 being suggested by the plots.\n\n\nARIMAX Model\n\nAuto Arima ModelAuto Arima ResidualsARIMAX ModelARIMA (1,1,1) PlotARIMA (1,1,1) ModelCross Validation\n\n\n\n\nCode\nxreg <- cbind(Inflation = normalized_data_ts[, \"inflation\"],\n              Unemployment = normalized_data_ts[, \"unemployment\"])\nfit.auto <- auto.arima(normalized_data_ts[, \"PG.Adjusted\"], xreg = xreg)\nsummary(fit.auto)\n\n\nSeries: normalized_data_ts[, \"PG.Adjusted\"] \nRegression with ARIMA(0,1,0) errors \n\nCoefficients:\n       drift  Inflation  Unemployment\n      0.0442     0.0139       -0.1199\ns.e.  0.0271     0.0857        0.0381\n\nsigma^2 = 0.03803:  log likelihood = 12.55\nAIC=-17.1   AICc=-16.23   BIC=-9.38\n\nTraining set error measures:\n                        ME      RMSE       MAE      MPE     MAPE      MASE\nTraining set -1.921083e-05 0.1873552 0.1337599 409.1758 431.0459 0.4244207\n                  ACF1\nTraining set 0.1245836\n\n\n\n\n\n\nCode\ncheckresiduals(fit.auto)\n\n\n\n\n\n\n    Ljung-Box test\n\ndata:  Residuals from Regression with ARIMA(0,1,0) errors\nQ* = 12.832, df = 8, p-value = 0.1178\n\nModel df: 0.   Total lags used: 8\n\n\n\n\n\n\nCode\nARIMA.c=function(p1,q1,q2,data){\ntemp=c()\nd=1\ni=1\ntemp= data.frame()\nls=matrix(rep(NA,6*30),nrow=30)\n\n\nfor (p in p1)#\n{\n  for(q in q1:q2)#\n  {\n    for(d in 0:1)\n    {\n      \n      if(p+d+q<=6)\n      {\n        \n        model<- Arima(data,order=c(p,d,q))\n        ls[i,]= c(p,d,q,model$aic,model$bic,model$aicc)\n        i=i+1\n        #print(i)\n        \n      }\n      \n    }\n  }\n}\n\n\ntemp= as.data.frame(ls)\nnames(temp)= c(\"p\",\"d\",\"q\",\"AIC\",\"BIC\",\"AICc\")\n\ntemp\n}\n\noutput <- ARIMA.c(1,1,8,data=residuals(fit))\n\noutput[which.min(output$AIC),] \n\n\n  p d q      AIC      BIC     AICc\n2 1 1 1 19.41041 25.20589 19.92105\n\n\nCode\noutput[which.min(output$BIC),]\n\n\n  p d q      AIC      BIC     AICc\n2 1 1 1 19.41041 25.20589 19.92105\n\n\nCode\noutput[which.min(output$AICc),]\n\n\n  p d q      AIC      BIC     AICc\n2 1 1 1 19.41041 25.20589 19.92105\n\n\n\n\n\n\nCode\nset.seed(1234)\n\nmodel_output <- capture.output(sarima(fit.res, 1,1,1)) \n\n\n\n\n\n\n\n\n\nCode\ncat(model_output[30:61], model_output[length(model_output)], sep = \"\\n\")\n\n\n$fit\n\nCall:\narima(x = xdata, order = c(p, d, q), seasonal = list(order = c(P, D, Q), period = S), \n    xreg = constant, transform.pars = trans, fixed = fixed, optim.control = list(trace = trc, \n        REPORT = 1, reltol = tol))\n\nCoefficients:\n         ar1      ma1  constant\n      0.5118  -0.2678   -0.0053\ns.e.  0.3621   0.3920    0.0573\n\nsigma^2 estimated as 0.07602:  log likelihood = -6.7,  aic = 21.4\n\n$degrees_of_freedom\n[1] 48\n\n$ttable\n         Estimate     SE t.value p.value\nar1        0.5118 0.3621  1.4137  0.1639\nma1       -0.2678 0.3920 -0.6833  0.4977\nconstant  -0.0053 0.0573 -0.0933  0.9261\n\n$AIC\n[1] 0.4196409\n\n$AICc\n[1] 0.4296534\n\n$BIC\n[1] 0.5711566\n\n\n\n\n\n\nCode\nn=length(fit.res)\nk= 51\n \n \nrmse1 <- matrix(NA, (n-k),4)\nrmse2 <- matrix(NA, (n-k),4)\nrmse3 <- matrix(NA, (n-k),4)\n\n\nst <- tsp(fit.res)[1]+(k-5)/4 \n\nfor(i in 1:(n-k))\n{\n  xtrain <- window(fit.res, end=st + i/4)\n  xtest <- window(fit.res, start=st + (i+1)/4, end=st + (i+4)/4)\n  \n  #ARIMA(0,1,0) ARIMA(1,1,1)\n  \n  fit <- Arima(xtrain, order=c(0,1,0),\n                include.drift=TRUE, method=\"ML\")\n  fcast <- forecast(fit, h=4)\n  \n  fit2 <- Arima(xtrain, order=c(1,1,1),\n                include.drift=TRUE, method=\"ML\")\n  fcast2 <- forecast(fit2, h=4)\n\n\n  rmse1[i,1:length(xtest)]   <- sqrt((fcast$mean-xtest)^2)\n  rmse2[i,1:length(xtest)] <- sqrt((fcast2$mean-xtest)^2)\n}\n\nplot(1:4,colMeans(rmse1,na.rm=TRUE), type=\"l\",col=2, xlab=\"horizon\", ylab=\"RMSE\")\nlines(1:4, colMeans(rmse2,na.rm=TRUE), type=\"l\",col=3)\nlegend(\"topleft\",legend=c(\"fit1\",\"fit2\"),col=2:4,lty=1)\n\n\n\n\n\n\n\n\nBased on the results of the auto.arima function, the suggested best model is ARIMA(0,1,0). However, when we manually test different ARIMA models, we find that ARIMA(1,1,1) has the lowest values for AIC, BIC, and AICC. Additionally, both models have similar standardized residual plots, with means close to 0, indicating a good fit. The ACF plot of residuals also shows no significant lags, further indicating a well-fitted model.\nTo determine the best model, we conduct cross-validation and compare the RMSE values of both models. The results show that ARIMA(1,1,1) has lower RMSE values than ARIMA(0,1,0), indicating that it is the better model.\nWe can then proceed to choose the best GARCH model using ARIMA(1,1,1) as the base model.\n\n\nSquared Residuals\n\nPlotACF PlotPACF Plot\n\n\n\n\nCode\nfit <- lm(PG.Adjusted ~ inflation+unemployment, data=normalized_numeric_df)\nfit.res<-ts(residuals(fit),star=decimal_date(as.Date(\"2010-01-01\",format = \"%Y-%m-%d\")),frequency = 4)\nfit <- Arima(fit.res,order=c(1,1,1))\nres=fit$res\nplot(res^2,main='Squared Residuals')\n\n\n\n\n\n\n\n\n\nCode\nacf(res^2,24, main = \"ACF Residuals Square\")\n\n\n\n\n\n\n\n\n\nCode\npacf(res^2,24, main = \"PACF Residuals Square\")\n\n\n\n\n\n\n\n\nFrom the squared residuals of the best ARIMA model, it can be observed that the ACF plot and PACF plot indicate that the residuals are not autocorrelated and are white noise, indicating a good fit of the model. Based on the squared residuals of the best ARIMA model, we can see that the ACF and PACF plots indicate that most of the values lie between the blue lines. Additionally, the p-value is 2 and q-value is 2. This suggests that the model has a good fit and that there is no significant autocorrelation or partial autocorrelation in the residuals. Now we can proceed by fitting GARCH Model for p and q values.\n\n\nGARCH Model\n\nModelGRACH(1,1)GRACH(2,1)GRACH(1,2)\n\n\n\n\nCode\nmodel <- list() ## set counter\ncc <- 1\nfor (p in 1:2) {\n  for (q in 1:2) {\n  \nmodel[[cc]] <- garch(res,order=c(q,p),trace=F)\ncc <- cc + 1\n}\n} \n\n## get AIC values for model evaluation\nGARCH_AIC <- sapply(model, AIC) ## model with lowest AIC is the best\nwhich(GARCH_AIC == min(GARCH_AIC))\n\n\n[1] 4\n\n\nCode\nmodel[[which(GARCH_AIC == min(GARCH_AIC))]]\n\n\n\nCall:\ngarch(x = res, order = c(q, p), trace = F)\n\nCoefficient(s):\n       a0         a1         a2         b1         b2  \n2.256e-02  3.047e-02  3.867e-01  1.882e-15  1.963e-01  \n\n\n\n\n\n\nCode\nsummary(garchFit(~garch(1,1),res, trace=F))\n\n\n\nTitle:\n GARCH Modelling \n\nCall:\n garchFit(formula = ~garch(1, 1), data = res, trace = F) \n\nMean and Variance Equation:\n data ~ garch(1, 1)\n<environment: 0x13afee968>\n [data = res]\n\nConditional Distribution:\n norm \n\nCoefficient(s):\n         mu        omega       alpha1        beta1  \n-4.6666e-04   1.9303e-05   1.4788e-01   9.0107e-01  \n\nStd. Errors:\n based on Hessian \n\nError Analysis:\n         Estimate  Std. Error  t value Pr(>|t|)    \nmu     -0.0004667   0.0313902   -0.015    0.988    \nomega   0.0000193   0.0047540    0.004    0.997    \nalpha1  0.1478752   0.0991860    1.491    0.136    \nbeta1   0.9010691   0.1030303    8.746   <2e-16 ***\n---\nSignif. codes:  0 '***' 0.001 '**' 0.01 '*' 0.05 '.' 0.1 ' ' 1\n\nLog Likelihood:\n -3.885054    normalized:  -0.07471257 \n\nDescription:\n Sat Jan  6 19:52:30 2024 by user:  \n\n\nStandardised Residuals Tests:\n                                Statistic p-Value     \n Jarque-Bera Test   R    Chi^2  1.934794  0.380071    \n Shapiro-Wilk Test  R    W      0.9777846 0.4365295   \n Ljung-Box Test     R    Q(10)  9.637841  0.4728202   \n Ljung-Box Test     R    Q(15)  15.96448  0.3844161   \n Ljung-Box Test     R    Q(20)  27.64264  0.1181323   \n Ljung-Box Test     R^2  Q(10)  27.23902  0.002386871 \n Ljung-Box Test     R^2  Q(15)  45.57077  6.220242e-05\n Ljung-Box Test     R^2  Q(20)  51.54676  0.0001325294\n LM Arch Test       R    TR^2   28.74877  0.004292679 \n\nInformation Criterion Statistics:\n      AIC       BIC       SIC      HQIC \n0.3032713 0.4533670 0.2925260 0.3608144 \n\n\n\n\n\n\nCode\nsummary(garchFit(~garch(2,1),res, trace=F))\n\n\n\nTitle:\n GARCH Modelling \n\nCall:\n garchFit(formula = ~garch(2, 1), data = res, trace = F) \n\nMean and Variance Equation:\n data ~ garch(2, 1)\n<environment: 0x118dace58>\n [data = res]\n\nConditional Distribution:\n norm \n\nCoefficient(s):\n         mu        omega       alpha1       alpha2        beta1  \n-0.01258589   0.00315889   0.00000001   0.23548477   0.76820409  \n\nStd. Errors:\n based on Hessian \n\nError Analysis:\n         Estimate  Std. Error  t value Pr(>|t|)    \nmu     -1.259e-02   3.345e-02   -0.376  0.70674    \nomega   3.159e-03   6.361e-03    0.497  0.61945    \nalpha1  1.000e-08   1.290e-01    0.000  1.00000    \nalpha2  2.355e-01   1.984e-01    1.187  0.23522    \nbeta1   7.682e-01   2.043e-01    3.760  0.00017 ***\n---\nSignif. codes:  0 '***' 0.001 '**' 0.01 '*' 0.05 '.' 0.1 ' ' 1\n\nLog Likelihood:\n -2.576856    normalized:  -0.04955492 \n\nDescription:\n Sat Jan  6 19:52:30 2024 by user:  \n\n\nStandardised Residuals Tests:\n                                Statistic p-Value     \n Jarque-Bera Test   R    Chi^2  0.8890177 0.6411391   \n Shapiro-Wilk Test  R    W      0.9811744 0.576967    \n Ljung-Box Test     R    Q(10)  9.405623  0.494089    \n Ljung-Box Test     R    Q(15)  15.23836  0.434388    \n Ljung-Box Test     R    Q(20)  26.41236  0.1526206   \n Ljung-Box Test     R^2  Q(10)  23.65179  0.008580279 \n Ljung-Box Test     R^2  Q(15)  42.63602  0.0001792918\n Ljung-Box Test     R^2  Q(20)  50.08671  0.0002152343\n LM Arch Test       R    TR^2   26.23078  0.009954924 \n\nInformation Criterion Statistics:\n      AIC       BIC       SIC      HQIC \n0.2914175 0.4790371 0.2750005 0.3633465 \n\n\n\n\n\n\nCode\nsummary(garchFit(~garch(1,2),res, trace=F))\n\n\n\nTitle:\n GARCH Modelling \n\nCall:\n garchFit(formula = ~garch(1, 2), data = res, trace = F) \n\nMean and Variance Equation:\n data ~ garch(1, 2)\n<environment: 0x1186fdc58>\n [data = res]\n\nConditional Distribution:\n norm \n\nCoefficient(s):\n         mu        omega       alpha1        beta1        beta2  \n-0.00103287   0.00049634   0.14694954   0.89300793   0.00000001  \n\nStd. Errors:\n based on Hessian \n\nError Analysis:\n         Estimate  Std. Error  t value Pr(>|t|)\nmu     -1.033e-03   3.265e-02   -0.032    0.975\nomega   4.963e-04   5.285e-03    0.094    0.925\nalpha1  1.469e-01   1.622e-01    0.906    0.365\nbeta1   8.930e-01   1.440e+00    0.620    0.535\nbeta2   1.000e-08   1.394e+00    0.000    1.000\n\nLog Likelihood:\n -4.088473    normalized:  -0.07862448 \n\nDescription:\n Sat Jan  6 19:52:30 2024 by user:  \n\n\nStandardised Residuals Tests:\n                                Statistic p-Value     \n Jarque-Bera Test   R    Chi^2  2.312166  0.3147165   \n Shapiro-Wilk Test  R    W      0.9763829 0.3856933   \n Ljung-Box Test     R    Q(10)  9.746935  0.4629691   \n Ljung-Box Test     R    Q(15)  16.0425   0.3792326   \n Ljung-Box Test     R    Q(20)  27.41614  0.123957    \n Ljung-Box Test     R^2  Q(10)  27.39304  0.002256152 \n Ljung-Box Test     R^2  Q(15)  44.98849  7.689356e-05\n Ljung-Box Test     R^2  Q(20)  50.74762  0.0001729551\n LM Arch Test       R    TR^2   29.18198  0.003702039 \n\nInformation Criterion Statistics:\n      AIC       BIC       SIC      HQIC \n0.3495567 0.5371762 0.3331396 0.4214856 \n\n\n\n\n\nBased on the analysis of the different GARCH models, it appears that GARCH(1,1) is the optimal choice. Although the AIC values of the different models are relatively similar, we can further evaluate their significance to make a final determination. Upon closer inspection, it appears that GARCH(1,1) has significantly better values than the other models, indicating that it is the most appropriate choice. Therefore, we can conclude that the GARCH(1,1) model is the best fit for the data.\n\n\nBest Model\n\nARIMA ModelGARCH ModelVolatility\n\n\n\n\nCode\n#fiting an ARIMA model to the Inflation variable\ninflation_fit<-auto.arima(normalized_numeric_df$inflation) \nfinflation<-forecast(inflation_fit)\n\n#fitting an ARIMA model to the Unemployment variable\nunemployment_fit<-auto.arima(normalized_numeric_df$unemployment) \nfunemployment<-forecast(unemployment_fit)\n\n# best model fit for forcasting\nxreg <- cbind(Inflation = normalized_data_ts[, \"inflation\"],\n              Unemployment = normalized_data_ts[, \"unemployment\"])\n\n\n\nsummary(arima.fit<-Arima(normalized_data_ts[, \"PG.Adjusted\"],order=c(1,1,1),xreg=xreg),include.drift = TRUE)\n\n\nSeries: normalized_data_ts[, \"PG.Adjusted\"] \nRegression with ARIMA(1,1,1) errors \n\nCoefficients:\n         ar1      ma1  Inflation  Unemployment\n      0.3380  -0.1329     0.0428       -0.1152\ns.e.  0.4362   0.4426     0.0897        0.0383\n\nsigma^2 = 0.03922:  log likelihood = 12.27\nAIC=-14.55   AICc=-13.21   BIC=-4.89\n\nTraining set error measures:\n                     ME      RMSE       MAE      MPE     MAPE      MASE\nTraining set 0.02921533 0.1882886 0.1408886 283.1477 316.0763 0.4470402\n                    ACF1\nTraining set -0.03829366\n\n\n\n\n\n\nCode\nsummary(final.fit <- garchFit(~garch(1,1), res,trace = F))\n\n\n\nTitle:\n GARCH Modelling \n\nCall:\n garchFit(formula = ~garch(1, 1), data = res, trace = F) \n\nMean and Variance Equation:\n data ~ garch(1, 1)\n<environment: 0x111921518>\n [data = res]\n\nConditional Distribution:\n norm \n\nCoefficient(s):\n         mu        omega       alpha1        beta1  \n-4.6666e-04   1.9303e-05   1.4788e-01   9.0107e-01  \n\nStd. Errors:\n based on Hessian \n\nError Analysis:\n         Estimate  Std. Error  t value Pr(>|t|)    \nmu     -0.0004667   0.0313902   -0.015    0.988    \nomega   0.0000193   0.0047540    0.004    0.997    \nalpha1  0.1478752   0.0991860    1.491    0.136    \nbeta1   0.9010691   0.1030303    8.746   <2e-16 ***\n---\nSignif. codes:  0 '***' 0.001 '**' 0.01 '*' 0.05 '.' 0.1 ' ' 1\n\nLog Likelihood:\n -3.885054    normalized:  -0.07471257 \n\nDescription:\n Sat Jan  6 19:52:31 2024 by user:  \n\n\nStandardised Residuals Tests:\n                                Statistic p-Value     \n Jarque-Bera Test   R    Chi^2  1.934794  0.380071    \n Shapiro-Wilk Test  R    W      0.9777846 0.4365295   \n Ljung-Box Test     R    Q(10)  9.637841  0.4728202   \n Ljung-Box Test     R    Q(15)  15.96448  0.3844161   \n Ljung-Box Test     R    Q(20)  27.64264  0.1181323   \n Ljung-Box Test     R^2  Q(10)  27.23902  0.002386871 \n Ljung-Box Test     R^2  Q(15)  45.57077  6.220242e-05\n Ljung-Box Test     R^2  Q(20)  51.54676  0.0001325294\n LM Arch Test       R    TR^2   28.74877  0.004292679 \n\nInformation Criterion Statistics:\n      AIC       BIC       SIC      HQIC \n0.3032713 0.4533670 0.2925260 0.3608144 \n\n\n\n\n\n\nCode\nht <- final.fit@h.t #a numeric vector with the conditional variances (h.t = sigma.t^delta)\n\n#############################\ndata=data.frame(final)\ndata$Date<-as.Date(data$Date,\"%Y-%m-%d\")\n\n\ndata2= data.frame(ht,data$Date)\nggplot(data2, aes(y = ht, x = data.Date)) + geom_line(col = '#43A098') + ylab('Conditional Variance') + xlab('Date')\n\n\n\n\n\n\n\n\nFrom the ARIMA(1,1,1), we see that the training set error measures also suggest a good fit, with low mean absolute error, root mean squared error, and autocorrelation of the residuals. GATCH(1,1) model model is used to estimate the volatility of the standardized residuals of the previous regression model. The model includes a mean equation that estimates the mean of the residuals and a variance equation that models the conditional variance of the residuals. The coefficients of the mean equation suggest that the mean of the residuals is close to zero. The variance equation coefficients suggest that the conditional variance of the residuals is dependent on the past conditional variances and the past squared standardized residuals. The model’s log-likelihood value is -3.885, and the AIC, BIC, SIC, and HQIC values are all relatively low, indicating a good fit of the model. The standardized residuals tests indicate that the residuals are approximately normally distributed and that there is no significant autocorrelation in the residuals.\nThe volatility of the model seems high in 2020 but has decreased gradually in the past few months. This could indicate that the asset’s price was experiencing a lot of fluctuations in 2020, but the market has stabilized recently.\n\n\nModel Diagnostics\n\nResidualsQQ PlotBox Test\n\n\n\n\nCode\nfit2<-garch(res,order=c(1,1),trace=F)\ncheckresiduals(fit2) \n\n\n\n\n\n\n\n\n\nCode\nqqnorm(fit2$residuals, pch = 1)\nqqline(fit2$residuals, col = \"blue\", lwd = 2)\n\n\n\n\n\n\n\n\n\nCode\nBox.test (fit2$residuals, type = \"Ljung\")\n\n\n\n    Box-Ljung test\n\ndata:  fit2$residuals\nX-squared = 0.020613, df = 1, p-value = 0.8858\n\n\nThe ACF plot of the residuals shows all the values between the blue lines, which indicates that the residuals are not significantly autocorrelated. The range of values for the residual plot between -2 and 2 is considered acceptable. Additionally, the QQ plot of the residuals shows a linear plot on the line, which is another good indication that the residuals are normally distributed. The QQ plot is a valuable tool to assess if the residuals follow a normal distribution, and in this case, the plot suggests that the residuals do indeed follow a normal distribution.\nThe Box-Ljung test, a p-value of 0.9008 indicates that the model’s residuals are not significantly autocorrelated, meaning that the model has captured most of the information in the data. This result is good because it suggests that the model is a good fit for the data and has accounted for most of the underlying patterns in the data. Therefore, we can rely on the model’s predictions and use them to make informed decisions.\n\n\n\n\n\nForecast\n\n\nCode\npredict(final.fit, n.ahead = 5, plot=TRUE)\n\n\n\n\n\n   meanForecast meanError standardDeviation lowerInterval upperInterval\n1 -0.0004666581 0.4689871         0.4689871    -0.9196645     0.9187312\n2 -0.0004666581 0.4803472         0.4803472    -0.9419299     0.9409966\n3 -0.0004666581 0.4919816         0.4919816    -0.9647328     0.9637995\n4 -0.0004666581 0.5038967         0.5038967    -0.9880861     0.9871528\n5 -0.0004666581 0.5160996         0.5160996    -1.0120032     1.0110699\n\n\nThe forecasted plot is based on the best model ARIMAX(1,1,1)+GARCH(1,1). This model takes into account the autoregressive and moving average components of the data, as well as the impact of exogenous variables on the time series. Additionally, the GARCH component of the model accounts for the volatility clustering in the data. Overall, this model is well-suited to make accurate predictions about future values of the time series.\n\n\nEquation of the Model\nThe equation of the ARIMAX(1,1,1) model is:\n\\(Y(t) = c + \\phi_1(Y{(t-1)} - X{(t-1)}) + \\theta_1\\epsilon{(t-1)} + \\epsilon(t)\\)\nwhere, \\(Y(t)\\) is the time series variable, \\(X(t-1)\\) is the exogenous variable, \\(c\\) is a constant, \\(\\phi_1\\) and \\(\\theta_1\\) are the parameters, and \\(\\epsilon(t)\\) is the error term.\nThe equation of the GARCH(1,1) model is:\n\\(\\sigma^2(t) = \\alpha_0+\\alpha_1\\epsilon_t^2(t-1)+ \\beta_1\\sigma^2(t-1)\\)\nwhere \\(\\sigma^2_t\\) is the conditional variance at time \\(t\\), \\(\\alpha_0\\) is a constant, \\(\\alpha_1\\) and \\(\\beta_1\\) are the parameters, and \\(\\epsilon_t\\) is the error term.\nThe combined equation of the ARIMAX(1,1,1)+GARCH(1,1) model is:\n\\(Y(t) = c + \\phi_1(Y(t-1) - X(t-1)) + \\theta_1\\epsilon(t-1) + \\epsilon(t)\\)\n\\(\\epsilon(t) = \\sigma(t) * \\epsilon~(t)\\)\n\\(\\sigma^2(t) = \\alpha_0+\\alpha_1\\epsilon_t^2(t-1)+ \\beta_1\\sigma^2(t-1)\\)"
  },
  {
    "objectID": "arch_Communication_Services_Sector_Fund.html",
    "href": "arch_Communication_Services_Sector_Fund.html",
    "title": "",
    "section": "",
    "text": "Impact of Macroeconomic Factors on AT&T Stock Price\n\n\n\nAT&T Inc. is a multinational telecommunications conglomerate that provides a range of services including wireless communication, internet, television, and digital entertainment. It is one of the largest communication service providers in the United States and has a significant presence in other countries as well. In recent years, AT&T Inc. has been focusing on expanding its 5G network and acquiring media companies to strengthen its digital entertainment offerings.\nOne of the top companies in the consumer staple sector is Procter & Gamble Co. (T), which produces a wide range of products including personal care, cleaning, and food and beverage items. To analyze the stock price behavior of Procter & Gamble, an ARIMAX+ARCH/GARCH model can be employed.\nBy using an ARIMAX+ARCH/GARCH model to analyze the stock price behavior of AT&T, we can gain insights into how macroeconomic factors impact the company’s performance. For example, an increase in GDP growth rate or a decrease in unemployment rate may lead to increased consumer spending and a rise in AT&T’s stock price. Conversely, an increase in inflation or interest rates may lead to a decrease in consumer spending and a decline in AT&T’s stock price.\n\n\nTime series Plot\n\nAT&TDifferentitaed Chart SeriesGDP GrowthInflationInterestUnemployment\n\n\n\n\nCode\n# get data\noptions(\"getSymbols.warning4.0\"=FALSE)\noptions(\"getSymbols.yahoo.warning\"=FALSE)\n\n\ndata.info = getSymbols(\"T\",src='yahoo', from = '2010-01-01',to = \"2023-03-01\",auto.assign = FALSE)\ndata = getSymbols(\"T\",src='yahoo', from = '2010-01-01',to = \"2023-03-01\")\ndf <- data.frame(Date=index(T),coredata(T))\n\n# create Bollinger Bands\nbbands <- BBands(T[,c(\"T.High\",\"T.Low\",\"T.Close\")])\n\n# join and subset data\ndf <- subset(cbind(df, data.frame(bbands[,1:3])), Date >= \"2010-01-01\")\n\n\n# colors column for increasing and decreasing\nfor (i in 1:length(df[,1])) {\n  if (df$T.Close[i] >= df$T.Open[i]) {\n      df$direction[i] = 'Increasing'\n  } else {\n      df$direction[i] = 'Decreasing'\n  }\n}\n\ni <- list(line = list(color = '#73C6B6'))\nd <- list(line = list(color = '#7F7F7F'))\n\n# plot candlestick chart\n\nfig <- df %>% plot_ly(x = ~Date, type=\"candlestick\",\n          open = ~T.Open, close = ~T.Close,\n          high = ~T.High, low = ~T.Low, name = \"T\",\n          increasing = i, decreasing = d) \nfig <- fig %>% add_lines(x = ~Date, y = ~up , name = \"B Bands\",\n            line = list(color = '#ccc', width = 0.5),\n            legendgroup = \"Bollinger Bands\",\n            hoverinfo = \"none\", inherit = F) \nfig <- fig %>% add_lines(x = ~Date, y = ~dn, name = \"B Bands\",\n            line = list(color = '#ccc', width = 0.5),\n            legendgroup = \"Bollinger Bands\", inherit = F,\n            showlegend = FALSE, hoverinfo = \"none\") \nfig <- fig %>% add_lines(x = ~Date, y = ~mavg, name = \"Mv Avg\",\n            line = list(color = '#C052B3', width = 0.5),\n            hoverinfo = \"none\", inherit = F) \nfig <- fig %>% layout(yaxis = list(title = \"Price\"))\n\n# plot volume bar chart\nfig2 <- df \nfig2 <- fig2 %>% plot_ly(x=~Date, y=~T.Volume, type='bar', name = \"T Volume\",\n          color = ~direction, colors = c('#73C6B6','#7F7F7F')) \nfig2 <- fig2 %>% layout(yaxis = list(title = \"Volume\"))\n\n# create rangeselector buttons\nrs <- list(visible = TRUE, x = 0.5, y = -0.055,\n           xanchor = 'center', yref = 'paper',\n           font = list(size = 9),\n           buttons = list(\n             list(count=1,\n                  label='RESET',\n                  step='all'),\n             list(count=3,\n                  label='3 YR',\n                  step='year',\n                  stepmode='backward'),\n             list(count=1,\n                  label='1 YR',\n                  step='year',\n                  stepmode='backward'),\n             list(count=1,\n                  label='1 MO',\n                  step='month',\n                  stepmode='backward')\n           ))\n\n# subplot with shared x axis\nfig <- subplot(fig, fig2, heights = c(0.7,0.2), nrows=2,\n             shareX = TRUE, titleY = TRUE)\nfig <- fig %>% layout(title = paste(\"AT&T Stock Price: January 2010 - March 2023\"),\n         xaxis = list(rangeselector = rs),\n         legend = list(orientation = 'h', x = 0.5, y = 1,\n                       xanchor = 'center', yref = 'paper',\n                       font = list(size = 10),\n                       bgcolor = 'transparent'))\n\nfig\n\n\n\n\n\n\n\n\n\n\nCode\nlog(data.info$`T.Adjusted`) %>% diff() %>% chartSeries(theme=chartTheme('white'),up.col='#73C6B6')\n\n\n\n\n\n\n\n\n\nCode\n#import the data\ngdp <- read.csv(\"DATA/RAW DATA/gdp-growth.csv\")\n\n#change date format\ngdp$Date <- as.Date(gdp$DATE , \"%m/%d/%Y\")\n\n#drop DATE column\ngdp <- subset(gdp, select = -c(1))\n\n#export the cleaned data\ngdp_clean <- gdp\nwrite.csv(gdp_clean, \"DATA/CLEANED DATA/gdp_clean_data.csv\", row.names=FALSE)\n\n#plot gdp growth rate \nfig <- plot_ly(gdp, x = ~Date, y = ~value, type = 'scatter', mode = 'lines',line = list(color = 'rgb(240, 128, 128)'))\nfig <- fig %>% layout(title = \"U.S GPD Growth Rate: 2010 - 2022\",xaxis = list(title = \"Time\"),yaxis = list(title =\"GDP Growth Rate\"))\nfig\n\n\n\n\n\n\n\n\n\n\nCode\n#import the data\ninflation_rate <- read.csv(\"DATA/RAW DATA/inflation-rate.csv\")\n\n#cleaning the data\n#remove unwanted columns\ninflation_rate_clean <- subset(inflation_rate, select = -c(1,HALF1,HALF2))\n\n#convert the data to time series data\ninflation_data_ts <- ts(as.vector(t(as.matrix(inflation_rate_clean))), start=c(2010,1), end=c(2023,2), frequency=12)\n\n#export the data\nwrite.csv(inflation_rate_clean, \"DATA/CLEANED DATA/inflation_rate_clean_data.csv\", row.names=FALSE)\n\n\n#plot inflation rate \nfig <- autoplot(inflation_data_ts, ylab = \"Inflation Rate\", color=\"#FFA07A\")+ggtitle(\"U.S Inflation Rate: January 2010 - February 2023\")+theme_bw()\nggplotly(fig)\n\n\n\n\n\n\n\n\n\n\nCode\n#import the data\ninterest_data <- read.csv(\"DATA/RAW DATA/interest-rate.csv\")\n\n#change date format\ninterest_data$Date <- as.Date(interest_data$Date , \"%m/%d/%Y\")\n\n#export the cleaned data\ninterest_clean_data <- interest_data\nwrite.csv(interest_clean_data, \"DATA/CLEANED DATA/interest_rate_clean_data.csv\", row.names=FALSE)\n\n#plot interest rate \nfig <- plot_ly(interest_data, x = ~Date, y = ~value, type = 'scatter', mode = 'lines',line = list(color='rgb(219, 112, 147)'))\nfig <- fig %>% layout(title = \"U.S Interest Rate: January 2010 - March 2023\",xaxis = list(title = \"Time\"),yaxis = list(title =\"Interest Rate\"))\nfig\n\n\n\n\n\n\n\n\n\n\nCode\n#import the data\nunemployment_rate <- read.csv(\"DATA/RAW DATA/unemployment-rate.csv\")\n\n#change date format\nunemployment_rate$Date <- as.Date(unemployment_rate$Date , \"%m/%d/%Y\")\n\n# export the data\nwrite.csv(unemployment_rate, \"DATA/CLEANED DATA/unemployment_rate_clean_data.csv\", row.names=FALSE)\n\n#plot unemployment rate \nfig <- plot_ly(unemployment_rate, x = ~Date, y = ~Value, type = 'scatter', mode = 'lines',line = list(color = 'rgb(189, 183, 107)'))\nfig <- fig %>% layout(title = \"U.S Unemployment Rate: January 2010 - March 2023\",xaxis = list(title = \"Time\"),yaxis = list(title =\"Unemployment Rate\"))\nfig\n\n\n\n\n\n\n\n\n\nFrom 2010 to 2014, AT&T’s stock price had a relatively steady increase, likely due to the company’s consistent revenue growth and strong financial performance. However, from 2014 to 2016, AT&T’s stock price experienced some volatility, which could be attributed to various factors such as changes in regulatory policies and increased competition in the telecommunication industry.\nIn 2016, AT&T’s stock price began to recover, likely due to the company’s acquisition of Time Warner, which expanded its portfolio of media and entertainment assets. This trend continued into 2017 and 2018, with AT&T’s stock price reaching an all-time high in mid-2018.\nHowever, from late 2018 to early 2020, AT&T’s stock price experienced some decline, which could be attributed to factors such as the company’s high debt load and increasing competition from new players in the telecommunication industry.\nThe outbreak of the COVID-19 pandemic in early 2020 caused a brief dip in AT&T’s stock price, as investors were uncertain about the impact of the pandemic on the company’s operations and financial performance. Nevertheless, AT&T’s strong position in the telecommunication and media industries helped it to quickly rebound and continue its growth trend throughout 2020 and into early 2021.\nSince early 2021, AT&T’s stock price has experienced some volatility, likely due to a combination of factors such as global economic uncertainty and fluctuations in consumer demand for the company’s products and services.\nSince early 2020, AT&T’s stock price has experienced some volatility, likely due to a combination of factors such as global economic uncertainty and fluctuations in consumer demand for the company’s products.\nAs discussed before, the macroeconomic factors of GDP growth, inflation, interest rates, and unemployment rate are closely interrelated and play a crucial role in the overall health and stability of an economy. From 2010 to 2023, the global economy experienced a mix of ups and downs, with periods of strong GDP growth followed by slowdowns and recessions.\nThe second plot shows the first difference of the logarithm of the adjusted AT&T stock price. Taking the first difference removes any long-term trends and transforms the time series into a stationary process. From the plot, we can observe that the first difference of the logarithm of the AT&T stock price appears to be stationary, as the mean and variance are roughly constant over time.\n\n\nEnodogenous and Exogenous Variables\n\nPlotCorrelation HeatmapCCF GDPCCF InterestCCF InflationCCF Unemployment\n\n\n\n\nCode\nnumeric_data <- c(\"T.Adjusted\",\"gdp\", \"interest\", \"inflation\", \"unemployment\")\nnumeric_data <- final[, numeric_data]\nnormalized_data_numeric <- scale(numeric_data)\nnormalized_data <- ts(normalized_data_numeric, start = c(2010, 1), end = c(2021,10),frequency = 4)\nts_plot(normalized_data,\n        title = \"Normalized Time Series Data for T Stock and Macroeconomic Variables\",\n        Ytitle = \"Normalized Values\",\n        Xtitle = \"Year\")\n\n\n\n\n\n\n\n\n\n\nCode\n# Get upper triangle of the correlation matrix\nget_upper_tri <- function(cormat){\n    cormat[lower.tri(cormat)]<- NA\n    return(cormat)\n}\ncormat <- round(cor(normalized_data_numeric),2)\n\nupper_tri <- get_upper_tri(cormat)\n\nmelted_cormat <- melt(upper_tri, na.rm = TRUE)\n# Create a ggheatmap\nggheatmap <- ggplot(melted_cormat, aes(Var2, Var1, fill = value))+\n geom_tile(color = \"white\")+\n scale_fill_gradient2(low = \"blue\", high = \"red\", mid = \"white\", \n   midpoint = 0, limit = c(-1,1), space = \"Lab\", \n    name=\"Pearson\\nCorrelation\") +\n  theme_minimal()+ # minimal theme\n theme(axis.text.x = element_text(angle = 45, vjust = 1, \n    size = 12, hjust = 1))+\n coord_fixed()\n\nggheatmap + \ngeom_text(aes(Var2, Var1, label = value), color = \"black\", size = 4) +\ntheme(\n  axis.title.x = element_blank(),\n  axis.title.y = element_blank(),\n  panel.grid.major = element_blank(),\n  panel.border = element_blank(),\n  panel.background = element_blank(),\n  axis.ticks = element_blank(),\n  legend.justification = c(1, 0),\n  legend.position = c(0.6, 0.7),\n  legend.direction = \"horizontal\")+\n  guides(fill = guide_colorbar(barwidth = 7, barheight = 1,\n                title.position = \"top\", title.hjust = 0.5))\n\n\n\n\n\n\n\n\n\nCode\npar(mfrow=c(1,1))\nccf_result <- ccf(normalized_data[, c(\"T.Adjusted\")], normalized_data[, c(\"gdp\")], \n    lag.max = 300,\n    main = \"Cros-Correlation Plot for T Stock Price and GDP Growth Rate \",\n    ylab = \"CCF\")\n\n\n\n\n\nCode\ncat(\"The sum of cross correlation function is\", sum(abs(ccf_result$acf)))\n\n\nThe sum of cross correlation function is 5.957981\n\n\n\n\n\n\nCode\npar(mfrow=c(1,1))\nccf_result <- ccf(normalized_data[, c(\"T.Adjusted\")], normalized_data[, c(\"interest\")], \n    lag.max = 300,\n    main = \"Cros-Correlation Plot for T Stock Price and Interest Rate\",\n    ylab = \"CCF\")\n\n\n\n\n\nCode\ncat(\"The sum of cross correlation function is\", sum(abs(ccf_result$acf)))\n\n\nThe sum of cross correlation function is 12.0267\n\n\n\n\n\n\nCode\npar(mfrow=c(1,1))\nccf_result <- ccf(normalized_data[, c(\"T.Adjusted\")], normalized_data[, c(\"inflation\")], \n    lag.max = 300,\n    main = \"Cros-Correlation Plot for T Stock Price and Inflation Rate\",\n    ylab = \"CCF\")\n\n\n\n\n\nCode\ncat(\"The sum of cross correlation function is\", sum(abs(ccf_result$acf)))\n\n\nThe sum of cross correlation function is 16.72642\n\n\n\n\n\n\nCode\npar(mfrow=c(1,1))\nccf_result <- ccf(normalized_data[, c(\"T.Adjusted\")], normalized_data[, c(\"unemployment\")], \n    lag.max = 300,\n    main = \"Cros-Correlation Plot for T Stock Priceand Unemployment Rate\",\n    ylab = \"CCF\")\n\n\n\n\n\nCode\ncat(\"The sum of cross correlation function is\", sum(abs(ccf_result$acf)))\n\n\nThe sum of cross correlation function is 20.3674\n\n\n\n\n\nThe Normalized Time Series Data for Stock Price and Macroeconomic Variables plot shows the same variables as the first plot but has been normalized to a common range of 0 to 1 using the scale() function in R, which standardizes the variables to have a mean of 0 and a standard deviation of 1. The heatmap analysis of the normalized data reveals that inflation and unemployment rate exhibit strong positive correlations with the stock price indices, indicating that these variables may significantly influence stock price movements. On the other hand, weaker correlations were observed between the stock price indices and GDP and interest rates, suggesting that these variables may have less impact on stock price fluctuations. The cross-correlation feature plots confirm these findings, indicating that inflation and unemployment rate are more suitable feature variables for the ARIMAX model when predicting AT&T movements.\nFinal Exogenous variables: Macroeconomic indicators: Inflation rate and unemployment rate.\n\n\nEnodogenous and Exogenous Variables Plot\n\nPlotCheck the stationarity\n\n\n\n\nCode\nfinal_data <- final %>%dplyr::select( Date,T.Adjusted, inflation,unemployment)\nnumeric_data <- c(\"T.Adjusted\", \"inflation\",\"unemployment\")\nnumeric_data <- final_data[, numeric_data]\nnormalized_data_numeric <- scale(numeric_data)\nnormalized_numeric_df <- data.frame(normalized_data_numeric)\nnormalized_data_ts <- ts(normalized_data_numeric, start = c(2010, 1), frequency = 4)\n\nautoplot(normalized_data_ts, facets=TRUE) +\n  xlab(\"Year\") + ylab(\"\") +\n  ggtitle(\"AT&T Stock Price, Inflation Rate and Unemployment Rate in USA 2010-2023\")\n\n\n\n\n\n\n\n\n\nCode\n# Convert your multivariate time series data to a matrix\nfinal_data_ts_multivariate <- as.matrix(normalized_data_ts)\n\n# Check for stationarity using Phillips-Perron test\nphillips_perron_test <- ur.pp(final_data_ts_multivariate)  \nsummary(phillips_perron_test)\n\n\n\n################################## \n# Phillips-Perron Unit Root Test # \n################################## \n\nTest regression with intercept \n\n\nCall:\nlm(formula = y ~ y.l1)\n\nResiduals:\n    Min      1Q  Median      3Q     Max \n-1.3311 -0.1828 -0.0604  0.1125  4.4755 \n\nCoefficients:\n            Estimate Std. Error t value Pr(>|t|)    \n(Intercept) 0.005902   0.041678   0.142    0.888    \ny.l1        0.845352   0.042100  20.079   <2e-16 ***\n---\nSignif. codes:  0 '***' 0.001 '**' 0.01 '*' 0.05 '.' 0.1 ' ' 1\n\nResidual standard error: 0.5189 on 153 degrees of freedom\nMultiple R-squared:  0.7249,    Adjusted R-squared:  0.7231 \nF-statistic: 403.2 on 1 and 153 DF,  p-value: < 2.2e-16\n\n\nValue of test-statistic, type: Z-alpha  is: -22.8539 \n\n         aux. Z statistics\nZ-tau-mu            0.1434\n\n\n\n\n\nThe results of the Phillips-Perron unit root test indicate strong evidence against the null hypothesis of a unit root, as the p-value for the coefficient of the lagged variable is less than the significance level of 0.05. This suggests that the variable y, which is being tested for stationarity, is likely stationary. Furthermore, the test statistic Z-tau-mu is 0.1434, which is smaller than the critical value of Z-alpha (-22.8539), providing further evidence of stationarity.\nTo determine whether the linear model requires an ARCH model, an ARCH test is conducted. The ACF and PACF plots are also used to identify suitable model values.\n\n\nModel Fitting\n\nPlotARCH TestACF PlotPACF Plot\n\n\n\n\nCode\nnormalized_numeric_df$T.Adjusted<-ts(normalized_numeric_df$T.Adjusted,star=decimal_date(as.Date(\"2010-01-01\",format = \"%Y-%m-%d\")),frequency = 4)\nnormalized_numeric_df$inflation<-ts(normalized_numeric_df$inflation,star=decimal_date(as.Date(\"2010-01-01\",format = \"%Y-%m-%d\")),frequency = 4)\nnormalized_numeric_df$unemployment<-ts(normalized_numeric_df$unemployment,star=decimal_date(as.Date(\"2010-01-01\",format = \"%Y-%m-%d\")),frequency = 4)\n\nfit <- lm(T.Adjusted ~ inflation+unemployment, data=normalized_numeric_df)\nfit.res<-ts(residuals(fit),star=decimal_date(as.Date(\"2010-01-01\",format = \"%Y-%m-%d\")),frequency = 4)\n############## Then look at the residuals ############\nreturns <- fit.res  %>% diff()\nautoplot(returns)+ggtitle(\"Linear Model Returns\")\n\n\n\n\n\n\n\n\n\nCode\nbyd.archTest <- ArchTest(fit.res, lags = 1, demean = TRUE)\nbyd.archTest\n\n\n\n    ARCH LM-test; Null hypothesis: no ARCH effects\n\ndata:  fit.res\nChi-squared = 12.762, df = 1, p-value = 0.0003537\n\n\n\n\n\n\nCode\nggAcf(returns) +ggtitle(\"ACF for returns\")\n\n\n\n\n\n\n\n\n\nCode\nggPacf(returns) +ggtitle(\"PACF for returns\")\n\n\n\n\n\n\n\n\nThe ARCH LM-test was conducted with the null hypothesis of no ARCH effects. The test resulted in a chi-squared value of 12.762 with one degree of freedom, and a very low p-value of 0.0003537. This suggests strong evidence against the null hypothesis, indicating the presence of ARCH effects in the data.\nBased on the ACF and PACF plots, it appears that there is some significant autocorrelation and partial autocorrelation at multiple lags, which suggests that an ARIMA model may not be sufficient to capture the time series behavior. Additionally, the values for p and q appear to be relatively high, with p = 0and q = 0 being suggested by the plots.\n\n\nARIMAX Model\n\nAuto Arima ModelAuto Arima Residuals\n\n\n\n\nCode\nxreg <- cbind(Inflation = normalized_data_ts[, \"inflation\"],\n              Unemployment = normalized_data_ts[, \"unemployment\"])\nfit.auto <- auto.arima(normalized_data_ts[, \"T.Adjusted\"], xreg = xreg)\nsummary(fit.auto)\n\n\nSeries: normalized_data_ts[, \"T.Adjusted\"] \nRegression with ARIMA(0,1,0) errors \n\nCoefficients:\n      Inflation  Unemployment\n         -0.156       -0.3069\ns.e.      0.150        0.0681\n\nsigma^2 = 0.119:  log likelihood = -17.07\nAIC=40.14   AICc=40.65   BIC=45.94\n\nTraining set error measures:\n                     ME      RMSE       MAE       MPE     MAPE      MASE\nTraining set 0.03193585 0.3349059 0.2596331 -52.59156 81.78785 0.5318546\n                   ACF1\nTraining set -0.1104956\n\n\n\n\n\n\nCode\ncheckresiduals(fit.auto)\n\n\n\n\n\n\n    Ljung-Box test\n\ndata:  Residuals from Regression with ARIMA(0,1,0) errors\nQ* = 1.767, df = 8, p-value = 0.9873\n\nModel df: 0.   Total lags used: 8\n\n\n\n\n\nBased on the results of the auto.arima function, the suggested best model is ARIMA(0,1,0) which is the same as the manual choosen arima model. So, we can then proceed to choose the best GARCH model using ARIMA(0,1,0) as the base model.\n\n\nSquared Residuals\n\nPlotACF PlotPACF PlotACF AbsoluteARCH Model\n\n\n\n\nCode\nfit <- lm(T.Adjusted ~ inflation+unemployment, data=normalized_numeric_df)\nfit.res<-ts(residuals(fit),star=decimal_date(as.Date(\"2010-01-01\",format = \"%Y-%m-%d\")),frequency = 4)\nfit <- Arima(fit.res,order=c(0,1,0))\nres=fit$res\nplot(res^2,main='Squared Residuals')\n\n\n\n\n\n\n\n\n\nCode\nacf(res^2,24, main = \"ACF Residuals Square\")\n\n\n\n\n\n\n\n\n\nCode\npacf(res^2,24, main = \"PACF Residuals Square\")\n\n\n\n\n\n\n\n\n\nCode\nacf(abs(res), main = \"ACF of Absolute Residuals\")\n\n\n\n\n\n\n\nCode\npacf(abs(res), main = \"PACF of Absolute Residuals\")\n\n\n\n\n\n\n\n\n\nCode\nsummary(garchFit(~garch(1,0),res, trace=F))\n\n\n\nTitle:\n GARCH Modelling \n\nCall:\n garchFit(formula = ~garch(1, 0), data = res, trace = F) \n\nMean and Variance Equation:\n data ~ garch(1, 0)\n<environment: 0x11c534e80>\n [data = res]\n\nConditional Distribution:\n norm \n\nCoefficient(s):\n       mu      omega     alpha1  \n-0.014923   0.061675   1.000000  \n\nStd. Errors:\n based on Hessian \n\nError Analysis:\n        Estimate  Std. Error  t value Pr(>|t|)\nmu      -0.01492     0.04410   -0.338    0.735\nomega    0.06167     0.05922    1.041    0.298\nalpha1   1.00000     0.91139    1.097    0.273\n\nLog Likelihood:\n -24.54426    normalized:  -0.4720049 \n\nDescription:\n Sat Jan  6 19:51:47 2024 by user:  \n\n\nStandardised Residuals Tests:\n                                Statistic p-Value     \n Jarque-Bera Test   R    Chi^2  21.09326  2.628191e-05\n Shapiro-Wilk Test  R    W      0.9298325 0.004420054 \n Ljung-Box Test     R    Q(10)  14.55688  0.1490716   \n Ljung-Box Test     R    Q(15)  23.42808  0.07546427  \n Ljung-Box Test     R    Q(20)  30.70205  0.05924875  \n Ljung-Box Test     R^2  Q(10)  3.989278  0.9478294   \n Ljung-Box Test     R^2  Q(15)  4.464594  0.9957709   \n Ljung-Box Test     R^2  Q(20)  7.691274  0.9937212   \n LM Arch Test       R    TR^2   5.066894  0.9557072   \n\nInformation Criterion Statistics:\n     AIC      BIC      SIC     HQIC \n1.059394 1.171966 1.053209 1.102552 \n\n\n\n\n\nFrom the squared residuals of the best ARIMA model, it can be observed that the ACF plot and PACF plot indicate that the residuals are not autocorrelated and are white noise, indicating a good fit of the model. Based on the squared residuals of the best ARIMA model, we can see that the ACF and PACF plots indicate that most of the values lie between the blue lines. Additionally, the p-value is 1 and q-value is 0. This suggests that the model has a good fit and that there is no significant autocorrelation or partial autocorrelation in the residuals.\nThe best models are ARIMA(0,1,0) and ARCH(1)\n\n\nBest Model\n\nARIMA ModelGARCH ModelVolatility\n\n\n\n\nCode\n#fiting an ARIMA model to the Inflation variable\ninflation_fit<-auto.arima(normalized_numeric_df$inflation) \nfinflation<-forecast(inflation_fit)\n\n#fitting an ARIMA model to the Unemployment variable\nunemployment_fit<-auto.arima(normalized_numeric_df$unemployment) \nfunemployment<-forecast(unemployment_fit)\n\n# best model fit for forcasting\nxreg <- cbind(Inflation = normalized_data_ts[, \"inflation\"],\n              Unemployment = normalized_data_ts[, \"unemployment\"])\n\n\n\nsummary(arima.fit<-Arima(normalized_data_ts[, \"T.Adjusted\"],order=c(0,1,0),xreg=xreg),include.drift = TRUE)\n\n\nSeries: normalized_data_ts[, \"T.Adjusted\"] \nRegression with ARIMA(0,1,0) errors \n\nCoefficients:\n      Inflation  Unemployment\n         -0.156       -0.3069\ns.e.      0.150        0.0681\n\nsigma^2 = 0.119:  log likelihood = -17.07\nAIC=40.14   AICc=40.65   BIC=45.94\n\nTraining set error measures:\n                     ME      RMSE       MAE       MPE     MAPE      MASE\nTraining set 0.03193585 0.3349059 0.2596331 -52.59156 81.78785 0.5318546\n                   ACF1\nTraining set -0.1104956\n\n\n\n\n\n\nCode\nsummary(final.fit <- garchFit(~garch(1,0), res,trace = F))\n\n\n\nTitle:\n GARCH Modelling \n\nCall:\n garchFit(formula = ~garch(1, 0), data = res, trace = F) \n\nMean and Variance Equation:\n data ~ garch(1, 0)\n<environment: 0x11df83050>\n [data = res]\n\nConditional Distribution:\n norm \n\nCoefficient(s):\n       mu      omega     alpha1  \n-0.014923   0.061675   1.000000  \n\nStd. Errors:\n based on Hessian \n\nError Analysis:\n        Estimate  Std. Error  t value Pr(>|t|)\nmu      -0.01492     0.04410   -0.338    0.735\nomega    0.06167     0.05922    1.041    0.298\nalpha1   1.00000     0.91139    1.097    0.273\n\nLog Likelihood:\n -24.54426    normalized:  -0.4720049 \n\nDescription:\n Sat Jan  6 19:51:47 2024 by user:  \n\n\nStandardised Residuals Tests:\n                                Statistic p-Value     \n Jarque-Bera Test   R    Chi^2  21.09326  2.628191e-05\n Shapiro-Wilk Test  R    W      0.9298325 0.004420054 \n Ljung-Box Test     R    Q(10)  14.55688  0.1490716   \n Ljung-Box Test     R    Q(15)  23.42808  0.07546427  \n Ljung-Box Test     R    Q(20)  30.70205  0.05924875  \n Ljung-Box Test     R^2  Q(10)  3.989278  0.9478294   \n Ljung-Box Test     R^2  Q(15)  4.464594  0.9957709   \n Ljung-Box Test     R^2  Q(20)  7.691274  0.9937212   \n LM Arch Test       R    TR^2   5.066894  0.9557072   \n\nInformation Criterion Statistics:\n     AIC      BIC      SIC     HQIC \n1.059394 1.171966 1.053209 1.102552 \n\n\n\n\n\n\nCode\nht <- final.fit@h.t #a numeric vector with the conditional variances (h.t = sigma.t^delta)\n\n#############################\ndata=data.frame(final)\ndata$Date<-as.Date(data$Date,\"%Y-%m-%d\")\n\n\ndata2= data.frame(ht,data$Date)\nggplot(data2, aes(y = ht, x = data.Date)) + geom_line(col = '#73C6B6') + ylab('Conditional Variance') + xlab('Date')\n\n\n\n\n\n\n\n\nFrom the ARIMA(0,1,0), we see that the training set error measures also suggest a good fit, with low mean absolute error, root mean squared error, and autocorrelation of the residuals. GATCH(1,0) model model is used to estimate the volatility of the standardized residuals of the previous regression model. The model includes a mean equation that estimates the mean of the residuals and a variance equation that models the conditional variance of the residuals. The coefficients of the mean equation suggest that the mean of the residuals is close to zero. The variance equation coefficients suggest that the conditional variance of the residuals is dependent on the past conditional variances and the past squared standardized residuals. The model’s the AIC, BIC, SIC, and HQIC values are all relatively low, indicating a good fit of the model. The standardized residuals tests indicate that the residuals are approximately normally distributed and that there is no significant autocorrelation in the residuals.\nThe volatility of the model seems high in 2020 but has decreased gradually in the past few months. This could indicate that the asset’s price was experiencing a lot of fluctuations in 2020, but the market has stabilized recently.\n\n\nModel Diagnostics\n\nResidualsQQ PlotBox Test\n\n\n\n\nCode\nfit2<-garch(res,order=c(1,0),trace=F)\ncheckresiduals(fit2) \n\n\n\n\n\n\n\n\n\nCode\nqqnorm(fit2$residuals, pch = 1)\nqqline(fit2$residuals, col = \"blue\", lwd = 2)\n\n\n\n\n\n\n\n\n\nCode\nBox.test (fit2$residuals, type = \"Ljung\")\n\n\n\n    Box-Ljung test\n\ndata:  fit2$residuals\nX-squared = 0.20159, df = 1, p-value = 0.6534\n\n\nThe ACF plot of the residuals shows all the values between the blue lines, which indicates that the residuals are not significantly autocorrelated. The range of values for the residual plot between -2 and 2 is considered acceptable. Additionally, the QQ plot of the residuals shows a linear plot on the line, which is another good indication that the residuals are normally distributed. The QQ plot is a valuable tool to assess if the residuals follow a normal distribution, and in this case, the plot suggests that the residuals do indeed follow a normal distribution.\nThe Box-Ljung test, a p-value of 0.6534 indicates that the model’s residuals are not significantly autocorrelated, meaning that the model has captured most of the information in the data. This result is good because it suggests that the model is a good fit for the data and has accounted for most of the underlying patterns in the data. Therefore, we can rely on the model’s predictions and use them to make informed decisions.\n\n\n\n\n\nForecast\n\n\nCode\npredict(final.fit, n.ahead = 5, plot=TRUE)\n\n\n\n\n\n  meanForecast meanError standardDeviation lowerInterval upperInterval\n1  -0.01492345  1.240661          1.240661     -2.446574      2.416727\n2  -0.01492345  1.265272          1.265272     -2.494812      2.464965\n3  -0.01492345  1.289414          1.289414     -2.542129      2.512282\n4  -0.01492345  1.313112          1.313112     -2.588576      2.558729\n5  -0.01492345  1.336390          1.336390     -2.634200      2.604353\n\n\nThe forecasted plot is based on the best model ARIMAX(0,1,0)+ARCH(1,0). This model takes into account the autoregressive and moving average components of the data, as well as the impact of exogenous variables on the time series. Additionally, the GARCH component of the model accounts for the volatility clustering in the data. Overall, this model is well-suited to make accurate predictions about future values of the time series.\n\n\nEquation of the Model\nThe equation of the ARIMAX(0,1,0) model is:\n\\(Y(t) = Y(t-1) + \\epsilon(t)\\)\nwhere \\(Y(t)\\) is the time series variable and \\(\\epsilon(t)\\) is the error term.\nThe equation of the ARCH(1) model is:\n\\(\\sigma_{t}^{2}=\\alpha_{0}+\\alpha_{1}\\epsilon_{t-1}^{2}\\)\nwhere \\(\\sigma^2_t\\) is the conditional variance at time \\(t\\), \\(\\alpha_0\\) is a constant, \\(\\alpha_1\\) and \\(\\beta_1\\) are the parameters, and \\(\\epsilon_t\\) is the error term.\nThe combined equation of the ARIMAX(1,1,1)+GARCH(1,1) model is:\n\\(Y(t) = Y(t-1) + \\epsilon(t)\\)\n\\(\\sigma_{t}^{2}=\\alpha_{0}+\\alpha_{1}\\epsilon_{t-1}^{2}\\)"
  },
  {
    "objectID": "arch_Energy_Sector_Fund.html",
    "href": "arch_Energy_Sector_Fund.html",
    "title": "",
    "section": "",
    "text": "Impact of Macroeconomic Factors on ExxonMobil Stock Price\n\n\n\nOne of the top companies in the energy sector is ExxonMobil Corporation (XOM), which is one of the largest publicly traded international oil and gas companies in the world. ExxonMobil engages in exploration, production, transportation, and sale of crude oil, natural gas, and petroleum products. With operations in over 70 countries, ExxonMobil has a significant global presence and is a major player in the energy industry. As of 2021, ExxonMobil also has investments in alternative energy technologies such as biofuels and carbon capture and storage.\nBy using an ARIMAX+ARCH/GARCH model to analyze the stock price behavior of ExxonMobil, we can gain insights into how macroeconomic factors impact the company’s performance. For example, an increase in GDP growth rate or a decrease in unemployment rate may lead to increased consumer spending and a rise in ExxonMobil’s stock price. Conversely, an increase in inflation or interest rates may lead to a decrease in consumer spending and a decline in ExxonMobil’s stock price.\n\n\nTime series Plot\n\nExxonMobilDifferentitaed Chart SeriesGDP GrowthInflationInterestUnemployment\n\n\n\n\nCode\n# get data\noptions(\"getSymbols.warning4.0\"=FALSE)\noptions(\"getSymbols.yahoo.warning\"=FALSE)\n\n\ndata.info = getSymbols(\"XOM\",src='yahoo', from = '2010-01-01',to = \"2023-03-01\",auto.assign = FALSE)\ndata = getSymbols(\"XOM\",src='yahoo', from = '2010-01-01',to = \"2023-03-01\")\ndf <- data.frame(Date=index(XOM),coredata(XOM))\n\n# create Bollinger Bands\nbbands <- BBands(XOM[,c(\"XOM.High\",\"XOM.Low\",\"XOM.Close\")])\n\n# join and subset data\ndf <- subset(cbind(df, data.frame(bbands[,1:3])), Date >= \"2010-01-01\")\n\n\n# colors column for increasing and decreasing\nfor (i in 1:length(df[,1])) {\n  if (df$XOM.Close[i] >= df$XOM.Open[i]) {\n      df$direction[i] = 'Increasing'\n  } else {\n      df$direction[i] = 'Decreasing'\n  }\n}\n\ni <- list(line = list(color = '#F9E79F'))\nd <- list(line = list(color = '#7F7F7F'))\n\n# plot candlestick chart\n\nfig <- df %>% plot_ly(x = ~Date, type=\"candlestick\",\n          open = ~XOM.Open, close = ~XOM.Close,\n          high = ~XOM.High, low = ~XOM.Low, name = \"XOM\",\n          increasing = i, decreasing = d) \nfig <- fig %>% add_lines(x = ~Date, y = ~up , name = \"B Bands\",\n            line = list(color = '#ccc', width = 0.5),\n            legendgroup = \"Bollinger Bands\",\n            hoverinfo = \"none\", inherit = F) \nfig <- fig %>% add_lines(x = ~Date, y = ~dn, name = \"B Bands\",\n            line = list(color = '#ccc', width = 0.5),\n            legendgroup = \"Bollinger Bands\", inherit = F,\n            showlegend = FALSE, hoverinfo = \"none\") \nfig <- fig %>% add_lines(x = ~Date, y = ~mavg, name = \"Mv Avg\",\n            line = list(color = '#C052B3', width = 0.5),\n            hoverinfo = \"none\", inherit = F) \nfig <- fig %>% layout(yaxis = list(title = \"Price\"))\n\n# plot volume bar chart\nfig2 <- df \nfig2 <- fig2 %>% plot_ly(x=~Date, y=~XOM.Volume, type='bar', name = \"XOM Volume\",\n          color = ~direction, colors = c('#F9E79F','#7F7F7F')) \nfig2 <- fig2 %>% layout(yaxis = list(title = \"Volume\"))\n\n# create rangeselector buttons\nrs <- list(visible = TRUE, x = 0.5, y = -0.055,\n           xanchor = 'center', yref = 'paper',\n           font = list(size = 9),\n           buttons = list(\n             list(count=1,\n                  label='RESET',\n                  step='all'),\n             list(count=3,\n                  label='3 YR',\n                  step='year',\n                  stepmode='backward'),\n             list(count=1,\n                  label='1 YR',\n                  step='year',\n                  stepmode='backward'),\n             list(count=1,\n                  label='1 MO',\n                  step='month',\n                  stepmode='backward')\n           ))\n\n# subplot with shared x axis\nfig <- subplot(fig, fig2, heights = c(0.7,0.2), nrows=2,\n             shareX = TRUE, titleY = TRUE)\nfig <- fig %>% layout(title = paste(\"ExxonMobil Stock Price: January 2010 - March 2023\"),\n         xaxis = list(rangeselector = rs),\n         legend = list(orientation = 'h', x = 0.5, y = 1,\n                       xanchor = 'center', yref = 'paper',\n                       font = list(size = 10),\n                       bgcolor = 'transparent'))\n\nfig\n\n\n\n\n\n\n\n\n\n\nCode\nlog(data.info$`XOM.Adjusted`) %>% diff() %>% chartSeries(theme=chartTheme('white'),up.col='#F9E79F')\n\n\n\n\n\n\n\n\n\nCode\n#import the data\ngdp <- read.csv(\"DATA/RAW DATA/gdp-growth.csv\")\n\n#change date format\ngdp$Date <- as.Date(gdp$DATE , \"%m/%d/%Y\")\n\n#drop DATE column\ngdp <- subset(gdp, select = -c(1))\n\n#export the cleaned data\ngdp_clean <- gdp\nwrite.csv(gdp_clean, \"DATA/CLEANED DATA/gdp_clean_data.csv\", row.names=FALSE)\n\n#plot gdp growth rate \nfig <- plot_ly(gdp, x = ~Date, y = ~value, type = 'scatter', mode = 'lines',line = list(color = 'rgb(240, 128, 128)'))\nfig <- fig %>% layout(title = \"U.S GPD Growth Rate: 2010 - 2022\",xaxis = list(title = \"Time\"),yaxis = list(title =\"GDP Growth Rate\"))\nfig\n\n\n\n\n\n\n\n\n\n\nCode\n#import the data\ninflation_rate <- read.csv(\"DATA/RAW DATA/inflation-rate.csv\")\n\n#cleaning the data\n#remove unwanted columns\ninflation_rate_clean <- subset(inflation_rate, select = -c(1,HALF1,HALF2))\n\n#convert the data to time series data\ninflation_data_ts <- ts(as.vector(t(as.matrix(inflation_rate_clean))), start=c(2010,1), end=c(2023,2), frequency=12)\n\n#export the data\nwrite.csv(inflation_rate_clean, \"DATA/CLEANED DATA/inflation_rate_clean_data.csv\", row.names=FALSE)\n\n\n#plot inflation rate \nfig <- autoplot(inflation_data_ts, ylab = \"Inflation Rate\", color=\"#FFA07A\")+ggtitle(\"U.S Inflation Rate: January 2010 - February 2023\")+theme_bw()\nggplotly(fig)\n\n\n\n\n\n\n\n\n\n\nCode\n#import the data\ninterest_data <- read.csv(\"DATA/RAW DATA/interest-rate.csv\")\n\n#change date format\ninterest_data$Date <- as.Date(interest_data$Date , \"%m/%d/%Y\")\n\n#export the cleaned data\ninterest_clean_data <- interest_data\nwrite.csv(interest_clean_data, \"DATA/CLEANED DATA/interest_rate_clean_data.csv\", row.names=FALSE)\n\n#plot interest rate \nfig <- plot_ly(interest_data, x = ~Date, y = ~value, type = 'scatter', mode = 'lines',line = list(color='rgb(219, 112, 147)'))\nfig <- fig %>% layout(title = \"U.S Interest Rate: January 2010 - March 2023\",xaxis = list(title = \"Time\"),yaxis = list(title =\"Interest Rate\"))\nfig\n\n\n\n\n\n\n\n\n\n\nCode\n#import the data\nunemployment_rate <- read.csv(\"DATA/RAW DATA/unemployment-rate.csv\")\n\n#change date format\nunemployment_rate$Date <- as.Date(unemployment_rate$Date , \"%m/%d/%Y\")\n\n# export the data\nwrite.csv(unemployment_rate, \"DATA/CLEANED DATA/unemployment_rate_clean_data.csv\", row.names=FALSE)\n\n#plot unemployment rate \nfig <- plot_ly(unemployment_rate, x = ~Date, y = ~Value, type = 'scatter', mode = 'lines',line = list(color = 'rgb(189, 183, 107)'))\nfig <- fig %>% layout(title = \"U.S Unemployment Rate: January 2010 - March 2023\",xaxis = list(title = \"Time\"),yaxis = list(title =\"Unemployment Rate\"))\nfig\n\n\n\n\n\n\n\n\n\nExxonMobil (XOM) is one of the largest integrated oil and gas companies in the world. From 2010 to 2023, XOM’s stock price experienced some significant fluctuations.\nIn the early part of the decade, XOM’s stock price steadily increased as the global demand for oil and gas remained strong. However, the company’s stock price began to decline in mid-2014, as the oversupply of oil and gas in the global market led to a drop in prices. This decline was further exacerbated by the COVID-19 pandemic in 2020, which caused a sharp drop in global demand for oil and gas.\nDespite these challenges, XOM’s stock price has shown signs of recovery in recent years, as the company has taken steps to reduce its costs and increase its focus on natural gas and chemicals. In addition, the company has made significant investments in renewable energy, which could help it to diversify its revenue streams in the long term.\nSince mid 2020, ExxonMobil’s stock price has experienced some volatility, likely due to a combination of factors such as global economic uncertainty and fluctuations in consumer demand for the company’s products.\nAs discussed before, the macroeconomic factors of GDP growth, inflation, interest rates, and unemployment rate are closely interrelated and play a crucial role in the overall health and stability of an economy. From 2010 to 2023, the global economy experienced a mix of ups and downs, with periods of strong GDP growth followed by slowdowns and recessions.\nThe second plot shows the first difference of the logarithm of the adjusted ExxonMobil stock price. Taking the first difference removes any long-term trends and transforms the time series into a stationary process. From the plot, we can observe that the first difference of the logarithm of the ExxonMobil stock price appears to be stationary, as the mean and variance are roughly constant over time.\n\n\nEnodogenous and Exogenous Variables\n\nPlotCorrelation HeatmapCCF GDPCCF InterestCCF InflationCCF Unemployment\n\n\n\n\nCode\nnumeric_data <- c(\"XOM.Adjusted\",\"gdp\", \"interest\", \"inflation\", \"unemployment\")\nnumeric_data <- final[, numeric_data]\nnormalized_data_numeric <- scale(numeric_data)\nnormalized_data <- ts(normalized_data_numeric, start = c(2010, 1), end = c(2021,10),frequency = 4)\nts_plot(normalized_data,\n        title = \"Normalized Time Series Data for XOM Stock and Macroeconomic Variables\",\n        Ytitle = \"Normalized Values\",\n        Xtitle = \"Year\")\n\n\n\n\n\n\n\n\n\n\nCode\n# Get upper triangle of the correlation matrix\nget_upper_tri <- function(cormat){\n    cormat[lower.tri(cormat)]<- NA\n    return(cormat)\n}\ncormat <- round(cor(normalized_data_numeric),2)\n\nupper_tri <- get_upper_tri(cormat)\n\nmelted_cormat <- melt(upper_tri, na.rm = TRUE)\n# Create a ggheatmap\nggheatmap <- ggplot(melted_cormat, aes(Var2, Var1, fill = value))+\n geom_tile(color = \"white\")+\n scale_fill_gradient2(low = \"blue\", high = \"red\", mid = \"white\", \n   midpoint = 0, limit = c(-1,1), space = \"Lab\", \n    name=\"Pearson\\nCorrelation\") +\n  theme_minimal()+ # minimal theme\n theme(axis.text.x = element_text(angle = 45, vjust = 1, \n    size = 12, hjust = 1))+\n coord_fixed()\n\nggheatmap + \ngeom_text(aes(Var2, Var1, label = value), color = \"black\", size = 4) +\ntheme(\n  axis.title.x = element_blank(),\n  axis.title.y = element_blank(),\n  panel.grid.major = element_blank(),\n  panel.border = element_blank(),\n  panel.background = element_blank(),\n  axis.ticks = element_blank(),\n  legend.justification = c(1, 0),\n  legend.position = c(0.6, 0.7),\n  legend.direction = \"horizontal\")+\n  guides(fill = guide_colorbar(barwidth = 7, barheight = 1,\n                title.position = \"top\", title.hjust = 0.5))\n\n\n\n\n\n\n\n\n\nCode\npar(mfrow=c(1,1))\nccf_result <- ccf(normalized_data[, c(\"XOM.Adjusted\")], normalized_data[, c(\"gdp\")], \n    lag.max = 300,\n    main = \"Cros-Correlation Plot for XOM Stock Price and GDP Growth Rate \",\n    ylab = \"CCF\")\n\n\n\n\n\nCode\ncat(\"The sum of cross correlation function is\", sum(abs(ccf_result$acf)))\n\n\nThe sum of cross correlation function is 8.584101\n\n\n\n\n\n\nCode\npar(mfrow=c(1,1))\nccf_result <- ccf(normalized_data[, c(\"XOM.Adjusted\")], normalized_data[, c(\"interest\")], \n    lag.max = 300,\n    main = \"Cros-Correlation Plot for XOM Stock Price and Interest Rate\",\n    ylab = \"CCF\")\n\n\n\n\n\nCode\ncat(\"The sum of cross correlation function is\", sum(abs(ccf_result$acf)))\n\n\nThe sum of cross correlation function is 13.78063\n\n\n\n\n\n\nCode\npar(mfrow=c(1,1))\nccf_result <- ccf(normalized_data[, c(\"XOM.Adjusted\")], normalized_data[, c(\"inflation\")], \n    lag.max = 300,\n    main = \"Cros-Correlation Plot for XOM Stock Price and Inflation Rate\",\n    ylab = \"CCF\")\n\n\n\n\n\nCode\ncat(\"The sum of cross correlation function is\", sum(abs(ccf_result$acf)))\n\n\nThe sum of cross correlation function is 13.02693\n\n\n\n\n\n\nCode\npar(mfrow=c(1,1))\nccf_result <- ccf(normalized_data[, c(\"XOM.Adjusted\")], normalized_data[, c(\"unemployment\")], \n    lag.max = 300,\n    main = \"Cros-Correlation Plot for XOM Stock Priceand Unemployment Rate\",\n    ylab = \"CCF\")\n\n\n\n\n\nCode\ncat(\"The sum of cross correlation function is\", sum(abs(ccf_result$acf)))\n\n\nThe sum of cross correlation function is 11.66149\n\n\n\n\n\nThe Normalized Time Series Data for Stock Price and Macroeconomic Variables plot shows the same variables as the first plot but has been normalized to a common range of 0 to 1 using the scale() function in R, which standardizes the variables to have a mean of 0 and a standard deviation of 1. The heatmap analysis of the normalized data reveals that inflation and unemployment rate exhibit strong positive correlations with the stock price indices, indicating that these variables may significantly influence stock price movements. On the other hand, weaker correlations were observed between the stock price indices and GDP and interest rates, suggesting that these variables may have less impact on stock price fluctuations. The cross-correlation feature plots confirm these findings, indicating that inflation and unemployment rate are more suitable feature variables for the ARIMAX model when predicting ExxonMobil movements.\nFinal Exogenous variables: Macroeconomic indicators: Inflation rate and unemployment rate.\n\n\nEnodogenous and Exogenous Variables Plot\n\nPlotCheck the stationarity\n\n\n\n\nCode\nfinal_data <- final %>%dplyr::select( Date,XOM.Adjusted, inflation,unemployment)\nnumeric_data <- c(\"XOM.Adjusted\", \"inflation\",\"unemployment\")\nnumeric_data <- final_data[, numeric_data]\nnormalized_data_numeric <- scale(numeric_data)\nnormalized_numeric_df <- data.frame(normalized_data_numeric)\nnormalized_data_ts <- ts(normalized_data_numeric, start = c(2010, 1), frequency = 4)\n\nautoplot(normalized_data_ts, facets=TRUE) +\n  xlab(\"Year\") + ylab(\"\") +\n  ggtitle(\"ExxonMobil Stock Price, Inflation Rate and Unemployment Rate in USA 2010-2023\")\n\n\n\n\n\n\n\n\n\nCode\n# Convert your multivariate time series data to a matrix\nfinal_data_ts_multivariate <- as.matrix(normalized_data_ts)\n\n# Check for stationarity using Phillips-Perron test\nphillips_perron_test <- ur.pp(final_data_ts_multivariate)  \nsummary(phillips_perron_test)\n\n\n\n################################## \n# Phillips-Perron Unit Root Test # \n################################## \n\nTest regression with intercept \n\n\nCall:\nlm(formula = y ~ y.l1)\n\nResiduals:\n    Min      1Q  Median      3Q     Max \n-2.6785 -0.2163 -0.0613  0.1641  4.4054 \n\nCoefficients:\n            Estimate Std. Error t value Pr(>|t|)    \n(Intercept) 0.003019   0.049779   0.061    0.952    \ny.l1        0.780682   0.050284  15.526   <2e-16 ***\n---\nSignif. codes:  0 '***' 0.001 '**' 0.01 '*' 0.05 '.' 0.1 ' ' 1\n\nResidual standard error: 0.6197 on 153 degrees of freedom\nMultiple R-squared:  0.6117,    Adjusted R-squared:  0.6092 \nF-statistic:   241 on 1 and 153 DF,  p-value: < 2.2e-16\n\n\nValue of test-statistic, type: Z-alpha  is: -35.294 \n\n         aux. Z statistics\nZ-tau-mu            0.0608\n\n\n\n\n\nThe results of the Phillips-Perron unit root test indicate strong evidence against the null hypothesis of a unit root, as the p-value for the coefficient of the lagged variable is less than the significance level of 0.05. This suggests that the variable y, which is being tested for stationarity, is likely stationary. Furthermore, the test statistic Z-tau-mu is 0.0608, which is smaller than the critical value of Z-alpha (-35.294), providing further evidence of stationarity.\nTo determine whether the linear model requires an ARCH model, an ARCH test is conducted. The ACF and PACF plots are also used to identify suitable model values.\n\n\nModel Fitting\n\nPlotARCH TestACF PlotPACF Plot\n\n\n\n\nCode\nnormalized_numeric_df$XOM.Adjusted<-ts(normalized_numeric_df$XOM.Adjusted,star=decimal_date(as.Date(\"2010-01-01\",format = \"%Y-%m-%d\")),frequency = 4)\nnormalized_numeric_df$inflation<-ts(normalized_numeric_df$inflation,star=decimal_date(as.Date(\"2010-01-01\",format = \"%Y-%m-%d\")),frequency = 4)\nnormalized_numeric_df$unemployment<-ts(normalized_numeric_df$unemployment,star=decimal_date(as.Date(\"2010-01-01\",format = \"%Y-%m-%d\")),frequency = 4)\n\nfit <- lm(XOM.Adjusted ~ inflation+unemployment, data=normalized_numeric_df)\nfit.res<-ts(residuals(fit),star=decimal_date(as.Date(\"2010-01-01\",format = \"%Y-%m-%d\")),frequency = 4)\n############## Then look at the residuals ############\nreturns <- fit.res  %>% diff()\nautoplot(returns)+ggtitle(\"Linear Model Returns\")\n\n\n\n\n\n\n\n\n\nCode\nbyd.archTest <- ArchTest(fit.res, lags = 1, demean = TRUE)\nbyd.archTest\n\n\n\n    ARCH LM-test; Null hypothesis: no ARCH effects\n\ndata:  fit.res\nChi-squared = 6.3634, df = 1, p-value = 0.01165\n\n\n\n\n\n\nCode\nggAcf(returns) +ggtitle(\"ACF for returns\")\n\n\n\n\n\n\n\n\n\nCode\nggPacf(returns) +ggtitle(\"PACF for returns\")\n\n\n\n\n\n\n\n\nThe ARCH LM-test was conducted with the null hypothesis of no ARCH effects. The test resulted in a chi-squared value of 36.896 with one degree of freedom, and a very low p-value of 0.01165. This suggests strong evidence against the null hypothesis, indicating the presence of ARCH effects in the data.\nBased on the ACF and PACF plots, it appears that there is some significant autocorrelation and partial autocorrelation at multiple lags, which suggests that an ARIMA model may not be sufficient to capture the time series behavior. Additionally, the values for p and q appear to be relatively high, with p = 0 and q = 0 being suggested by the plots.\n\n\nARIMAX Model\n\nAuto Arima ModelAuto Arima Residuals\n\n\n\n\nCode\nxreg <- cbind(Inflation = normalized_data_ts[, \"inflation\"],\n              Unemployment = normalized_data_ts[, \"unemployment\"])\nfit.auto <- auto.arima(normalized_data_ts[, \"XOM.Adjusted\"], xreg = xreg)\nsummary(fit.auto)\n\n\nSeries: normalized_data_ts[, \"XOM.Adjusted\"] \nRegression with ARIMA(0,1,0) errors \n\nCoefficients:\n      Inflation  Unemployment\n         0.4785       -0.3455\ns.e.     0.2235        0.1014\n\nsigma^2 = 0.2642:  log likelihood = -37.4\nAIC=80.8   AICc=81.32   BIC=86.6\n\nTraining set error measures:\n                     ME      RMSE       MAE      MPE     MAPE      MASE\nTraining set 0.02751442 0.4989418 0.3970164 40.31754 217.5685 0.4290685\n                   ACF1\nTraining set -0.1780103\n\n\n\n\n\n\nCode\ncheckresiduals(fit.auto)\n\n\n\n\n\n\n    Ljung-Box test\n\ndata:  Residuals from Regression with ARIMA(0,1,0) errors\nQ* = 5.972, df = 8, p-value = 0.6504\n\nModel df: 0.   Total lags used: 8\n\n\n\n\n\nBased on the results of the auto.arima function, the suggested best model is ARIMA(0,1,0) which is the same as the manual choosen arima model. So, we can then proceed to choose the best GARCH model using ARIMA(0,1,0) as the base model.\nWe can then proceed to choose the best GARCH model using ARIMA(0,1,0) as the base model.\n\n\nSquared Residuals\n\nPlotACF PlotPACF PlotGRACH(1,1)\n\n\n\n\nCode\nfit <- lm(XOM.Adjusted ~ inflation+unemployment, data=normalized_numeric_df)\nfit.res<-ts(residuals(fit),star=decimal_date(as.Date(\"2010-01-01\",format = \"%Y-%m-%d\")),frequency = 4)\nfit <- Arima(fit.res,order=c(0,1,0))\nres=fit$res\nplot(res^2,main='Squared Residuals')\n\n\n\n\n\n\n\n\n\nCode\nacf(res^2,24, main = \"ACF Residuals Square\")\n\n\n\n\n\n\n\n\n\nCode\npacf(res^2,24, main = \"PACF Residuals Square\")\n\n\n\n\n\n\n\n\n\nCode\nsummary(garchFit(~garch(1,1),res, trace=F))\n\n\n\nTitle:\n GARCH Modelling \n\nCall:\n garchFit(formula = ~garch(1, 1), data = res, trace = F) \n\nMean and Variance Equation:\n data ~ garch(1, 1)\n<environment: 0x129301df8>\n [data = res]\n\nConditional Distribution:\n norm \n\nCoefficient(s):\n       mu      omega     alpha1      beta1  \n0.0090955  0.0010266  0.0948428  0.9289551  \n\nStd. Errors:\n based on Hessian \n\nError Analysis:\n        Estimate  Std. Error  t value Pr(>|t|)    \nmu      0.009096    0.066163    0.137    0.891    \nomega   0.001027    0.025018    0.041    0.967    \nalpha1  0.094843    0.089034    1.065    0.287    \nbeta1   0.928955    0.128326    7.239 4.52e-13 ***\n---\nSignif. codes:  0 '***' 0.001 '**' 0.01 '*' 0.05 '.' 0.1 ' ' 1\n\nLog Likelihood:\n -38.10686    normalized:  -0.7328243 \n\nDescription:\n Sat Jan  6 19:52:51 2024 by user:  \n\n\nStandardised Residuals Tests:\n                                Statistic p-Value  \n Jarque-Bera Test   R    Chi^2  1.133832  0.5672723\n Shapiro-Wilk Test  R    W      0.9863451 0.8107207\n Ljung-Box Test     R    Q(10)  6.442407  0.7768264\n Ljung-Box Test     R    Q(15)  10.9601   0.7554174\n Ljung-Box Test     R    Q(20)  13.75256  0.842817 \n Ljung-Box Test     R^2  Q(10)  11.81937  0.2973276\n Ljung-Box Test     R^2  Q(15)  13.76547  0.5433859\n Ljung-Box Test     R^2  Q(20)  16.25398  0.7007478\n LM Arch Test       R    TR^2   13.2983   0.3477368\n\nInformation Criterion Statistics:\n     AIC      BIC      SIC     HQIC \n1.619495 1.769590 1.608749 1.677038 \n\n\n\n\n\nFrom the squared residuals of the best ARIMA model, it can be observed that the ACF plot and PACF plot indicate that the residuals are not autocorrelated and are white noise, indicating a good fit of the model. Based on the squared residuals of the best ARIMA model, we can see that the ACF and PACF plots indicate that most of the values lie between the blue lines. Additionally, the p-value is 1 and q-value is 1. This suggests that the model has a good fit and that there is no significant autocorrelation or partial autocorrelation in the residuals.\nThe bst model is ARIMA(0,1,0) and GARCH(1,1). #### Best Model\n\nARIMA ModelGARCH ModelVolatility\n\n\n\n\nCode\n#fiting an ARIMA model to the Inflation variable\ninflation_fit<-auto.arima(normalized_numeric_df$inflation) \nfinflation<-forecast(inflation_fit)\n\n#fitting an ARIMA model to the Unemployment variable\nunemployment_fit<-auto.arima(normalized_numeric_df$unemployment) \nfunemployment<-forecast(unemployment_fit)\n\n# best model fit for forcasting\nxreg <- cbind(Inflation = normalized_data_ts[, \"inflation\"],\n              Unemployment = normalized_data_ts[, \"unemployment\"])\n\n\n\nsummary(arima.fit<-Arima(normalized_data_ts[, \"XOM.Adjusted\"],order=c(0,1,0),xreg=xreg),include.drift = TRUE)\n\n\nSeries: normalized_data_ts[, \"XOM.Adjusted\"] \nRegression with ARIMA(0,1,0) errors \n\nCoefficients:\n      Inflation  Unemployment\n         0.4785       -0.3455\ns.e.     0.2235        0.1014\n\nsigma^2 = 0.2642:  log likelihood = -37.4\nAIC=80.8   AICc=81.32   BIC=86.6\n\nTraining set error measures:\n                     ME      RMSE       MAE      MPE     MAPE      MASE\nTraining set 0.02751442 0.4989418 0.3970164 40.31754 217.5685 0.4290685\n                   ACF1\nTraining set -0.1780103\n\n\n\n\n\n\nCode\nsummary(final.fit <- garchFit(~garch(1,1), res,trace = F))\n\n\n\nTitle:\n GARCH Modelling \n\nCall:\n garchFit(formula = ~garch(1, 1), data = res, trace = F) \n\nMean and Variance Equation:\n data ~ garch(1, 1)\n<environment: 0x1383385d0>\n [data = res]\n\nConditional Distribution:\n norm \n\nCoefficient(s):\n       mu      omega     alpha1      beta1  \n0.0090955  0.0010266  0.0948428  0.9289551  \n\nStd. Errors:\n based on Hessian \n\nError Analysis:\n        Estimate  Std. Error  t value Pr(>|t|)    \nmu      0.009096    0.066163    0.137    0.891    \nomega   0.001027    0.025018    0.041    0.967    \nalpha1  0.094843    0.089034    1.065    0.287    \nbeta1   0.928955    0.128326    7.239 4.52e-13 ***\n---\nSignif. codes:  0 '***' 0.001 '**' 0.01 '*' 0.05 '.' 0.1 ' ' 1\n\nLog Likelihood:\n -38.10686    normalized:  -0.7328243 \n\nDescription:\n Sat Jan  6 19:52:52 2024 by user:  \n\n\nStandardised Residuals Tests:\n                                Statistic p-Value  \n Jarque-Bera Test   R    Chi^2  1.133832  0.5672723\n Shapiro-Wilk Test  R    W      0.9863451 0.8107207\n Ljung-Box Test     R    Q(10)  6.442407  0.7768264\n Ljung-Box Test     R    Q(15)  10.9601   0.7554174\n Ljung-Box Test     R    Q(20)  13.75256  0.842817 \n Ljung-Box Test     R^2  Q(10)  11.81937  0.2973276\n Ljung-Box Test     R^2  Q(15)  13.76547  0.5433859\n Ljung-Box Test     R^2  Q(20)  16.25398  0.7007478\n LM Arch Test       R    TR^2   13.2983   0.3477368\n\nInformation Criterion Statistics:\n     AIC      BIC      SIC     HQIC \n1.619495 1.769590 1.608749 1.677038 \n\n\n\n\n\n\nCode\nht <- final.fit@h.t #a numeric vector with the conditional variances (h.t = sigma.t^delta)\n\n#############################\ndata=data.frame(final)\ndata$Date<-as.Date(data$Date,\"%Y-%m-%d\")\n\n\ndata2= data.frame(ht,data$Date)\nggplot(data2, aes(y = ht, x = data.Date)) + geom_line(col = '#F9E79F') + ylab('Conditional Variance') + xlab('Date')\n\n\n\n\n\n\n\n\nFrom the ARIMA(0,1,0), we see that the training set error measures also suggest a good fit, with low mean absolute error, root mean squared error, and autocorrelation of the residuals. GATCH(1,1) model model is used to estimate the volatility of the standardized residuals of the previous regression model. The model includes a mean equation that estimates the mean of the residuals and a variance equation that models the conditional variance of the residuals. The coefficients of the mean equation suggest that the mean of the residuals is close to zero. The variance equation coefficients suggest that the conditional variance of the residuals is dependent on the past conditional variances and the past squared standardized residuals. The model’s the AIC, BIC, SIC, and HQIC values are all relatively low, indicating a good fit of the model. The standardized residuals tests indicate that the residuals are approximately normally distributed and that there is no significant autocorrelation in the residuals.\nThe volatility of the model seems high in 2020 but has decreased gradually in the past few months. This could indicate that the asset’s price was experiencing a lot of fluctuations in 2020, but the market has stabilized recently.\n\n\nModel Diagnostics\n\nResidualsQQ PlotBox Test\n\n\n\n\nCode\nfit2<-garch(res,order=c(1,1),trace=F)\ncheckresiduals(fit2) \n\n\nWarning in modeldf.default(object): Could not find appropriate degrees of\nfreedom for this model.\n\n\n\n\n\n\n\n\n\nCode\nqqnorm(fit2$residuals, pch = 1)\nqqline(fit2$residuals, col = \"blue\", lwd = 2)\n\n\n\n\n\n\n\n\n\nCode\nBox.test (fit2$residuals, type = \"Ljung\")\n\n\n\n    Box-Ljung test\n\ndata:  fit2$residuals\nX-squared = 0.88262, df = 1, p-value = 0.3475\n\n\nThe ACF plot of the residuals shows all the values between the blue lines, which indicates that the residuals are not significantly autocorrelated. The range of values for the residual plot between -2 and 2 is considered acceptable. Additionally, the QQ plot of the residuals shows a linear plot on the line, which is another good indication that the residuals are normally distributed. The QQ plot is a valuable tool to assess if the residuals follow a normal distribution, and in this case, the plot suggests that the residuals do indeed follow a normal distribution.\nThe Box-Ljung test, a p-value of 0.3475 indicates that the model’s residuals are not significantly autocorrelated, meaning that the model has captured most of the information in the data. This result is good because it suggests that the model is a good fit for the data and has accounted for most of the underlying patterns in the data. Therefore, we can rely on the model’s predictions and use them to make informed decisions.\n\n\n\n\n\nForecast\n\n\nCode\npredict(final.fit, n.ahead = 5, plot=TRUE)\n\n\n\n\n\n  meanForecast meanError standardDeviation lowerInterval upperInterval\n1  0.009095548 0.7486587         0.7486587     -1.458249      1.476440\n2  0.009095548 0.7581919         0.7581919     -1.476933      1.495124\n3  0.009095548 0.7678294         0.7678294     -1.495822      1.514014\n4  0.009095548 0.7775725         0.7775725     -1.514919      1.533110\n5  0.009095548 0.7874226         0.7874226     -1.534224      1.552416\n\n\nThe forecasted plot is based on the best model ARIMAX(0,1,0)+GARCH(1,1). This model takes into account the autoregressive and moving average components of the data, as well as the impact off exogenous variables on the time series. Additionally, the GARCH component of the model accounts for the volatility clustering in the data. Overall, this model is well-suited to make accurate predictions about future values of the time series.\n\n\nEquation of the Model\nThe equation of the ARIMAX(0,1,0) model is:\n\\(Y(t) = Y(t-1) + \\epsilon(t)\\)\nwhere \\(Y(t)\\) is the time series variable and \\(\\epsilon(t)\\) is the error term.\nThe equation of the GARCH(1,1) model is:\n\\(\\sigma^2(t) = \\alpha_0+\\alpha_1\\epsilon_t^2(t-1)+ \\beta_1\\sigma^2(t-1)\\)\nwhere \\(\\sigma^2_t\\) is the conditional variance at time \\(t\\), \\(\\alpha_0\\) is a constant, \\(\\alpha_1\\) and \\(\\beta_1\\) are the parameters, and \\(\\epsilon_t\\) is the error term.\nThe combined equation of the ARIMAX(0,1,0)+GARCH(1,1) model is:\n\\(Y(t) = Y(t-1) + \\epsilon(t)\\)\n\\(\\epsilon(t) = \\sigma(t) * \\epsilon~(t)\\)\n\\(\\sigma^2(t) = \\alpha_0+\\alpha_1\\epsilon_t^2(t-1)+ \\beta_1\\sigma^2(t-1)\\)"
  },
  {
    "objectID": "arch_Financial_Sector_Fund.html",
    "href": "arch_Financial_Sector_Fund.html",
    "title": "",
    "section": "",
    "text": "Impact of Macroeconomic Factors on JPMorgan Chase & Co. Stock Price\n\n\n\nJPMorgan Chase & Co. is a multinational investment bank and financial services company that provides a range of services such as commercial banking, investment banking, asset management, and private banking.\nTo analyze the stock price behavior of JPMorgan Chase & Co., an ARIMAX+ARCH/GARCH model can also be employed. This type of model can help to identify how macroeconomic factors such as interest rates, inflation, and GDP growth rate may affect the company’s performance.\n\n\nTime series Plot\n\nJPMorgan Chase & Co.Differentitaed Chart SeriesGDP GrowthInflationInterestUnemployment\n\n\n\n\nCode\n# get data\noptions(\"getSymbols.warning4.0\"=FALSE)\noptions(\"getSymbols.yahoo.warning\"=FALSE)\n\n\ndata.info = getSymbols(\"JPM\",src='yahoo', from = '2010-01-01',to = \"2023-03-01\",auto.assign = FALSE)\ndata = getSymbols(\"JPM\",src='yahoo', from = '2010-01-01',to = \"2023-03-01\")\ndf <- data.frame(Date=index(JPM),coredata(JPM))\n\n# create Bollinger Bands\nbbands <- BBands(JPM[,c(\"JPM.High\",\"JPM.Low\",\"JPM.Close\")])\n\n# join and subset data\ndf <- subset(cbind(df, data.frame(bbands[,1:3])), Date >= \"2010-01-01\")\n\n\n# colors column for increasing and decreasing\nfor (i in 1:length(df[,1])) {\n  if (df$JPM.Close[i] >= df$JPM.Open[i]) {\n      df$direction[i] = 'Increasing'\n  } else {\n      df$direction[i] = 'Decreasing'\n  }\n}\n\ni <- list(line = list(color = '#F1948A'))\nd <- list(line = list(color = '#7F7F7F'))\n\n# plot candlestick chart\n\nfig <- df %>% plot_ly(x = ~Date, type=\"candlestick\",\n          open = ~JPM.Open, close = ~JPM.Close,\n          high = ~JPM.High, low = ~JPM.Low, name = \"JPM\",\n          increasing = i, decreasing = d) \nfig <- fig %>% add_lines(x = ~Date, y = ~up , name = \"B Bands\",\n            line = list(color = '#ccc', width = 0.5),\n            legendgroup = \"Bollinger Bands\",\n            hoverinfo = \"none\", inherit = F) \nfig <- fig %>% add_lines(x = ~Date, y = ~dn, name = \"B Bands\",\n            line = list(color = '#ccc', width = 0.5),\n            legendgroup = \"Bollinger Bands\", inherit = F,\n            showlegend = FALSE, hoverinfo = \"none\") \nfig <- fig %>% add_lines(x = ~Date, y = ~mavg, name = \"Mv Avg\",\n            line = list(color = '#C052B3', width = 0.5),\n            hoverinfo = \"none\", inherit = F) \nfig <- fig %>% layout(yaxis = list(title = \"Price\"))\n\n# plot volume bar chart\nfig2 <- df \nfig2 <- fig2 %>% plot_ly(x=~Date, y=~JPM.Volume, type='bar', name = \"JPM Volume\",\n          color = ~direction, colors = c('#F1948A','#7F7F7F')) \nfig2 <- fig2 %>% layout(yaxis = list(title = \"Volume\"))\n\n# create rangeselector buttons\nrs <- list(visible = TRUE, x = 0.5, y = -0.055,\n           xanchor = 'center', yref = 'paper',\n           font = list(size = 9),\n           buttons = list(\n             list(count=1,\n                  label='RESET',\n                  step='all'),\n             list(count=3,\n                  label='3 YR',\n                  step='year',\n                  stepmode='backward'),\n             list(count=1,\n                  label='1 YR',\n                  step='year',\n                  stepmode='backward'),\n             list(count=1,\n                  label='1 MO',\n                  step='month',\n                  stepmode='backward')\n           ))\n\n# subplot with shared x axis\nfig <- subplot(fig, fig2, heights = c(0.7,0.2), nrows=2,\n             shareX = TRUE, titleY = TRUE)\nfig <- fig %>% layout(title = paste(\"JPMorgan Chase & Co.  Stock Price: January 2010 - March 2023\"),\n         xaxis = list(rangeselector = rs),\n         legend = list(orientation = 'h', x = 0.5, y = 1,\n                       xanchor = 'center', yref = 'paper',\n                       font = list(size = 10),\n                       bgcolor = 'transparent'))\n\nfig\n\n\n\n\n\n\n\n\n\n\nCode\nlog(data.info$`JPM.Adjusted`) %>% diff() %>% chartSeries(theme=chartTheme('white'),up.col='#F1948A')\n\n\n\n\n\n\n\n\n\nCode\n#import the data\ngdp <- read.csv(\"DATA/RAW DATA/gdp-growth.csv\")\n\n#change date format\ngdp$Date <- as.Date(gdp$DATE , \"%m/%d/%Y\")\n\n#drop DATE column\ngdp <- subset(gdp, select = -c(1))\n\n#export the cleaned data\ngdp_clean <- gdp\nwrite.csv(gdp_clean, \"DATA/CLEANED DATA/gdp_clean_data.csv\", row.names=FALSE)\n\n#plot gdp growth rate \nfig <- plot_ly(gdp, x = ~Date, y = ~value, type = 'scatter', mode = 'lines',line = list(color = 'rgb(240, 128, 128)'))\nfig <- fig %>% layout(title = \"U.S GPD Growth Rate: 2010 - 2022\",xaxis = list(title = \"Time\"),yaxis = list(title =\"GDP Growth Rate\"))\nfig\n\n\n\n\n\n\n\n\n\n\nCode\n#import the data\ninflation_rate <- read.csv(\"DATA/RAW DATA/inflation-rate.csv\")\n\n#cleaning the data\n#remove unwanted columns\ninflation_rate_clean <- subset(inflation_rate, select = -c(1,HALF1,HALF2))\n\n#convert the data to time series data\ninflation_data_ts <- ts(as.vector(t(as.matrix(inflation_rate_clean))), start=c(2010,1), end=c(2023,2), frequency=12)\n\n#export the data\nwrite.csv(inflation_rate_clean, \"DATA/CLEANED DATA/inflation_rate_clean_data.csv\", row.names=FALSE)\n\n\n#plot inflation rate \nfig <- autoplot(inflation_data_ts, ylab = \"Inflation Rate\", color=\"#FFA07A\")+ggtitle(\"U.S Inflation Rate: January 2010 - February 2023\")+theme_bw()\nggplotly(fig)\n\n\n\n\n\n\n\n\n\n\nCode\n#import the data\ninterest_data <- read.csv(\"DATA/RAW DATA/interest-rate.csv\")\n\n#change date format\ninterest_data$Date <- as.Date(interest_data$Date , \"%m/%d/%Y\")\n\n#export the cleaned data\ninterest_clean_data <- interest_data\nwrite.csv(interest_clean_data, \"DATA/CLEANED DATA/interest_rate_clean_data.csv\", row.names=FALSE)\n\n#plot interest rate \nfig <- plot_ly(interest_data, x = ~Date, y = ~value, type = 'scatter', mode = 'lines',line = list(color='rgb(219, 112, 147)'))\nfig <- fig %>% layout(title = \"U.S Interest Rate: January 2010 - March 2023\",xaxis = list(title = \"Time\"),yaxis = list(title =\"Interest Rate\"))\nfig\n\n\n\n\n\n\n\n\n\n\nCode\n#import the data\nunemployment_rate <- read.csv(\"DATA/RAW DATA/unemployment-rate.csv\")\n\n#change date format\nunemployment_rate$Date <- as.Date(unemployment_rate$Date , \"%m/%d/%Y\")\n\n# export the data\nwrite.csv(unemployment_rate, \"DATA/CLEANED DATA/unemployment_rate_clean_data.csv\", row.names=FALSE)\n\n#plot unemployment rate \nfig <- plot_ly(unemployment_rate, x = ~Date, y = ~Value, type = 'scatter', mode = 'lines',line = list(color = 'rgb(189, 183, 107)'))\nfig <- fig %>% layout(title = \"U.S Unemployment Rate: January 2010 - March 2023\",xaxis = list(title = \"Time\"),yaxis = list(title =\"Unemployment Rate\"))\nfig\n\n\n\n\n\n\n\n\n\nOver the years, JPMorgan Chase & Co. has weathered various economic challenges, including the global financial crisis of 2008. Since then, the company has implemented various measures to strengthen its balance sheet and improve its risk management practices. As a result, JPMorgan Chase & Co. has been able to maintain its position as one of the most financially sound banks in the industry.\nFrom 2010 to 2014, JPMorgan Chase & Co.’s stock price steadily increased, reflecting the company’s strong financial performance and consistent dividend payouts. However, from 2014 to 2016, the company’s stock price experienced a decline, which could be attributed to a combination of factors such as slowing sales growth and increased competition in the consumer goods industry.\nFrom 2016 to 2018, JPMorgan Chase & Co.’s stock price began to recover, likely due to the company’s efforts to streamline its operations and focus on core brands. This trend continued into 2019, with JPMorgan Chase & Co.’s stock price reaching an all-time high in mid-2019.\nThe outbreak of the COVID-19 pandemic in early 2020 caused a brief dip in JPMorgan Chase & Co.’s stock price, as investors were uncertain about the impact of the pandemic on the company’s operations and financial performance. However, the company’s strong position in the banking industry and its ability to adapt to changing market conditions helped it to quickly rebound and continue its growth trend throughout 2020 and into early 2021.\nSince early 2021, JPMorgan Chase & Co.’s stock price has experienced some volatility, likely due to a combination of factors such as global economic uncertainty and fluctuations in consumer demand for the company’s products and services.\nAs discussed before, the macroeconomic factors of GDP growth, inflation, interest rates, and unemployment rate are closely interrelated and play a crucial role in the overall health and stability of an economy. From 2010 to 2023, the global economy experienced a mix of ups and downs, with periods of strong GDP growth followed by slowdowns and recessions.\nThe second plot shows the first difference of the logarithm of the adjusted JPMorgan Chase & Co. stock price. Taking the first difference removes any long-term trends and transforms the time series into a stationary process. From the plot, we can observe that the first difference of the logarithm of the JPMorgan Chase & Co. stock price appears to be stationary, as the mean and variance are roughly constant over time.\n\n\nEnodogenous and Exogenous Variables\n\nPlotCorrelation HeatmapCCF GDPCCF InterestCCF InflationCCF Unemployment\n\n\n\n\nCode\nnumeric_data <- c(\"JPM.Adjusted\",\"gdp\", \"interest\", \"inflation\", \"unemployment\")\nnumeric_data <- final[, numeric_data]\nnormalized_data_numeric <- scale(numeric_data)\nnormalized_data <- ts(normalized_data_numeric, start = c(2010, 1), end = c(2021,10),frequency = 4)\nts_plot(normalized_data,\n        title = \"Normalized Time Series Data for JPM Stock and Macroeconomic Variables\",\n        Ytitle = \"Normalized Values\",\n        Xtitle = \"Year\")\n\n\n\n\n\n\n\n\n\n\nCode\n# Get upper triangle of the correlation matrix\nget_upper_tri <- function(cormat){\n    cormat[lower.tri(cormat)]<- NA\n    return(cormat)\n}\ncormat <- round(cor(normalized_data_numeric),2)\n\nupper_tri <- get_upper_tri(cormat)\n\nmelted_cormat <- melt(upper_tri, na.rm = TRUE)\n# Create a ggheatmap\nggheatmap <- ggplot(melted_cormat, aes(Var2, Var1, fill = value))+\n geom_tile(color = \"white\")+\n scale_fill_gradient2(low = \"blue\", high = \"red\", mid = \"white\", \n   midpoint = 0, limit = c(-1,1), space = \"Lab\", \n    name=\"Pearson\\nCorrelation\") +\n  theme_minimal()+ # minimal theme\n theme(axis.text.x = element_text(angle = 45, vjust = 1, \n    size = 12, hjust = 1))+\n coord_fixed()\n\nggheatmap + \ngeom_text(aes(Var2, Var1, label = value), color = \"black\", size = 4) +\ntheme(\n  axis.title.x = element_blank(),\n  axis.title.y = element_blank(),\n  panel.grid.major = element_blank(),\n  panel.border = element_blank(),\n  panel.background = element_blank(),\n  axis.ticks = element_blank(),\n  legend.justification = c(1, 0),\n  legend.position = c(0.6, 0.7),\n  legend.direction = \"horizontal\")+\n  guides(fill = guide_colorbar(barwidth = 7, barheight = 1,\n                title.position = \"top\", title.hjust = 0.5))\n\n\n\n\n\n\n\n\n\nCode\npar(mfrow=c(1,1))\nccf_result <- ccf(normalized_data[, c(\"JPM.Adjusted\")], normalized_data[, c(\"gdp\")], \n    lag.max = 300,\n    main = \"Cros-Correlation Plot for JPM Stock Price and GDP Growth Rate \",\n    ylab = \"CCF\")\n\n\n\n\n\nCode\ncat(\"The sum of cross correlation function is\", sum(abs(ccf_result$acf)))\n\n\nThe sum of cross correlation function is 5.286491\n\n\n\n\n\n\nCode\npar(mfrow=c(1,1))\nccf_result <- ccf(normalized_data[, c(\"JPM.Adjusted\")], normalized_data[, c(\"interest\")], \n    lag.max = 300,\n    main = \"Cros-Correlation Plot for JPM Stock Price and Interest Rate\",\n    ylab = \"CCF\")\n\n\n\n\n\nCode\ncat(\"The sum of cross correlation function is\", sum(abs(ccf_result$acf)))\n\n\nThe sum of cross correlation function is 10.89197\n\n\n\n\n\n\nCode\npar(mfrow=c(1,1))\nccf_result <- ccf(normalized_data[, c(\"JPM.Adjusted\")], normalized_data[, c(\"inflation\")], \n    lag.max = 300,\n    main = \"Cros-Correlation Plot for JPM Stock Price and Inflation Rate\",\n    ylab = \"CCF\")\n\n\n\n\n\nCode\ncat(\"The sum of cross correlation function is\", sum(abs(ccf_result$acf)))\n\n\nThe sum of cross correlation function is 17.12742\n\n\n\n\n\n\nCode\npar(mfrow=c(1,1))\nccf_result <- ccf(normalized_data[, c(\"JPM.Adjusted\")], normalized_data[, c(\"unemployment\")], \n    lag.max = 300,\n    main = \"Cros-Correlation Plot for JPM Stock Priceand Unemployment Rate\",\n    ylab = \"CCF\")\n\n\n\n\n\nCode\ncat(\"The sum of cross correlation function is\", sum(abs(ccf_result$acf)))\n\n\nThe sum of cross correlation function is 20.36533\n\n\n\n\n\nThe Normalized Time Series Data for Stock Price and Macroeconomic Variables plot shows the same variables as the first plot but has been normalized to a common range of 0 to 1 using the scale() function in R, which standardizes the variables to have a mean of 0 and a standard deviation of 1. The heatmap analysis of the normalized data reveals that inflation and unemployment rate exhibit strong positive correlations with the stock price indices, indicating that these variables may significantly influence stock price movements. On the other hand, weaker correlations were observed between the stock price indices and GDP and interest rates, suggesting that these variables may have less impact on stock price fluctuations. The cross-correlation feature plots confirm these findings, indicating that inflation and unemployment rate are more suitable feature variables for the ARIMAX model when predicting JPMorgan Chase & Co. movements.\nFinal Exogenous variables: Macroeconomic indicators: Inflation rate and unemployment rate.\n\n\nEnodogenous and Exogenous Variables Plot\n\nPlotCheck the stationarity\n\n\n\n\nCode\nfinal_data <- final %>%dplyr::select( Date,JPM.Adjusted, inflation,unemployment)\nnumeric_data <- c(\"JPM.Adjusted\", \"inflation\",\"unemployment\")\nnumeric_data <- final_data[, numeric_data]\nnormalized_data_numeric <- scale(numeric_data)\nnormalized_numeric_df <- data.frame(normalized_data_numeric)\nnormalized_data_ts <- ts(normalized_data_numeric, start = c(2010, 1), frequency = 4)\n\nautoplot(normalized_data_ts, facets=TRUE) +\n  xlab(\"Year\") + ylab(\"\") +\n  ggtitle(\"JPMorgan Chase & Co.  Stock Price, Inflation Rate and Unemployment Rate in USA 2010-2023\")\n\n\n\n\n\n\n\n\n\nCode\n# Convert your multivariate time series data to a matrix\nfinal_data_ts_multivariate <- as.matrix(normalized_data_ts)\n\n# Check for stationarity using Phillips-Perron test\nphillips_perron_test <- ur.pp(final_data_ts_multivariate)  \nsummary(phillips_perron_test)\n\n\n\n################################## \n# Phillips-Perron Unit Root Test # \n################################## \n\nTest regression with intercept \n\n\nCall:\nlm(formula = y ~ y.l1)\n\nResiduals:\n    Min      1Q  Median      3Q     Max \n-1.3694 -0.1559 -0.0528  0.0968  4.4945 \n\nCoefficients:\n             Estimate Std. Error t value Pr(>|t|)    \n(Intercept) 0.0007685  0.0411891   0.019    0.985    \ny.l1        0.8576417  0.0416064  20.613   <2e-16 ***\n---\nSignif. codes:  0 '***' 0.001 '**' 0.01 '*' 0.05 '.' 0.1 ' ' 1\n\nResidual standard error: 0.5128 on 153 degrees of freedom\nMultiple R-squared:  0.7353,    Adjusted R-squared:  0.7335 \nF-statistic: 424.9 on 1 and 153 DF,  p-value: < 2.2e-16\n\n\nValue of test-statistic, type: Z-alpha  is: -23.1811 \n\n         aux. Z statistics\nZ-tau-mu            0.0193\n\n\n\n\n\nThe results of the Phillips-Perron unit root test indicate strong evidence against the null hypothesis of a unit root, as the p-value for the coefficient of the lagged variable is less than the significance level of 0.05. This suggests that the variable y, which is being tested for stationarity, is likely stationary. Furthermore, the test statistic Z-tau-mu is 0.0193, which is smaller than the critical value of Z-alpha (-23.1811), providing further evidence of stationarity.\nTo determine whether the linear model requires an ARCH model, an ARCH test is conducted. The ACF and PACF plots are also used to identify suitable model values.\n\n\nModel Fitting\n\nPlotARCH TestACF PlotPACF Plot\n\n\n\n\nCode\nnormalized_numeric_df$JPM.Adjusted<-ts(normalized_numeric_df$JPM.Adjusted,star=decimal_date(as.Date(\"2010-01-01\",format = \"%Y-%m-%d\")),frequency = 4)\nnormalized_numeric_df$inflation<-ts(normalized_numeric_df$inflation,star=decimal_date(as.Date(\"2010-01-01\",format = \"%Y-%m-%d\")),frequency = 4)\nnormalized_numeric_df$unemployment<-ts(normalized_numeric_df$unemployment,star=decimal_date(as.Date(\"2010-01-01\",format = \"%Y-%m-%d\")),frequency = 4)\n\nfit <- lm(JPM.Adjusted ~ inflation+unemployment, data=normalized_numeric_df)\nfit.res<-ts(residuals(fit),star=decimal_date(as.Date(\"2010-01-01\",format = \"%Y-%m-%d\")),frequency = 4)\n############## Then look at the residuals ############\nreturns <- fit.res  %>% diff()\nautoplot(returns)+ggtitle(\"Linear Model Returns\")\n\n\n\n\n\n\n\n\n\nCode\nbyd.archTest <- ArchTest(fit.res, lags = 1, demean = TRUE)\nbyd.archTest\n\n\n\n    ARCH LM-test; Null hypothesis: no ARCH effects\n\ndata:  fit.res\nChi-squared = 25.657, df = 1, p-value = 4.077e-07\n\n\n\n\n\n\nCode\nggAcf(returns) +ggtitle(\"ACF for returns\")\n\n\n\n\n\n\n\n\n\nCode\nggPacf(returns) +ggtitle(\"PACF for returns\")\n\n\n\n\n\n\n\n\nThe ARCH LM-test was conducted with the null hypothesis of no ARCH effects. The test resulted in a chi-squared value of 25.657 with one degree of freedom, and a very low p-value of 4.077e-07. This suggests strong evidence against the null hypothesis, indicating the presence of ARCH effects in the data.\nBased on the ACF and PACF plots, it appears that there is some significant autocorrelation and partial autocorrelation at multiple lags, which suggests that an ARIMA model may not be sufficient to capture the time series behavior. Additionally, the values for p and q appear to be relatively high, with p = 3 and q = 3 being suggested by the plots.\n\n\nARIMAX Model\n\nAuto Arima ModelAuto Arima ResidualsARIMAX ModelARIMA (1,1,1) PlotARIMA (1,1,1) ModelCross Validation\n\n\n\n\nCode\nxreg <- cbind(Inflation = normalized_data_ts[, \"inflation\"],\n              Unemployment = normalized_data_ts[, \"unemployment\"])\nfit.auto <- auto.arima(normalized_data_ts[, \"JPM.Adjusted\"], xreg = xreg)\nsummary(fit.auto)\n\n\nSeries: normalized_data_ts[, \"JPM.Adjusted\"] \nRegression with ARIMA(0,1,0) errors \n\nCoefficients:\n      Inflation  Unemployment\n         0.1048       -0.2416\ns.e.     0.1070        0.0486\n\nsigma^2 = 0.06062:  log likelihood = 0.14\nAIC=5.73   AICc=6.24   BIC=11.52\n\nTraining set error measures:\n                     ME      RMSE       MAE      MPE     MAPE      MASE\nTraining set 0.01856899 0.2389951 0.1664957 7.224073 29.02619 0.4044532\n                 ACF1\nTraining set 0.227372\n\n\n\n\n\n\nCode\ncheckresiduals(fit.auto)\n\n\n\n\n\n\n    Ljung-Box test\n\ndata:  Residuals from Regression with ARIMA(0,1,0) errors\nQ* = 7.2584, df = 8, p-value = 0.509\n\nModel df: 0.   Total lags used: 8\n\n\n\n\n\n\nCode\nARIMA.c=function(p1,p2,q1,q2,data){\ntemp=c()\nd=1\ni=1\ntemp= data.frame()\nls=matrix(rep(NA,6*30),nrow=30)\n\n\nfor (p in p1:p2)#\n{\n  for(q in q1:q2)#\n  {\n    for(d in 0:1)\n    {\n      \n      if(p+d+q<=6)\n      {\n        \n        model<- Arima(data,order=c(p,d,q))\n        ls[i,]= c(p,d,q,model$aic,model$bic,model$aicc)\n        i=i+1\n        #print(i)\n        \n      }\n      \n    }\n  }\n}\n\n\ntemp= as.data.frame(ls)\nnames(temp)= c(\"p\",\"d\",\"q\",\"AIC\",\"BIC\",\"AICc\")\n\ntemp\n}\n\noutput <- ARIMA.c(1,3,1,3,data=residuals(fit))\n\noutput[which.min(output$AIC),] \n\n\n  p d q      AIC      BIC     AICc\n2 1 1 1 20.18394 25.97942 20.69458\n\n\nCode\noutput[which.min(output$BIC),]\n\n\n  p d q      AIC      BIC     AICc\n2 1 1 1 20.18394 25.97942 20.69458\n\n\nCode\noutput[which.min(output$AICc),]\n\n\n  p d q      AIC      BIC     AICc\n2 1 1 1 20.18394 25.97942 20.69458\n\n\n\n\n\n\nCode\nset.seed(1234)\n\nmodel_output <- capture.output(sarima(fit.res, 1,1,1)) \n\n\n\n\n\n\n\n\n\nCode\ncat(model_output[39:69], model_output[length(model_output)], sep = \"\\n\")\n\n\n$fit\n\nCall:\narima(x = xdata, order = c(p, d, q), seasonal = list(order = c(P, D, Q), period = S), \n    xreg = constant, transform.pars = trans, fixed = fixed, optim.control = list(trace = trc, \n        REPORT = 1, reltol = tol))\n\nCoefficients:\n          ar1     ma1  constant\n      -0.5131  0.8051   -0.0107\ns.e.   0.3978  0.3186    0.0462\n\nsigma^2 estimated as 0.07681:  log likelihood = -7.07,  aic = 22.13\n\n$degrees_of_freedom\n[1] 48\n\n$ttable\n         Estimate     SE t.value p.value\nar1       -0.5131 0.3978 -1.2900  0.2032\nma1        0.8051 0.3186  2.5267  0.0149\nconstant  -0.0107 0.0462 -0.2321  0.8174\n\n$AIC\n[1] 0.4339237\n\n$AICc\n[1] 0.4439362\n\n$BIC\n[1] 0.5854395\n\n\n\n\n\n\nCode\nn=length(fit.res)\nk= 51\n \n \nrmse1 <- matrix(NA, (n-k),4)\nrmse2 <- matrix(NA, (n-k),4)\nrmse3 <- matrix(NA, (n-k),4)\n\n\nst <- tsp(fit.res)[1]+(k-5)/4 \n\nfor(i in 1:(n-k))\n{\n  xtrain <- window(fit.res, end=st + i/4)\n  xtest <- window(fit.res, start=st + (i+1)/4, end=st + (i+4)/4)\n  \n  #ARIMA(0,1,0) ARIMA(1,1,1)\n  \n  fit <- Arima(xtrain, order=c(0,1,0),\n                include.drift=TRUE, method=\"ML\")\n  fcast <- forecast(fit, h=4)\n  \n  fit2 <- Arima(xtrain, order=c(1,1,1),\n                include.drift=TRUE, method=\"ML\")\n  fcast2 <- forecast(fit2, h=4)\n\n\n  rmse1[i,1:length(xtest)]   <- sqrt((fcast$mean-xtest)^2)\n  rmse2[i,1:length(xtest)] <- sqrt((fcast2$mean-xtest)^2)\n}\n\nplot(1:4,colMeans(rmse1,na.rm=TRUE), type=\"l\",col=2, xlab=\"horizon\", ylab=\"RMSE\")\nlines(1:4, colMeans(rmse2,na.rm=TRUE), type=\"l\",col=3)\nlegend(\"topleft\",legend=c(\"fit1\",\"fit2\"),col=2:4,lty=1)\n\n\n\n\n\n\n\n\nBased on the results of the auto.arima function, the suggested best model is ARIMA(0,1,0). However, when we manually test different ARIMA models, we find that ARIMA(1,1,1) has the lowest values for AIC, BIC, and AICC. Additionally, both models have similar standardized residual plots, with means close to 0, indicating a good fit. The ACF plot of residuals also shows no significant lags, further indicating a well-fitted model.\nTo determine the best model, we conduct cross-validation and compare the RMSE values of both models. The results show that ARIMA(0,1,0) has lower RMSE values than ARIMA(1,1,1), indicating that it is the better model.\nWe can then proceed to choose the best GARCH model using ARIMA(0,1,0) as the base model.\n\n\nSquared Residuals\n\nPlotACF PlotPACF Plot\n\n\n\n\nCode\nfit <- lm(JPM.Adjusted ~ inflation+unemployment, data=normalized_numeric_df)\nfit.res<-ts(residuals(fit),star=decimal_date(as.Date(\"2010-01-01\",format = \"%Y-%m-%d\")),frequency = 4)\nfit <- Arima(fit.res,order=c(0,1,0))\nres=fit$res\nplot(res^2,main='Squared Residuals')\n\n\n\n\n\n\n\n\n\nCode\nacf(res^2,24, main = \"ACF Residuals Square\")\n\n\n\n\n\n\n\n\n\nCode\npacf(res^2,24, main = \"PACF Residuals Square\")\n\n\n\n\n\n\n\n\nFrom the squared residuals of the best ARIMA model, it can be observed that the ACF plot and PACF plot indicate that the residuals are not autocorrelated and are white noise, indicating a good fit of the model. Based on the squared residuals of the best ARIMA model, we can see that the ACF and PACF plots indicate that most of the values lie between the blue lines. Additionally, the p-value is 1 and q-value is 2. This suggests that the model has a good fit and that there is no significant autocorrelation or partial autocorrelation in the residuals. Now we can proceed by fitting GARCH Model for p and q values.\n\n\nGARCH Model\n\nModelGRACH(1,1)GRACH(1,2)\n\n\n\n\nCode\nmodel <- list() ## set counter\ncc <- 1\nfor (p in 1) {\n  for (q in 1:2) {\n  \nmodel[[cc]] <- garch(res,order=c(q,p),trace=F)\ncc <- cc + 1\n}\n} \n\n## get AIC values for model evaluation\nGARCH_AIC <- sapply(model, AIC) ## model with lowest AIC is the best\nwhich(GARCH_AIC == min(GARCH_AIC))\n\n\n[1] 1\n\n\nCode\nmodel[[which(GARCH_AIC == min(GARCH_AIC))]]\n\n\n\nCall:\ngarch(x = res, order = c(q, p), trace = F)\n\nCoefficient(s):\n      a0        a1        b1  \n0.001917  0.305848  0.736524  \n\n\n\n\n\n\nCode\nsummary(garchFit(~garch(1,1),res, trace=F))\n\n\n\nTitle:\n GARCH Modelling \n\nCall:\n garchFit(formula = ~garch(1, 1), data = res, trace = F) \n\nMean and Variance Equation:\n data ~ garch(1, 1)\n<environment: 0x117619968>\n [data = res]\n\nConditional Distribution:\n norm \n\nCoefficient(s):\n       mu      omega     alpha1      beta1  \n0.0204567  0.0019945  0.3210361  0.7204176  \n\nStd. Errors:\n based on Hessian \n\nError Analysis:\n        Estimate  Std. Error  t value Pr(>|t|)    \nmu      0.020457    0.026942    0.759   0.4477    \nomega   0.001995    0.003465    0.576   0.5649    \nalpha1  0.321036    0.168345    1.907   0.0565 .  \nbeta1   0.720418    0.122860    5.864 4.53e-09 ***\n---\nSignif. codes:  0 '***' 0.001 '**' 0.01 '*' 0.05 '.' 0.1 ' ' 1\n\nLog Likelihood:\n -0.1441207    normalized:  -0.002771552 \n\nDescription:\n Sat Jan  6 19:53:10 2024 by user:  \n\n\nStandardised Residuals Tests:\n                                Statistic p-Value  \n Jarque-Bera Test   R    Chi^2  1.637721  0.4409338\n Shapiro-Wilk Test  R    W      0.979193  0.4921128\n Ljung-Box Test     R    Q(10)  5.279382  0.8717508\n Ljung-Box Test     R    Q(15)  11.33346  0.7286177\n Ljung-Box Test     R    Q(20)  15.95038  0.7196977\n Ljung-Box Test     R^2  Q(10)  10.91981  0.3638   \n Ljung-Box Test     R^2  Q(15)  19.91269  0.1753078\n Ljung-Box Test     R^2  Q(20)  20.83651  0.4068112\n LM Arch Test       R    TR^2   15.33254  0.2237526\n\nInformation Criterion Statistics:\n      AIC       BIC       SIC      HQIC \n0.1593893 0.3094849 0.1486439 0.2169324 \n\n\n\n\n\n\nCode\nsummary(garchFit(~garch(1,2),res, trace=F))\n\n\n\nTitle:\n GARCH Modelling \n\nCall:\n garchFit(formula = ~garch(1, 2), data = res, trace = F) \n\nMean and Variance Equation:\n data ~ garch(1, 2)\n<environment: 0x136d4e6f0>\n [data = res]\n\nConditional Distribution:\n norm \n\nCoefficient(s):\n        mu       omega      alpha1       beta1       beta2  \n0.02043647  0.00220287  0.32580190  0.70965382  0.00000001  \n\nStd. Errors:\n based on Hessian \n\nError Analysis:\n        Estimate  Std. Error  t value Pr(>|t|)\nmu     2.044e-02   2.738e-02    0.746    0.455\nomega  2.203e-03   3.617e-03    0.609    0.542\nalpha1 3.258e-01   2.562e-01    1.272    0.203\nbeta1  7.097e-01   8.327e-01    0.852    0.394\nbeta2  1.000e-08   6.657e-01    0.000    1.000\n\nLog Likelihood:\n -0.2447918    normalized:  -0.004707535 \n\nDescription:\n Sat Jan  6 19:53:10 2024 by user:  \n\n\nStandardised Residuals Tests:\n                                Statistic p-Value  \n Jarque-Bera Test   R    Chi^2  1.745873  0.417723 \n Shapiro-Wilk Test  R    W      0.9787466 0.4740299\n Ljung-Box Test     R    Q(10)  5.34259   0.8671499\n Ljung-Box Test     R    Q(15)  11.45855  0.719463 \n Ljung-Box Test     R    Q(20)  15.86405  0.7250215\n Ljung-Box Test     R^2  Q(10)  11.0293   0.3552398\n Ljung-Box Test     R^2  Q(15)  20.21143  0.163971 \n Ljung-Box Test     R^2  Q(20)  21.07113  0.3929568\n LM Arch Test       R    TR^2   16.27218  0.1790854\n\nInformation Criterion Statistics:\n      AIC       BIC       SIC      HQIC \n0.2017228 0.3893424 0.1853057 0.2736517 \n\n\n\n\n\nBased on the analysis of the different GARCH models, it appears that GARCH(1,1) is the optimal choice. Although the AIC values of the different models are relatively similar, we can further evaluate their significance to make a final determination. Upon closer inspection, it appears that GARCH(1,1) has significantly better values than the other models, indicating that it is the most appropriate choice. Therefore, we can conclude that the GARCH(1,1) model is the best fit for the data.\n\n\nBest Model\n\nARIMA ModelGARCH ModelVolatility\n\n\n\n\nCode\n#fiting an ARIMA model to the Inflation variable\ninflation_fit<-auto.arima(normalized_numeric_df$inflation) \nfinflation<-forecast(inflation_fit)\n\n#fitting an ARIMA model to the Unemployment variable\nunemployment_fit<-auto.arima(normalized_numeric_df$unemployment) \nfunemployment<-forecast(unemployment_fit)\n\n# best model fit for forcasting\nxreg <- cbind(Inflation = normalized_data_ts[, \"inflation\"],\n              Unemployment = normalized_data_ts[, \"unemployment\"])\n\n\n\nsummary(arima.fit<-Arima(normalized_data_ts[, \"JPM.Adjusted\"],order=c(0,1,0),xreg=xreg),include.drift = TRUE)\n\n\nSeries: normalized_data_ts[, \"JPM.Adjusted\"] \nRegression with ARIMA(0,1,0) errors \n\nCoefficients:\n      Inflation  Unemployment\n         0.1048       -0.2416\ns.e.     0.1070        0.0486\n\nsigma^2 = 0.06062:  log likelihood = 0.14\nAIC=5.73   AICc=6.24   BIC=11.52\n\nTraining set error measures:\n                     ME      RMSE       MAE      MPE     MAPE      MASE\nTraining set 0.01856899 0.2389951 0.1664957 7.224073 29.02619 0.4044532\n                 ACF1\nTraining set 0.227372\n\n\n\n\n\n\nCode\nsummary(final.fit <- garchFit(~garch(1,1), res,trace = F))\n\n\n\nTitle:\n GARCH Modelling \n\nCall:\n garchFit(formula = ~garch(1, 1), data = res, trace = F) \n\nMean and Variance Equation:\n data ~ garch(1, 1)\n<environment: 0x111b40320>\n [data = res]\n\nConditional Distribution:\n norm \n\nCoefficient(s):\n       mu      omega     alpha1      beta1  \n0.0204567  0.0019945  0.3210361  0.7204176  \n\nStd. Errors:\n based on Hessian \n\nError Analysis:\n        Estimate  Std. Error  t value Pr(>|t|)    \nmu      0.020457    0.026942    0.759   0.4477    \nomega   0.001995    0.003465    0.576   0.5649    \nalpha1  0.321036    0.168345    1.907   0.0565 .  \nbeta1   0.720418    0.122860    5.864 4.53e-09 ***\n---\nSignif. codes:  0 '***' 0.001 '**' 0.01 '*' 0.05 '.' 0.1 ' ' 1\n\nLog Likelihood:\n -0.1441207    normalized:  -0.002771552 \n\nDescription:\n Sat Jan  6 19:53:10 2024 by user:  \n\n\nStandardised Residuals Tests:\n                                Statistic p-Value  \n Jarque-Bera Test   R    Chi^2  1.637721  0.4409338\n Shapiro-Wilk Test  R    W      0.979193  0.4921128\n Ljung-Box Test     R    Q(10)  5.279382  0.8717508\n Ljung-Box Test     R    Q(15)  11.33346  0.7286177\n Ljung-Box Test     R    Q(20)  15.95038  0.7196977\n Ljung-Box Test     R^2  Q(10)  10.91981  0.3638   \n Ljung-Box Test     R^2  Q(15)  19.91269  0.1753078\n Ljung-Box Test     R^2  Q(20)  20.83651  0.4068112\n LM Arch Test       R    TR^2   15.33254  0.2237526\n\nInformation Criterion Statistics:\n      AIC       BIC       SIC      HQIC \n0.1593893 0.3094849 0.1486439 0.2169324 \n\n\n\n\n\n\nCode\nht <- final.fit@h.t #a numeric vector with the conditional variances (h.t = sigma.t^delta)\n\n#############################\ndata=data.frame(final)\ndata$Date<-as.Date(data$Date,\"%Y-%m-%d\")\n\n\ndata2= data.frame(ht,data$Date)\nggplot(data2, aes(y = ht, x = data.Date)) + geom_line(col = '#F1948A') + ylab('Conditional Variance') + xlab('Date')\n\n\n\n\n\n\n\n\nFrom the ARIMA(1,1,1), we see that the training set error measures also suggest a good fit, with low mean absolute error, root mean squared error, and autocorrelation of the residuals. GATCH(1,1) model model is used to estimate the volatility of the standardized residuals of the previous regression model. The model includes a mean equation that estimates the mean of the residuals and a variance equation that models the conditional variance of the residuals. The coefficients of the mean equation suggest that the mean of the residuals is close to zero. The variance equation coefficients suggest that the conditional variance of the residuals is dependent on the past conditional variances and the past squared standardized residuals. The model’s AIC, BIC, SIC, and HQIC values are all relatively low, indicating a good fit of the model. The standardized residuals tests indicate that the residuals are approximately normally distributed and that there is no significant autocorrelation in the residuals.\nThe volatility of the model seems high in 2020 but has decreased gradually in the past few months. This could indicate that the asset’s price was experiencing a lot of fluctuations in 2020, but the market has stabilized recently.\n\n\nModel Diagnostics\n\nResidualsQQ PlotBox Test\n\n\n\n\nCode\nfit2<-garch(res,order=c(1,1),trace=F)\ncheckresiduals(fit2) \n\n\n\n\n\n\n\n\n\nCode\nqqnorm(fit2$residuals, pch = 1)\nqqline(fit2$residuals, col = \"blue\", lwd = 2)\n\n\n\n\n\n\n\n\n\nCode\nBox.test (fit2$residuals, type = \"Ljung\")\n\n\n\n    Box-Ljung test\n\ndata:  fit2$residuals\nX-squared = 0.42691, df = 1, p-value = 0.5135\n\n\nThe ACF plot of the residuals shows all the values between the blue lines, which indicates that the residuals are not significantly autocorrelated. The range of values for the residual plot between -2 and 2 is considered acceptable. Additionally, the QQ plot of the residuals shows a linear plot on the line, which is another good indication that the residuals are normally distributed. The QQ plot is a valuable tool to assess if the residuals follow a normal distribution, and in this case, the plot suggests that the residuals do indeed follow a normal distribution.\nThe Box-Ljung test, a p-value of 0.5135 indicates that the model’s residuals are not significantly autocorrelated, meaning that the model has captured most of the information in the data. This result is good because it suggests that the model is a good fit for the data and has accounted for most of the underlying patterns in the data. Therefore, we can rely on the model’s predictions and use them to make informed decisions.\n\n\n\n\n\nForecast\n\n\nCode\npredict(final.fit, n.ahead = 5, plot=TRUE)\n\n\n\n\n\n  meanForecast meanError standardDeviation lowerInterval upperInterval\n1   0.02045667 0.5405365         0.5405365     -1.038975      1.079889\n2   0.02045667 0.5534313         0.5534313     -1.064249      1.105162\n3   0.02045667 0.5665487         0.5665487     -1.089958      1.130872\n4   0.02045667 0.5798945         0.5798945     -1.116116      1.157029\n5   0.02045667 0.5934747         0.5934747     -1.142732      1.183646\n\n\nThe forecasted plot is based on the best model ARIMAX(0,1,0)+GARCH(1,1). This model takes into account the autoregressive and moving average components of the data, as well as the impact of exogenous variables on the time series. Additionally, the GARCH component of the model accounts for the volatility clustering in the data. Overall, this model is well-suited to make accurate predictions about future values of the time series.\n\n\nEquation of the Model\nThe equation of the ARIMAX(0,1,0) model is:\n\\(Y(t) = Y(t-1) + \\epsilon(t)\\)\nwhere \\(Y(t)\\) is the time series variable and \\(\\epsilon(t)\\) is the error term.\nThe equation of the GARCH(1,1) model is:\n\\(\\sigma^2(t) = \\alpha_0+\\alpha_1\\epsilon_t^2(t-1)+ \\beta_1\\sigma^2(t-1)\\)\nwhere \\(\\sigma^2_t\\) is the conditional variance at time \\(t\\), \\(\\alpha_0\\) is a constant, \\(\\alpha_1\\) and \\(\\beta_1\\) are the parameters, and \\(\\epsilon_t\\) is the error term.\nThe combined equation of the ARIMAX(0,1,0)+GARCH(1,1) model is:\n\\(Y(t) = Y(t-1) + \\epsilon(t)\\)\n\\(\\epsilon(t) = \\sigma(t) * \\epsilon~(t)\\)\n\\(\\sigma^2(t) = \\alpha_0+\\alpha_1\\epsilon_t^2(t-1)+ \\beta_1\\sigma^2(t-1)\\)"
  },
  {
    "objectID": "arch_Health_Care_Sector_Fund.html",
    "href": "arch_Health_Care_Sector_Fund.html",
    "title": "",
    "section": "",
    "text": "Impact of Macroeconomic Factors on CVS Stock Price\n\n\n\nCVS Health Corp. is one of the top companies in the healthcare sector, providing a wide range of services including pharmacy, health insurance, and retail clinics. An ARIMAX+ARCH/GARCH model can be used to analyze the stock price behavior of CVS Health Corp.\nBy using an ARIMAX+ARCH/GARCH model to analyze the stock price behavior of CVS Health Corp., we can gain insights into how macroeconomic factors impact the company’s performance. For example, changes in healthcare policies, government regulations, and interest rates may affect the company’s financial performance and subsequently its stock price. Additionally, demographic trends such as an aging population and the increasing prevalence of chronic diseases may also impact the demand for healthcare services provided by CVS Health Corp. and in turn affect the company’s stock price.\n\n\nTime series Plot\n\nCVSDifferentitaed Chart SeriesGDP GrowthInflationInterestUnemployment\n\n\n\n\nCode\n# get data\noptions(\"getSymbols.warning4.0\"=FALSE)\noptions(\"getSymbols.yahoo.warning\"=FALSE)\n\n\ndata.info = getSymbols(\"CVS\",src='yahoo', from = '2010-01-01',to = \"2023-03-01\",auto.assign = FALSE)\ndata = getSymbols(\"CVS\",src='yahoo', from = '2010-01-01',to = \"2023-03-01\")\ndf <- data.frame(Date=index(CVS),coredata(CVS))\n\n# create Bollinger Bands\nbbands <- BBands(CVS[,c(\"CVS.High\",\"CVS.Low\",\"CVS.Close\")])\n\n# join and subset data\ndf <- subset(cbind(df, data.frame(bbands[,1:3])), Date >= \"2010-01-01\")\n\n\n# colors column for increasing and decreasing\nfor (i in 1:length(df[,1])) {\n  if (df$CVS.Close[i] >= df$CVS.Open[i]) {\n      df$direction[i] = 'Increasing'\n  } else {\n      df$direction[i] = 'Decreasing'\n  }\n}\n\ni <- list(line = list(color = '#AD1457'))\nd <- list(line = list(color = '#7F7F7F'))\n\n# plot candlestick chart\n\nfig <- df %>% plot_ly(x = ~Date, type=\"candlestick\",\n          open = ~CVS.Open, close = ~CVS.Close,\n          high = ~CVS.High, low = ~CVS.Low, name = \"CVS\",\n          increasing = i, decreasing = d) \nfig <- fig %>% add_lines(x = ~Date, y = ~up , name = \"B Bands\",\n            line = list(color = '#ccc', width = 0.5),\n            legendgroup = \"Bollinger Bands\",\n            hoverinfo = \"none\", inherit = F) \nfig <- fig %>% add_lines(x = ~Date, y = ~dn, name = \"B Bands\",\n            line = list(color = '#ccc', width = 0.5),\n            legendgroup = \"Bollinger Bands\", inherit = F,\n            showlegend = FALSE, hoverinfo = \"none\") \nfig <- fig %>% add_lines(x = ~Date, y = ~mavg, name = \"Mv Avg\",\n            line = list(color = '#C052B3', width = 0.5),\n            hoverinfo = \"none\", inherit = F) \nfig <- fig %>% layout(yaxis = list(title = \"Price\"))\n\n# plot volume bar chart\nfig2 <- df \nfig2 <- fig2 %>% plot_ly(x=~Date, y=~CVS.Volume, type='bar', name = \"CVS Volume\",\n          color = ~direction, colors = c('#AD1457','#7F7F7F')) \nfig2 <- fig2 %>% layout(yaxis = list(title = \"Volume\"))\n\n# create rangeselector buttons\nrs <- list(visible = TRUE, x = 0.5, y = -0.055,\n           xanchor = 'center', yref = 'paper',\n           font = list(size = 9),\n           buttons = list(\n             list(count=1,\n                  label='RESET',\n                  step='all'),\n             list(count=3,\n                  label='3 YR',\n                  step='year',\n                  stepmode='backward'),\n             list(count=1,\n                  label='1 YR',\n                  step='year',\n                  stepmode='backward'),\n             list(count=1,\n                  label='1 MO',\n                  step='month',\n                  stepmode='backward')\n           ))\n\n# subplot with shared x axis\nfig <- subplot(fig, fig2, heights = c(0.7,0.2), nrows=2,\n             shareX = TRUE, titleY = TRUE)\nfig <- fig %>% layout(title = paste(\"CVS Stock Price: January 2010 - March 2023\"),\n         xaxis = list(rangeselector = rs),\n         legend = list(orientation = 'h', x = 0.5, y = 1,\n                       xanchor = 'center', yref = 'paper',\n                       font = list(size = 10),\n                       bgcolor = 'transparent'))\n\nfig\n\n\n\n\n\n\n\n\n\n\nCode\nlog(data.info$`CVS.Adjusted`) %>% diff() %>% chartSeries(theme=chartTheme('white'),up.col='#AD1457')\n\n\n\n\n\n\n\n\n\nCode\n#import the data\ngdp <- read.csv(\"DATA/RAW DATA/gdp-growth.csv\")\n\n#change date format\ngdp$Date <- as.Date(gdp$DATE , \"%m/%d/%Y\")\n\n#drop DATE column\ngdp <- subset(gdp, select = -c(1))\n\n#export the cleaned data\ngdp_clean <- gdp\nwrite.csv(gdp_clean, \"DATA/CLEANED DATA/gdp_clean_data.csv\", row.names=FALSE)\n\n#plot gdp growth rate \nfig <- plot_ly(gdp, x = ~Date, y = ~value, type = 'scatter', mode = 'lines',line = list(color = 'rgb(240, 128, 128)'))\nfig <- fig %>% layout(title = \"U.S GPD Growth Rate: 2010 - 2022\",xaxis = list(title = \"Time\"),yaxis = list(title =\"GDP Growth Rate\"))\nfig\n\n\n\n\n\n\n\n\n\n\nCode\n#import the data\ninflation_rate <- read.csv(\"DATA/RAW DATA/inflation-rate.csv\")\n\n#cleaning the data\n#remove unwanted columns\ninflation_rate_clean <- subset(inflation_rate, select = -c(1,HALF1,HALF2))\n\n#convert the data to time series data\ninflation_data_ts <- ts(as.vector(t(as.matrix(inflation_rate_clean))), start=c(2010,1), end=c(2023,2), frequency=12)\n\n#export the data\nwrite.csv(inflation_rate_clean, \"DATA/CLEANED DATA/inflation_rate_clean_data.csv\", row.names=FALSE)\n\n\n#plot inflation rate \nfig <- autoplot(inflation_data_ts, ylab = \"Inflation Rate\", color=\"#FFA07A\")+ggtitle(\"U.S Inflation Rate: January 2010 - February 2023\")+theme_bw()\nggplotly(fig)\n\n\n\n\n\n\n\n\n\n\nCode\n#import the data\ninterest_data <- read.csv(\"DATA/RAW DATA/interest-rate.csv\")\n\n#change date format\ninterest_data$Date <- as.Date(interest_data$Date , \"%m/%d/%Y\")\n\n#export the cleaned data\ninterest_clean_data <- interest_data\nwrite.csv(interest_clean_data, \"DATA/CLEANED DATA/interest_rate_clean_data.csv\", row.names=FALSE)\n\n#plot interest rate \nfig <- plot_ly(interest_data, x = ~Date, y = ~value, type = 'scatter', mode = 'lines',line = list(color='rgb(219, 112, 147)'))\nfig <- fig %>% layout(title = \"U.S Interest Rate: January 2010 - March 2023\",xaxis = list(title = \"Time\"),yaxis = list(title =\"Interest Rate\"))\nfig\n\n\n\n\n\n\n\n\n\n\nCode\n#import the data\nunemployment_rate <- read.csv(\"DATA/RAW DATA/unemployment-rate.csv\")\n\n#change date format\nunemployment_rate$Date <- as.Date(unemployment_rate$Date , \"%m/%d/%Y\")\n\n# export the data\nwrite.csv(unemployment_rate, \"DATA/CLEANED DATA/unemployment_rate_clean_data.csv\", row.names=FALSE)\n\n#plot unemployment rate \nfig <- plot_ly(unemployment_rate, x = ~Date, y = ~Value, type = 'scatter', mode = 'lines',line = list(color = 'rgb(189, 183, 107)'))\nfig <- fig %>% layout(title = \"U.S Unemployment Rate: January 2010 - March 2023\",xaxis = list(title = \"Time\"),yaxis = list(title =\"Unemployment Rate\"))\nfig\n\n\n\n\n\n\n\n\n\nThe company’s stock price has exhibited a similar pattern to that of the broader healthcare sector, with fluctuations that are often driven by changes in healthcare policy, market competition, and macroeconomic conditions.\nFrom 2010 to 2015, CVS’s stock price experienced a steady rise, as the company expanded its retail pharmacy and healthcare services businesses. However, in 2015, CVS announced its plans to acquire health insurer Aetna, which led to some investor uncertainty and caused a temporary dip in the company’s stock price.\nFollowing the acquisition, CVS’s stock price remained relatively stable until early 2020, when the COVID-19 pandemic began to impact the healthcare industry. While the pandemic initially caused some volatility in CVS’s stock price, the company’s strong position in the healthcare sector and its focus on expanding its digital capabilities helped to stabilize its performance.\nSince mid-2020, CVS’s stock price has exhibited a generally upward trend, likely due to the company’s efforts to enhance its digital and e-commerce offerings, expand its healthcare services, and streamline its operations. In early 2021, CVS also announced plans to acquire health insurer Centene Corp’s Illinois unit, which could help to further bolster its position in the healthcare industry.\nSince early 2021, CVS’s stock price has experienced some volatility, likely due to a combination of factors such as global economic uncertainty and fluctuations in consumer demand for the company’s products.\nAs discussed before, the macroeconomic factors of GDP growth, inflation, interest rates, and unemployment rate are closely interrelated and play a crucial role in the overall health and stability of an economy. From 2010 to 2023, the global economy experienced a mix of ups and downs, with periods of strong GDP growth followed by slowdowns and recessions.\nThe second plot shows the first difference of the logarithm of the adjusted CVS stock price. Taking the first difference removes any long-term trends and transforms the time series into a stationary process. From the plot, we can observe that the first difference of the logarithm of the CVS stock price appears to be stationary, as the mean and variance are roughly constant over time.\n\n\nEnodogenous and Exogenous Variables\n\nPlotCorrelation HeatmapCCF GDPCCF InterestCCF InflationCCF Unemployment\n\n\n\n\nCode\nnumeric_data <- c(\"CVS.Adjusted\",\"gdp\", \"interest\", \"inflation\", \"unemployment\")\nnumeric_data <- final[, numeric_data]\nnormalized_data_numeric <- scale(numeric_data)\nnormalized_data <- ts(normalized_data_numeric, start = c(2010, 1), end = c(2021,10),frequency = 4)\nts_plot(normalized_data,\n        title = \"Normalized Time Series Data for CVS Stock and Macroeconomic Variables\",\n        Ytitle = \"Normalized Values\",\n        Xtitle = \"Year\")\n\n\n\n\n\n\n\n\n\n\nCode\n# Get upper triangle of the correlation matrix\nget_upper_tri <- function(cormat){\n    cormat[lower.tri(cormat)]<- NA\n    return(cormat)\n}\ncormat <- round(cor(normalized_data_numeric),2)\n\nupper_tri <- get_upper_tri(cormat)\n\nmelted_cormat <- melt(upper_tri, na.rm = TRUE)\n# Create a ggheatmap\nggheatmap <- ggplot(melted_cormat, aes(Var2, Var1, fill = value))+\n geom_tile(color = \"white\")+\n scale_fill_gradient2(low = \"blue\", high = \"red\", mid = \"white\", \n   midpoint = 0, limit = c(-1,1), space = \"Lab\", \n    name=\"Pearson\\nCorrelation\") +\n  theme_minimal()+ # minimal theme\n theme(axis.text.x = element_text(angle = 45, vjust = 1, \n    size = 12, hjust = 1))+\n coord_fixed()\n\nggheatmap + \ngeom_text(aes(Var2, Var1, label = value), color = \"black\", size = 4) +\ntheme(\n  axis.title.x = element_blank(),\n  axis.title.y = element_blank(),\n  panel.grid.major = element_blank(),\n  panel.border = element_blank(),\n  panel.background = element_blank(),\n  axis.ticks = element_blank(),\n  legend.justification = c(1, 0),\n  legend.position = c(0.6, 0.7),\n  legend.direction = \"horizontal\")+\n  guides(fill = guide_colorbar(barwidth = 7, barheight = 1,\n                title.position = \"top\", title.hjust = 0.5))\n\n\n\n\n\n\n\n\n\nCode\npar(mfrow=c(1,1))\nccf_result <- ccf(normalized_data[, c(\"CVS.Adjusted\")], normalized_data[, c(\"gdp\")], \n    lag.max = 300,\n    main = \"Cros-Correlation Plot for CVS Stock Price and GDP Growth Rate \",\n    ylab = \"CCF\")\n\n\n\n\n\nCode\ncat(\"The sum of cross correlation function is\", sum(abs(ccf_result$acf)))\n\n\nThe sum of cross correlation function is 6.66753\n\n\n\n\n\n\nCode\npar(mfrow=c(1,1))\nccf_result <- ccf(normalized_data[, c(\"CVS.Adjusted\")], normalized_data[, c(\"interest\")], \n    lag.max = 300,\n    main = \"Cros-Correlation Plot for CVS Stock Price and Interest Rate\",\n    ylab = \"CCF\")\n\n\n\n\n\nCode\ncat(\"The sum of cross correlation function is\", sum(abs(ccf_result$acf)))\n\n\nThe sum of cross correlation function is 14.04391\n\n\n\n\n\n\nCode\npar(mfrow=c(1,1))\nccf_result <- ccf(normalized_data[, c(\"CVS.Adjusted\")], normalized_data[, c(\"inflation\")], \n    lag.max = 300,\n    main = \"Cros-Correlation Plot for CVS Stock Price and Inflation Rate\",\n    ylab = \"CCF\")\n\n\n\n\n\nCode\ncat(\"The sum of cross correlation function is\", sum(abs(ccf_result$acf)))\n\n\nThe sum of cross correlation function is 15.34855\n\n\n\n\n\n\nCode\npar(mfrow=c(1,1))\nccf_result <- ccf(normalized_data[, c(\"CVS.Adjusted\")], normalized_data[, c(\"unemployment\")], \n    lag.max = 300,\n    main = \"Cros-Correlation Plot for CVS Stock Priceand Unemployment Rate\",\n    ylab = \"CCF\")\n\n\n\n\n\nCode\ncat(\"The sum of cross correlation function is\", sum(abs(ccf_result$acf)))\n\n\nThe sum of cross correlation function is 15.40655\n\n\n\n\n\nThe Normalized Time Series Data for Stock Price and Macroeconomic Variables plot shows the same variables as the first plot but has been normalized to a common range of 0 to 1 using the scale() function in R, which standardizes the variables to have a mean of 0 and a standard deviation of 1. The heatmap analysis of the normalized data reveals that inflation and unemployment rate exhibit strong positive correlations with the stock price indices, indicating that these variables may significantly influence stock price movements. On the other hand, weaker correlations were observed between the stock price indices and GDP and interest rates, suggesting that these variables may have less impact on stock price fluctuations. The cross-correlation feature plots confirm these findings, indicating that inflation and unemployment rate are more suitable feature variables for the ARIMAX model when predicting CVS movements.\nFinal Exogenous variables: Macroeconomic indicators: Inflation rate and unemployment rate.\n\n\nEnodogenous and Exogenous Variables Plot\n\nPlotCheck the stationarity\n\n\n\n\nCode\nfinal_data <- final %>%dplyr::select( Date,CVS.Adjusted, inflation,unemployment)\nnumeric_data <- c(\"CVS.Adjusted\", \"inflation\",\"unemployment\")\nnumeric_data <- final_data[, numeric_data]\nnormalized_data_numeric <- scale(numeric_data)\nnormalized_numeric_df <- data.frame(normalized_data_numeric)\nnormalized_data_ts <- ts(normalized_data_numeric, start = c(2010, 1), frequency = 4)\n\nautoplot(normalized_data_ts, facets=TRUE) +\n  xlab(\"Year\") + ylab(\"\") +\n  ggtitle(\"CVS Stock Price, Inflation Rate and Unemployment Rate in USA 2010-2023\")\n\n\n\n\n\n\n\n\n\nCode\n# Convert your multivariate time series data to a matrix\nfinal_data_ts_multivariate <- as.matrix(normalized_data_ts)\n\n# Check for stationarity using Phillips-Perron test\nphillips_perron_test <- ur.pp(final_data_ts_multivariate)  \nsummary(phillips_perron_test)\n\n\n\n################################## \n# Phillips-Perron Unit Root Test # \n################################## \n\nTest regression with intercept \n\n\nCall:\nlm(formula = y ~ y.l1)\n\nResiduals:\n    Min      1Q  Median      3Q     Max \n-1.9919 -0.1733 -0.0624  0.1315  4.4728 \n\nCoefficients:\n            Estimate Std. Error t value Pr(>|t|)    \n(Intercept) 0.004471   0.042538   0.105    0.916    \ny.l1        0.841642   0.042969  19.587   <2e-16 ***\n---\nSignif. codes:  0 '***' 0.001 '**' 0.01 '*' 0.05 '.' 0.1 ' ' 1\n\nResidual standard error: 0.5296 on 153 degrees of freedom\nMultiple R-squared:  0.7149,    Adjusted R-squared:  0.713 \nF-statistic: 383.7 on 1 and 153 DF,  p-value: < 2.2e-16\n\n\nValue of test-statistic, type: Z-alpha  is: -25.4391 \n\n         aux. Z statistics\nZ-tau-mu            0.1044\n\n\n\n\n\nThe results of the Phillips-Perron unit root test indicate strong evidence against the null hypothesis of a unit root, as the p-value for the coefficient of the lagged variable is less than the significance level of 0.05. This suggests that the variable y, which is being tested for stationarity, is likely stationary. Furthermore, the test statistic Z-tau-mu is 0.1044, which is smaller than the critical value of Z-alpha (-25.4391), providing further evidence of stationarity.\nTo determine whether the linear model requires an ARCH model, an ARCH test is conducted. The ACF and PACF plots are also used to identify suitable model values.\n\n\nModel Fitting\n\nPlotARCH TestACF PlotPACF Plot\n\n\n\n\nCode\nnormalized_numeric_df$CVS.Adjusted<-ts(normalized_numeric_df$CVS.Adjusted,star=decimal_date(as.Date(\"2010-01-01\",format = \"%Y-%m-%d\")),frequency = 4)\nnormalized_numeric_df$inflation<-ts(normalized_numeric_df$inflation,star=decimal_date(as.Date(\"2010-01-01\",format = \"%Y-%m-%d\")),frequency = 4)\nnormalized_numeric_df$unemployment<-ts(normalized_numeric_df$unemployment,star=decimal_date(as.Date(\"2010-01-01\",format = \"%Y-%m-%d\")),frequency = 4)\n\nfit <- lm(CVS.Adjusted ~ inflation+unemployment, data=normalized_numeric_df)\nfit.res<-ts(residuals(fit),star=decimal_date(as.Date(\"2010-01-01\",format = \"%Y-%m-%d\")),frequency = 4)\n############## Then look at the residuals ############\nreturns <- fit.res  %>% diff()\nautoplot(returns)+ggtitle(\"Linear Model Returns\")\n\n\n\n\n\n\n\n\n\nCode\nbyd.archTest <- ArchTest(fit.res, lags = 1, demean = TRUE)\nbyd.archTest\n\n\n\n    ARCH LM-test; Null hypothesis: no ARCH effects\n\ndata:  fit.res\nChi-squared = 6.232, df = 1, p-value = 0.01255\n\n\n\n\n\n\nCode\nggAcf(returns) +ggtitle(\"ACF for returns\")\n\n\n\n\n\n\n\n\n\nCode\nggPacf(returns) +ggtitle(\"PACF for returns\")\n\n\n\n\n\n\n\n\nThe ARCH LM-test was conducted with the null hypothesis of no ARCH effects. The test resulted in a chi-squared value of 6.232 with one degree of freedom, and a low p-value of 0.01255. This suggests strong evidence against the null hypothesis, indicating the presence of ARCH effects in the data.\nBased on the ACF and PACF plots, it appears that there is some significant autocorrelation and partial autocorrelation at multiple lags, which suggests that an ARIMA model may not be sufficient to capture the time series behavior. Additionally, the values for p and q appear to be relatively high, with p = 3 and q = 8 being suggested by the plots.\n\n\nARIMAX Model\n\nAuto Arima ModelAuto Arima ResidualsARIMAX ModelARIMA (2,0,1) PlotARIMA (2,0,1) ModelARIMA (1,1,1) PlotARIMA (1,1,1) ModelCross Validation\n\n\n\n\nCode\nxreg <- cbind(Inflation = normalized_data_ts[, \"inflation\"],\n              Unemployment = normalized_data_ts[, \"unemployment\"])\nfit.auto <- auto.arima(normalized_data_ts[, \"CVS.Adjusted\"], xreg = xreg)\nsummary(fit.auto)\n\n\nSeries: normalized_data_ts[, \"CVS.Adjusted\"] \nRegression with ARIMA(0,1,0)(0,0,1)[4] errors \n\nCoefficients:\n        sma1  Inflation  Unemployment\n      0.4196     0.2311       -0.0734\ns.e.  0.1281     0.1097        0.0511\n\nsigma^2 = 0.07929:  log likelihood = -6.57\nAIC=21.15   AICc=22.02   BIC=28.88\n\nTraining set error measures:\n                     ME      RMSE       MAE      MPE     MAPE      MASE\nTraining set 0.03193475 0.2705407 0.2096642 36.92293 74.53086 0.4005753\n                    ACF1\nTraining set -0.07923522\n\n\n\n\n\n\nCode\ncheckresiduals(fit.auto)\n\n\n\n\n\n\n    Ljung-Box test\n\ndata:  Residuals from Regression with ARIMA(0,1,0)(0,0,1)[4] errors\nQ* = 7.2257, df = 7, p-value = 0.4058\n\nModel df: 1.   Total lags used: 8\n\n\n\n\n\n\nCode\nARIMA.c=function(p1,p2,q1,q2,data){\ntemp=c()\nd=1\ni=1\ntemp= data.frame()\nls=matrix(rep(NA,6*30),nrow=30)\n\n\nfor (p in p1:p2)#\n{\n  for(q in q1:q2)#\n  {\n    for(d in 0:1)\n    {\n      \n      if(p+d+q<=6)\n      {\n        \n        model<- Arima(data,order=c(p,d,q))\n        ls[i,]= c(p,d,q,model$aic,model$bic,model$aicc)\n        i=i+1\n        #print(i)\n        \n      }\n      \n    }\n  }\n}\n\n\ntemp= as.data.frame(ls)\nnames(temp)= c(\"p\",\"d\",\"q\",\"AIC\",\"BIC\",\"AICc\")\n\ntemp\n}\n\noutput <- ARIMA.c(1,8,1,2,data=residuals(fit))\n\noutput[which.min(output$AIC),] \n\n\n  p d q      AIC      BIC     AICc\n5 2 0 1 59.21013 68.96635 60.51448\n\n\nCode\noutput[which.min(output$BIC),]\n\n\n  p d q      AIC      BIC     AICc\n2 1 1 1 60.59251 66.38799 61.10315\n\n\nCode\noutput[which.min(output$AICc),]\n\n\n  p d q      AIC      BIC     AICc\n5 2 0 1 59.21013 68.96635 60.51448\n\n\n\n\n\n\nCode\nset.seed(1234)\n\nmodel_output <- capture.output(sarima(fit.res, 2,0,1)) \n\n\n\n\n\n\n\n\n\nCode\ncat(model_output[38:69], model_output[length(model_output)], sep = \"\\n\")\n\n\n$fit\n\nCall:\narima(x = xdata, order = c(p, d, q), seasonal = list(order = c(P, D, Q), period = S), \n    xreg = xmean, include.mean = FALSE, transform.pars = trans, fixed = fixed, \n    optim.control = list(trace = trc, REPORT = 1, reltol = tol))\n\nCoefficients:\n          ar1     ar2     ma1    xmean\n      -0.0494  0.5842  1.0000  -0.0355\ns.e.   0.1158  0.1153  0.0813   0.2141\n\nsigma^2 estimated as 0.1426:  log likelihood = -24.61,  aic = 59.21\n\n$degrees_of_freedom\n[1] 48\n\n$ttable\n      Estimate     SE t.value p.value\nar1    -0.0494 0.1158 -0.4267  0.6715\nar2     0.5842 0.1153  5.0647  0.0000\nma1     1.0000 0.0813 12.2957  0.0000\nxmean  -0.0355 0.2141 -0.1659  0.8690\n\n$AIC\n[1] 1.138656\n\n$AICc\n[1] 1.155023\n\n$BIC\n[1] 1.326276\n\n\n\n\n\n\nCode\nset.seed(1234)\n\nmodel_output <- capture.output(sarima(fit.res, 1,1,1)) \n\n\n\n\n\n\n\n\n\nCode\ncat(model_output[40:70], model_output[length(model_output)], sep = \"\\n\")\n\n\n$fit\n\nCall:\narima(x = xdata, order = c(p, d, q), seasonal = list(order = c(P, D, Q), period = S), \n    xreg = constant, transform.pars = trans, fixed = fixed, optim.control = list(trace = trc, \n        REPORT = 1, reltol = tol))\n\nCoefficients:\n          ar1     ma1  constant\n      -0.8170  1.0000    0.0126\ns.e.   0.0886  0.0711    0.0625\n\nsigma^2 estimated as 0.1647:  log likelihood = -27.28,  aic = 62.55\n\n$degrees_of_freedom\n[1] 48\n\n$ttable\n         Estimate     SE t.value p.value\nar1       -0.8170 0.0886 -9.2196  0.0000\nma1        1.0000 0.0711 14.0599  0.0000\nconstant   0.0126 0.0625  0.2024  0.8404\n\n$AIC\n[1] 1.226501\n\n$AICc\n[1] 1.236513\n\n$BIC\n[1] 1.378016\n\n\n\n\n\n\nCode\nn=length(fit.res)\nk= 51\n \n \nrmse1 <- matrix(NA, (n-k),4)\nrmse2 <- matrix(NA, (n-k),4)\nrmse3 <- matrix(NA, (n-k),4)\n\n\nst <- tsp(fit.res)[1]+(k-5)/4 \n\nfor(i in 1:(n-k))\n{\n  xtrain <- window(fit.res, end=st + i/4)\n  xtest <- window(fit.res, start=st + (i+1)/4, end=st + (i+4)/4)\n  \n  #ARIMA(0,1,0) ARIMA(1,1,1)\n  \n  fit <- Arima(xtrain, order=c(2,0,1),\n                include.drift=TRUE, method=\"ML\")\n  fcast <- forecast(fit, h=4)\n  \n  fit2 <- Arima(xtrain, order=c(1,1,1),\n                include.drift=TRUE, method=\"ML\")\n  fcast2 <- forecast(fit2, h=4)\n\n  fit3 <- Arima(xtrain, order=c(0,1,0),seasonal=c(0,0,1),\n                include.drift=TRUE, method=\"ML\")\n  fcast3 <- forecast(fit2, h=4)\n  \n  rmse1[i,1:length(xtest)]   <- sqrt((fcast$mean-xtest)^2)\n  rmse2[i,1:length(xtest)] <- sqrt((fcast2$mean-xtest)^2)\n  rmse3[i,1:length(xtest)] <- sqrt((fcast3$mean-xtest)^2)\n}\n\nplot(1:4,colMeans(rmse1,na.rm=TRUE), type=\"l\",col=2, xlab=\"horizon\", ylab=\"RMSE\")\nlines(1:4, colMeans(rmse2,na.rm=TRUE), type=\"l\",col=3)\nlines(1:4, colMeans(rmse3,na.rm=TRUE), type=\"l\",col=3)\nlegend(\"topleft\",legend=c(\"fit1\",\"fit2\",\"fit3\"),col=2:4,lty=1)\n\n\n\n\n\n\n\n\nBased on the results of the auto.arima function, the suggested best model is ARIMA(0,1,0)(0,0,1)[4]. However, when we manually test different ARIMA models, we find that ARIMA(1,1,1) has the lowest values for AIC and AICC and (2,0,1) has the lowest BIC value. Additionally,when comparing the models, RMSE values are for ARIMA(1,1,1) and ARIMA(0,1,0)(0,0,1)[4], but the AIC and BIC value is way lesser in ARIMA(1,1,1) than ARIMA(0,1,0)(0,0,1)[4]. So, out best model id ARIMA(1,1,1)\nWe can then proceed to choose the best GARCH model using ARIMA(1,1,1) as the base model.\n\n\nSquared Residuals\n\nPlotACF PlotPACF PlotACF AbsoluteARCH Model\n\n\n\n\nCode\nfit <- lm(CVS.Adjusted ~ inflation+unemployment, data=normalized_numeric_df)\nfit.res<-ts(residuals(fit),star=decimal_date(as.Date(\"2010-01-01\",format = \"%Y-%m-%d\")),frequency = 4)\nfit <- Arima(fit.res,order=c(1,1,1))\nres=fit$res\nplot(res^2,main='Squared Residuals')\n\n\n\n\n\n\n\n\n\nCode\nacf(res^2,24, main = \"ACF Residuals Square\")\n\n\n\n\n\n\n\n\n\nCode\npacf(res^2,24, main = \"PACF Residuals Square\")\n\n\n\n\n\n\n\n\n\nCode\nacf(abs(res), main = \"ACF of Absolute Residuals\")\n\n\n\n\n\n\n\nCode\npacf(abs(res), main = \"PACF of Absolute Residuals\")\n\n\n\n\n\n\n\n\n\nCode\nsummary(garchFit(~garch(1,0),res, trace=F))\n\n\n\nTitle:\n GARCH Modelling \n\nCall:\n garchFit(formula = ~garch(1, 0), data = res, trace = F) \n\nMean and Variance Equation:\n data ~ garch(1, 0)\n<environment: 0x11097ef60>\n [data = res]\n\nConditional Distribution:\n norm \n\nCoefficient(s):\n       mu      omega     alpha1  \n-0.004657   0.074527   0.772521  \n\nStd. Errors:\n based on Hessian \n\nError Analysis:\n        Estimate  Std. Error  t value Pr(>|t|)   \nmu     -0.004657    0.046134   -0.101  0.91959   \nomega   0.074527    0.022857    3.261  0.00111 **\nalpha1  0.772521    0.378439    2.041  0.04122 * \n---\nSignif. codes:  0 '***' 0.001 '**' 0.01 '*' 0.05 '.' 0.1 ' ' 1\n\nLog Likelihood:\n -21.43535    normalized:  -0.4122182 \n\nDescription:\n Sat Jan  6 19:53:57 2024 by user:  \n\n\nStandardised Residuals Tests:\n                                Statistic p-Value     \n Jarque-Bera Test   R    Chi^2  37.97519  5.672745e-09\n Shapiro-Wilk Test  R    W      0.9085432 0.000724795 \n Ljung-Box Test     R    Q(10)  7.703958  0.6577284   \n Ljung-Box Test     R    Q(15)  11.08966  0.7462132   \n Ljung-Box Test     R    Q(20)  20.53374  0.4250177   \n Ljung-Box Test     R^2  Q(10)  2.098718  0.995526    \n Ljung-Box Test     R^2  Q(15)  3.245988  0.9993468   \n Ljung-Box Test     R^2  Q(20)  4.060657  0.9999474   \n LM Arch Test       R    TR^2   3.304628  0.9929844   \n\nInformation Criterion Statistics:\n      AIC       BIC       SIC      HQIC \n0.9398210 1.0523927 0.9336356 0.9829783 \n\n\n\n\n\nFrom the squared residuals of the best ARIMA model, it can be observed that the ACF plot and PACF plot indicate that the residuals are not autocorrelated and are white noise, indicating a good fit of the model. Based on the squared residuals of the best ARIMA model, we can see that the ACF and PACF plots indicate that most of the values lie between the blue lines. Additionally, the p-value is 1. This suggests that the model has a good fit and that there is no significant autocorrelation or partial autocorrelation in the residuals.\nThe best models are ARIMA(1,1,1) and ARCH(1)\n\n\nBest Model\n\nARIMA ModelGARCH ModelVolatility\n\n\n\n\nCode\n#fiting an ARIMA model to the Inflation variable\ninflation_fit<-auto.arima(normalized_numeric_df$inflation) \nfinflation<-forecast(inflation_fit)\n\n#fitting an ARIMA model to the Unemployment variable\nunemployment_fit<-auto.arima(normalized_numeric_df$unemployment) \nfunemployment<-forecast(unemployment_fit)\n\n# best model fit for forcasting\nxreg <- cbind(Inflation = normalized_data_ts[, \"inflation\"],\n              Unemployment = normalized_data_ts[, \"unemployment\"])\n\n\n\nsummary(arima.fit<-Arima(normalized_data_ts[, \"CVS.Adjusted\"],order=c(1,1,1),xreg=xreg),include.drift = TRUE)\n\n\nSeries: normalized_data_ts[, \"CVS.Adjusted\"] \nRegression with ARIMA(1,1,1) errors \n\nCoefficients:\n          ar1     ma1  Inflation  Unemployment\n      -0.3281  0.3754     0.2266       -0.1084\ns.e.   0.8664  0.8446     0.1377        0.0637\n\nsigma^2 = 0.09542:  log likelihood = -10.37\nAIC=30.75   AICc=32.08   BIC=40.4\n\nTraining set error measures:\n                     ME      RMSE       MAE      MPE     MAPE      MASE\nTraining set 0.04128572 0.2936735 0.2223189 35.05635 89.53581 0.4247529\n                    ACF1\nTraining set -0.05055536\n\n\n\n\n\n\nCode\nsummary(final.fit <- garchFit(~garch(1,0), res,trace = F))\n\n\n\nTitle:\n GARCH Modelling \n\nCall:\n garchFit(formula = ~garch(1, 0), data = res, trace = F) \n\nMean and Variance Equation:\n data ~ garch(1, 0)\n<environment: 0x124e42400>\n [data = res]\n\nConditional Distribution:\n norm \n\nCoefficient(s):\n       mu      omega     alpha1  \n-0.004657   0.074527   0.772521  \n\nStd. Errors:\n based on Hessian \n\nError Analysis:\n        Estimate  Std. Error  t value Pr(>|t|)   \nmu     -0.004657    0.046134   -0.101  0.91959   \nomega   0.074527    0.022857    3.261  0.00111 **\nalpha1  0.772521    0.378439    2.041  0.04122 * \n---\nSignif. codes:  0 '***' 0.001 '**' 0.01 '*' 0.05 '.' 0.1 ' ' 1\n\nLog Likelihood:\n -21.43535    normalized:  -0.4122182 \n\nDescription:\n Sat Jan  6 19:53:58 2024 by user:  \n\n\nStandardised Residuals Tests:\n                                Statistic p-Value     \n Jarque-Bera Test   R    Chi^2  37.97519  5.672745e-09\n Shapiro-Wilk Test  R    W      0.9085432 0.000724795 \n Ljung-Box Test     R    Q(10)  7.703958  0.6577284   \n Ljung-Box Test     R    Q(15)  11.08966  0.7462132   \n Ljung-Box Test     R    Q(20)  20.53374  0.4250177   \n Ljung-Box Test     R^2  Q(10)  2.098718  0.995526    \n Ljung-Box Test     R^2  Q(15)  3.245988  0.9993468   \n Ljung-Box Test     R^2  Q(20)  4.060657  0.9999474   \n LM Arch Test       R    TR^2   3.304628  0.9929844   \n\nInformation Criterion Statistics:\n      AIC       BIC       SIC      HQIC \n0.9398210 1.0523927 0.9336356 0.9829783 \n\n\n\n\n\n\nCode\nht <- final.fit@h.t #a numeric vector with the conditional variances (h.t = sigma.t^delta)\n\n#############################\ndata=data.frame(final)\ndata$Date<-as.Date(data$Date,\"%Y-%m-%d\")\n\n\ndata2= data.frame(ht,data$Date)\nggplot(data2, aes(y = ht, x = data.Date)) + geom_line(col = '#AD1457') + ylab('Conditional Variance') + xlab('Date')\n\n\n\n\n\n\n\n\nFrom the ARIMA(1,1,1), we see that the training set error measures also suggest a good fit, with low mean absolute error, root mean squared error, and autocorrelation of the residuals. GATCH(1,0) model model is used to estimate the volatility of the standardized residuals of the previous regression model. The model includes a mean equation that estimates the mean of the residuals and a variance equation that models the conditional variance of the residuals. The coefficients of the mean equation suggest that the mean of the residuals is close to zero. The variance equation coefficients suggest that the conditional variance of the residuals is dependent on the past conditional variances and the past squared standardized residuals. The model’s the AIC, BIC, SIC, and HQIC values are all relatively low, indicating a good fit of the model. The standardized residuals tests indicate that the residuals are approximately normally distributed and that there is no significant autocorrelation in the residuals.\nThe volatility of the model seems high in 2020 but has decreased gradually in the past few months. This could indicate that the asset’s price was experiencing a lot of fluctuations in 2020, but the market has stabilized recently.\n\n\nModel Diagnostics\n\nResidualsQQ PlotBox Test\n\n\n\n\nCode\nfit2<-garch(res,order=c(1,0),trace=F)\ncheckresiduals(fit2) \n\n\n\n\n\n\n\n\n\nCode\nqqnorm(fit2$residuals, pch = 1)\nqqline(fit2$residuals, col = \"blue\", lwd = 2)\n\n\n\n\n\n\n\n\n\nCode\nBox.test (fit2$residuals, type = \"Ljung\")\n\n\n\n    Box-Ljung test\n\ndata:  fit2$residuals\nX-squared = 0.48064, df = 1, p-value = 0.4881\n\n\nThe ACF plot of the residuals shows all the values between the blue lines, which indicates that the residuals are not significantly autocorrelated. The range of values for the residual plot between -2 and 2 is considered acceptable. Additionally, the QQ plot of the residuals shows a linear plot on the line, which is another good indication that the residuals are normally distributed. The QQ plot is a valuable tool to assess if the residuals follow a normal distribution, and in this case, the plot suggests that the residuals do indeed follow a normal distribution.\nThe Box-Ljung test, a p-value of 0.8885 indicates that the model’s residuals are not significantly autocorrelated, meaning that the model has captured most of the information in the data. This result is good because it suggests that the model is a good fit for the data and has accounted for most of the underlying patterns in the data. Therefore, we can rely on the model’s predictions and use them to make informed decisions.\n\n\n\n\n\nForecast\n\n\nCode\npredict(final.fit, n.ahead = 5, plot=TRUE)\n\n\n\n\n\n  meanForecast meanError standardDeviation lowerInterval upperInterval\n1 -0.004657039 0.2997059         0.2997059    -0.5920698     0.5827557\n2 -0.004657039 0.3793652         0.3793652    -0.7481991     0.7388850\n3 -0.004657039 0.4309373         0.4309373    -0.8492786     0.8399645\n4 -0.004657039 0.4668938         0.4668938    -0.9197521     0.9104381\n5 -0.004657039 0.4928784         0.4928784    -0.9706809     0.9613668\n\n\nThe forecasted plot is based on the best model ARIMAX(1,1,1)+GARCH(1,1). This model takes into account the autoregressive and moving average components of the data, as well as the impact of exogenous variables on the time series. Additionally, the GARCH component of the model accounts for the volatility clustering in the data. Overall, this model is well-suited to make accurate predictions about future values of the time series.\n\n\nEquation of the Model\nThe equation of the ARIMAX(1,1,1) model is:\n\\(Y(t) = c + \\phi_1(Y{(t-1)} - X{(t-1)}) + \\theta_1\\epsilon{(t-1)} + \\epsilon(t)\\)\nwhere, \\(Y(t)\\) is the time series variable, \\(X(t-1)\\) is the exogenous variable, \\(c\\) is a constant, \\(\\phi_1\\) and \\(\\theta_1\\) are the parameters, and \\(\\epsilon(t)\\) is the error term.\nThe equation of the ARCH(1) model is:\n\\(\\sigma_{t}^{2}=\\alpha_{0}+\\alpha_{1}\\epsilon_{t-1}^{2}\\)\nwhere \\(\\sigma^2_t\\) is the conditional variance at time \\(t\\), \\(\\alpha_0\\) is a constant, \\(\\alpha_1\\) and \\(\\beta_1\\) are the parameters, and \\(\\epsilon_t\\) is the error term.\nThe combined equation of the ARIMAX(1,1,1)+GARCH(1,1) model is:\n\\(Y(t) = c + \\phi_1(Y(t-1) - X(t-1)) + \\theta_1\\epsilon(t-1) + \\epsilon(t)\\)\n\\(\\sigma_{t}^{2}=\\alpha_{0}+\\alpha_{1}\\epsilon_{t-1}^{2}\\)"
  },
  {
    "objectID": "arch_Industrial_Sector_Fund.html",
    "href": "arch_Industrial_Sector_Fund.html",
    "title": "",
    "section": "",
    "text": "Impact of Macroeconomic Factors on General Electric Stock Price\n\n\n\nOne of the top companies in the industrial sector is General Electric (GE), which is a multinational conglomerate that produces a wide range of products including aircraft engines, power generation equipment, renewable energy solutions, and medical imaging devices. To analyze the stock price behavior of General Electric, an ARIMAX+ARCH/GARCH model can be employed.\nBy using an ARIMAX+ARCH/GARCH model to analyze the stock price behavior of General Electric, we can gain insights into how macroeconomic factors impact the company’s performance. For example, an increase in government spending on infrastructure projects or a rise in demand for commercial aircraft may lead to increased revenue and a rise in General Electric’s stock price. On the other hand, global economic uncertainty or changes in trade policies may lead to a decline in General Electric’s stock price.\n\n\nTime series Plot\n\nGeneral ElectricDifferentitaed Chart SeriesGDP GrowthInflationInterestUnemployment\n\n\n\n\nCode\n# get data\noptions(\"getSymbols.warning4.0\"=FALSE)\noptions(\"getSymbols.yahoo.warning\"=FALSE)\n\n\ndata.info = getSymbols(\"GE\",src='yahoo', from = '2010-01-01',to = \"2023-03-01\",auto.assign = FALSE)\ndata = getSymbols(\"GE\",src='yahoo', from = '2010-01-01',to = \"2023-03-01\")\ndf <- data.frame(Date=index(GE),coredata(GE))\n\n# create Bollinger Bands\nbbands <- BBands(GE[,c(\"GE.High\",\"GE.Low\",\"GE.Close\")])\n\n# join and subset data\ndf <- subset(cbind(df, data.frame(bbands[,1:3])), Date >= \"2010-01-01\")\n\n\n# colors column for increasing and decreasing\nfor (i in 1:length(df[,1])) {\n  if (df$GE.Close[i] >= df$GE.Open[i]) {\n      df$direction[i] = 'Increasing'\n  } else {\n      df$direction[i] = 'Decreasing'\n  }\n}\n\ni <- list(line = list(color = '#FDD835'))\nd <- list(line = list(color = '#7F7F7F'))\n\n# plot candlestick chart\n\nfig <- df %>% plot_ly(x = ~Date, type=\"candlestick\",\n          open = ~GE.Open, close = ~GE.Close,\n          high = ~GE.High, low = ~GE.Low, name = \"GE\",\n          increasing = i, decreasing = d) \nfig <- fig %>% add_lines(x = ~Date, y = ~up , name = \"B Bands\",\n            line = list(color = '#ccc', width = 0.5),\n            legendgroup = \"Bollinger Bands\",\n            hoverinfo = \"none\", inherit = F) \nfig <- fig %>% add_lines(x = ~Date, y = ~dn, name = \"B Bands\",\n            line = list(color = '#ccc', width = 0.5),\n            legendgroup = \"Bollinger Bands\", inherit = F,\n            showlegend = FALSE, hoverinfo = \"none\") \nfig <- fig %>% add_lines(x = ~Date, y = ~mavg, name = \"Mv Avg\",\n            line = list(color = '#C052B3', width = 0.5),\n            hoverinfo = \"none\", inherit = F) \nfig <- fig %>% layout(yaxis = list(title = \"Price\"))\n\n# plot volume bar chart\nfig2 <- df \nfig2 <- fig2 %>% plot_ly(x=~Date, y=~GE.Volume, type='bar', name = \"GE Volume\",\n          color = ~direction, colors = c('#FDD835','#7F7F7F')) \nfig2 <- fig2 %>% layout(yaxis = list(title = \"Volume\"))\n\n# create rangeselector buttons\nrs <- list(visible = TRUE, x = 0.5, y = -0.055,\n           xanchor = 'center', yref = 'paper',\n           font = list(size = 9),\n           buttons = list(\n             list(count=1,\n                  label='RESET',\n                  step='all'),\n             list(count=3,\n                  label='3 YR',\n                  step='year',\n                  stepmode='backward'),\n             list(count=1,\n                  label='1 YR',\n                  step='year',\n                  stepmode='backward'),\n             list(count=1,\n                  label='1 MO',\n                  step='month',\n                  stepmode='backward')\n           ))\n\n# subplot with shared x axis\nfig <- subplot(fig, fig2, heights = c(0.7,0.2), nrows=2,\n             shareX = TRUE, titleY = TRUE)\nfig <- fig %>% layout(title = paste(\"General Electric  Stock Price: January 2010 - March 2023\"),\n         xaxis = list(rangeselector = rs),\n         legend = list(orientation = 'h', x = 0.5, y = 1,\n                       xanchor = 'center', yref = 'paper',\n                       font = list(size = 10),\n                       bgcolor = 'transparent'))\n\nfig\n\n\n\n\n\n\n\n\n\n\nCode\nlog(data.info$`GE.Adjusted`) %>% diff() %>% chartSeries(theme=chartTheme('white'),up.col='#FDD835')\n\n\n\n\n\n\n\n\n\nCode\n#import the data\ngdp <- read.csv(\"DATA/RAW DATA/gdp-growth.csv\")\n\n#change date format\ngdp$Date <- as.Date(gdp$DATE , \"%m/%d/%Y\")\n\n#drop DATE column\ngdp <- subset(gdp, select = -c(1))\n\n#export the cleaned data\ngdp_clean <- gdp\nwrite.csv(gdp_clean, \"DATA/CLEANED DATA/gdp_clean_data.csv\", row.names=FALSE)\n\n#plot gdp growth rate \nfig <- plot_ly(gdp, x = ~Date, y = ~value, type = 'scatter', mode = 'lines',line = list(color = 'rgb(240, 128, 128)'))\nfig <- fig %>% layout(title = \"U.S GPD Growth Rate: 2010 - 2022\",xaxis = list(title = \"Time\"),yaxis = list(title =\"GDP Growth Rate\"))\nfig\n\n\n\n\n\n\n\n\n\n\nCode\n#import the data\ninflation_rate <- read.csv(\"DATA/RAW DATA/inflation-rate.csv\")\n\n#cleaning the data\n#remove unwanted columns\ninflation_rate_clean <- subset(inflation_rate, select = -c(1,HALF1,HALF2))\n\n#convert the data to time series data\ninflation_data_ts <- ts(as.vector(t(as.matrix(inflation_rate_clean))), start=c(2010,1), end=c(2023,2), frequency=12)\n\n#export the data\nwrite.csv(inflation_rate_clean, \"DATA/CLEANED DATA/inflation_rate_clean_data.csv\", row.names=FALSE)\n\n\n#plot inflation rate \nfig <- autoplot(inflation_data_ts, ylab = \"Inflation Rate\", color=\"#FFA07A\")+ggtitle(\"U.S Inflation Rate: January 2010 - February 2023\")+theme_bw()\nggplotly(fig)\n\n\n\n\n\n\n\n\n\n\nCode\n#import the data\ninterest_data <- read.csv(\"DATA/RAW DATA/interest-rate.csv\")\n\n#change date format\ninterest_data$Date <- as.Date(interest_data$Date , \"%m/%d/%Y\")\n\n#export the cleaned data\ninterest_clean_data <- interest_data\nwrite.csv(interest_clean_data, \"DATA/CLEANED DATA/interest_rate_clean_data.csv\", row.names=FALSE)\n\n#plot interest rate \nfig <- plot_ly(interest_data, x = ~Date, y = ~value, type = 'scatter', mode = 'lines',line = list(color='rgb(219, 112, 147)'))\nfig <- fig %>% layout(title = \"U.S Interest Rate: January 2010 - March 2023\",xaxis = list(title = \"Time\"),yaxis = list(title =\"Interest Rate\"))\nfig\n\n\n\n\n\n\n\n\n\n\nCode\n#import the data\nunemployment_rate <- read.csv(\"DATA/RAW DATA/unemployment-rate.csv\")\n\n#change date format\nunemployment_rate$Date <- as.Date(unemployment_rate$Date , \"%m/%d/%Y\")\n\n# export the data\nwrite.csv(unemployment_rate, \"DATA/CLEANED DATA/unemployment_rate_clean_data.csv\", row.names=FALSE)\n\n#plot unemployment rate \nfig <- plot_ly(unemployment_rate, x = ~Date, y = ~Value, type = 'scatter', mode = 'lines',line = list(color = 'rgb(189, 183, 107)'))\nfig <- fig %>% layout(title = \"U.S Unemployment Rate: January 2010 - March 2023\",xaxis = list(title = \"Time\"),yaxis = list(title =\"Unemployment Rate\"))\nfig\n\n\n\n\n\n\n\n\n\nFrom 2010 to 2014, GE experienced a steady increase in its business operations due to strong financial performance and consistent dividend payouts. However, from 2014 to 2016, the company faced challenges such as slowing sales growth and increased competition in the consumer goods industry, resulting in a decline in its business operations.\nIn response, GE began to streamline its operations and focus on core brands, which helped the company to recover from 2016 to 2018. This trend continued into 2019, with GE reaching an all-time high in mid-2019. However, the COVID-19 pandemic caused a brief dip in the company’s business operations in 2020, and GE’s stock price experienced some volatility in early 2021 due to global economic uncertainty and fluctuations in consumer demand for the company’s products.\nSince early 2021, General Electric ’s stock price has experienced some volatility, likely due to a combination of factors such as global economic uncertainty and fluctuations in consumer demand for the company’s products.\nAs discussed before, the macroeconomic factors of GDP growth, inflation, interest rates, and unemployment rate are closely interrelated and play a crucial role in the overall health and stability of an economy. From 2010 to 2023, the global economy experienced a mix of GE and downs, with periods of strong GDP growth followed by slowdowns and recessions.\nThe second plot shows the first difference of the logarithm of the adjusted General Electric stock price. Taking the first difference removes any long-term trends and transforms the time series into a stationary process. From the plot, we can observe that the first difference of the logarithm of the General Electric stock price appears to be stationary, as the mean and variance are roughly constant over time.\n\n\nEnodogenous and Exogenous Variables\n\nPlotCorrelation HeatmapCCF GDPCCF InterestCCF InflationCCF Unemployment\n\n\n\n\nCode\nnumeric_data <- c(\"GE.Adjusted\",\"gdp\", \"interest\", \"inflation\", \"unemployment\")\nnumeric_data <- final[, numeric_data]\nnormalized_data_numeric <- scale(numeric_data)\nnormalized_data <- ts(normalized_data_numeric, start = c(2010, 1), end = c(2021,10),frequency = 4)\nts_plot(normalized_data,\n        title = \"Normalized Time Series Data for GE Stock and Macroeconomic Variables\",\n        Ytitle = \"Normalized Values\",\n        Xtitle = \"Year\")\n\n\n\n\n\n\n\n\n\n\nCode\n# Get upper triangle of the correlation matrix\nget_upper_tri <- function(cormat){\n    cormat[lower.tri(cormat)]<- NA\n    return(cormat)\n}\ncormat <- round(cor(normalized_data_numeric),2)\n\nupper_tri <- get_upper_tri(cormat)\n\nmelted_cormat <- melt(upper_tri, na.rm = TRUE)\n# Create a ggheatmap\nggheatmap <- ggplot(melted_cormat, aes(Var2, Var1, fill = value))+\n geom_tile(color = \"white\")+\n scale_fill_gradient2(low = \"blue\", high = \"red\", mid = \"white\", \n   midpoint = 0, limit = c(-1,1), space = \"Lab\", \n    name=\"Pearson\\nCorrelation\") +\n  theme_minimal()+ # minimal theme\n theme(axis.text.x = element_text(angle = 45, vjust = 1, \n    size = 12, hjust = 1))+\n coord_fixed()\n\nggheatmap + \ngeom_text(aes(Var2, Var1, label = value), color = \"black\", size = 4) +\ntheme(\n  axis.title.x = element_blank(),\n  axis.title.y = element_blank(),\n  panel.grid.major = element_blank(),\n  panel.border = element_blank(),\n  panel.background = element_blank(),\n  axis.ticks = element_blank(),\n  legend.justification = c(1, 0),\n  legend.position = c(0.6, 0.7),\n  legend.direction = \"horizontal\")+\n  guides(fill = guide_colorbar(barwidth = 7, barheight = 1,\n                title.position = \"top\", title.hjust = 0.5))\n\n\n\n\n\n\n\n\n\nCode\npar(mfrow=c(1,1))\nccf_result <- ccf(normalized_data[, c(\"GE.Adjusted\")], normalized_data[, c(\"gdp\")], \n    lag.max = 300,\n    main = \"Cros-Correlation Plot for GE Stock Price and GDP Growth Rate \",\n    ylab = \"CCF\")\n\n\n\n\n\nCode\ncat(\"The sum of cross correlation function is\", sum(abs(ccf_result$acf)))\n\n\nThe sum of cross correlation function is 5.53155\n\n\n\n\n\n\nCode\npar(mfrow=c(1,1))\nccf_result <- ccf(normalized_data[, c(\"GE.Adjusted\")], normalized_data[, c(\"interest\")], \n    lag.max = 300,\n    main = \"Cros-Correlation Plot for GE Stock Price and Interest Rate\",\n    ylab = \"CCF\")\n\n\n\n\n\nCode\ncat(\"The sum of cross correlation function is\", sum(abs(ccf_result$acf)))\n\n\nThe sum of cross correlation function is 12.52537\n\n\n\n\n\n\nCode\npar(mfrow=c(1,1))\nccf_result <- ccf(normalized_data[, c(\"GE.Adjusted\")], normalized_data[, c(\"inflation\")], \n    lag.max = 300,\n    main = \"Cros-Correlation Plot for GE Stock Price and Inflation Rate\",\n    ylab = \"CCF\")\n\n\n\n\n\nCode\ncat(\"The sum of cross correlation function is\", sum(abs(ccf_result$acf)))\n\n\nThe sum of cross correlation function is 16.47864\n\n\n\n\n\n\nCode\npar(mfrow=c(1,1))\nccf_result <- ccf(normalized_data[, c(\"GE.Adjusted\")], normalized_data[, c(\"unemployment\")], \n    lag.max = 300,\n    main = \"Cros-Correlation Plot for GE Stock Priceand Unemployment Rate\",\n    ylab = \"CCF\")\n\n\n\n\n\nCode\ncat(\"The sum of cross correlation function is\", sum(abs(ccf_result$acf)))\n\n\nThe sum of cross correlation function is 19.28415\n\n\n\n\n\nThe Normalized Time Series Data for Stock Price and Macroeconomic Variables plot shows the same variables as the first plot but has been normalized to a common range of 0 to 1 using the scale() function in R, which standardizes the variables to have a mean of 0 and a standard deviation of 1. The heatmap analysis of the normalized data reveals that inflation and unemployment rate exhibit strong positive correlations with the stock price indices, indicating that these variables may significantly influence stock price movements. On the other hand, weaker correlations were observed between the stock price indices and GDP and interest rates, suggesting that these variables may have less impact on stock price fluctuations. The cross-correlation feature plots confirm these findings, indicating that inflation and unemployment rate are more suitable feature variables for the ARIMAX model when predicting General Electric movements.\nFinal Exogenous variables: Macroeconomic indicators: Inflation rate and unemployment rate.\n\n\nEnodogenous and Exogenous Variables Plot\n\nPlotCheck the stationarity\n\n\n\n\nCode\nfinal_data <- final %>%dplyr::select( Date,GE.Adjusted, inflation,unemployment)\nnumeric_data <- c(\"GE.Adjusted\", \"inflation\",\"unemployment\")\nnumeric_data <- final_data[, numeric_data]\nnormalized_data_numeric <- scale(numeric_data)\nnormalized_numeric_df <- data.frame(normalized_data_numeric)\nnormalized_data_ts <- ts(normalized_data_numeric, start = c(2010, 1), frequency = 4)\n\nautoplot(normalized_data_ts, facets=TRUE) +\n  xlab(\"Year\") + ylab(\"\") +\n  ggtitle(\"General Electric  Stock Price, Inflation Rate and Unemployment Rate in USA 2010-2023\")\n\n\n\n\n\n\n\n\n\nCode\n# Convert your multivariate time series data to a matrix\nfinal_data_ts_multivariate <- as.matrix(normalized_data_ts)\n\n# Check for stationarity using Phillips-Perron test\nphillips_perron_test <- ur.pp(final_data_ts_multivariate)  \nsummary(phillips_perron_test)\n\n\n\n################################## \n# Phillips-Perron Unit Root Test # \n################################## \n\nTest regression with intercept \n\n\nCall:\nlm(formula = y ~ y.l1)\n\nResiduals:\n    Min      1Q  Median      3Q     Max \n-1.3707 -0.1831 -0.0486  0.1363  4.4969 \n\nCoefficients:\n              Estimate Std. Error t value Pr(>|t|)    \n(Intercept) -0.0006863  0.0412911  -0.017    0.987    \ny.l1         0.8584440  0.0417094  20.582   <2e-16 ***\n---\nSignif. codes:  0 '***' 0.001 '**' 0.01 '*' 0.05 '.' 0.1 ' ' 1\n\nResidual standard error: 0.5141 on 153 degrees of freedom\nMultiple R-squared:  0.7347,    Adjusted R-squared:  0.7329 \nF-statistic: 423.6 on 1 and 153 DF,  p-value: < 2.2e-16\n\n\nValue of test-statistic, type: Z-alpha  is: -22.1435 \n\n         aux. Z statistics\nZ-tau-mu           -0.0164\n\n\n\n\n\nThe results of the Phillips-Perron unit root test indicate strong evidence against the null hypothesis of a unit root, as the p-value for the coefficient of the lagged variable is less than the significance level of 0.05. This suggests that the variable y, which is being tested for stationarity, is likely stationary. Furthermore, the test statistic Z-tau-mu is 0.0164, which is smaller than the critical value of Z-alpha (-22.1435), providing further evidence of stationarity.\nTo determine whether the linear model requires an ARCH model, an ARCH test is conducted. The ACF and PACF plots are also used to identify suitable model values.\n\n\nModel Fitting\n\nPlotARCH TestACF PlotPACF Plot\n\n\n\n\nCode\nnormalized_numeric_df$GE.Adjusted<-ts(normalized_numeric_df$GE.Adjusted,star=decimal_date(as.Date(\"2010-01-01\",format = \"%Y-%m-%d\")),frequency = 4)\nnormalized_numeric_df$inflation<-ts(normalized_numeric_df$inflation,star=decimal_date(as.Date(\"2010-01-01\",format = \"%Y-%m-%d\")),frequency = 4)\nnormalized_numeric_df$unemployment<-ts(normalized_numeric_df$unemployment,star=decimal_date(as.Date(\"2010-01-01\",format = \"%Y-%m-%d\")),frequency = 4)\n\nfit <- lm(GE.Adjusted ~ inflation+unemployment, data=normalized_numeric_df)\nfit.res<-ts(residuals(fit),star=decimal_date(as.Date(\"2010-01-01\",format = \"%Y-%m-%d\")),frequency = 4)\n############## Then look at the residuals ############\nreturns <- fit.res  %>% diff()\nautoplot(returns)+ggtitle(\"Linear Model Returns\")\n\n\n\n\n\n\n\n\n\nCode\nbyd.archTest <- ArchTest(fit.res, lags = 1, demean = TRUE)\nbyd.archTest\n\n\n\n    ARCH LM-test; Null hypothesis: no ARCH effects\n\ndata:  fit.res\nChi-squared = 25.401, df = 1, p-value = 4.657e-07\n\n\n\n\n\n\nCode\nggAcf(returns) +ggtitle(\"ACF for returns\")\n\n\n\n\n\n\n\n\n\nCode\nggPacf(returns) +ggtitle(\"PACF for returns\")\n\n\n\n\n\n\n\n\nThe ARCH LM-test was conducted with the null hypothesis of no ARCH effects. The test resulted in a chi-squared value of 25.401 with one degree of freedom, and a very low p-value of 4.657e-07. This suggests strong evidence against the null hypothesis, indicating the presence of ARCH effects in the data.\nBased on the ACF and PACF plots, it appears that there is some significant autocorrelation and partial autocorrelation at multiple lags, which suggests that an ARIMA model may not be sufficient to capture the time series behavior. Additionally, the values for p and q appear to be relatively high, with p = 0 and q = 9 being suggested by the plots.\n\n\nARIMAX Model\n\nAuto Arima ModelAuto Arima ResidualsARIMA (0,1,9) PlotARIMA (0,1,9) ModelCross Validation\n\n\n\n\nCode\nxreg <- cbind(Inflation = normalized_data_ts[, \"inflation\"],\n              Unemployment = normalized_data_ts[, \"unemployment\"])\nfit.auto <- auto.arima(normalized_data_ts[, \"GE.Adjusted\"], xreg = xreg)\nsummary(fit.auto)\n\n\nSeries: normalized_data_ts[, \"GE.Adjusted\"] \nRegression with ARIMA(0,1,0) errors \n\nCoefficients:\n      Inflation  Unemployment\n         0.0262       -0.1293\ns.e.     0.1538        0.0698\n\nsigma^2 = 0.1252:  log likelihood = -18.35\nAIC=42.71   AICc=43.22   BIC=48.5\n\nTraining set error measures:\n                      ME     RMSE       MAE       MPE     MAPE      MASE\nTraining set -0.01726622 0.343433 0.2723993 -30.29885 65.14651 0.4320897\n                   ACF1\nTraining set 0.09578302\n\n\n\n\n\n\nCode\ncheckresiduals(fit.auto)\n\n\n\n\n\n\n    Ljung-Box test\n\ndata:  Residuals from Regression with ARIMA(0,1,0) errors\nQ* = 4.2215, df = 8, p-value = 0.8366\n\nModel df: 0.   Total lags used: 8\n\n\n\n\n\n\nCode\nset.seed(1234)\n\nmodel_output <- capture.output(sarima(fit.res,0,1,9)) \n\n\n\n\n\n\n\n\n\nCode\ncat(model_output[124:160], model_output[length(model_output)], sep = \"\\n\")\n\n\n\nCall:\narima(x = xdata, order = c(p, d, q), seasonal = list(order = c(P, D, Q), period = S), \n    xreg = constant, transform.pars = trans, fixed = fixed, optim.control = list(trace = trc, \n        REPORT = 1, reltol = tol))\n\nCoefficients:\n         ma1      ma2     ma3     ma4      ma5      ma6     ma7      ma8\n      0.0971  -0.0973  0.0659  0.2270  -0.2299  -0.1028  0.2258  -0.3203\ns.e.  0.1525   0.1693  0.1704  0.1934   0.1897   0.1696  0.1777   0.1800\n          ma9  constant\n      -0.8656   -0.0174\ns.e.   0.1923    0.0208\n\nsigma^2 estimated as 0.08399:  log likelihood = -16.28,  aic = 54.56\n\n$degrees_of_freedom\n[1] 41\n\n$ttable\n         Estimate     SE t.value p.value\nma1        0.0971 0.1525  0.6369  0.5277\nma2       -0.0973 0.1693 -0.5748  0.5686\nma3        0.0659 0.1704  0.3868  0.7009\nma4        0.2270 0.1934  1.1737  0.2473\nma5       -0.2299 0.1897 -1.2118  0.2325\nma6       -0.1028 0.1696 -0.6058  0.5480\nma7        0.2258 0.1777  1.2709  0.2109\nma8       -0.3203 0.1800 -1.7802  0.0825\nma9       -0.8656 0.1923 -4.5006  0.0001\nconstant  -0.0174 0.0208 -0.8327  0.4098\n\n$AIC\n[1] 1.06981\n\n$AICc\n[1] 1.177653\n\n\n\n\n\n\nCode\nn=length(fit.res)\nk= 51\n \n \nrmse1 <- matrix(NA, (n-k),4)\nrmse2 <- matrix(NA, (n-k),4)\nrmse3 <- matrix(NA, (n-k),4)\n\n\nst <- tsp(fit.res)[1]+(k-5)/4 \n\nfor(i in 1:(n-k))\n{\n  xtrain <- window(fit.res, end=st + i/4)\n  xtest <- window(fit.res, start=st + (i+1)/4, end=st + (i+4)/4)\n  \n  #ARIMA(0,1,0) ARIMA(1,1,1)\n  \n  fit <- Arima(xtrain, order=c(0,1,0),\n                include.drift=TRUE, method=\"ML\")\n  fcast <- forecast(fit, h=4)\n  \n  fit2 <- Arima(xtrain, order=c(0,1,9),\n                include.drift=TRUE, method=\"ML\")\n  fcast2 <- forecast(fit2, h=4)\n\n\n  rmse1[i,1:length(xtest)]   <- sqrt((fcast$mean-xtest)^2)\n  rmse2[i,1:length(xtest)] <- sqrt((fcast2$mean-xtest)^2)\n}\n\nplot(1:4,colMeans(rmse1,na.rm=TRUE), type=\"l\",col=2, xlab=\"horizon\", ylab=\"RMSE\")\nlines(1:4, colMeans(rmse2,na.rm=TRUE), type=\"l\",col=3)\nlegend(\"topleft\",legend=c(\"fit1\",\"fit2\"),col=2:4,lty=1)\n\n\n\n\n\n\n\n\nThe auto.arima function suggests the ARIMA(0,1,0) model, but the acf and pacf plots suggest a simpler ARIMA(0,1,9) model. Upon comparison, the AIC, BIC values and the RMSE values of the ARIMA(0,1,0) model are much lower than the other mode, indicating that it is the better choice.\nWe can then proceed to choose the best GARCH model using ARIMA(0,1,0) as the base model.\n\n\nSquared Residuals\n\nPlotACF PlotPACF Plot\n\n\n\n\nCode\nfit <- lm(GE.Adjusted ~ inflation+unemployment, data=normalized_numeric_df)\nfit.res<-ts(residuals(fit),star=decimal_date(as.Date(\"2010-01-01\",format = \"%Y-%m-%d\")),frequency = 4)\nfit <- Arima(fit.res,order=c(0,1,0))\nres=fit$res\nplot(res^2,main='Squared Residuals')\n\n\n\n\n\n\n\n\n\nCode\nacf(res^2,24, main = \"ACF Residuals Square\")\n\n\n\n\n\n\n\n\n\nCode\npacf(res^2,24, main = \"PACF Residuals Square\")\n\n\n\n\n\n\n\n\nFrom the squared residuals of the best ARIMA model, it can be observed that the ACF plot and PACF plot indicate that the residuals are not autocorrelated and are white noise, indicating a good fit of the model. Based on the squared residuals of the best ARIMA model, we can see that the ACF and PACF plots indicate that most of the values lie between the blue lines. Additionally, the p-value is 2 and q-value is 2. This suggests that the model has a good fit and that there is no significant autocorrelation or partial autocorrelation in the residuals. Now we can proceed by fitting GARCH Model for p and q values.\n\n\nGARCH Model\n\nModelGRACH(1,1)GRACH(2,1)GRACH(1,2)\n\n\n\n\nCode\nmodel <- list() ## set counter\ncc <- 1\nfor (p in 1:2) {\n  for (q in 1:2) {\n  \nmodel[[cc]] <- garch(res,order=c(q,p),trace=F)\ncc <- cc + 1\n}\n} \n\n## get AIC values for model evaluation\nGARCH_AIC <- sapply(model, AIC) ## model with lowest AIC is the best\nwhich(GARCH_AIC == min(GARCH_AIC))\n\n\n[1] 1\n\n\nCode\nmodel[[which(GARCH_AIC == min(GARCH_AIC))]]\n\n\n\nCall:\ngarch(x = res, order = c(q, p), trace = F)\n\nCoefficient(s):\n       a0         a1         b1  \n1.080e-01  4.104e-01  1.072e-14  \n\n\n\n\n\n\nCode\nsummary(garchFit(~garch(1,1),res, trace=F))\n\n\n\nTitle:\n GARCH Modelling \n\nCall:\n garchFit(formula = ~garch(1, 1), data = res, trace = F) \n\nMean and Variance Equation:\n data ~ garch(1, 1)\n<environment: 0x148b87ee8>\n [data = res]\n\nConditional Distribution:\n norm \n\nCoefficient(s):\n       mu      omega     alpha1      beta1  \n0.0026982  0.0219391  0.1528910  0.7287887  \n\nStd. Errors:\n based on Hessian \n\nError Analysis:\n        Estimate  Std. Error  t value Pr(>|t|)  \nmu      0.002698    0.056443    0.048   0.9619  \nomega   0.021939    0.033878    0.648   0.5173  \nalpha1  0.152891    0.161894    0.944   0.3450  \nbeta1   0.728789    0.314483    2.317   0.0205 *\n---\nSignif. codes:  0 '***' 0.001 '**' 0.01 '*' 0.05 '.' 0.1 ' ' 1\n\nLog Likelihood:\n -26.01023    normalized:  -0.5001967 \n\nDescription:\n Sat Jan  6 19:55:44 2024 by user:  \n\n\nStandardised Residuals Tests:\n                                Statistic p-Value  \n Jarque-Bera Test   R    Chi^2  0.4666824 0.7918834\n Shapiro-Wilk Test  R    W      0.9819011 0.6096038\n Ljung-Box Test     R    Q(10)  12.73464  0.2388891\n Ljung-Box Test     R    Q(15)  15.02448  0.4496551\n Ljung-Box Test     R    Q(20)  18.2661   0.5698836\n Ljung-Box Test     R^2  Q(10)  15.00172  0.1319993\n Ljung-Box Test     R^2  Q(15)  18.03358  0.2609042\n Ljung-Box Test     R^2  Q(20)  23.85804  0.2486481\n LM Arch Test       R    TR^2   10.46567  0.5751782\n\nInformation Criterion Statistics:\n     AIC      BIC      SIC     HQIC \n1.154240 1.304335 1.143494 1.211783 \n\n\n\n\n\n\nCode\nsummary(garchFit(~garch(2,1),res, trace=F))\n\n\n\nTitle:\n GARCH Modelling \n\nCall:\n garchFit(formula = ~garch(2, 1), data = res, trace = F) \n\nMean and Variance Equation:\n data ~ garch(2, 1)\n<environment: 0x13fb35128>\n [data = res]\n\nConditional Distribution:\n norm \n\nCoefficient(s):\n        mu       omega      alpha1      alpha2       beta1  \n0.00269818  0.02535776  0.16597577  0.00000001  0.69408610  \n\nStd. Errors:\n based on Hessian \n\nError Analysis:\n        Estimate  Std. Error  t value Pr(>|t|)  \nmu     2.698e-03   5.762e-02    0.047   0.9627  \nomega  2.536e-02   4.246e-02    0.597   0.5503  \nalpha1 1.660e-01   2.491e-01    0.666   0.5052  \nalpha2 1.000e-08   2.846e-01    0.000   1.0000  \nbeta1  6.941e-01   3.998e-01    1.736   0.0825 .\n---\nSignif. codes:  0 '***' 0.001 '**' 0.01 '*' 0.05 '.' 0.1 ' ' 1\n\nLog Likelihood:\n -26.07056    normalized:  -0.5013568 \n\nDescription:\n Sat Jan  6 19:55:44 2024 by user:  \n\n\nStandardised Residuals Tests:\n                                Statistic p-Value  \n Jarque-Bera Test   R    Chi^2  0.4948671 0.7808021\n Shapiro-Wilk Test  R    W      0.9819355 0.6111646\n Ljung-Box Test     R    Q(10)  13.05933  0.2203734\n Ljung-Box Test     R    Q(15)  15.32623  0.4281834\n Ljung-Box Test     R    Q(20)  18.53299  0.5523415\n Ljung-Box Test     R^2  Q(10)  15.58891  0.1120207\n Ljung-Box Test     R^2  Q(15)  18.56009  0.2343648\n Ljung-Box Test     R^2  Q(20)  24.51462  0.220635 \n LM Arch Test       R    TR^2   10.70846  0.5540547\n\nInformation Criterion Statistics:\n     AIC      BIC      SIC     HQIC \n1.195021 1.382641 1.178604 1.266950 \n\n\n\n\n\n\nCode\nsummary(garchFit(~garch(1,2),res, trace=F))\n\n\n\nTitle:\n GARCH Modelling \n\nCall:\n garchFit(formula = ~garch(1, 2), data = res, trace = F) \n\nMean and Variance Equation:\n data ~ garch(1, 2)\n<environment: 0x11d50c518>\n [data = res]\n\nConditional Distribution:\n norm \n\nCoefficient(s):\n        mu       omega      alpha1       beta1       beta2  \n0.00269818  0.04213930  0.30074757  0.00000001  0.46607609  \n\nStd. Errors:\n based on Hessian \n\nError Analysis:\n        Estimate  Std. Error  t value Pr(>|t|)    \nmu     2.698e-03   3.484e-02    0.077 0.938266    \nomega  4.214e-02   5.345e-02    0.788 0.430434    \nalpha1 3.007e-01   8.078e-02    3.723 0.000197 ***\nbeta1  1.000e-08         NaN      NaN      NaN    \nbeta2  4.661e-01   3.142e-01    1.484 0.137934    \n---\nSignif. codes:  0 '***' 0.001 '**' 0.01 '*' 0.05 '.' 0.1 ' ' 1\n\nLog Likelihood:\n -25.62945    normalized:  -0.4928741 \n\nDescription:\n Sat Jan  6 19:55:44 2024 by user:  \n\n\nStandardised Residuals Tests:\n                                Statistic p-Value  \n Jarque-Bera Test   R    Chi^2  0.6040771 0.7393096\n Shapiro-Wilk Test  R    W      0.984106  0.7108861\n Ljung-Box Test     R    Q(10)  13.05999  0.2203371\n Ljung-Box Test     R    Q(15)  15.24153  0.4341636\n Ljung-Box Test     R    Q(20)  18.46496  0.5568079\n Ljung-Box Test     R^2  Q(10)  14.60171  0.1472715\n Ljung-Box Test     R^2  Q(15)  17.70743  0.2783592\n Ljung-Box Test     R^2  Q(20)  22.47545  0.3152782\n LM Arch Test       R    TR^2   10.36302  0.5841457\n\nInformation Criterion Statistics:\n     AIC      BIC      SIC     HQIC \n1.178056 1.365675 1.161639 1.249985 \n\n\n\n\n\nBased on the analysis of the different GARCH models, it appears that GARCH(1,1) is the optimal choice. Although the AIC values of the different models are relatively similar, we can further evaluate their significance to make a final determination. Upon closer inspection, it appears that GARCH(1,1) has significantly better values than the other models, indicating that it is the most appropriate choice. Therefore, we can conclude that the GARCH(1,1) model is the best fit for the data.\n\n\nBest Model\n\nARIMA ModelGARCH ModelVolatility\n\n\n\n\nCode\n#fiting an ARIMA model to the Inflation variable\ninflation_fit<-auto.arima(normalized_numeric_df$inflation) \nfinflation<-forecast(inflation_fit)\n\n#fitting an ARIMA model to the Unemployment variable\nunemployment_fit<-auto.arima(normalized_numeric_df$unemployment) \nfunemployment<-forecast(unemployment_fit)\n\n# best model fit for forcasting\nxreg <- cbind(Inflation = normalized_data_ts[, \"inflation\"],\n              Unemployment = normalized_data_ts[, \"unemployment\"])\n\n\n\nsummary(arima.fit<-Arima(normalized_data_ts[, \"GE.Adjusted\"],order=c(0,1,0),xreg=xreg),include.drift = TRUE)\n\n\nSeries: normalized_data_ts[, \"GE.Adjusted\"] \nRegression with ARIMA(0,1,0) errors \n\nCoefficients:\n      Inflation  Unemployment\n         0.0262       -0.1293\ns.e.     0.1538        0.0698\n\nsigma^2 = 0.1252:  log likelihood = -18.35\nAIC=42.71   AICc=43.22   BIC=48.5\n\nTraining set error measures:\n                      ME     RMSE       MAE       MPE     MAPE      MASE\nTraining set -0.01726622 0.343433 0.2723993 -30.29885 65.14651 0.4320897\n                   ACF1\nTraining set 0.09578302\n\n\n\n\n\n\nCode\nsummary(final.fit <- garchFit(~garch(1,1), res,trace = F))\n\n\n\nTitle:\n GARCH Modelling \n\nCall:\n garchFit(formula = ~garch(1, 1), data = res, trace = F) \n\nMean and Variance Equation:\n data ~ garch(1, 1)\n<environment: 0x13b3483b0>\n [data = res]\n\nConditional Distribution:\n norm \n\nCoefficient(s):\n       mu      omega     alpha1      beta1  \n0.0026982  0.0219391  0.1528910  0.7287887  \n\nStd. Errors:\n based on Hessian \n\nError Analysis:\n        Estimate  Std. Error  t value Pr(>|t|)  \nmu      0.002698    0.056443    0.048   0.9619  \nomega   0.021939    0.033878    0.648   0.5173  \nalpha1  0.152891    0.161894    0.944   0.3450  \nbeta1   0.728789    0.314483    2.317   0.0205 *\n---\nSignif. codes:  0 '***' 0.001 '**' 0.01 '*' 0.05 '.' 0.1 ' ' 1\n\nLog Likelihood:\n -26.01023    normalized:  -0.5001967 \n\nDescription:\n Sat Jan  6 19:55:44 2024 by user:  \n\n\nStandardised Residuals Tests:\n                                Statistic p-Value  \n Jarque-Bera Test   R    Chi^2  0.4666824 0.7918834\n Shapiro-Wilk Test  R    W      0.9819011 0.6096038\n Ljung-Box Test     R    Q(10)  12.73464  0.2388891\n Ljung-Box Test     R    Q(15)  15.02448  0.4496551\n Ljung-Box Test     R    Q(20)  18.2661   0.5698836\n Ljung-Box Test     R^2  Q(10)  15.00172  0.1319993\n Ljung-Box Test     R^2  Q(15)  18.03358  0.2609042\n Ljung-Box Test     R^2  Q(20)  23.85804  0.2486481\n LM Arch Test       R    TR^2   10.46567  0.5751782\n\nInformation Criterion Statistics:\n     AIC      BIC      SIC     HQIC \n1.154240 1.304335 1.143494 1.211783 \n\n\n\n\n\n\nCode\nht <- final.fit@h.t #a numeric vector with the conditional variances (h.t = sigma.t^delta)\n\n#############################\ndata=data.frame(final)\ndata$Date<-as.Date(data$Date,\"%Y-%m-%d\")\n\n\ndata2= data.frame(ht,data$Date)\nggplot(data2, aes(y = ht, x = data.Date)) + geom_line(col = '#FDD835') + ylab('Conditional Variance') + xlab('Date')\n\n\n\n\n\n\n\n\nFrom the ARIMA(0,1,0), we see that the training set error measures also suggest a good fit, with low mean absolute error, root mean squared error, and autocorrelation of the residuals. GATCH(1,1) model model is used to estimate the volatility of the standardized residuals of the previous regression model. The model includes a mean equation that estimates the mean of the residuals and a variance equation that models the conditional variance of the residuals. The coefficients of the mean equation suggest that the mean of the residuals is close to zero. The variance equation coefficients suggest that the conditional variance of the residuals is dependent on the past conditional variances and the past squared standardized residuals. The model’s AIC, BIC, SIC, and HQIC values are all relatively low, indicating a good fit of the model. The standardized residuals tests indicate that the residuals are approximately normally distributed and that there is no significant autocorrelation in the residuals.\nThe volatility of the model seems high in 2020 but has decreased gradually in the past few months. This could indicate that the asset’s price was experiencing a lot of fluctuations in 2020, but the market has stabilized recently.\n\n\nModel Diagnostics\n\nResidualsQQ PlotBox Test\n\n\n\n\nCode\nfit2<-garch(res,order=c(1,1),trace=F)\ncheckresiduals(fit2) \n\n\n\n\n\n\n\n\n\nCode\nqqnorm(fit2$residuals, pch = 1)\nqqline(fit2$residuals, col = \"blue\", lwd = 2)\n\n\n\n\n\n\n\n\n\nCode\nBox.test (fit2$residuals, type = \"Ljung\")\n\n\n\n    Box-Ljung test\n\ndata:  fit2$residuals\nX-squared = 0.27111, df = 1, p-value = 0.6026\n\n\nThe ACF plot of the residuals shows all the values between the blue lines, which indicates that the residuals are not significantly autocorrelated. The range of values for the residual plot between -2 and 2 is considered acceptable. Additionally, the QQ plot of the residuals shows a linear plot on the line, which is another good indication that the residuals are normally distributed. The QQ plot is a valuable tool to assess if the residuals follow a normal distribution, and in this case, the plot suggests that the residuals do indeed follow a normal distribution.\nThe Box-Ljung test, a p-value of 0.6026 indicates that the model’s residuals are not significantly autocorrelated, meaning that the model has captured most of the information in the data. This result is good because it suggests that the model is a good fit for the data and has accounted for most of the underlying patterns in the data. Therefore, we can rely on the model’s predictions and use them to make informed decisions.\n\n\n\n\n\nForecast\n\n\nCode\npredict(final.fit, n.ahead = 5, plot=TRUE)\n\n\n\n\n\n  meanForecast meanError standardDeviation lowerInterval upperInterval\n1  0.002698178 0.4377440         0.4377440    -0.8552644     0.8606607\n2  0.002698178 0.4369055         0.4369055    -0.8536209     0.8590172\n3  0.002698178 0.4361648         0.4361648    -0.8521692     0.8575655\n4  0.002698178 0.4355108         0.4355108    -0.8508872     0.8562836\n5  0.002698178 0.4349332         0.4349332    -0.8497553     0.8551517\n\n\nThe forecasted plot is based on the best model ARIMAX(0,1,0)+GARCH(1,1). This model takes into account the autoregressive and moving average components of the data, as well as the impact of exogenous variables on the time series. Additionally, the GARCH component of the model accounts for the volatility clustering in the data. Overall, this model is well-suited to make accurate predictions about future values of the time series.\n\n\nEquation of the Model\nThe equation of the ARIMAX(0,1,0) model is:\n\\(Y(t) = Y(t-1) + \\epsilon(t)\\)\nwhere \\(Y(t)\\) is the time series variable and \\(\\epsilon(t)\\) is the error term.\nThe equation of the GARCH(1,1) model is:\n\\(\\sigma^2(t) = \\alpha_0+\\alpha_1\\epsilon_t^2(t-1)+ \\beta_1\\sigma^2(t-1)\\)\nwhere \\(\\sigma^2_t\\) is the conditional variance at time \\(t\\), \\(\\alpha_0\\) is a constant, \\(\\alpha_1\\) and \\(\\beta_1\\) are the parameters, and \\(\\epsilon_t\\) is the error term.\nThe combined equation of the ARIMAX(1,1,1)+GARCH(1,1) model is:\n\\(Y(t) = Y(t-1) + \\epsilon(t)\\)\n\\(\\epsilon(t) = \\sigma(t) * \\epsilon~(t)\\)\n\\(\\sigma^2(t) = \\alpha_0+\\alpha_1\\epsilon_t^2(t-1)+ \\beta_1\\sigma^2(t-1)\\)"
  },
  {
    "objectID": "arch_Materials_Sector_Fund.html",
    "href": "arch_Materials_Sector_Fund.html",
    "title": "",
    "section": "",
    "text": "Impact of Macroeconomic Factors on Dow Inc. Stock Price\n\n\n\nOne of the best companies in the material sector is Dow Inc. (DOW), a multinational chemical corporation that produces a wide range of products including plastics, chemicals, and agricultural products. As of 2021, Dow Inc. is ranked as one of the largest chemical producers in the world, with operations in over 160 countries.\nBy using an ARIMAX+ARCH/GARCH model to analyze the stock price behavior of Dow, we can gain insights into how macroeconomic factors impact the company’s performance. For example, an increase in GDP growth rate or a decrease in unemployment rate may lead to increased consumer spending and a rise in Dow’s stock price. Conversely, an increase in inflation or interest rates may lead to a decrease in consumer spending and a decline in Dow’s stock price.\n\n\nTime series Plot\n\nDow Inc.Differentitaed Chart SeriesGDP GrowthInflationInterestUnemployment\n\n\n\n\nCode\n# get data\noptions(\"getSymbols.warning4.0\"=FALSE)\noptions(\"getSymbols.yahoo.warning\"=FALSE)\n\n\ndata.info = getSymbols(\"DOW\",src='yahoo', from = '2020-01-01',to = \"2023-03-01\",auto.assign = FALSE)\ndata = getSymbols(\"DOW\",src='yahoo', from = '2020-01-01',to = \"2023-03-01\")\ndf <- data.frame(Date=index(DOW),coredata(DOW))\n\n# create Bollinger Bands\nbbands <- BBands(DOW[,c(\"DOW.High\",\"DOW.Low\",\"DOW.Close\")])\n\n# join and subset data\ndf <- subset(cbind(df, data.frame(bbands[,1:3])), Date >= \"2020-01-01\")\n\n\n# colors column for increasing and decreasing\nfor (i in 1:length(df[,1])) {\n  if (df$DOW.Close[i] >= df$DOW.Open[i]) {\n      df$direction[i] = 'Increasing'\n  } else {\n      df$direction[i] = 'Decreasing'\n  }\n}\n\ni <- list(line = list(color = '#DC7633'))\nd <- list(line = list(color = '#7F7F7F'))\n\n# plot candlestick chart\n\nfig <- df %>% plot_ly(x = ~Date, type=\"candlestick\",\n          open = ~DOW.Open, close = ~DOW.Close,\n          high = ~DOW.High, low = ~DOW.Low, name = \"DOW\",\n          increasing = i, decreasing = d) \nfig <- fig %>% add_lines(x = ~Date, y = ~up , name = \"B Bands\",\n            line = list(color = '#ccc', width = 0.5),\n            legendgroup = \"Bollinger Bands\",\n            hoverinfo = \"none\", inherit = F) \nfig <- fig %>% add_lines(x = ~Date, y = ~dn, name = \"B Bands\",\n            line = list(color = '#ccc', width = 0.5),\n            legendgroup = \"Bollinger Bands\", inherit = F,\n            showlegend = FALSE, hoverinfo = \"none\") \nfig <- fig %>% add_lines(x = ~Date, y = ~mavg, name = \"Mv Avg\",\n            line = list(color = '#C052B3', width = 0.5),\n            hoverinfo = \"none\", inherit = F) \nfig <- fig %>% layout(yaxis = list(title = \"Price\"))\n\n# plot volume bar chart\nfig2 <- df \nfig2 <- fig2 %>% plot_ly(x=~Date, y=~DOW.Volume, type='bar', name = \"DOW Volume\",\n          color = ~direction, colors = c('#DC7633','#7F7F7F')) \nfig2 <- fig2 %>% layout(yaxis = list(title = \"Volume\"))\n\n# create rangeselector buttons\nrs <- list(visible = TRUE, x = 0.5, y = -0.055,\n           xanchor = 'center', yref = 'paper',\n           font = list(size = 9),\n           buttons = list(\n             list(count=1,\n                  label='RESET',\n                  step='all'),\n             list(count=3,\n                  label='3 YR',\n                  step='year',\n                  stepmode='backward'),\n             list(count=1,\n                  label='1 YR',\n                  step='year',\n                  stepmode='backward'),\n             list(count=1,\n                  label='1 MO',\n                  step='month',\n                  stepmode='backward')\n           ))\n\n# subplot with shared x axis\nfig <- subplot(fig, fig2, heights = c(0.7,0.2), nrows=2,\n             shareX = TRUE, titleY = TRUE)\nfig <- fig %>% layout(title = paste(\"Dow Inc. Stock Price: January 2020 - March 2023\"),\n         xaxis = list(rangeselector = rs),\n         legend = list(orientation = 'h', x = 0.5, y = 1,\n                       xanchor = 'center', yref = 'paper',\n                       font = list(size = 10),\n                       bgcolor = 'transparent'))\n\nfig\n\n\n\n\n\n\n\n\n\n\nCode\nlog(data.info$`DOW.Adjusted`) %>% diff() %>% chartSeries(theme=chartTheme('white'),up.col='#DC7633')\n\n\n\n\n\n\n\n\n\nCode\n#import the data\ngdp <- read.csv(\"DATA/RAW DATA/gdp-growth.csv\")\n\n#change date format\ngdp$Date <- as.Date(gdp$DATE , \"%m/%d/%Y\")\n\n#drop DATE column\ngdp <- subset(gdp, select = -c(1))\n\n#export the cleaned data\ngdp_clean <- gdp\nwrite.csv(gdp_clean, \"DATA/CLEANED DATA/gdp_clean_data.csv\", row.names=FALSE)\n\n#plot gdp growth rate \nfig <- plot_ly(gdp, x = ~Date, y = ~value, type = 'scatter', mode = 'lines',line = list(color = 'rgb(240, 128, 128)'))\nfig <- fig %>% layout(title = \"U.S GPD Growth Rate: 2010 - 2022\",xaxis = list(title = \"Time\"),yaxis = list(title =\"GDP Growth Rate\"))\nfig\n\n\n\n\n\n\n\n\n\n\nCode\n#import the data\ninflation_rate <- read.csv(\"DATA/RAW DATA/inflation-rate.csv\")\n\n#cleaning the data\n#remove unwanted columns\ninflation_rate_clean <- subset(inflation_rate, select = -c(1,HALF1,HALF2))\n\n#convert the data to time series data\ninflation_data_ts <- ts(as.vector(t(as.matrix(inflation_rate_clean))), start=c(2010,1), end=c(2023,2), frequency=12)\n\n#export the data\nwrite.csv(inflation_rate_clean, \"DATA/CLEANED DATA/inflation_rate_clean_data.csv\", row.names=FALSE)\n\n\n#plot inflation rate \nfig <- autoplot(inflation_data_ts, ylab = \"Inflation Rate\", color=\"#FFA07A\")+ggtitle(\"U.S Inflation Rate: January 2020 - February 2023\")+theme_bw()\nggplotly(fig)\n\n\n\n\n\n\n\n\n\n\nCode\n#import the data\ninterest_data <- read.csv(\"DATA/RAW DATA/interest-rate.csv\")\n\n#change date format\ninterest_data$Date <- as.Date(interest_data$Date , \"%m/%d/%Y\")\n\n#export the cleaned data\ninterest_clean_data <- interest_data\nwrite.csv(interest_clean_data, \"DATA/CLEANED DATA/interest_rate_clean_data.csv\", row.names=FALSE)\n\n#plot interest rate \nfig <- plot_ly(interest_data, x = ~Date, y = ~value, type = 'scatter', mode = 'lines',line = list(color='rgb(219, 112, 147)'))\nfig <- fig %>% layout(title = \"U.S Interest Rate: January 2010 - March 2023\",xaxis = list(title = \"Time\"),yaxis = list(title =\"Interest Rate\"))\nfig\n\n\n\n\n\n\n\n\n\n\nCode\n#import the data\nunemployment_rate <- read.csv(\"DATA/RAW DATA/unemployment-rate.csv\")\n\n#change date format\nunemployment_rate$Date <- as.Date(unemployment_rate$Date , \"%m/%d/%Y\")\n\n# export the data\nwrite.csv(unemployment_rate, \"DATA/CLEANED DATA/unemployment_rate_clean_data.csv\", row.names=FALSE)\n\n#plot unemployment rate \nfig <- plot_ly(unemployment_rate, x = ~Date, y = ~Value, type = 'scatter', mode = 'lines',line = list(color = 'rgb(189, 183, 107)'))\nfig <- fig %>% layout(title = \"U.S Unemployment Rate: January 2010 - March 2023\",xaxis = list(title = \"Time\"),yaxis = list(title =\"Unemployment Rate\"))\nfig\n\n\n\n\n\n\n\n\n\nIn 2020, the COVID-19 pandemic had a significant impact on Dow Inc.’s stock price, as the company’s operations were affected by global supply chain disruptions and a decline in demand for its products. The company’s stock price reached a low point in March 2020 but began to recover gradually in the following months as markets responded positively to government stimulus measures and signs of economic recovery.\nIn 2021, Dow Inc.’s stock price continued to trend upwards, reflecting the company’s strong financial performance and positive outlook for its core businesses. The company’s focus on sustainability and innovation, as well as its strategic partnerships and acquisitions, have helped to position it for long-term growth and success.\nOverall, the stock market of Dow Inc. from 2020 to 2023 has been characterized by volatility and uncertainty, but the company’s resilience and ability to adapt to changing market conditions have allowed it to emerge as a leader in the materials science industry.\nAs discussed before, the macroeconomic factors of GDP growth, inflation, interest rates, and unemployment rate are closely interrelated and play a crucial role in the overall health and stability of an economy. From 2020 to 2023, the global economy experienced a mix of ups and downs, with periods of strong GDP growth followed by slowdowns and recessions.\nThe second plot shows the first difference of the logarithm of the adjusted Dow Inc. stock price. Taking the first difference removes any long-term trends and transforms the time series into a stationary process. From the plot, we can observe that the first difference of the logarithm of the Dow Inc. stock price appears to be stationary, as the mean and variance are roughly constant over time.\n\n\nEnodogenous and Exogenous Variables\n\nPlotCorrelation HeatmapCCF GDPCCF InterestCCF InflationCCF Unemployment\n\n\n\n\nCode\nnumeric_data <- c(\"DOW.Adjusted\",\"gdp\", \"interest\", \"inflation\", \"unemployment\")\nnumeric_data <- final[, numeric_data]\nnormalized_data_numeric <- scale(numeric_data)\nnormalized_data <- ts(normalized_data_numeric, start = c(2020, 1), end = c(2021,10),frequency = 4)\nts_plot(normalized_data,\n        title = \"Normalized Time Series Data for DOW Stock and Macroeconomic Variables\",\n        Ytitle = \"Normalized Values\",\n        Xtitle = \"Year\")\n\n\n\n\n\n\n\n\n\n\nCode\n# Get upper triangle of the correlation matrix\nget_upper_tri <- function(cormat){\n    cormat[lower.tri(cormat)]<- NA\n    return(cormat)\n}\ncormat <- round(cor(normalized_data_numeric),2)\n\nupper_tri <- get_upper_tri(cormat)\n\nmelted_cormat <- melt(upper_tri, na.rm = TRUE)\n# Create a ggheatmap\nggheatmap <- ggplot(melted_cormat, aes(Var2, Var1, fill = value))+\n geom_tile(color = \"white\")+\n scale_fill_gradient2(low = \"blue\", high = \"red\", mid = \"white\", \n   midpoint = 0, limit = c(-1,1), space = \"Lab\", \n    name=\"Pearson\\nCorrelation\") +\n  theme_minimal()+ # minimal theme\n theme(axis.text.x = element_text(angle = 45, vjust = 1, \n    size = 12, hjust = 1))+\n coord_fixed()\n\nggheatmap + \ngeom_text(aes(Var2, Var1, label = value), color = \"black\", size = 4) +\ntheme(\n  axis.title.x = element_blank(),\n  axis.title.y = element_blank(),\n  panel.grid.major = element_blank(),\n  panel.border = element_blank(),\n  panel.background = element_blank(),\n  axis.ticks = element_blank(),\n  legend.justification = c(1, 0),\n  legend.position = c(0.6, 0.7),\n  legend.direction = \"horizontal\")+\n  guides(fill = guide_colorbar(barwidth = 7, barheight = 1,\n                title.position = \"top\", title.hjust = 0.5))\n\n\n\n\n\n\n\n\n\nCode\npar(mfrow=c(1,1))\nccf_result <- ccf(normalized_data[, c(\"DOW.Adjusted\")], normalized_data[, c(\"gdp\")], \n    lag.max = 300,\n    main = \"Cros-Correlation Plot for DOW Stock Price and GDP Growth Rate \",\n    ylab = \"CCF\")\n\n\n\n\n\nCode\ncat(\"The sum of cross correlation function is\", sum(abs(ccf_result$acf)))\n\n\nThe sum of cross correlation function is 6.093042\n\n\n\n\n\n\nCode\npar(mfrow=c(1,1))\nccf_result <- ccf(normalized_data[, c(\"DOW.Adjusted\")], normalized_data[, c(\"interest\")], \n    lag.max = 300,\n    main = \"Cros-Correlation Plot for DOW Stock Price and Interest Rate\",\n    ylab = \"CCF\")\n\n\n\n\n\nCode\ncat(\"The sum of cross correlation function is\", sum(abs(ccf_result$acf)))\n\n\nThe sum of cross correlation function is 6.720415\n\n\n\n\n\n\nCode\npar(mfrow=c(1,1))\nccf_result <- ccf(normalized_data[, c(\"DOW.Adjusted\")], normalized_data[, c(\"inflation\")], \n    lag.max = 300,\n    main = \"Cros-Correlation Plot for DOW Stock Price and Inflation Rate\",\n    ylab = \"CCF\")\n\n\n\n\n\nCode\ncat(\"The sum of cross correlation function is\", sum(abs(ccf_result$acf)))\n\n\nThe sum of cross correlation function is 7.439477\n\n\n\n\n\n\nCode\npar(mfrow=c(1,1))\nccf_result <- ccf(normalized_data[, c(\"DOW.Adjusted\")], normalized_data[, c(\"unemployment\")], \n    lag.max = 300,\n    main = \"Cros-Correlation Plot for DOW Stock Priceand Unemployment Rate\",\n    ylab = \"CCF\")\n\n\n\n\n\nCode\ncat(\"The sum of cross correlation function is\", sum(abs(ccf_result$acf)))\n\n\nThe sum of cross correlation function is 5.23597\n\n\n\n\n\nThe Normalized Time Series Data for Stock Price and Macroeconomic Variables plot shows the same variables as the first plot but has been normalized to a common range of 0 to 1 using the scale() function in R, which standardizes the variables to have a mean of 0 and a standard deviation of 1. The heatmap analysis of the normalized data reveals that inflation rate exhibit strong positive correlations with the stock price indices, indicating that these variables may significantly influence stock price movements. On the other hand, weaker correlations were observed between the stock price indices and GDP and interest rates, suggesting that these variables may have less impact on stock price fluctuations. The cross-correlation feature plots confirm these findings, indicating that inflation rate are more suitable feature variables for the ARIMAX model when predicting Dow Inc. movements.\nFinal Exogenous variables: Macroeconomic indicator: Inflation rate\n\n\nEnodogenous and Exogenous Variables Plot\n\nPlotCheck the stationarity\n\n\n\n\nCode\nfinal_data <- final %>%dplyr::select( Date,DOW.Adjusted, inflation)\nnumeric_data <- c(\"DOW.Adjusted\", \"inflation\")\nnumeric_data <- final_data[, numeric_data]\nnormalized_data_numeric <- scale(numeric_data)\nnormalized_numeric_df <- data.frame(normalized_data_numeric)\nnormalized_data_ts <- ts(normalized_data_numeric, start = c(2020, 1), frequency = 4)\n\nautoplot(normalized_data_ts, facets=TRUE) +\n  xlab(\"Year\") + ylab(\"\") +\n  ggtitle(\"Dow Inc. Stock Price, Inflation Rate in USA 2020-2023\")\n\n\n\n\n\n\n\n\n\nCode\n# Convert your multivariate time series data to a matrix\nfinal_data_ts_multivariate <- as.matrix(normalized_data_ts)\n\n# Check for stationarity using Phillips-Perron test\nphillips_perron_test <- ur.pp(final_data_ts_multivariate)  \nsummary(phillips_perron_test)\n\n\n\n################################## \n# Phillips-Perron Unit Root Test # \n################################## \n\nTest regression with intercept \n\n\nCall:\nlm(formula = y ~ y.l1)\n\nResiduals:\n    Min      1Q  Median      3Q     Max \n-2.1410 -0.3354  0.1386  0.4878  0.8967 \n\nCoefficients:\n            Estimate Std. Error t value Pr(>|t|)    \n(Intercept)  0.05182    0.14270   0.363     0.72    \ny.l1         0.77281    0.15149   5.101 4.72e-05 ***\n---\nSignif. codes:  0 '***' 0.001 '**' 0.01 '*' 0.05 '.' 0.1 ' ' 1\n\nResidual standard error: 0.6832 on 21 degrees of freedom\nMultiple R-squared:  0.5534,    Adjusted R-squared:  0.5322 \nF-statistic: 26.02 on 1 and 21 DF,  p-value: 4.72e-05\n\n\nValue of test-statistic, type: Z-alpha  is: -6.0229 \n\n         aux. Z statistics\nZ-tau-mu            0.3401\n\n\n\n\n\nThe results of the Phillips-Perron unit root test indicate strong evidence against the null hypothesis of a unit root, as the p-value for the coefficient of the lagged variable is less than the significance level of 0.05. This suggests that the variable y, which is being tested for stationarity, is likely stationary. Furthermore, the test statistic Z-tau-mu is 0.3401, which is smaller than the critical value of Z-alpha (-6.0229), providing further evidence of stationarity.\nTo determine whether the linear model requires an ARCH model, an ARCH test is conducted. The ACF and PACF plots are also used to identify suitable model values.\n\n\nModel Fitting\n\nPlotARCH Test\n\n\n\n\nCode\nnormalized_numeric_df$DOW.Adjusted<-ts(normalized_numeric_df$DOW.Adjusted,star=decimal_date(as.Date(\"2020-01-01\",format = \"%Y-%m-%d\")),frequency = 4)\nnormalized_numeric_df$inflation<-ts(normalized_numeric_df$inflation,star=decimal_date(as.Date(\"2020-01-01\",format = \"%Y-%m-%d\")),frequency = 4)\n\n\nfit <- lm(DOW.Adjusted ~ inflation, data=normalized_numeric_df)\nfit.res<-ts(residuals(fit),star=decimal_date(as.Date(\"2020-01-01\",format = \"%Y-%m-%d\")),frequency = 4)\n############## Then look at the residuals ############\nreturns <- fit.res  %>% diff()\nautoplot(returns)+ggtitle(\"Linear Model Returns\")\n\n\n\n\n\n\n\n\n\nCode\nbyd.archTest <- ArchTest(fit.res, lags = 1, demean = TRUE)\nbyd.archTest\n\n\n\n    ARCH LM-test; Null hypothesis: no ARCH effects\n\ndata:  fit.res\nChi-squared = 0.37793, df = 1, p-value = 0.5387\n\n\n\n\n\nThe ARCH LM-test was conducted with the null hypothesis of no ARCH effects. The test resulted in a chi-squared value of 0.37793 with one degree of freedom, and a very low p-value of 0.5387. This suggests strong evidence for the null hypothesis, indicating the no presence of ARCH effects in the data."
  },
  {
    "objectID": "arch_Real_Estate_Sector_Fund.html",
    "href": "arch_Real_Estate_Sector_Fund.html",
    "title": "",
    "section": "",
    "text": "Impact of Macroeconomic Factors on Simon Property Group Inc Stock Price\n\n\n\nOne of the best companies in the real estate sector is Simon Property Group Inc. (SPG). Simon Property Group is a real estate investment trust (REIT) that owns and operates shopping malls, premium outlets, and mixed-use properties in the United States, Europe, and Asia. The company has a strong track record of generating steady rental income from its properties and has consistently paid out dividends to its shareholders. Additionally, Simon Property Group has a diversified portfolio of properties, which helps to mitigate risk and provide stable returns.\nBy using an ARIMAX+ARCH/GARCH model to analyze the stock price behavior ofSimon Property Group Inc., we can gain insights into how macroeconomic factors impact the company’s performance. For example, an increase in GDP growth rate or a decrease in unemployment rate may lead to increased consumer spending and a rise in Simon Property Group’s stock price. Conversely, an increase in inflation or interest rates may lead to a decrease in consumer spending and a decline in Simon Property Group’s stock price.\n\n\nTime series Plot\n\nSimon Property Group IncDifferentitaed Chart SeriesGDP GrowthInflationInterestUnemployment\n\n\n\n\nCode\n# get data\noptions(\"getSymbols.warning4.0\"=FALSE)\noptions(\"getSymbols.yahoo.warning\"=FALSE)\n\n\ndata.info = getSymbols(\"SPG\",src='yahoo', from = '2010-01-01',to = \"2023-03-01\",auto.assign = FALSE)\ndata = getSymbols(\"SPG\",src='yahoo', from = '2010-01-01',to = \"2023-03-01\")\ndf <- data.frame(Date=index(SPG),coredata(SPG))\n\n# create Bollinger Bands\nbbands <- BBands(SPG[,c(\"SPG.High\",\"SPG.Low\",\"SPG.Close\")])\n\n# join and subset data\ndf <- subset(cbind(df, data.frame(bbands[,1:3])), Date >= \"2010-01-01\")\n\n\n# colors column for increasing and decreasing\nfor (i in 1:length(df[,1])) {\n  if (df$SPG.Close[i] >= df$SPG.Open[i]) {\n      df$direction[i] = 'Increasing'\n  } else {\n      df$direction[i] = 'Decreasing'\n  }\n}\n\ni <- list(line = list(color = '#C39BD3'))\nd <- list(line = list(color = '#7F7F7F'))\n\n# plot candlestick chart\n\nfig <- df %>% plot_ly(x = ~Date, type=\"candlestick\",\n          open = ~SPG.Open, close = ~SPG.Close,\n          high = ~SPG.High, low = ~SPG.Low, name = \"SPG\",\n          increasing = i, decreasing = d) \nfig <- fig %>% add_lines(x = ~Date, y = ~up , name = \"B Bands\",\n            line = list(color = '#ccc', width = 0.5),\n            legendgroup = \"Bollinger Bands\",\n            hoverinfo = \"none\", inherit = F) \nfig <- fig %>% add_lines(x = ~Date, y = ~dn, name = \"B Bands\",\n            line = list(color = '#ccc', width = 0.5),\n            legendgroup = \"Bollinger Bands\", inherit = F,\n            showlegend = FALSE, hoverinfo = \"none\") \nfig <- fig %>% add_lines(x = ~Date, y = ~mavg, name = \"Mv Avg\",\n            line = list(color = '#C052B3', width = 0.5),\n            hoverinfo = \"none\", inherit = F) \nfig <- fig %>% layout(yaxis = list(title = \"Price\"))\n\n# plot volume bar chart\nfig2 <- df \nfig2 <- fig2 %>% plot_ly(x=~Date, y=~SPG.Volume, type='bar', name = \"SPG Volume\",\n          color = ~direction, colors = c('#C39BD3','#7F7F7F')) \nfig2 <- fig2 %>% layout(yaxis = list(title = \"Volume\"))\n\n# create rangeselector buttons\nrs <- list(visible = TRUE, x = 0.5, y = -0.055,\n           xanchor = 'center', yref = 'paper',\n           font = list(size = 9),\n           buttons = list(\n             list(count=1,\n                  label='RESET',\n                  step='all'),\n             list(count=3,\n                  label='3 YR',\n                  step='year',\n                  stepmode='backward'),\n             list(count=1,\n                  label='1 YR',\n                  step='year',\n                  stepmode='backward'),\n             list(count=1,\n                  label='1 MO',\n                  step='month',\n                  stepmode='backward')\n           ))\n\n# subplot with shared x axis\nfig <- subplot(fig, fig2, heights = c(0.7,0.2), nrows=2,\n             shareX = TRUE, titleY = TRUE)\nfig <- fig %>% layout(title = paste(\"Simon Property Group Inc Stock Price: January 2010 - March 2023\"),\n         xaxis = list(rangeselector = rs),\n         legend = list(orientation = 'h', x = 0.5, y = 1,\n                       xanchor = 'center', yref = 'paper',\n                       font = list(size = 10),\n                       bgcolor = 'transparent'))\n\nfig\n\n\n\n\n\n\n\n\n\n\nCode\nlog(data.info$`SPG.Adjusted`) %>% diff() %>% chartSeries(theme=chartTheme('white'),up.col='#C39BD3')\n\n\n\n\n\n\n\n\n\nCode\n#import the data\ngdp <- read.csv(\"DATA/RAW DATA/gdp-growth.csv\")\n\n#change date format\ngdp$Date <- as.Date(gdp$DATE , \"%m/%d/%Y\")\n\n#drop DATE column\ngdp <- subset(gdp, select = -c(1))\n\n#export the cleaned data\ngdp_clean <- gdp\nwrite.csv(gdp_clean, \"DATA/CLEANED DATA/gdp_clean_data.csv\", row.names=FALSE)\n\n#plot gdp growth rate \nfig <- plot_ly(gdp, x = ~Date, y = ~value, type = 'scatter', mode = 'lines',line = list(color = 'rgb(240, 128, 128)'))\nfig <- fig %>% layout(title = \"U.S GPD Growth Rate: 2010 - 2022\",xaxis = list(title = \"Time\"),yaxis = list(title =\"GDP Growth Rate\"))\nfig\n\n\n\n\n\n\n\n\n\n\nCode\n#import the data\ninflation_rate <- read.csv(\"DATA/RAW DATA/inflation-rate.csv\")\n\n#cleaning the data\n#remove unwanted columns\ninflation_rate_clean <- subset(inflation_rate, select = -c(1,HALF1,HALF2))\n\n#convert the data to time series data\ninflation_data_ts <- ts(as.vector(t(as.matrix(inflation_rate_clean))), start=c(2010,1), end=c(2023,2), frequency=12)\n\n#export the data\nwrite.csv(inflation_rate_clean, \"DATA/CLEANED DATA/inflation_rate_clean_data.csv\", row.names=FALSE)\n\n\n#plot inflation rate \nfig <- autoplot(inflation_data_ts, ylab = \"Inflation Rate\", color=\"#FFA07A\")+ggtitle(\"U.S Inflation Rate: January 2010 - February 2023\")+theme_bw()\nggplotly(fig)\n\n\n\n\n\n\n\n\n\n\nCode\n#import the data\ninterest_data <- read.csv(\"DATA/RAW DATA/interest-rate.csv\")\n\n#change date format\ninterest_data$Date <- as.Date(interest_data$Date , \"%m/%d/%Y\")\n\n#export the cleaned data\ninterest_clean_data <- interest_data\nwrite.csv(interest_clean_data, \"DATA/CLEANED DATA/interest_rate_clean_data.csv\", row.names=FALSE)\n\n#plot interest rate \nfig <- plot_ly(interest_data, x = ~Date, y = ~value, type = 'scatter', mode = 'lines',line = list(color='rgb(219, 112, 147)'))\nfig <- fig %>% layout(title = \"U.S Interest Rate: January 2010 - March 2023\",xaxis = list(title = \"Time\"),yaxis = list(title =\"Interest Rate\"))\nfig\n\n\n\n\n\n\n\n\n\n\nCode\n#import the data\nunemployment_rate <- read.csv(\"DATA/RAW DATA/unemployment-rate.csv\")\n\n#change date format\nunemployment_rate$Date <- as.Date(unemployment_rate$Date , \"%m/%d/%Y\")\n\n# export the data\nwrite.csv(unemployment_rate, \"DATA/CLEANED DATA/unemployment_rate_clean_data.csv\", row.names=FALSE)\n\n#plot unemployment rate \nfig <- plot_ly(unemployment_rate, x = ~Date, y = ~Value, type = 'scatter', mode = 'lines',line = list(color = 'rgb(189, 183, 107)'))\nfig <- fig %>% layout(title = \"U.S Unemployment Rate: January 2010 - March 2023\",xaxis = list(title = \"Time\"),yaxis = list(title =\"Unemployment Rate\"))\nfig\n\n\n\n\n\n\n\n\n\nFrom 2010 to early 2020, SPG’s stock price exhibited a generally upward trend, with occasional dips and plateaus along the way.\nIn particular, SPG’s stock price experienced a strong rebound following the economic downturn of 2008-2009, reaching pre-recession levels by early 2010. From there, the stock price continued to climb, reaching an all-time high in mid-2016. However, in the years that followed, SPG’s stock price experienced more volatility and fluctuation.\nIn early 2020, the outbreak of the COVID-19 pandemic led to a sharp decline in SPG’s stock price, as investors became concerned about the impact of the pandemic on the retail industry and consumer spending. However, since mid-2020, SPG’s stock price has shown signs of recovery, likely due to the gradual reopening of retail properties and improving economic conditions.\nOverall, SPG’s stock price has been influenced by a range of factors over the years, including economic conditions, consumer spending, competition in the retail industry, and changes in the real estate market.\nSince early 2020, Simon Property Group Inc’s stock price has experienced some volatility, likely due to a combination of factors such as global economic uncertainty and fluctuations in consumer demand for the company’s products.\nAs discussed before, the macroeconomic factors of GDP growth, inflation, interest rates, and unemployment rate are closely interrelated and play a crucial role in the overall health and stability of an economy. From 2010 to 2023, the global economy experienced a mix of ups and downs, with periods of strong GDP growth followed by slowdowns and recessions.\nThe second plot shows the first difference of the logarithm of the adjusted Simon Property Group Inc stock price. Taking the first difference removes any long-term trends and transforms the time series into a stationary process. From the plot, we can observe that the first difference of the logarithm of the Simon Property Group Inc stock price appears to be stationary, as the mean and variance are roughly constant over time.\n\n\nEnodogenous and Exogenous Variables\n\nPlotCorrelation HeatmapCCF GDPCCF InterestCCF InflationCCF Unemployment\n\n\n\n\nCode\nnumeric_data <- c(\"SPG.Adjusted\",\"gdp\", \"interest\", \"inflation\", \"unemployment\")\nnumeric_data <- final[, numeric_data]\nnormalized_data_numeric <- scale(numeric_data)\nnormalized_data <- ts(normalized_data_numeric, start = c(2010, 1), end = c(2021,10),frequency = 4)\nts_plot(normalized_data,\n        title = \"Normalized Time Series Data for SPG Stock and Macroeconomic Variables\",\n        Ytitle = \"Normalized Values\",\n        Xtitle = \"Year\")\n\n\n\n\n\n\n\n\n\n\nCode\n# Get upper triangle of the correlation matrix\nget_upper_tri <- function(cormat){\n    cormat[lower.tri(cormat)]<- NA\n    return(cormat)\n}\ncormat <- round(cor(normalized_data_numeric),2)\n\nupper_tri <- get_upper_tri(cormat)\n\nmelted_cormat <- melt(upper_tri, na.rm = TRUE)\n# Create a ggheatmap\nggheatmap <- ggplot(melted_cormat, aes(Var2, Var1, fill = value))+\n geom_tile(color = \"white\")+\n scale_fill_gradient2(low = \"blue\", high = \"red\", mid = \"white\", \n   midpoint = 0, limit = c(-1,1), space = \"Lab\", \n    name=\"Pearson\\nCorrelation\") +\n  theme_minimal()+ # minimal theme\n theme(axis.text.x = element_text(angle = 45, vjust = 1, \n    size = 12, hjust = 1))+\n coord_fixed()\n\nggheatmap + \ngeom_text(aes(Var2, Var1, label = value), color = \"black\", size = 4) +\ntheme(\n  axis.title.x = element_blank(),\n  axis.title.y = element_blank(),\n  panel.grid.major = element_blank(),\n  panel.border = element_blank(),\n  panel.background = element_blank(),\n  axis.ticks = element_blank(),\n  legend.justification = c(1, 0),\n  legend.position = c(0.6, 0.7),\n  legend.direction = \"horizontal\")+\n  guides(fill = guide_colorbar(barwidth = 7, barheight = 1,\n                title.position = \"top\", title.hjust = 0.5))\n\n\n\n\n\n\n\n\n\nCode\npar(mfrow=c(1,1))\nccf_result <- ccf(normalized_data[, c(\"SPG.Adjusted\")], normalized_data[, c(\"gdp\")], \n    lag.max = 300,\n    main = \"Cros-Correlation Plot for SPG Stock Price and GDP Growth Rate \",\n    ylab = \"CCF\")\n\n\n\n\n\nCode\ncat(\"The sum of cross correlation function is\", sum(abs(ccf_result$acf)))\n\n\nThe sum of cross correlation function is 8.137909\n\n\n\n\n\n\nCode\npar(mfrow=c(1,1))\nccf_result <- ccf(normalized_data[, c(\"SPG.Adjusted\")], normalized_data[, c(\"interest\")], \n    lag.max = 300,\n    main = \"Cros-Correlation Plot for SPG Stock Price and Interest Rate\",\n    ylab = \"CCF\")\n\n\n\n\n\nCode\ncat(\"The sum of cross correlation function is\", sum(abs(ccf_result$acf)))\n\n\nThe sum of cross correlation function is 9.091511\n\n\n\n\n\n\nCode\npar(mfrow=c(1,1))\nccf_result <- ccf(normalized_data[, c(\"SPG.Adjusted\")], normalized_data[, c(\"inflation\")], \n    lag.max = 300,\n    main = \"Cros-Correlation Plot for SPG Stock Price and Inflation Rate\",\n    ylab = \"CCF\")\n\n\n\n\n\nCode\ncat(\"The sum of cross correlation function is\", sum(abs(ccf_result$acf)))\n\n\nThe sum of cross correlation function is 15.82909\n\n\n\n\n\n\nCode\npar(mfrow=c(1,1))\nccf_result <- ccf(normalized_data[, c(\"SPG.Adjusted\")], normalized_data[, c(\"unemployment\")], \n    lag.max = 300,\n    main = \"Cros-Correlation Plot for SPG Stock Priceand Unemployment Rate\",\n    ylab = \"CCF\")\n\n\n\n\n\nCode\ncat(\"The sum of cross correlation function is\", sum(abs(ccf_result$acf)))\n\n\nThe sum of cross correlation function is 17.5948\n\n\n\n\n\nThe Normalized Time Series Data for Stock Price and Macroeconomic Variables plot shows the same variables as the first plot but has been normalized to a common range of 0 to 1 using the scale() function in R, which standardizes the variables to have a mean of 0 and a standard deviation of 1. The heatmap analysis of the normalized data reveals that inflation and unemployment rate exhibit strong positive correlations with the stock price indices, indicating that these variables may significantly influence stock price movements. On the other hand, weaker correlations were observed between the stock price indices and GDP and interest rates, suggesting that these variables may have less impact on stock price fluctuations. The cross-correlation feature plots confirm these findings, indicating that inflation and unemployment rate are more suitable feature variables for the ARIMAX model when predicting Simon Property Group Inc movements.\nFinal Exogenous variables: Macroeconomic indicators: Inflation rate and unemployment rate.\n\n\nEnodogenous and Exogenous Variables Plot\n\nPlotCheck the stationarity\n\n\n\n\nCode\nfinal_data <- final %>%dplyr::select( Date,SPG.Adjusted, inflation,unemployment)\nnumeric_data <- c(\"SPG.Adjusted\", \"inflation\",\"unemployment\")\nnumeric_data <- final_data[, numeric_data]\nnormalized_data_numeric <- scale(numeric_data)\nnormalized_numeric_df <- data.frame(normalized_data_numeric)\nnormalized_data_ts <- ts(normalized_data_numeric, start = c(2010, 1), frequency = 4)\n\nautoplot(normalized_data_ts, facets=TRUE) +\n  xlab(\"Year\") + ylab(\"\") +\n  ggtitle(\"Simon Property Group Inc Stock Price, Inflation Rate and Unemployment Rate in USA 2010-2023\")\n\n\n\n\n\n\n\n\n\nCode\n# Convert your multivariate time series data to a matrix\nfinal_data_ts_multivariate <- as.matrix(normalized_data_ts)\n\n# Check for stationarity using Phillips-Perron test\nphillips_perron_test <- ur.pp(final_data_ts_multivariate)  \nsummary(phillips_perron_test)\n\n\n\n################################## \n# Phillips-Perron Unit Root Test # \n################################## \n\nTest regression with intercept \n\n\nCall:\nlm(formula = y ~ y.l1)\n\nResiduals:\n    Min      1Q  Median      3Q     Max \n-2.4356 -0.1842 -0.0432  0.1307  4.4556 \n\nCoefficients:\n            Estimate Std. Error t value Pr(>|t|)    \n(Intercept) 0.006439   0.043712   0.147    0.883    \ny.l1        0.828205   0.044155  18.757   <2e-16 ***\n---\nSignif. codes:  0 '***' 0.001 '**' 0.01 '*' 0.05 '.' 0.1 ' ' 1\n\nResidual standard error: 0.5442 on 153 degrees of freedom\nMultiple R-squared:  0.6969,    Adjusted R-squared:  0.6949 \nF-statistic: 351.8 on 1 and 153 DF,  p-value: < 2.2e-16\n\n\nValue of test-statistic, type: Z-alpha  is: -27.5627 \n\n         aux. Z statistics\nZ-tau-mu            0.1461\n\n\n\n\n\nThe results of the Phillips-Perron unit root test indicate strong evidence against the null hypothesis of a unit root, as the p-value for the coefficient of the lagged variable is less than the significance level of 0.05. This suggests that the variable y, which is being tested for stationarity, is likely stationary. Furthermore, the test statistic Z-tau-mu is 0.1461, which is smaller than the critical value of Z-alpha (-27.5627), providing further evidence of stationarity.\nTo determine whether the linear model requires an ARCH model, an ARCH test is conducted. The ACF and PACF plots are also used to identify suitable model values.\n\n\nModel Fitting\n\nPlotARCH TestACF PlotPACF Plot\n\n\n\n\nCode\nnormalized_numeric_df$SPG.Adjusted<-ts(normalized_numeric_df$SPG.Adjusted,star=decimal_date(as.Date(\"2010-01-01\",format = \"%Y-%m-%d\")),frequency = 4)\nnormalized_numeric_df$inflation<-ts(normalized_numeric_df$inflation,star=decimal_date(as.Date(\"2010-01-01\",format = \"%Y-%m-%d\")),frequency = 4)\nnormalized_numeric_df$unemployment<-ts(normalized_numeric_df$unemployment,star=decimal_date(as.Date(\"2010-01-01\",format = \"%Y-%m-%d\")),frequency = 4)\n\nfit <- lm(SPG.Adjusted ~ inflation+unemployment, data=normalized_numeric_df)\nfit.res<-ts(residuals(fit),star=decimal_date(as.Date(\"2010-01-01\",format = \"%Y-%m-%d\")),frequency = 4)\n############## Then look at the residuals ############\nreturns <- fit.res  %>% diff()\nautoplot(returns)+ggtitle(\"Linear Model Returns\")\n\n\n\n\n\n\n\n\n\nCode\nbyd.archTest <- ArchTest(fit.res, lags = 1, demean = TRUE)\nbyd.archTest\n\n\n\n    ARCH LM-test; Null hypothesis: no ARCH effects\n\ndata:  fit.res\nChi-squared = 4.086, df = 1, p-value = 0.04324\n\n\n\n\n\n\nCode\nggAcf(returns) +ggtitle(\"ACF for returns\")\n\n\n\n\n\n\n\n\n\nCode\nggPacf(returns) +ggtitle(\"PACF for returns\")\n\n\n\n\n\n\n\n\nThe ARCH LM-test was conducted with the null hypothesis of no ARCH effects. The test resulted in a chi-squared value of 4.086 with one degree of freedom, and a very low p-value of 0.04324. This suggests strong evidence against the null hypothesis, indicating the presence of ARCH effects in the data.\nBased on the ACF and PACF plots, it appears that there is some significant autocorrelation and partial autocorrelation at multiple lags, which suggests that an ARIMA model may not be sufficient to capture the time series behavior. Additionally, the values for p and q appear to be relatively high, with p = 2 and q = 2 being suggested by the plots.\n\n\nARIMAX Model\n\nAuto Arima ModelAuto Arima ResidualsARIMAX ModelARIMA (1,0,1) PlotARIMA (1,1,1) ModelCross Validation\n\n\n\n\nCode\nxreg <- cbind(Inflation = normalized_data_ts[, \"inflation\"],\n              Unemployment = normalized_data_ts[, \"unemployment\"])\nfit.auto <- auto.arima(normalized_data_ts[, \"SPG.Adjusted\"], xreg = xreg)\nsummary(fit.auto)\n\n\nSeries: normalized_data_ts[, \"SPG.Adjusted\"] \nRegression with ARIMA(1,0,0)(1,0,0)[4] errors \n\nCoefficients:\n         ar1     sar1  Inflation  Unemployment\n      0.9534  -0.3300     0.2778       -0.3783\ns.e.  0.0495   0.1831     0.1652        0.0677\n\nsigma^2 = 0.1182:  log likelihood = -17.37\nAIC=44.75   AICc=46.05   BIC=54.5\n\nTraining set error measures:\n                      ME      RMSE       MAE       MPE     MAPE      MASE\nTraining set -0.01361405 0.3303231 0.2707078 -56.23605 97.35222 0.3436879\n                   ACF1\nTraining set 0.05506966\n\n\n\n\n\n\nCode\ncheckresiduals(fit.auto)\n\n\n\n\n\n\n    Ljung-Box test\n\ndata:  Residuals from Regression with ARIMA(1,0,0)(1,0,0)[4] errors\nQ* = 3.0527, df = 6, p-value = 0.8022\n\nModel df: 2.   Total lags used: 8\n\n\n\n\n\n\nCode\nARIMA.c=function(p1,p2,q1,q2,data){\ntemp=c()\nd=1\ni=1\ntemp= data.frame()\nls=matrix(rep(NA,6*30),nrow=30)\n\n\nfor (p in p1:p2)#\n{\n  for(q in q1:q2)#\n  {\n    for(d in 0:1)\n    {\n      \n      if(p+d+q<=6)\n      {\n        \n        model<- Arima(data,order=c(p,d,q))\n        ls[i,]= c(p,d,q,model$aic,model$bic,model$aicc)\n        i=i+1\n        #print(i)\n        \n      }\n      \n    }\n  }\n}\n\n\ntemp= as.data.frame(ls)\nnames(temp)= c(\"p\",\"d\",\"q\",\"AIC\",\"BIC\",\"AICc\")\n\ntemp\n}\n\noutput <- ARIMA.c(1,2,1,2,data=residuals(fit))\n\noutput[which.min(output$AIC),] \n\n\n  p d q      AIC      BIC     AICc\n1 1 0 1 65.50305 73.30802 66.35411\n\n\nCode\noutput[which.min(output$BIC),]\n\n\n  p d q      AIC      BIC     AICc\n1 1 0 1 65.50305 73.30802 66.35411\n\n\nCode\noutput[which.min(output$AICc),]\n\n\n  p d q      AIC      BIC     AICc\n1 1 0 1 65.50305 73.30802 66.35411\n\n\n\n\n\n\nCode\nset.seed(1234)\n\nmodel_output <- capture.output(sarima(fit.res, 1,0,1)) \n\n\n\n\n\n\n\n\n\nCode\ncat(model_output[25:56], model_output[length(model_output)], sep = \"\\n\")\n\n\n$fit\n\nCall:\narima(x = xdata, order = c(p, d, q), seasonal = list(order = c(P, D, Q), period = S), \n    xreg = xmean, include.mean = FALSE, transform.pars = trans, fixed = fixed, \n    optim.control = list(trace = trc, REPORT = 1, reltol = tol))\n\nCoefficients:\n         ar1     ma1    xmean\n      0.2774  0.4238  -0.0205\ns.e.  0.2080  0.1863   0.1133\n\nsigma^2 estimated as 0.1752:  log likelihood = -28.75,  aic = 65.5\n\n$degrees_of_freedom\n[1] 49\n\n$ttable\n      Estimate     SE t.value p.value\nar1     0.2774 0.2080  1.3340  0.1884\nma1     0.4238 0.1863  2.2748  0.0273\nxmean  -0.0205 0.1133 -0.1813  0.8569\n\n$AIC\n[1] 1.259674\n\n$AICc\n[1] 1.269289\n\n$BIC\n[1] 1.40977\n\n\n\n\n\n\nCode\nn=length(fit.res)\nk= 51\n \n \nrmse1 <- matrix(NA, (n-k),4)\nrmse2 <- matrix(NA, (n-k),4)\nrmse3 <- matrix(NA, (n-k),4)\n\n\nst <- tsp(fit.res)[1]+(k-5)/4 \n\nfor(i in 1:(n-k))\n{\n  xtrain <- window(fit.res, end=st + i/4)\n  xtest <- window(fit.res, start=st + (i+1)/4, end=st + (i+4)/4)\n  \n  #ARIMA(0,1,0) ARIMA(1,1,1)\n  \n  fit <- Arima(xtrain, order=c(0,1,0),\n                include.drift=TRUE, method=\"ML\")\n  fcast <- forecast(fit, h=4)\n  \n  fit2 <- Arima(xtrain, order=c(1,0,0), seasonal = c(1,0,0),\n                include.drift=TRUE, method=\"ML\")\n  fcast2 <- forecast(fit2, h=4)\n\n\n  rmse1[i,1:length(xtest)]   <- sqrt((fcast$mean-xtest)^2)\n  rmse2[i,1:length(xtest)] <- sqrt((fcast2$mean-xtest)^2)\n}\n\nplot(1:4,colMeans(rmse1,na.rm=TRUE), type=\"l\",col=2, xlab=\"horizon\", ylab=\"RMSE\")\nlines(1:4, colMeans(rmse2,na.rm=TRUE), type=\"l\",col=3)\nlegend(\"topleft\",legend=c(\"fit1\",\"fit2\"),col=2:4,lty=1)\n\n\n\n\n\n\n\n\nBased on the results of the auto.arima function, the suggested best model is ARIMA(1,0,0)(1,0,0)[4]. However, when we manually test different ARIMA models, we find that ARIMA(0,1,0) has the lowest values for AIC, BIC, and AICC. Additionally, both models have similar standardized residual plots, with means close to 0, indicating a good fit. The ACF plot of residuals also shows no significant lags, further indicating a well-fitted model.\nTo determine the best model, we conduct cross-validation and compare the RMSE values of both models. The results show that ARIMA(0,1,0) has lower RMSE values than the other model, indicating that it is the better model.\nWe can then proceed to choose the best GARCH model using ARIMA(0,1,0) as the base model.\n\n\nSquared Residuals\n\nPlotACF PlotPACF PlotGRACH(1,2)\n\n\n\n\nCode\nfit <- lm(SPG.Adjusted ~ inflation+unemployment, data=normalized_numeric_df)\nfit.res<-ts(residuals(fit),star=decimal_date(as.Date(\"2010-01-01\",format = \"%Y-%m-%d\")),frequency = 4)\nfit <- Arima(fit.res,order=c(0,1,0))\nres=fit$res\nplot(res^2,main='Squared Residuals')\n\n\n\n\n\n\n\n\n\nCode\nacf(res^2,24, main = \"ACF Residuals Square\")\n\n\n\n\n\n\n\n\n\nCode\npacf(res^2,24, main = \"PACF Residuals Square\")\n\n\n\n\n\n\n\n\n\nCode\nsummary(garchFit(~garch(2,1),res, trace=F))\n\n\n\nTitle:\n GARCH Modelling \n\nCall:\n garchFit(formula = ~garch(2, 1), data = res, trace = F) \n\nMean and Variance Equation:\n data ~ garch(2, 1)\n<environment: 0x14041fd60>\n [data = res]\n\nConditional Distribution:\n norm \n\nCoefficient(s):\n       mu      omega     alpha1     alpha2      beta1  \n0.0452885  0.0005951  0.1460472  0.3886609  0.6648638  \n\nStd. Errors:\n based on Hessian \n\nError Analysis:\n        Estimate  Std. Error  t value Pr(>|t|)    \nmu     0.0452885   0.0388984    1.164 0.244311    \nomega  0.0005951   0.0190009    0.031 0.975015    \nalpha1 0.1460472   0.1482029    0.985 0.324401    \nalpha2 0.3886609   0.2958817    1.314 0.188991    \nbeta1  0.6648638   0.1933161    3.439 0.000583 ***\n---\nSignif. codes:  0 '***' 0.001 '**' 0.01 '*' 0.05 '.' 0.1 ' ' 1\n\nLog Likelihood:\n -28.51055    normalized:  -0.5482798 \n\nDescription:\n Sat Jan  6 19:56:24 2024 by user:  \n\n\nStandardised Residuals Tests:\n                                Statistic p-Value     \n Jarque-Bera Test   R    Chi^2  22.15704  1.544041e-05\n Shapiro-Wilk Test  R    W      0.9265577 0.003306988 \n Ljung-Box Test     R    Q(10)  7.382861  0.6888733   \n Ljung-Box Test     R    Q(15)  11.82493  0.6922294   \n Ljung-Box Test     R    Q(20)  18.67743  0.5428737   \n Ljung-Box Test     R^2  Q(10)  3.102511  0.9789076   \n Ljung-Box Test     R^2  Q(15)  3.78577   0.9983586   \n Ljung-Box Test     R^2  Q(20)  5.344281  0.9995367   \n LM Arch Test       R    TR^2   3.42089   0.9917725   \n\nInformation Criterion Statistics:\n     AIC      BIC      SIC     HQIC \n1.288867 1.476487 1.272450 1.360796 \n\n\n\n\n\nFrom the squared residuals of the best ARIMA model, it can be observed that the ACF plot and PACF plot indicate that the residuals are not autocorrelated and are white noise, indicating a good fit of the model. Based on the squared residuals of the best ARIMA model, we can see that the ACF and PACF plots indicate that most of the values lie between the blue lines. Additionally, the p-value is 2 and q-value is 1.\nSo, the best model is ARIMA(0,1,0) and GARCH(2,1).\n\n\nBest Model\n\nARIMA ModelGARCH ModelVolatility\n\n\n\n\nCode\n#fiting an ARIMA model to the Inflation variable\ninflation_fit<-auto.arima(normalized_numeric_df$inflation) \nfinflation<-forecast(inflation_fit)\n\n#fitting an ARIMA model to the Unemployment variable\nunemployment_fit<-auto.arima(normalized_numeric_df$unemployment) \nfunemployment<-forecast(unemployment_fit)\n\n# best model fit for forcasting\nxreg <- cbind(Inflation = normalized_data_ts[, \"inflation\"],\n              Unemployment = normalized_data_ts[, \"unemployment\"])\n\n\n\nsummary(arima.fit<-Arima(normalized_data_ts[, \"SPG.Adjusted\"],order=c(0,1,0),xreg=xreg),include.drift = TRUE)\n\n\nSeries: normalized_data_ts[, \"SPG.Adjusted\"] \nRegression with ARIMA(0,1,0) errors \n\nCoefficients:\n      Inflation  Unemployment\n         0.4599       -0.4115\ns.e.     0.1513        0.0687\n\nsigma^2 = 0.1211:  log likelihood = -17.51\nAIC=41.03   AICc=41.54   BIC=46.82\n\nTraining set error measures:\n                      ME      RMSE       MAE       MPE     MAPE      MASE\nTraining set -0.02507976 0.3378129 0.2655551 -55.51772 91.68396 0.3371461\n                   ACF1\nTraining set 0.04744896\n\n\n\n\n\n\nCode\nsummary(final.fit <- garchFit(~garch(2,1), res,trace = F))\n\n\n\nTitle:\n GARCH Modelling \n\nCall:\n garchFit(formula = ~garch(2, 1), data = res, trace = F) \n\nMean and Variance Equation:\n data ~ garch(2, 1)\n<environment: 0x1372b06a0>\n [data = res]\n\nConditional Distribution:\n norm \n\nCoefficient(s):\n       mu      omega     alpha1     alpha2      beta1  \n0.0452885  0.0005951  0.1460472  0.3886609  0.6648638  \n\nStd. Errors:\n based on Hessian \n\nError Analysis:\n        Estimate  Std. Error  t value Pr(>|t|)    \nmu     0.0452885   0.0388984    1.164 0.244311    \nomega  0.0005951   0.0190009    0.031 0.975015    \nalpha1 0.1460472   0.1482029    0.985 0.324401    \nalpha2 0.3886609   0.2958817    1.314 0.188991    \nbeta1  0.6648638   0.1933161    3.439 0.000583 ***\n---\nSignif. codes:  0 '***' 0.001 '**' 0.01 '*' 0.05 '.' 0.1 ' ' 1\n\nLog Likelihood:\n -28.51055    normalized:  -0.5482798 \n\nDescription:\n Sat Jan  6 19:56:25 2024 by user:  \n\n\nStandardised Residuals Tests:\n                                Statistic p-Value     \n Jarque-Bera Test   R    Chi^2  22.15704  1.544041e-05\n Shapiro-Wilk Test  R    W      0.9265577 0.003306988 \n Ljung-Box Test     R    Q(10)  7.382861  0.6888733   \n Ljung-Box Test     R    Q(15)  11.82493  0.6922294   \n Ljung-Box Test     R    Q(20)  18.67743  0.5428737   \n Ljung-Box Test     R^2  Q(10)  3.102511  0.9789076   \n Ljung-Box Test     R^2  Q(15)  3.78577   0.9983586   \n Ljung-Box Test     R^2  Q(20)  5.344281  0.9995367   \n LM Arch Test       R    TR^2   3.42089   0.9917725   \n\nInformation Criterion Statistics:\n     AIC      BIC      SIC     HQIC \n1.288867 1.476487 1.272450 1.360796 \n\n\n\n\n\n\nCode\nht <- final.fit@h.t #a numeric vector with the conditional variances (h.t = sigma.t^delta)\n\n#############################\ndata=data.frame(final)\ndata$Date<-as.Date(data$Date,\"%Y-%m-%d\")\n\n\ndata2= data.frame(ht,data$Date)\nggplot(data2, aes(y = ht, x = data.Date)) + geom_line(col = '#C39BD3') + ylab('Conditional Variance') + xlab('Date')\n\n\n\n\n\n\n\n\nFrom the ARIMA(0,1,0), we see that the training set error measures also suggest a good fit, with low mean absolute error, root mean squared error, and autocorrelation of the residuals. GATCH(2,1) model model is used to estimate the volatility of the standardized residuals of the previous regression model. The model includes a mean equation that estimates the mean of the residuals and a variance equation that models the conditional variance of the residuals. The coefficients of the mean equation suggest that the mean of the residuals is close to zero. The variance equation coefficients suggest that the conditional variance of the residuals is dependent on the past conditional variances and the past squared standardized residuals. The model’s AIC, BIC, SIC, and HQIC values are all relatively low, indicating a good fit of the model. The standardized residuals tests indicate that the residuals are approximately normally distributed and that there is no significant autocorrelation in the residuals.\nThe volatility of the model seems high in 2020 but has decreased gradually in the past few months. This could indicate that the asset’s price was experiencing a lot of fluctuations in 2020, but the market has stabilized recently.\n\n\nModel Diagnostics\n\nResidualsQQ PlotBox Test\n\n\n\n\nCode\nfit2<-garch(res,order=c(2,1),trace=F)\ncheckresiduals(fit2) \n\n\n\n\n\n\n\n\n\nCode\nqqnorm(fit2$residuals, pch = 1)\nqqline(fit2$residuals, col = \"blue\", lwd = 2)\n\n\n\n\n\n\n\n\n\nCode\nBox.test (fit2$residuals, type = \"Ljung\")\n\n\n\n    Box-Ljung test\n\ndata:  fit2$residuals\nX-squared = 0.41445, df = 1, p-value = 0.5197\n\n\nThe ACF plot of the residuals shows all the values between the blue lines, which indicates that the residuals are not significantly autocorrelated. The range of values for the residual plot between -2 and 4 is considered acceptable. Additionally, the QQ plot of the residuals shows a linear plot on the line, which is another good indication that the residuals are normally distributed. The QQ plot is a valuable tool to assess if the residuals follow a normal distribution, and in this case, the plot suggests that the residuals do indeed follow a normal distribution.\nThe Box-Ljung test, a p-value of 0.5186 indicates that the model’s residuals are not significantly autocorrelated, meaning that the model has captured most of the information in the data. This result is good because it suggests that the model is a good fit for the data and has accounted for most of the underlying patterns in the data. Therefore, we can rely on the model’s predictions and use them to make informed decisions.\n\n\n\n\n\nForecast\n\n\nCode\npredict(final.fit, n.ahead = 5, plot=TRUE)\n\n\n\n\n\n  meanForecast meanError standardDeviation lowerInterval upperInterval\n1   0.04528854  1.106059          1.106059     -2.122548      2.213125\n2   0.04528854  0.996482          0.996482     -1.907780      1.998357\n3   0.04528854  1.131939          1.131939     -2.173271      2.263848\n4   0.04528854  1.193958          1.193958     -2.294825      2.385402\n5   0.04528854  1.286298          1.286298     -2.475809      2.566386\n\n\nThe forecasted plot is based on the best model ARIMAX(0,1,0)+GARCH(2,1). This model takes into account the autoregressive and moving average components of the data, as well as the impact of exogenous variables on the time series. Additionally, the GARCH component of the model accounts for the volatility clustering in the data. Overall, this model is well-suited to make accurate predictions about future values of the time series.\n\n\nEquation of the Model\nThe equation of the ARIMAX(0,1,0) model is:\n\\(Y(t) = Y(t-1) + \\epsilon(t)\\)\nwhere \\(Y(t)\\) is the time series variable and \\(\\epsilon(t)\\) is the error term. The equation of the GARCH(2,1) model is:\n\\(\\sigma_t^2 = \\omega + \\alpha_1\\varepsilon^2_(t-1) + \\alpha_2\\varepsilon^2_(t-2) + \\beta_1\\sigma^2(t-1)\\)\nwhere \\(\\sigma_t^2\\) represents the conditional variance of the time series at time \\(t\\), \\(\\varepsilon_t\\) represents the error term or innovation at time \\(t\\), \\(\\omega\\) is a constant representing the long-run average variance, and \\(\\alpha_1\\), \\(\\alpha_2\\), and \\(\\beta_1\\) are the coefficients to be estimated.\nThe combined equation of the ARIMAX(1,1,1)+GARCH(1,1) model is:\n\\(Y(t) = Y(t-1) + \\epsilon(t)\\)\n\\(\\epsilon(t) = \\sigma(t) * \\epsilon~(t)\\)\n\\(\\sigma_t^2 = \\omega + \\alpha_1\\varepsilon^2_(t-1) + \\alpha_2\\varepsilon^2_(t-2) + \\beta_1\\sigma^2(t-1)\\)"
  },
  {
    "objectID": "arch_Technology_Sector_Fund.html",
    "href": "arch_Technology_Sector_Fund.html",
    "title": "",
    "section": "",
    "text": "Impact of Macroeconomic Factors on Apple Stock Price\n\n\n\nThere are many great technology sector companies to choose from, but one of the best is undoubtedly Apple Inc. (AAPL). Apple has consistently ranked among the world’s most valuable companies, with a market capitalization of over $2 trillion as of 2021.\nApple’s success can be attributed to its innovative products, such as the iPhone, iPad, and MacBook, as well as its ecosystem of software and services, including the App Store, iCloud, and Apple Music. The company also has a strong brand reputation and a loyal customer base.\nApple’s financial performance has been impressive, with steady revenue growth and strong profits. The company has consistently returned value to its shareholders through share buybacks and dividend payments.\nBy using an ARIMAX+ARCH/GARCH model to analyze the stock price behavior of Apple, we can gain insights into how macroeconomic factors impact the company’s performance. For example, an increase in GDP growth rate or a decrease in unemployment rate may lead to increased consumer spending and a rise in Apple’s stock price. Conversely, an increase in inflation or interest rates may lead to a decrease in consumer spending and a decline in Apple’s stock price.\n\n\nTime series Plot\n\nAppleDifferentitaed Chart SeriesGDP GrowthInflationInterestUnemployment\n\n\n\n\nCode\n# get data\noptions(\"getSymbols.warning4.0\"=FALSE)\noptions(\"getSymbols.yahoo.warning\"=FALSE)\n\n\ndata.info = getSymbols(\"AAPL\",src='yahoo', from = '2010-01-01',to = \"2023-03-01\",auto.assign = FALSE)\ndata = getSymbols(\"AAPL\",src='yahoo', from = '2010-01-01',to = \"2023-03-01\")\ndf <- data.frame(Date=index(AAPL),coredata(AAPL))\n\n# create Bollinger Bands\nbbands <- BBands(AAPL[,c(\"AAPL.High\",\"AAPL.Low\",\"AAPL.Close\")])\n\n# join and subset data\ndf <- subset(cbind(df, data.frame(bbands[,1:3])), Date >= \"2010-01-01\")\n\n\n# colors column for increasing and decreasing\nfor (i in 1:length(df[,1])) {\n  if (df$AAPL.Close[i] >= df$AAPL.Open[i]) {\n      df$direction[i] = 'Increasing'\n  } else {\n      df$direction[i] = 'Decreasing'\n  }\n}\n\ni <- list(line = list(color = '#922B21'))\nd <- list(line = list(color = '#7F7F7F'))\n\n# plot candlestick chart\n\nfig <- df %>% plot_ly(x = ~Date, type=\"candlestick\",\n          open = ~AAPL.Open, close = ~AAPL.Close,\n          high = ~AAPL.High, low = ~AAPL.Low, name = \"AAPL\",\n          increasing = i, decreasing = d) \nfig <- fig %>% add_lines(x = ~Date, y = ~up , name = \"B Bands\",\n            line = list(color = '#ccc', width = 0.5),\n            legendgroup = \"Bollinger Bands\",\n            hoverinfo = \"none\", inherit = F) \nfig <- fig %>% add_lines(x = ~Date, y = ~dn, name = \"B Bands\",\n            line = list(color = '#ccc', width = 0.5),\n            legendgroup = \"Bollinger Bands\", inherit = F,\n            showlegend = FALSE, hoverinfo = \"none\") \nfig <- fig %>% add_lines(x = ~Date, y = ~mavg, name = \"Mv Avg\",\n            line = list(color = '#C052B3', width = 0.5),\n            hoverinfo = \"none\", inherit = F) \nfig <- fig %>% layout(yaxis = list(title = \"Price\"))\n\n# plot volume bar chart\nfig2 <- df \nfig2 <- fig2 %>% plot_ly(x=~Date, y=~AAPL.Volume, type='bar', name = \"AAPL Volume\",\n          color = ~direction, colors = c('#922B21','#7F7F7F')) \nfig2 <- fig2 %>% layout(yaxis = list(title = \"Volume\"))\n\n# create rangeselector buttons\nrs <- list(visible = TRUE, x = 0.5, y = -0.055,\n           xanchor = 'center', yref = 'paper',\n           font = list(size = 9),\n           buttons = list(\n             list(count=1,\n                  label='RESET',\n                  step='all'),\n             list(count=3,\n                  label='3 YR',\n                  step='year',\n                  stepmode='backward'),\n             list(count=1,\n                  label='1 YR',\n                  step='year',\n                  stepmode='backward'),\n             list(count=1,\n                  label='1 MO',\n                  step='month',\n                  stepmode='backward')\n           ))\n\n# subplot with shared x axis\nfig <- subplot(fig, fig2, heights = c(0.7,0.2), nrows=2,\n             shareX = TRUE, titleY = TRUE)\nfig <- fig %>% layout(title = paste(\"Apple Stock Price: January 2010 - March 2023\"),\n         xaxis = list(rangeselector = rs),\n         legend = list(orientation = 'h', x = 0.5, y = 1,\n                       xanchor = 'center', yref = 'paper',\n                       font = list(size = 10),\n                       bgcolor = 'transparent'))\n\nfig\n\n\n\n\n\n\n\n\n\n\nCode\nlog(data.info$`AAPL.Adjusted`) %>% diff() %>% chartSeries(theme=chartTheme('white'),up.col='#922B21')\n\n\n\n\n\n\n\n\n\nCode\n#import the data\ngdp <- read.csv(\"DATA/RAW DATA/gdp-growth.csv\")\n\n#change date format\ngdp$Date <- as.Date(gdp$DATE , \"%m/%d/%Y\")\n\n#drop DATE column\ngdp <- subset(gdp, select = -c(1))\n\n#export the cleaned data\ngdp_clean <- gdp\nwrite.csv(gdp_clean, \"DATA/CLEANED DATA/gdp_clean_data.csv\", row.names=FALSE)\n\n#plot gdp growth rate \nfig <- plot_ly(gdp, x = ~Date, y = ~value, type = 'scatter', mode = 'lines',line = list(color = 'rgb(240, 128, 128)'))\nfig <- fig %>% layout(title = \"U.S GPD Growth Rate: 2010 - 2022\",xaxis = list(title = \"Time\"),yaxis = list(title =\"GDP Growth Rate\"))\nfig\n\n\n\n\n\n\n\n\n\n\nCode\n#import the data\ninflation_rate <- read.csv(\"DATA/RAW DATA/inflation-rate.csv\")\n\n#cleaning the data\n#remove unwanted columns\ninflation_rate_clean <- subset(inflation_rate, select = -c(1,HALF1,HALF2))\n\n#convert the data to time series data\ninflation_data_ts <- ts(as.vector(t(as.matrix(inflation_rate_clean))), start=c(2010,1), end=c(2023,2), frequency=12)\n\n#export the data\nwrite.csv(inflation_rate_clean, \"DATA/CLEANED DATA/inflation_rate_clean_data.csv\", row.names=FALSE)\n\n\n#plot inflation rate \nfig <- autoplot(inflation_data_ts, ylab = \"Inflation Rate\", color=\"#FFA07A\")+ggtitle(\"U.S Inflation Rate: January 2010 - February 2023\")+theme_bw()\nggplotly(fig)\n\n\n\n\n\n\n\n\n\n\nCode\n#import the data\ninterest_data <- read.csv(\"DATA/RAW DATA/interest-rate.csv\")\n\n#change date format\ninterest_data$Date <- as.Date(interest_data$Date , \"%m/%d/%Y\")\n\n#export the cleaned data\ninterest_clean_data <- interest_data\nwrite.csv(interest_clean_data, \"DATA/CLEANED DATA/interest_rate_clean_data.csv\", row.names=FALSE)\n\n#plot interest rate \nfig <- plot_ly(interest_data, x = ~Date, y = ~value, type = 'scatter', mode = 'lines',line = list(color='rgb(219, 112, 147)'))\nfig <- fig %>% layout(title = \"U.S Interest Rate: January 2010 - March 2023\",xaxis = list(title = \"Time\"),yaxis = list(title =\"Interest Rate\"))\nfig\n\n\n\n\n\n\n\n\n\n\nCode\n#import the data\nunemployment_rate <- read.csv(\"DATA/RAW DATA/unemployment-rate.csv\")\n\n#change date format\nunemployment_rate$Date <- as.Date(unemployment_rate$Date , \"%m/%d/%Y\")\n\n# export the data\nwrite.csv(unemployment_rate, \"DATA/CLEANED DATA/unemployment_rate_clean_data.csv\", row.names=FALSE)\n\n#plot unemployment rate \nfig <- plot_ly(unemployment_rate, x = ~Date, y = ~Value, type = 'scatter', mode = 'lines',line = list(color = 'rgb(189, 183, 107)'))\nfig <- fig %>% layout(title = \"U.S Unemployment Rate: January 2010 - March 2023\",xaxis = list(title = \"Time\"),yaxis = list(title =\"Unemployment Rate\"))\nfig\n\n\n\n\n\n\n\n\n\nApple Inc.’s stock price has had a tumultuous journey from 2010 to 2023. In 2010, the company was just beginning to recover from the impact of the 2008 financial crisis, and its stock price was around $30 per share. However, the introduction of new products such as the iPad and iPhone in the following years propelled the company’s stock price to new heights, reaching over $700 per share in 2012.\nFrom 2013 to 2016, Apple’s stock price continued to fluctuate, experiencing highs and lows due to factors such as changes in consumer demand and increased competition in the technology sector. The company’s stock price began to recover in 2017, as investors became more optimistic about the potential of Apple’s new products such as the iPhone X and the Apple Watch.\nHowever, the outbreak of the COVID-19 pandemic in 2020 caused a brief dip in Apple’s stock price, as investors were uncertain about the impact of the pandemic on the company’s operations and financial performance. Nevertheless, Apple’s strong position in the technology sector and its ability to adapt to changing market conditions helped it to quickly rebound and continue its growth trend throughout 2020 and into early 2021.\nSince mid 2020, Apple’s stock price has experienced some volatility, likely due to a combination of factors such as global economic uncertainty and fluctuations in consumer demand for the company’s products.\nAs discussed before, the macroeconomic factors of GDP growth, inflation, interest rates, and unemployment rate are closely interrelated and play a crucial role in the overall health and stability of an economy. From 2010 to 2023, the global economy experienced a mix of ups and downs, with periods of strong GDP growth followed by slowdowns and recessions.\nThe second plot shows the first difference of the logarithm of the adjusted Apple stock price. Taking the first difference removes any long-term trends and transforms the time series into a stationary process. From the plot, we can observe that the first difference of the logarithm of the Apple stock price appears to be stationary, as the mean and variance are roughly constant over time.\n\n\nEnodogenous and Exogenous Variables\n\nPlotCorrelation HeatmapCCF GDPCCF InterestCCF InflationCCF Unemployment\n\n\n\n\nCode\nnumeric_data <- c(\"AAPL.Adjusted\",\"gdp\", \"interest\", \"inflation\", \"unemployment\")\nnumeric_data <- final[, numeric_data]\nnormalized_data_numeric <- scale(numeric_data)\nnormalized_data <- ts(normalized_data_numeric, start = c(2010, 1), end = c(2021,10),frequency = 4)\nts_plot(normalized_data,\n        title = \"Normalized Time Series Data for AAPL Stock and Macroeconomic Variables\",\n        Ytitle = \"Normalized Values\",\n        Xtitle = \"Year\")\n\n\n\n\n\n\n\n\n\n\nCode\n# Get upper triangle of the correlation matrix\nget_upper_tri <- function(cormat){\n    cormat[lower.tri(cormat)]<- NA\n    return(cormat)\n}\ncormat <- round(cor(normalized_data_numeric),2)\n\nupper_tri <- get_upper_tri(cormat)\n\nmelted_cormat <- melt(upper_tri, na.rm = TRUE)\n# Create a ggheatmap\nggheatmap <- ggplot(melted_cormat, aes(Var2, Var1, fill = value))+\n geom_tile(color = \"white\")+\n scale_fill_gradient2(low = \"blue\", high = \"red\", mid = \"white\", \n   midpoint = 0, limit = c(-1,1), space = \"Lab\", \n    name=\"Pearson\\nCorrelation\") +\n  theme_minimal()+ # minimal theme\n theme(axis.text.x = element_text(angle = 45, vjust = 1, \n    size = 12, hjust = 1))+\n coord_fixed()\n\nggheatmap + \ngeom_text(aes(Var2, Var1, label = value), color = \"black\", size = 4) +\ntheme(\n  axis.title.x = element_blank(),\n  axis.title.y = element_blank(),\n  panel.grid.major = element_blank(),\n  panel.border = element_blank(),\n  panel.background = element_blank(),\n  axis.ticks = element_blank(),\n  legend.justification = c(1, 0),\n  legend.position = c(0.6, 0.7),\n  legend.direction = \"horizontal\")+\n  guides(fill = guide_colorbar(barwidth = 7, barheight = 1,\n                title.position = \"top\", title.hjust = 0.5))\n\n\n\n\n\n\n\n\n\nCode\npar(mfrow=c(1,1))\nccf_result <- ccf(normalized_data[, c(\"AAPL.Adjusted\")], normalized_data[, c(\"gdp\")], \n    lag.max = 300,\n    main = \"Cros-Correlation Plot for AAPL Stock Price and GDP Growth Rate \",\n    ylab = \"CCF\")\n\n\n\n\n\nCode\ncat(\"The sum of cross correlation function is\", sum(abs(ccf_result$acf)))\n\n\nThe sum of cross correlation function is 5.344796\n\n\n\n\n\n\nCode\npar(mfrow=c(1,1))\nccf_result <- ccf(normalized_data[, c(\"AAPL.Adjusted\")], normalized_data[, c(\"interest\")], \n    lag.max = 300,\n    main = \"Cros-Correlation Plot for AAPL Stock Price and Interest Rate\",\n    ylab = \"CCF\")\n\n\n\n\n\nCode\ncat(\"The sum of cross correlation function is\", sum(abs(ccf_result$acf)))\n\n\nThe sum of cross correlation function is 12.1218\n\n\n\n\n\n\nCode\npar(mfrow=c(1,1))\nccf_result <- ccf(normalized_data[, c(\"AAPL.Adjusted\")], normalized_data[, c(\"inflation\")], \n    lag.max = 300,\n    main = \"Cros-Correlation Plot for AAPL Stock Price and Inflation Rate\",\n    ylab = \"CCF\")\n\n\n\n\n\nCode\ncat(\"The sum of cross correlation function is\", sum(abs(ccf_result$acf)))\n\n\nThe sum of cross correlation function is 14.80688\n\n\n\n\n\n\nCode\npar(mfrow=c(1,1))\nccf_result <- ccf(normalized_data[, c(\"AAPL.Adjusted\")], normalized_data[, c(\"unemployment\")], \n    lag.max = 300,\n    main = \"Cros-Correlation Plot for AAPL Stock Priceand Unemployment Rate\",\n    ylab = \"CCF\")\n\n\n\n\n\nCode\ncat(\"The sum of cross correlation function is\", sum(abs(ccf_result$acf)))\n\n\nThe sum of cross correlation function is 18.29428\n\n\n\n\n\nThe Normalized Time Series Data for Stock Price and Macroeconomic Variables plot shows the same variables as the first plot but has been normalized to a common range of 0 to 1 using the scale() function in R, which standardizes the variables to have a mean of 0 and a standard deviation of 1. The heatmap analysis of the normalized data reveals that inflation and unemployment rate exhibit strong positive correlations with the stock price indices, indicating that these variables may significantly influence stock price movements. On the other hand, weaker correlations were observed between the stock price indices and GDP and interest rates, suggesting that these variables may have less impact on stock price fluctuations. The cross-correlation feature plots confirm these findings, indicating that inflation and unemployment rate are more suitable feature variables for the ARIMAX model when predicting Apple movements.\nFinal Exogenous variables: Macroeconomic indicators: Inflation rate and unemployment rate.\n\n\nEnodogenous and Exogenous Variables Plot\n\nPlotCheck the stationarity\n\n\n\n\nCode\nfinal_data <- final %>%dplyr::select( Date,AAPL.Adjusted, inflation,unemployment)\nnumeric_data <- c(\"AAPL.Adjusted\", \"inflation\",\"unemployment\")\nnumeric_data <- final_data[, numeric_data]\nnormalized_data_numeric <- scale(numeric_data)\nnormalized_numeric_df <- data.frame(normalized_data_numeric)\nnormalized_data_ts <- ts(normalized_data_numeric, start = c(2010, 1), frequency = 4)\n\nautoplot(normalized_data_ts, facets=TRUE) +\n  xlab(\"Year\") + ylab(\"\") +\n  ggtitle(\"Apple Stock Price, Inflation Rate and Unemployment Rate in USA 2010-2023\")\n\n\n\n\n\n\n\n\n\nCode\n# Convert your multivariate time series data to a matrix\nfinal_data_ts_multivariate <- as.matrix(normalized_data_ts)\n\n# Check for stationarity using Phillips-Perron test\nphillips_perron_test <- ur.pp(final_data_ts_multivariate)  \nsummary(phillips_perron_test)\n\n\n\n################################## \n# Phillips-Perron Unit Root Test # \n################################## \n\nTest regression with intercept \n\n\nCall:\nlm(formula = y ~ y.l1)\n\nResiduals:\n    Min      1Q  Median      3Q     Max \n-2.2155 -0.1522 -0.0540  0.0799  4.4890 \n\nCoefficients:\n             Estimate Std. Error t value Pr(>|t|)    \n(Intercept) -0.000148   0.042080  -0.004    0.997    \ny.l1         0.851949   0.042506  20.043   <2e-16 ***\n---\nSignif. codes:  0 '***' 0.001 '**' 0.01 '*' 0.05 '.' 0.1 ' ' 1\n\nResidual standard error: 0.5239 on 153 degrees of freedom\nMultiple R-squared:  0.7242,    Adjusted R-squared:  0.7224 \nF-statistic: 401.7 on 1 and 153 DF,  p-value: < 2.2e-16\n\n\nValue of test-statistic, type: Z-alpha  is: -24.89 \n\n         aux. Z statistics\nZ-tau-mu           -0.0018\n\n\n\n\n\nThe results of the Phillips-Perron unit root test indicate strong evidence against the null hypothesis of a unit root, as the p-value for the coefficient of the lagged variable is less than the significance level of 0.05. This suggests that the variable y, which is being tested for stationarity, is likely stationary. Furthermore, the test statistic Z-tau-mu is -0.0018, which is smaller than the critical value of Z-alpha (-24.89), providing further evidence of stationarity.\nTo determine whether the linear model requires an ARCH model, an ARCH test is conducted. The ACF and PACF plots are also used to identify suitable model values.\n\n\nModel Fitting\n\nPlotARCH TestACF PlotPACF Plot\n\n\n\n\nCode\nnormalized_numeric_df$AAPL.Adjusted<-ts(normalized_numeric_df$AAPL.Adjusted,star=decimal_date(as.Date(\"2010-01-01\",format = \"%Y-%m-%d\")),frequency = 4)\nnormalized_numeric_df$inflation<-ts(normalized_numeric_df$inflation,star=decimal_date(as.Date(\"2010-01-01\",format = \"%Y-%m-%d\")),frequency = 4)\nnormalized_numeric_df$unemployment<-ts(normalized_numeric_df$unemployment,star=decimal_date(as.Date(\"2010-01-01\",format = \"%Y-%m-%d\")),frequency = 4)\n\nfit <- lm(AAPL.Adjusted ~ inflation+unemployment, data=normalized_numeric_df)\nfit.res<-ts(residuals(fit),star=decimal_date(as.Date(\"2010-01-01\",format = \"%Y-%m-%d\")),frequency = 4)\n############## Then look at the residuals ############\nreturns <- fit.res  %>% diff()\nautoplot(returns)+ggtitle(\"Linear Model Returns\")\n\n\n\n\n\n\n\n\n\nCode\nbyd.archTest <- ArchTest(fit.res, lags = 1, demean = TRUE)\nbyd.archTest\n\n\n\n    ARCH LM-test; Null hypothesis: no ARCH effects\n\ndata:  fit.res\nChi-squared = 24.051, df = 1, p-value = 9.382e-07\n\n\n\n\n\n\nCode\nggAcf(returns) +ggtitle(\"ACF for returns\")\n\n\n\n\n\n\n\n\n\nCode\nggPacf(returns) +ggtitle(\"PACF for returns\")\n\n\n\n\n\n\n\n\nThe ARCH LM-test was conducted with the null hypothesis of no ARCH effects. The test resulted in a chi-squared value of 24.051 with one degree of freedom, and a very low p-value of 9.383e-07. This suggests strong evidence against the null hypothesis, indicating the presence of ARCH effects in the data.\nBased on the ACF and PACF plots, it appears that there is some significant autocorrelation and partial autocorrelation at multiple lags, which suggests that an ARIMA model may not be sufficient to capture the time series behavior. Additionally, the values for p and q appear to be relatively high, with p = 0 and q = 0 being suggested by the plots.\n\n\nARIMAX Model\n\nAuto Arima ModelAuto Arima Residuals\n\n\n\n\nCode\nxreg <- cbind(Inflation = normalized_data_ts[, \"inflation\"],\n              Unemployment = normalized_data_ts[, \"unemployment\"])\nfit.auto <- auto.arima(normalized_data_ts[, \"AAPL.Adjusted\"], xreg = xreg)\nsummary(fit.auto)\n\n\nSeries: normalized_data_ts[, \"AAPL.Adjusted\"] \nRegression with ARIMA(0,1,0) errors \n\nCoefficients:\n      Inflation  Unemployment\n         0.1639       -0.1104\ns.e.     0.0896        0.0407\n\nsigma^2 = 0.04248:  log likelihood = 9.2\nAIC=-12.4   AICc=-11.89   BIC=-6.61\n\nTraining set error measures:\n                     ME      RMSE       MAE       MPE    MAPE      MASE\nTraining set 0.03812368 0.2000733 0.1274114 0.7800981 40.0008 0.4385178\n                  ACF1\nTraining set 0.1228277\n\n\n\n\n\n\nCode\ncheckresiduals(fit.auto)\n\n\n\n\n\n\n    Ljung-Box test\n\ndata:  Residuals from Regression with ARIMA(0,1,0) errors\nQ* = 4.5496, df = 8, p-value = 0.8044\n\nModel df: 0.   Total lags used: 8\n\n\n\n\n\nBased on the results of the auto.arima function, the suggested best model is ARIMA(0,1,0) which is the same as the manual choosen arima model. So, we can then proceed to choose the best GARCH model using ARIMA(0,1,0) as the base model.\nWe can then proceed to choose the best GARCH model using ARIMA(0,1,0) as the base model.\n\n\nSquared Residuals\n\nPlotACF PlotPACF PlotACF AbsoluteARCH Model\n\n\n\n\nCode\nfit <- lm(AAPL.Adjusted ~ inflation+unemployment, data=normalized_numeric_df)\nfit.res<-ts(residuals(fit),star=decimal_date(as.Date(\"2010-01-01\",format = \"%Y-%m-%d\")),frequency = 4)\nfit <- Arima(fit.res,order=c(0,1,0))\nres=fit$res\nplot(res^2,main='Squared Residuals')\n\n\n\n\n\n\n\n\n\nCode\nacf(res^2,24, main = \"ACF Residuals Square\")\n\n\n\n\n\n\n\n\n\nCode\npacf(res^2,24, main = \"PACF Residuals Square\")\n\n\n\n\n\n\n\n\n\nCode\nacf(abs(res), main = \"ACF of Absolute Residuals\")\n\n\n\n\n\n\n\nCode\npacf(abs(res), main = \"PACF of Absolute Residuals\")\n\n\n\n\n\n\n\n\n\nCode\nsummary(garchFit(~garch(1,0),res, trace=F))\n\n\n\nTitle:\n GARCH Modelling \n\nCall:\n garchFit(formula = ~garch(1, 0), data = res, trace = F) \n\nMean and Variance Equation:\n data ~ garch(1, 0)\n<environment: 0x13a282308>\n [data = res]\n\nConditional Distribution:\n norm \n\nCoefficient(s):\n       mu      omega     alpha1  \n0.0090525  0.0375268  0.6903879  \n\nStd. Errors:\n based on Hessian \n\nError Analysis:\n        Estimate  Std. Error  t value Pr(>|t|)   \nmu      0.009052    0.033063    0.274  0.78424   \nomega   0.037527    0.012149    3.089  0.00201 **\nalpha1  0.690388    0.365975    1.886  0.05924 . \n---\nSignif. codes:  0 '***' 0.001 '**' 0.01 '*' 0.05 '.' 0.1 ' ' 1\n\nLog Likelihood:\n -3.605681    normalized:  -0.06934001 \n\nDescription:\n Sat Jan  6 19:59:31 2024 by user:  \n\n\nStandardised Residuals Tests:\n                                Statistic p-Value  \n Jarque-Bera Test   R    Chi^2  2.85827   0.239516 \n Shapiro-Wilk Test  R    W      0.9672877 0.1618382\n Ljung-Box Test     R    Q(10)  7.91217   0.6374158\n Ljung-Box Test     R    Q(15)  8.892125  0.8830899\n Ljung-Box Test     R    Q(20)  11.42691  0.9343916\n Ljung-Box Test     R^2  Q(10)  12.0452   0.2820429\n Ljung-Box Test     R^2  Q(15)  12.59754  0.6333527\n Ljung-Box Test     R^2  Q(20)  14.78188  0.7887492\n LM Arch Test       R    TR^2   16.50618  0.1691362\n\nInformation Criterion Statistics:\n      AIC       BIC       SIC      HQIC \n0.2540646 0.3666364 0.2478793 0.2972220 \n\n\n\n\n\nFrom the squared residuals of the best ARIMA model, it can be observed that the ACF plot and PACF plot indicate that the residuals are not autocorrelated and are white noise, indicating a good fit of the model. Based on the squared residuals of the best ARIMA model, we can see that the ACF and PACF plots indicate that most of the values lie between the blue lines. Additionally, the p-value is 1 and q-value is 0. This suggests that the model has a good fit and that there is no significant autocorrelation or partial autocorrelation in the residuals.\nThe best models are ARIMA(0,1,0) and ARCH(1)\n\n\nBest Model\n\nARIMA ModelGARCH ModelVolatility\n\n\n\n\nCode\n#fiting an ARIMA model to the Inflation variable\ninflation_fit<-auto.arima(normalized_numeric_df$inflation) \nfinflation<-forecast(inflation_fit)\n\n#fitting an ARIMA model to the Unemployment variable\nunemployment_fit<-auto.arima(normalized_numeric_df$unemployment) \nfunemployment<-forecast(unemployment_fit)\n\n# best model fit for forcasting\nxreg <- cbind(Inflation = normalized_data_ts[, \"inflation\"],\n              Unemployment = normalized_data_ts[, \"unemployment\"])\n\n\n\nsummary(arima.fit<-Arima(normalized_data_ts[, \"AAPL.Adjusted\"],order=c(0,1,0),xreg=xreg),include.drift = TRUE)\n\n\nSeries: normalized_data_ts[, \"AAPL.Adjusted\"] \nRegression with ARIMA(0,1,0) errors \n\nCoefficients:\n      Inflation  Unemployment\n         0.1639       -0.1104\ns.e.     0.0896        0.0407\n\nsigma^2 = 0.04248:  log likelihood = 9.2\nAIC=-12.4   AICc=-11.89   BIC=-6.61\n\nTraining set error measures:\n                     ME      RMSE       MAE       MPE    MAPE      MASE\nTraining set 0.03812368 0.2000733 0.1274114 0.7800981 40.0008 0.4385178\n                  ACF1\nTraining set 0.1228277\n\n\n\n\n\n\nCode\nsummary(final.fit <- garchFit(~garch(1,0), res,trace = F))\n\n\n\nTitle:\n GARCH Modelling \n\nCall:\n garchFit(formula = ~garch(1, 0), data = res, trace = F) \n\nMean and Variance Equation:\n data ~ garch(1, 0)\n<environment: 0x13e4ee348>\n [data = res]\n\nConditional Distribution:\n norm \n\nCoefficient(s):\n       mu      omega     alpha1  \n0.0090525  0.0375268  0.6903879  \n\nStd. Errors:\n based on Hessian \n\nError Analysis:\n        Estimate  Std. Error  t value Pr(>|t|)   \nmu      0.009052    0.033063    0.274  0.78424   \nomega   0.037527    0.012149    3.089  0.00201 **\nalpha1  0.690388    0.365975    1.886  0.05924 . \n---\nSignif. codes:  0 '***' 0.001 '**' 0.01 '*' 0.05 '.' 0.1 ' ' 1\n\nLog Likelihood:\n -3.605681    normalized:  -0.06934001 \n\nDescription:\n Sat Jan  6 19:59:31 2024 by user:  \n\n\nStandardised Residuals Tests:\n                                Statistic p-Value  \n Jarque-Bera Test   R    Chi^2  2.85827   0.239516 \n Shapiro-Wilk Test  R    W      0.9672877 0.1618382\n Ljung-Box Test     R    Q(10)  7.91217   0.6374158\n Ljung-Box Test     R    Q(15)  8.892125  0.8830899\n Ljung-Box Test     R    Q(20)  11.42691  0.9343916\n Ljung-Box Test     R^2  Q(10)  12.0452   0.2820429\n Ljung-Box Test     R^2  Q(15)  12.59754  0.6333527\n Ljung-Box Test     R^2  Q(20)  14.78188  0.7887492\n LM Arch Test       R    TR^2   16.50618  0.1691362\n\nInformation Criterion Statistics:\n      AIC       BIC       SIC      HQIC \n0.2540646 0.3666364 0.2478793 0.2972220 \n\n\n\n\n\n\nCode\nht <- final.fit@h.t #a numeric vector with the conditional variances (h.t = sigma.t^delta)\n\n#############################\ndata=data.frame(final)\ndata$Date<-as.Date(data$Date,\"%Y-%m-%d\")\n\n\ndata2= data.frame(ht,data$Date)\nggplot(data2, aes(y = ht, x = data.Date)) + geom_line(col = '#922B21') + ylab('Conditional Variance') + xlab('Date')\n\n\n\n\n\n\n\n\nFrom the ARIMA(0,1,0), we see that the training set error measures also suggest a good fit, with low mean absolute error, root mean squared error, and autocorrelation of the residuals. GATCH(1,0) model model is used to estimate the volatility of the standardized residuals of the previous regression model. The model includes a mean equation that estimates the mean of the residuals and a variance equation that models the conditional variance of the residuals. The coefficients of the mean equation suggest that the mean of the residuals is close to zero. The variance equation coefficients suggest that the conditional variance of the residuals is dependent on the past conditional variances and the past squared standardized residuals. The model’s AIC, BIC, SIC, and HQIC values are all relatively low, indicating a good fit of the model. The standardized residuals tests indicate that the residuals are approximately normally distributed and that there is no significant autocorrelation in the residuals.\nThe volatility of the model seems high in 2020 but has decreased gradually in the past few months. This could indicate that the asset’s price was experiencing a lot of fluctuations in 2020, but the market has stabilized recently.\n\n\nModel Diagnostics\n\nResidualsQQ PlotBox Test\n\n\n\n\nCode\nfit2<-garch(res,order=c(1,0),trace=F)\ncheckresiduals(fit2) \n\n\nWarning in modeldf.default(object): Could not find appropriate degrees of\nfreedom for this model.\n\n\n\n\n\n\n\n\n\nCode\nqqnorm(fit2$residuals, pch = 1)\nqqline(fit2$residuals, col = \"blue\", lwd = 2)\n\n\n\n\n\n\n\n\n\nCode\nBox.test (fit2$residuals, type = \"Ljung\")\n\n\n\n    Box-Ljung test\n\ndata:  fit2$residuals\nX-squared = 2.6613, df = 1, p-value = 0.1028\n\n\nThe ACF plot of the residuals shows all the values between the blue lines, which indicates that the residuals are not significantly autocorrelated. The range of values for the residual plot between -2 and 2 is considered acceptable. Additionally, the QQ plot of the residuals shows a linear plot on the line, which is another good indication that the residuals are normally distributed. The QQ plot is a valuable tool to assess if the residuals follow a normal distribution, and in this case, the plot suggests that the residuals do indeed follow a normal distribution.\nThe Box-Ljung test, a p-value of 0.1028 indicates that the model’s residuals are not significantly autocorrelated, meaning that the model has captured most of the information in the data. This result is good because it suggests that the model is a good fit for the data and has accounted for most of the underlying patterns in the data. Therefore, we can rely on the model’s predictions and use them to make informed decisions.\n\n\n\n\n\nForecast\n\n\nCode\npredict(final.fit, n.ahead = 5, plot=TRUE)\n\n\n\n\n\n  meanForecast meanError standardDeviation lowerInterval upperInterval\n1   0.00905246 0.2478172         0.2478172    -0.4766603     0.4947653\n2   0.00905246 0.2827116         0.2827116    -0.5450521     0.5631570\n3   0.00905246 0.3044777         0.3044777    -0.5877128     0.6058177\n4   0.00905246 0.3186383         0.3186383    -0.6154671     0.6335720\n5   0.00905246 0.3280581         0.3280581    -0.6339296     0.6520345\n\n\nThe forecasted plot is based on the best model ARIMAX(0,1,0)+GARCH(1,0). This model takes into account the autoregressive and moving average components of the data, as well as the impact of exogenous variables on the time series. Additionally, the GARCH component of the model accounts for the volatility clustering in the data. Overall, this model is well-suited to make accurate predictions about future values of the time series.\n\n\nEquation of the Model\nThe equation of the ARIMAX(0,1,0) model is:\n\\(Y(t) = Y(t-1) + \\epsilon(t)\\)\nwhere \\(Y(t)\\) is the time series variable and \\(\\epsilon(t)\\) is the error term.\nThe equation of the ARCH(1) model is:\n\\(\\sigma_{t}^{2}=\\alpha_{0}+\\alpha_{1}\\epsilon_{t-1}^{2}\\)\nwhere \\(\\sigma^2_t\\) is the conditional variance at time \\(t\\), \\(\\alpha_0\\) is a constant, \\(\\alpha_1\\) and \\(\\beta_1\\) are the parameters, and \\(\\epsilon_t\\) is the error term.\nThe combined equation of the ARIMAX(1,1,1)+GARCH(1,1) model is:\n\\(Y(t) = Y(t-1) + \\epsilon(t)\\)\n\\(\\sigma_{t}^{2}=\\alpha_{0}+\\alpha_{1}\\epsilon_{t-1}^{2}\\)"
  },
  {
    "objectID": "arch_Utilities_Sector_Fund.html",
    "href": "arch_Utilities_Sector_Fund.html",
    "title": "",
    "section": "",
    "text": "Impact of Macroeconomic Factors on NextEra Energy Stock Price\n\n\n\nOne of the top companies in the utility sector is NextEra Energy, Inc. (NEE), which operates as a clean energy company. To analyze the stock price behavior of NextEra Energy, an ARIMAX+ARCH/GARCH model can be employed.\nBy using an ARIMAX+ARCH/GARCH model to analyze the stock price behavior of NextEra Energy, we can gain insights into how macroeconomic factors impact the company’s performance. For example, an increase in renewable energy adoption, government policies supporting clean energy, or a decrease in natural gas prices may lead to increased demand for NextEra Energy’s clean energy services and a rise in its stock price. Conversely, extreme weather conditions or natural disasters that impact NextEra Energy’s operations may lead to a decrease in its stock price. Additionally, changes in interest rates or inflation may impact NextEra Energy’s cost of capital, which may affect the company’s profitability and stock price.\n\n\nTime series Plot\n\nNEEDifferentitaed Chart SeriesGDP GrowthInflationInterestUnemployment\n\n\n\n\nCode\n# get data\noptions(\"getSymbols.warning4.0\"=FALSE)\noptions(\"getSymbols.yahoo.warning\"=FALSE)\n\n\ndata.info = getSymbols(\"NEE\",src='yahoo', from = '2010-01-01',to = \"2023-03-01\",auto.assign = FALSE)\ndata = getSymbols(\"NEE\",src='yahoo', from = '2010-01-01',to = \"2023-03-01\")\ndf <- data.frame(Date=index(NEE),coredata(NEE))\n\n# create Bollinger Bands\nbbands <- BBands(NEE[,c(\"NEE.High\",\"NEE.Low\",\"NEE.Close\")])\n\n# join and subset data\ndf <- subset(cbind(df, data.frame(bbands[,1:3])), Date >= \"2010-01-01\")\n\n\n# colors column for increasing and decreasing\nfor (i in 1:length(df[,1])) {\n  if (df$NEE.Close[i] >= df$NEE.Open[i]) {\n      df$direction[i] = 'Increasing'\n  } else {\n      df$direction[i] = 'Decreasing'\n  }\n}\n\ni <- list(line = list(color = '#7DCEA0'))\nd <- list(line = list(color = '#7F7F7F'))\n\n# plot candlestick chart\n\nfig <- df %>% plot_ly(x = ~Date, type=\"candlestick\",\n          open = ~NEE.Open, close = ~NEE.Close,\n          high = ~NEE.High, low = ~NEE.Low, name = \"NEE\",\n          increasing = i, decreasing = d) \nfig <- fig %>% add_lines(x = ~Date, y = ~up , name = \"B Bands\",\n            line = list(color = '#ccc', width = 0.5),\n            legendgroup = \"Bollinger Bands\",\n            hoverinfo = \"none\", inherit = F) \nfig <- fig %>% add_lines(x = ~Date, y = ~dn, name = \"B Bands\",\n            line = list(color = '#ccc', width = 0.5),\n            legendgroup = \"Bollinger Bands\", inherit = F,\n            showlegend = FALSE, hoverinfo = \"none\") \nfig <- fig %>% add_lines(x = ~Date, y = ~mavg, name = \"Mv Avg\",\n            line = list(color = '#C052B3', width = 0.5),\n            hoverinfo = \"none\", inherit = F) \nfig <- fig %>% layout(yaxis = list(title = \"Price\"))\n\n# plot volume bar chart\nfig2 <- df \nfig2 <- fig2 %>% plot_ly(x=~Date, y=~NEE.Volume, type='bar', name = \"NEE Volume\",\n          color = ~direction, colors = c('#7DCEA0','#7F7F7F')) \nfig2 <- fig2 %>% layout(yaxis = list(title = \"Volume\"))\n\n# create rangeselector buttons\nrs <- list(visible = TRUE, x = 0.5, y = -0.055,\n           xanchor = 'center', yref = 'paper',\n           font = list(size = 9),\n           buttons = list(\n             list(count=1,\n                  label='RESET',\n                  step='all'),\n             list(count=3,\n                  label='3 YR',\n                  step='year',\n                  stepmode='backward'),\n             list(count=1,\n                  label='1 YR',\n                  step='year',\n                  stepmode='backward'),\n             list(count=1,\n                  label='1 MO',\n                  step='month',\n                  stepmode='backward')\n           ))\n\n# subplot with shared x axis\nfig <- subplot(fig, fig2, heights = c(0.7,0.2), nrows=2,\n             shareX = TRUE, titleY = TRUE)\nfig <- fig %>% layout(title = paste(\"NEE Stock Price: January 2010 - March 2023\"),\n         xaxis = list(rangeselector = rs),\n         legend = list(orientation = 'h', x = 0.5, y = 1,\n                       xanchor = 'center', yref = 'paper',\n                       font = list(size = 10),\n                       bgcolor = 'transparent'))\n\nfig\n\n\n\n\n\n\n\n\n\n\nCode\nlog(data.info$`NEE.Adjusted`) %>% diff() %>% chartSeries(theme=chartTheme('white'),up.col='#7DCEA0')\n\n\n\n\n\n\n\n\n\nCode\n#import the data\ngdp <- read.csv(\"DATA/RAW DATA/gdp-growth.csv\")\n\n#change date format\ngdp$Date <- as.Date(gdp$DATE , \"%m/%d/%Y\")\n\n#drop DATE column\ngdp <- subset(gdp, select = -c(1))\n\n#export the cleaned data\ngdp_clean <- gdp\nwrite.csv(gdp_clean, \"DATA/CLEANED DATA/gdp_clean_data.csv\", row.names=FALSE)\n\n#plot gdp growth rate \nfig <- plot_ly(gdp, x = ~Date, y = ~value, type = 'scatter', mode = 'lines',line = list(color = 'rgb(240, 128, 128)'))\nfig <- fig %>% layout(title = \"U.S GPD Growth Rate: 2010 - 2022\",xaxis = list(title = \"Time\"),yaxis = list(title =\"GDP Growth Rate\"))\nfig\n\n\n\n\n\n\n\n\n\n\nCode\n#import the data\ninflation_rate <- read.csv(\"DATA/RAW DATA/inflation-rate.csv\")\n\n#cleaning the data\n#remove unwanted columns\ninflation_rate_clean <- subset(inflation_rate, select = -c(1,HALF1,HALF2))\n\n#convert the data to time series data\ninflation_data_ts <- ts(as.vector(t(as.matrix(inflation_rate_clean))), start=c(2010,1), end=c(2023,2), frequency=12)\n\n#export the data\nwrite.csv(inflation_rate_clean, \"DATA/CLEANED DATA/inflation_rate_clean_data.csv\", row.names=FALSE)\n\n\n#plot inflation rate \nfig <- autoplot(inflation_data_ts, ylab = \"Inflation Rate\", color=\"#FFA07A\")+ggtitle(\"U.S Inflation Rate: January 2010 - February 2023\")+theme_bw()\nggplotly(fig)\n\n\n\n\n\n\n\n\n\n\nCode\n#import the data\ninterest_data <- read.csv(\"DATA/RAW DATA/interest-rate.csv\")\n\n#change date format\ninterest_data$Date <- as.Date(interest_data$Date , \"%m/%d/%Y\")\n\n#export the cleaned data\ninterest_clean_data <- interest_data\nwrite.csv(interest_clean_data, \"DATA/CLEANED DATA/interest_rate_clean_data.csv\", row.names=FALSE)\n\n#plot interest rate \nfig <- plot_ly(interest_data, x = ~Date, y = ~value, type = 'scatter', mode = 'lines',line = list(color='rgb(219, 112, 147)'))\nfig <- fig %>% layout(title = \"U.S Interest Rate: January 2010 - March 2023\",xaxis = list(title = \"Time\"),yaxis = list(title =\"Interest Rate\"))\nfig\n\n\n\n\n\n\n\n\n\n\nCode\n#import the data\nunemployment_rate <- read.csv(\"DATA/RAW DATA/unemployment-rate.csv\")\n\n#change date format\nunemployment_rate$Date <- as.Date(unemployment_rate$Date , \"%m/%d/%Y\")\n\n# export the data\nwrite.csv(unemployment_rate, \"DATA/CLEANED DATA/unemployment_rate_clean_data.csv\", row.names=FALSE)\n\n#plot unemployment rate \nfig <- plot_ly(unemployment_rate, x = ~Date, y = ~Value, type = 'scatter', mode = 'lines',line = list(color = 'rgb(189, 183, 107)'))\nfig <- fig %>% layout(title = \"U.S Unemployment Rate: January 2010 - March 2023\",xaxis = list(title = \"Time\"),yaxis = list(title =\"Unemployment Rate\"))\nfig\n\n\n\n\n\n\n\n\n\nthe stock prices and volume of NextEra Energy (NEE) from January 2010 to March 2023. Overall, NEE has experienced a steady increase in stock prices over the period, with some fluctuations along the way.\nThe Bollinger Bands, which are represented by the upper and lower bands in the candlestick chart, show the standard deviation of the stock prices over a certain period of time. When the prices approach the upper band, it is an indication that the stock is overbought and may be due for a correction. Conversely, when the prices approach the lower band, it is an indication that the stock is oversold and may be due for a rebound.\nThe volume chart shows the trading volume of NEE during the same period. It is evident that the highest trading volumes occurred during periods of significant price changes, indicating high investor interest and activity.\nBased on the plot, it appears that NEE is a relatively stable and reliable investment option, with steady growth over the years. The stock has shown volatility for the year during 2021, that is because of COVID19.\nAs discussed before, the macroeconomic factors of GDP growth, inflation, interest rates, and unemployment rate are closely interrelated and play a crucial role in the overall health and stability of an economy. From 2010 to 2023, the global economy experienced a mix of ups and downs, with periods of strong GDP growth followed by slowdowns and recessions.\nThe second plot shows the first difference of the logarithm of the adjusted NEE stock price. Taking the first difference removes any long-term trends and transforms the time series into a stationary process. From the plot, we can observe that the first difference of the logarithm of the NEE stock price appears to be stationary, as the mean and variance are roughly constant over time.\n\n\nEnodogenous and Exogenous Variables\n\nPlotCorrelation HeatmapCCF GDPCCF InterestCCF InflationCCF Unemployment\n\n\n\n\nCode\nnumeric_data <- c(\"NEE.Adjusted\",\"gdp\", \"interest\", \"inflation\", \"unemployment\")\nnumeric_data <- final[, numeric_data]\nnormalized_data_numeric <- scale(numeric_data)\nnormalized_data <- ts(normalized_data_numeric, start = c(2010, 1), end = c(2021,10),frequency = 4)\nts_plot(normalized_data,\n        title = \"Normalized Time Series Data for NEE Stock and Macroeconomic Variables\",\n        Ytitle = \"Normalized Values\",\n        Xtitle = \"Year\")\n\n\n\n\n\n\n\n\n\n\nCode\n# Get upper triangle of the correlation matrix\nget_upper_tri <- function(cormat){\n    cormat[lower.tri(cormat)]<- NA\n    return(cormat)\n}\ncormat <- round(cor(normalized_data_numeric),2)\n\nupper_tri <- get_upper_tri(cormat)\n\nmelted_cormat <- melt(upper_tri, na.rm = TRUE)\n# Create a ggheatmap\nggheatmap <- ggplot(melted_cormat, aes(Var2, Var1, fill = value))+\n geom_tile(color = \"white\")+\n scale_fill_gradient2(low = \"blue\", high = \"red\", mid = \"white\", \n   midpoint = 0, limit = c(-1,1), space = \"Lab\", \n    name=\"Pearson\\nCorrelation\") +\n  theme_minimal()+ # minimal theme\n theme(axis.text.x = element_text(angle = 45, vjust = 1, \n    size = 12, hjust = 1))+\n coord_fixed()\n\nggheatmap + \ngeom_text(aes(Var2, Var1, label = value), color = \"black\", size = 4) +\ntheme(\n  axis.title.x = element_blank(),\n  axis.title.y = element_blank(),\n  panel.grid.major = element_blank(),\n  panel.border = element_blank(),\n  panel.background = element_blank(),\n  axis.ticks = element_blank(),\n  legend.justification = c(1, 0),\n  legend.position = c(0.6, 0.7),\n  legend.direction = \"horizontal\")+\n  guides(fill = guide_colorbar(barwidth = 7, barheight = 1,\n                title.position = \"top\", title.hjust = 0.5))\n\n\n\n\n\n\n\n\n\nCode\npar(mfrow=c(1,1))\nccf_result <- ccf(normalized_data[, c(\"NEE.Adjusted\")], normalized_data[, c(\"gdp\")], \n    lag.max = 300,\n    main = \"Cros-Correlation Plot for NEE Stock Price and GDP Growth Rate \",\n    ylab = \"CCF\")\n\n\n\n\n\nCode\ncat(\"The sum of cross correlation function is\", sum(abs(ccf_result$acf)))\n\n\nThe sum of cross correlation function is 4.619392\n\n\n\n\n\n\nCode\npar(mfrow=c(1,1))\nccf_result <- ccf(normalized_data[, c(\"NEE.Adjusted\")], normalized_data[, c(\"interest\")], \n    lag.max = 300,\n    main = \"Cros-Correlation Plot for NEE Stock Price and Interest Rate\",\n    ylab = \"CCF\")\n\n\n\n\n\nCode\ncat(\"The sum of cross correlation function is\", sum(abs(ccf_result$acf)))\n\n\nThe sum of cross correlation function is 11.24368\n\n\n\n\n\n\nCode\npar(mfrow=c(1,1))\nccf_result <- ccf(normalized_data[, c(\"NEE.Adjusted\")], normalized_data[, c(\"inflation\")], \n    lag.max = 300,\n    main = \"Cros-Correlation Plot for NEE Stock Price and Inflation Rate\",\n    ylab = \"CCF\")\n\n\n\n\n\nCode\ncat(\"The sum of cross correlation function is\", sum(abs(ccf_result$acf)))\n\n\nThe sum of cross correlation function is 16.46885\n\n\n\n\n\n\nCode\npar(mfrow=c(1,1))\nccf_result <- ccf(normalized_data[, c(\"NEE.Adjusted\")], normalized_data[, c(\"unemployment\")], \n    lag.max = 300,\n    main = \"Cros-Correlation Plot for NEE Stock Priceand Unemployment Rate\",\n    ylab = \"CCF\")\n\n\n\n\n\nCode\ncat(\"The sum of cross correlation function is\", sum(abs(ccf_result$acf)))\n\n\nThe sum of cross correlation function is 19.96215\n\n\n\n\n\nThe Normalized Time Series Data for Stock Price and Macroeconomic Variables plot shows the same variables as the first plot but has been normalized to a common range of 0 to 1 using the scale() function in R, which standardizes the variables to have a mean of 0 and a standard deviation of 1. The heatmap analysis of the normalized data reveals that inflation and unemployment rate exhibit strong positive correlations with the stock price indices, indicating that these variables may significantly influence stock price movements. On the other hand, weaker correlations were observed between the stock price indices and GDP and interest rates, suggesting that these variables may have less impact on stock price fluctuations. The cross-correlation feature plots confirm these findings, indicating that inflation and unemployment rate are more suitable feature variables for the ARIMAX model when predicting NEE movements.\nFinal Exogenous variables: Macroeconomic indicators: Inflation rate and unemployment rate.\n\n\nEnodogenous and Exogenous Variables Plot\n\nPlotCheck the stationarity\n\n\n\n\nCode\nfinal_data <- final %>%dplyr::select( Date,NEE.Adjusted, inflation,unemployment)\nnumeric_data <- c(\"NEE.Adjusted\", \"inflation\",\"unemployment\")\nnumeric_data <- final_data[, numeric_data]\nnormalized_data_numeric <- scale(numeric_data)\nnormalized_numeric_df <- data.frame(normalized_data_numeric)\nnormalized_data_ts <- ts(normalized_data_numeric, start = c(2010, 1), frequency = 4)\n\nautoplot(normalized_data_ts, facets=TRUE) +\n  xlab(\"Year\") + ylab(\"\") +\n  ggtitle(\"NEE Stock Price, Inflation Rate and Unemployment Rate in USA 2010-2023\")\n\n\n\n\n\n\n\n\n\nCode\n# Convert your multivariate time series data to a matrix\nfinal_data_ts_multivariate <- as.matrix(normalized_data_ts)\n\n# Check for stationarity using Phillips-Perron test\nphillips_perron_test <- ur.pp(final_data_ts_multivariate)  \nsummary(phillips_perron_test)\n\n\n\n################################## \n# Phillips-Perron Unit Root Test # \n################################## \n\nTest regression with intercept \n\n\nCall:\nlm(formula = y ~ y.l1)\n\nResiduals:\n    Min      1Q  Median      3Q     Max \n-2.1915 -0.1453 -0.0565  0.0819  4.4942 \n\nCoefficients:\n             Estimate Std. Error t value Pr(>|t|)    \n(Intercept) 0.0008174  0.0412102    0.02    0.984    \ny.l1        0.8574256  0.0416277   20.60   <2e-16 ***\n---\nSignif. codes:  0 '***' 0.001 '**' 0.01 '*' 0.05 '.' 0.1 ' ' 1\n\nResidual standard error: 0.5131 on 153 degrees of freedom\nMultiple R-squared:  0.735, Adjusted R-squared:  0.7332 \nF-statistic: 424.3 on 1 and 153 DF,  p-value: < 2.2e-16\n\n\nValue of test-statistic, type: Z-alpha  is: -23.1887 \n\n         aux. Z statistics\nZ-tau-mu            0.0204\n\n\n\n\n\nThe results of the Phillips-Perron unit root test indicate strong evidence against the null hypothesis of a unit root, as the p-value for the coefficient of the lagged variable is less than the significance level of 0.05. This suggests that the variable y, which is being tested for stationarity, is likely stationary. Furthermore, the test statistic Z-tau-mu is 0.0204, which is smaller than the critical value of Z-alpha (-23.1887), providing further evidence of stationarity.\nTo determine whether the linear model requires an ARCH model, an ARCH test is conducted. The ACF and PACF plots are also used to identify suitable model values.\n\n\nModel Fitting\n\nPlotARCH TestACF PlotPACF Plot\n\n\n\n\nCode\nnormalized_numeric_df$NEE.Adjusted<-ts(normalized_numeric_df$NEE.Adjusted,star=decimal_date(as.Date(\"2010-01-01\",format = \"%Y-%m-%d\")),frequency = 4)\nnormalized_numeric_df$inflation<-ts(normalized_numeric_df$inflation,star=decimal_date(as.Date(\"2010-01-01\",format = \"%Y-%m-%d\")),frequency = 4)\nnormalized_numeric_df$unemployment<-ts(normalized_numeric_df$unemployment,star=decimal_date(as.Date(\"2010-01-01\",format = \"%Y-%m-%d\")),frequency = 4)\n\nfit <- lm(NEE.Adjusted ~ inflation+unemployment, data=normalized_numeric_df)\nfit.res<-ts(residuals(fit),star=decimal_date(as.Date(\"2010-01-01\",format = \"%Y-%m-%d\")),frequency = 4)\n############## Then look at the residuals ############\nreturns <- fit.res  %>% diff()\nautoplot(returns)+ggtitle(\"Linear Model Returns\")\n\n\n\n\n\n\n\n\n\nCode\nbyd.archTest <- ArchTest(fit.res, lags = 1, demean = TRUE)\nbyd.archTest\n\n\n\n    ARCH LM-test; Null hypothesis: no ARCH effects\n\ndata:  fit.res\nChi-squared = 33.062, df = 1, p-value = 8.926e-09\n\n\n\n\n\n\nCode\nggAcf(returns) +ggtitle(\"ACF for returns\")\n\n\n\n\n\n\n\n\n\nCode\nggPacf(returns) +ggtitle(\"PACF for returns\")\n\n\n\n\n\n\n\n\nThe ARCH LM-test was conducted with the null hypothesis of no ARCH effects. The test resulted in a chi-squared value of 33.062 with one degree of freedom, and a very low p-value of 8.926e-09. This suggests strong evidence against the null hypothesis, indicating the presence of ARCH effects in the data.\nBased on the ACF and PACF plots, it appears that there is some significant autocorrelation and partial autocorrelation at multiple lags, which suggests that an ARIMA model may not be sufficient to capture the time series behavior. Additionally, the values for p and q appear to be relatively high, with p = 0 and q = 0 being suggested by the plots.\n\n\nARIMAX Model\n\nAuto Arima ModelAuto Arima ResidualsARIMA (0,1,0) PlotARIMA (0,1,0) Model\n\n\n\n\nCode\nxreg <- cbind(Inflation = normalized_data_ts[, \"inflation\"],\n              Unemployment = normalized_data_ts[, \"unemployment\"])\nfit.auto <- auto.arima(normalized_data_ts[, \"NEE.Adjusted\"], xreg = xreg)\nsummary(fit.auto)\n\n\nSeries: normalized_data_ts[, \"NEE.Adjusted\"] \nRegression with ARIMA(0,1,0)(0,0,1)[4] errors \n\nCoefficients:\n        sma1   drift  Inflation  Unemployment\n      0.5305  0.0468     0.1021       -0.0654\ns.e.  0.1278  0.0208     0.0400        0.0175\n\nsigma^2 = 0.01067:  log likelihood = 44.82\nAIC=-79.65   AICc=-78.32   BIC=-69.99\n\nTraining set error measures:\n                       ME       RMSE        MAE      MPE    MAPE      MASE\nTraining set 0.0002636712 0.09822197 0.07276084 1.973384 16.4772 0.2817091\n                  ACF1\nTraining set 0.1378295\n\n\n\n\n\n\nCode\ncheckresiduals(fit.auto)\n\n\n\n\n\n\n    Ljung-Box test\n\ndata:  Residuals from Regression with ARIMA(0,1,0)(0,0,1)[4] errors\nQ* = 5.6575, df = 7, p-value = 0.5803\n\nModel df: 1.   Total lags used: 8\n\n\n\n\n\n\nCode\nset.seed(1234)\n\nmodel_output <- capture.output(sarima(fit.res, 0,1,0)) \n\n\n\n\n\n\n\n\n\nCode\ncat(model_output[9:38], model_output[length(model_output)], sep = \"\\n\")\n\n\n$fit\n\nCall:\narima(x = xdata, order = c(p, d, q), seasonal = list(order = c(P, D, Q), period = S), \n    xreg = constant, transform.pars = trans, fixed = fixed, optim.control = list(trace = trc, \n        REPORT = 1, reltol = tol))\n\nCoefficients:\n      constant\n        0.0036\ns.e.    0.0341\n\nsigma^2 estimated as 0.05931:  log likelihood = -0.33,  aic = 4.66\n\n$degrees_of_freedom\n[1] 50\n\n$ttable\n         Estimate     SE t.value p.value\nconstant   0.0036 0.0341  0.1056  0.9163\n\n$AIC\n[1] 0.09127795\n\n$AICc\n[1] 0.09287859\n\n$BIC\n[1] 0.1670358\n\n\n\n\n\nThe auto.arima function suggests the ARIMA(0,1,0)(0,0,1)[4] model, but the acf and pacf plots suggest a simpler ARIMA(0,1,0) model. Upon comparison, the AIC and BIC values of the ARIMA(0,1,0) model are much lower than the other model, indicating that it is the better choice.\nWe can then proceed to choose the best GARCH model using ARIMA(0,1,0) as the base model.\n\n\nSquared Residuals\n\nPlotACF PlotPACF PlotGRACH(1,1)\n\n\n\n\nCode\nfit <- lm(NEE.Adjusted ~ inflation+unemployment, data=normalized_numeric_df)\nfit.res<-ts(residuals(fit),star=decimal_date(as.Date(\"2010-01-01\",format = \"%Y-%m-%d\")),frequency = 4)\nfit <- Arima(fit.res,order=c(0,1,0))\nres=fit$res\nplot(res^2,main='Squared Residuals')\n\n\n\n\n\n\n\n\n\nCode\nacf(res^2,24, main = \"ACF Residuals Square\")\n\n\n\n\n\n\n\n\n\nCode\npacf(res^2,24, main = \"PACF Residuals Square\")\n\n\n\n\n\n\n\n\n\nCode\nsummary(garchFit(~garch(1,1),res, trace=F))\n\n\n\nTitle:\n GARCH Modelling \n\nCall:\n garchFit(formula = ~garch(1, 1), data = res, trace = F) \n\nMean and Variance Equation:\n data ~ garch(1, 1)\n<environment: 0x1314e06a0>\n [data = res]\n\nConditional Distribution:\n norm \n\nCoefficient(s):\n       mu      omega     alpha1      beta1  \n0.0140516  0.0051686  0.2653371  0.6986082  \n\nStd. Errors:\n based on Hessian \n\nError Analysis:\n        Estimate  Std. Error  t value Pr(>|t|)    \nmu      0.014052    0.024565    0.572    0.567    \nomega   0.005169    0.004689    1.102    0.270    \nalpha1  0.265337    0.189150    1.403    0.161    \nbeta1   0.698608    0.152846    4.571 4.86e-06 ***\n---\nSignif. codes:  0 '***' 0.001 '**' 0.01 '*' 0.05 '.' 0.1 ' ' 1\n\nLog Likelihood:\n 5.384989    normalized:  0.1035575 \n\nDescription:\n Sat Jan  6 19:59:50 2024 by user:  \n\n\nStandardised Residuals Tests:\n                                Statistic p-Value     \n Jarque-Bera Test   R    Chi^2  46.08952  9.812695e-11\n Shapiro-Wilk Test  R    W      0.9212262 0.002081869 \n Ljung-Box Test     R    Q(10)  11.23043  0.3398492   \n Ljung-Box Test     R    Q(15)  14.68592  0.4742688   \n Ljung-Box Test     R    Q(20)  18.72223  0.539942    \n Ljung-Box Test     R^2  Q(10)  8.744675  0.5564943   \n Ljung-Box Test     R^2  Q(15)  9.787838  0.832869    \n Ljung-Box Test     R^2  Q(20)  10.55372  0.9569856   \n LM Arch Test       R    TR^2   16.96835  0.1507916   \n\nInformation Criterion Statistics:\n         AIC          BIC          SIC         HQIC \n-0.053268793  0.096826877 -0.064014104  0.004274344 \n\n\n\n\n\nFrom the squared residuals of the best ARIMA model, it can be observed that the ACF plot and PACF plot indicate that the residuals are not autocorrelated and are white noise, indicating a good fit of the model. Based on the squared residuals of the best ARIMA model, we can see that the ACF and PACF plots indicate that most of the values lie between the blue lines. Additionally, the p-value is 1 and q-value is 1. This suggests that the model has a good fit and that there is no significant autocorrelation or partial autocorrelation in the residuals.\nBased on the analysis of the different GARCH models, it appears that GARCH(1,1) is the best model.\n\n\nBest Model\n\nARIMA ModelGARCH ModelVolatility\n\n\n\n\nCode\n#fiting an ARIMA model to the Inflation variable\ninflation_fit<-auto.arima(normalized_numeric_df$inflation) \nfinflation<-forecast(inflation_fit)\n\n#fitting an ARIMA model to the Unemployment variable\nunemployment_fit<-auto.arima(normalized_numeric_df$unemployment) \nfunemployment<-forecast(unemployment_fit)\n\n# best model fit for forcasting\nxreg <- cbind(Inflation = normalized_data_ts[, \"inflation\"],\n              Unemployment = normalized_data_ts[, \"unemployment\"])\n\n\n\nsummary(arima.fit<-Arima(normalized_data_ts[, \"NEE.Adjusted\"],order=c(0,1,0),xreg=xreg),include.drift = TRUE)\n\n\nSeries: normalized_data_ts[, \"NEE.Adjusted\"] \nRegression with ARIMA(0,1,0) errors \n\nCoefficients:\n      Inflation  Unemployment\n         0.1026       -0.0633\ns.e.     0.0539        0.0245\n\nsigma^2 = 0.01536:  log likelihood = 35.15\nAIC=-64.29   AICc=-63.78   BIC=-58.5\n\nTraining set error measures:\n                     ME      RMSE      MAE      MPE     MAPE      MASE\nTraining set 0.04688101 0.1202993 0.087339 1.985271 20.98354 0.3381515\n                  ACF1\nTraining set 0.2016614\n\n\n\n\n\n\nCode\nsummary(final.fit <- garchFit(~garch(1,1), res,trace = F))\n\n\n\nTitle:\n GARCH Modelling \n\nCall:\n garchFit(formula = ~garch(1, 1), data = res, trace = F) \n\nMean and Variance Equation:\n data ~ garch(1, 1)\n<environment: 0x117e110a0>\n [data = res]\n\nConditional Distribution:\n norm \n\nCoefficient(s):\n       mu      omega     alpha1      beta1  \n0.0140516  0.0051686  0.2653371  0.6986082  \n\nStd. Errors:\n based on Hessian \n\nError Analysis:\n        Estimate  Std. Error  t value Pr(>|t|)    \nmu      0.014052    0.024565    0.572    0.567    \nomega   0.005169    0.004689    1.102    0.270    \nalpha1  0.265337    0.189150    1.403    0.161    \nbeta1   0.698608    0.152846    4.571 4.86e-06 ***\n---\nSignif. codes:  0 '***' 0.001 '**' 0.01 '*' 0.05 '.' 0.1 ' ' 1\n\nLog Likelihood:\n 5.384989    normalized:  0.1035575 \n\nDescription:\n Sat Jan  6 19:59:51 2024 by user:  \n\n\nStandardised Residuals Tests:\n                                Statistic p-Value     \n Jarque-Bera Test   R    Chi^2  46.08952  9.812695e-11\n Shapiro-Wilk Test  R    W      0.9212262 0.002081869 \n Ljung-Box Test     R    Q(10)  11.23043  0.3398492   \n Ljung-Box Test     R    Q(15)  14.68592  0.4742688   \n Ljung-Box Test     R    Q(20)  18.72223  0.539942    \n Ljung-Box Test     R^2  Q(10)  8.744675  0.5564943   \n Ljung-Box Test     R^2  Q(15)  9.787838  0.832869    \n Ljung-Box Test     R^2  Q(20)  10.55372  0.9569856   \n LM Arch Test       R    TR^2   16.96835  0.1507916   \n\nInformation Criterion Statistics:\n         AIC          BIC          SIC         HQIC \n-0.053268793  0.096826877 -0.064014104  0.004274344 \n\n\n\n\n\n\nCode\nht <- final.fit@h.t #a numeric vector with the conditional variances (h.t = sigma.t^delta)\n\n#############################\ndata=data.frame(final)\ndata$Date<-as.Date(data$Date,\"%Y-%m-%d\")\n\n\ndata2= data.frame(ht,data$Date)\nggplot(data2, aes(y = ht, x = data.Date)) + geom_line(col = '#7DCEA0') + ylab('Conditional Variance') + xlab('Date')\n\n\n\n\n\n\n\n\nFrom the ARIMA(0,1,0), we see that the training set error measures also suggest a good fit, with low mean absolute error, root mean squared error, and autocorrelation of the residuals. GATCH(1,1) model model is used to estimate the volatility of the standardized residuals of the previous regression model. The model includes a mean equation that estimates the mean of the residuals and a variance equation that models the conditional variance of the residuals. The coefficients of the mean equation suggest that the mean of the residuals is close to zero. The variance equation coefficients suggest that the conditional variance of the residuals is dependent on the past conditional variances and the past squared standardized residuals. The model’s AIC, BIC, SIC, and HQIC values are all relatively low, indicating a good fit of the model. The standardized residuals tests indicate that the residuals are approximately normally distributed and that there is no significant autocorrelation in the residuals.\nThe volatility of the model seems high in 2020 but has decreased gradually in the past few months. This could indicate that the asset’s price was experiencing a lot of fluctuations in 2020, but the market has stabilized recently.\n\n\nModel Diagnostics\n\nResidualsQQ PlotBox Test\n\n\n\n\nCode\nfit2<-garch(res,order=c(1,1),trace=F)\ncheckresiduals(fit2) #relatively doing a good job\n\n\n\n\n\n\n\n\n\nCode\nqqnorm(fit2$residuals, pch = 1)\nqqline(fit2$residuals, col = \"blue\", lwd = 2)\n\n\n\n\n\n\n\n\n\nCode\nBox.test (fit2$residuals, type = \"Ljung\")\n\n\n\n    Box-Ljung test\n\ndata:  fit2$residuals\nX-squared = 0.78499, df = 1, p-value = 0.3756\n\n\nThe ACF plot of the residuals shows all the values between the blue lines, which indicates that the residuals are not significantly autocorrelated. The range of values for the residual plot between -2 and 2 is considered acceptable. Additionally, the QQ plot of the residuals shows a linear plot on the line, which is another good indication that the residuals are normally distributed. The QQ plot is a valuable tool to assess if the residuals follow a normal distribution, and in this case, the plot suggests that the residuals do indeed follow a normal distribution.\nThe Box-Ljung test, a p-value of 0.3754 indicates that the model’s residuals are not significantly autocorrelated, meaning that the model has captured most of the information in the data. This result is good because it suggests that the model is a good fit for the data and has accounted for most of the underlying patterns in the data. Therefore, we can rely on the model’s predictions and use them to make informed decisions.\n\n\n\n\n\nForecast\n\n\nCode\npredict(final.fit, n.ahead = 5, plot=TRUE)\n\n\n\n\n\n  meanForecast meanError standardDeviation lowerInterval upperInterval\n1   0.01405162 0.3214469         0.3214469    -0.6159727     0.6440760\n2   0.01405162 0.3236838         0.3236838    -0.6203570     0.6484603\n3   0.01405162 0.3258256         0.3258256    -0.6245548     0.6526580\n4   0.01405162 0.3278768         0.3278768    -0.6285752     0.6566784\n5   0.01405162 0.3298421         0.3298421    -0.6324270     0.6605302\n\n\nThe forecasted plot is based on the best model ARIMAX(0,1,0)+GARCH(1,1). This model takes into account the autoregressive and moving average components of the data, as well as the impact of exogenous variables on the time series. Additionally, the GARCH component of the model accounts for the volatility clustering in the data. Overall, this model is well-suited to make accurate predictions about future values of the time series.\n\n\nEquation of the Model\nThe equation of the ARIMAX(0,1,0) model is:\n\\(Y(t) = Y(t-1) + \\epsilon(t)\\)\nwhere \\(Y(t)\\) is the time series variable and \\(\\epsilon(t)\\) is the error term.\nThe equation of the GARCH(1,1) model is:\n\\(\\sigma^2(t) = \\alpha_0+\\alpha_1\\epsilon_t^2(t-1)+ \\beta_1\\sigma^2(t-1)\\) where \\(\\sigma^2_t\\) is the conditional variance at time \\(t\\), \\(\\alpha_0\\) is a constant, \\(\\alpha_1\\) and \\(\\beta_1\\) are the parameters, and \\(\\epsilon_t\\) is the error term.\nThe combined equation of the ARIMAX(0,1,0)+GARCH(1,1) model is:\n\\(Y(t) = Y(t-1) + \\epsilon(t)\\)\n\\(\\epsilon(t) = \\sigma(t) * \\epsilon~(t)\\)\n\\(\\sigma^2(t) = \\alpha_0+\\alpha_1\\epsilon_t^2(t-1)+ \\beta_1\\sigma^2(t-1)\\)"
  },
  {
    "objectID": "arima_index_dow_jones.html",
    "href": "arima_index_dow_jones.html",
    "title": "ARIMA Model for Dow Jones Index",
    "section": "",
    "text": "During exploratory data analysis (EDA) of Dow Jones Index, it was observed that the raw data is often non-stationary, exhibiting trends, seasonality, and other complex patterns that can make modeling and forecasting difficult. To address this issue, the data is often differentiated to achieve stationarity, which is a key assumption of the ARIMA model.\nDifferencing involves calculating the difference between each data point and the previous data point, which can help to remove trends and seasonality from the data. The goal is to create a new series of data that is stationary, meaning that the statistical properties of the data do not change over time.\n\nDifferentiated Time Series Plot\n\nPlotACFPACFADF Test\n\n\n\n\nCode\n#import the data\ndf <- read.csv(\"DATA/CLEANED DATA/dji_raw_data.csv\")\n#convert to time series data\nmyts<-ts(df$DJI.Adjusted,frequency=252,start=c(2010,1,4), end = c(2023,3,1)) \n#First order differentiation\ndf1 <- diff(myts)\n# Plot \nmyts  %>% diff() %>% ggtsdisplay(main = \"First order differentiation\") \n\n\n\n\n\n\n\n\n\nCode\n#ACF plot \nggAcf(df1,60,main=\"ACF Plot: First order differentiation\") \n\n\nWarning in ggplot2::geom_segment(lineend = \"butt\", ...): Ignoring unknown\nparameters: `main`\n\n\n\n\n\n\n\n\n\nCode\nggPacf(df1,60,main=\"PACF Plot: First order differentiation\") \n\n\n\n\n\n\n\n\n\nCode\ntseries::adf.test(df1)\n\n\n\n    Augmented Dickey-Fuller Test\n\ndata:  df1\nDickey-Fuller = -15.31, Lag order = 14, p-value = 0.01\nalternative hypothesis: stationary\n\n\n\n\n\nAfter analyzing the ACF and PACF plots, it can be observed that most of the bar lines lie between the blue lines, indicating that the time series is stationary. This has been further confirmed through an Augmented Dickey-Fuller test as the p-value is less than 0.05. Once the data is stationary, the next step is to model it using ARIMA. The combination of ACF and PACF plots can help determine the appropriate values for p, d, and q in the ARIMA model. The order of differencing required to achieve stationarity gives the d value. The p value is determined by examining significant spikes at various lags in the PACF plot, while the q value is determined through significant spikes in the ACF plot. It is important to consider other methods such as grid search and information criteria to ensure the most appropriate model is selected.\nHere the parameters are d = 1 p = 0,7 (PACF Plot) q = 0,7 (ACF Plot)\n\n\nModel Selection\n\nARIMA ResultAuto Arima Model\n\n\n\n\nCode\n######################## Check for different combinations ########\nd=1\ni=1\ntemp = data.frame()\nls=matrix(rep(NA,6*8),nrow=8) #nrow = 8x1x1\n\n\nfor (p in 1:8)# p=01,2,3,4,5,6,7 :7\n{\n  for(q in 1)# q=0 :1\n  {\n    for(d in 1)# d=1 :1\n    {\n      \n      if(p-1+d+q-1<=8)\n      {\n        \n        model<- Arima(myts,order=c(p-1,d,q-1),include.drift=TRUE) \n        ls[i,]= c(p-1,d,q-1,model$aic,model$bic,model$aicc)\n        i=i+1\n        \n      }\n      \n    }\n  }\n}\n\nmodel = as.data.frame(ls)\nnames(model)= c(\"p\",\"d\",\"q\",\"AIC\",\"BIC\",\"AICc\")\n\n#temp\nknitr::kable(model)\n\n\n\n\n\np\nd\nq\nAIC\nBIC\nAICc\n\n\n\n\n0\n1\n0\n45358.07\n45370.26\n45358.07\n\n\n1\n1\n0\n45316.63\n45334.91\n45316.64\n\n\n2\n1\n0\n45299.67\n45324.05\n45299.68\n\n\n3\n1\n0\n45301.65\n45332.13\n45301.67\n\n\n4\n1\n0\n45293.77\n45330.34\n45293.80\n\n\n5\n1\n0\n45292.51\n45335.17\n45292.54\n\n\n6\n1\n0\n45268.12\n45316.88\n45268.17\n\n\n7\n1\n0\n45227.46\n45282.32\n45227.52\n\n\n\n\n\n\n\n\n\nCode\nauto.arima(myts)\n\n\nSeries: myts \nARIMA(2,1,0) with drift \n\nCoefficients:\n          ar1     ar2   drift\n      -0.1060  0.0759  7.1363\ns.e.   0.0174  0.0174  4.1056\n\nsigma^2 = 58682:  log likelihood = -22645.83\nAIC=45299.67   AICc=45299.68   BIC=45324.05\n\n\n\n\n\nThe given table provides different ARIMA models with their corresponding AIC and BIC values. By sorting the table according to each criterion, we can identify the models with the lowest values. The smallest AIC value corresponds to an ARIMA (2,1,0) model, while the ARIMA (0,1,0) model has the lowest BIC value. Additionally, the auto.arima function in R suggests an ARIMA (0,1,0) model as the best fit for the data. Since there are different models to choose from, it is important to perform model diagnostics to determine the best model for the data. These diagnostics can include checking for residual normality, checking for autocorrelation and partial autocorrelation in the residuals, and comparing the forecasted values to the actual values. By conducting these tests, we can determine which model provides the best fit for the data and is therefore the most appropriate to use for forecasting future values.\n\n\nModel Diagnostic\n\nModel 1 PlotModel 1Model 2 PlotModel 2\n\n\n\n\nCode\nmodel_output <- capture.output(sarima(myts,2,1,0))\n\n\n\n\n\n\n\n\n\nCode\ncat(model_output[12:42], model_output[length(model_output)], sep = \"\\n\") \n\n\nconverged\n$fit\n\nCall:\narima(x = xdata, order = c(p, d, q), seasonal = list(order = c(P, D, Q), period = S), \n    xreg = constant, transform.pars = trans, fixed = fixed, optim.control = list(trace = trc, \n        REPORT = 1, reltol = tol))\n\nCoefficients:\n          ar1     ar2  constant\n      -0.1060  0.0759    7.1363\ns.e.   0.0174  0.0174    4.1056\n\nsigma^2 estimated as 58628:  log likelihood = -22645.83,  aic = 45299.67\n\n$degrees_of_freedom\n[1] 3275\n\n$ttable\n         Estimate     SE t.value p.value\nar1       -0.1060 0.0174 -6.0862  0.0000\nar2        0.0759 0.0174  4.3613  0.0000\nconstant   7.1363 4.1056  1.7382  0.0823\n\n$AIC\n[1] 13.8193\n\n$AICc\n[1] 13.8193\n\n$BIC\n\n\n\n\n\n\nCode\nmodel_output <- capture.output(sarima(myts,0,1,0))\n\n\n\n\n\n\n\n\n\nCode\ncat(model_output[9:38], model_output[length(model_output)], sep = \"\\n\") \n\n\n$fit\n\nCall:\narima(x = xdata, order = c(p, d, q), seasonal = list(order = c(P, D, Q), period = S), \n    xreg = constant, transform.pars = trans, fixed = fixed, optim.control = list(trace = trc, \n        REPORT = 1, reltol = tol))\n\nCoefficients:\n      constant\n        7.1352\ns.e.    4.2696\n\nsigma^2 estimated as 59756:  log likelihood = -22677.04,  aic = 45358.07\n\n$degrees_of_freedom\n[1] 3277\n\n$ttable\n         Estimate     SE t.value p.value\nconstant   7.1352 4.2696  1.6712  0.0948\n\n$AIC\n[1] 13.83712\n\n$AICc\n[1] 13.83712\n\n$BIC\n[1] 13.84084\n\n\n\n\n\nTo determine which model diagnosis is the best, we need to consider some key factors such as the coefficient estimates and standard errors, AIC, BIC, p-values, and the log-likelihood. Comparing the two models, model 2 is the best because it has the lowest BIC value when compared and the AIC value are almost the same, which indicate better goodness of fit. Also, all the coefficient estimates in the model are significant, as evidenced by the p-values are better than model 1, which implies that there is a strong correlation between the variables. The t-values are high, indicating that the estimates are highly reliable.\nTherefore, based on the AIC and BIC values, the significance of the coefficient estimates, and the reliability of the estimates, we can conclude that model 2 is the best model diagnosis. Therefore, (0,1,0) is the best model for this time series.\nThe best model identified is ARIMA(0,1,0). By analyzing the standardized residuals plot, it can be observed that the mean is close to 0 and the variance is slightly higher than 1. Deviations from these values could indicate poor model fit. However, in this case, the model seems to be well-fitted. The ACF plot of residuals shows very few significant lags, which is a positive sign for the model. The qq-plot also suggests normality in the residuals. Additionally, the p-values for the Ljung-Box test are greater than 0.05, indicating that the residuals are independent, which is favorable for the model’s accuracy.\nThe equation for this model: \\[(1-\\phi_1B-\\phi_2B^2)(1-B)(Y_t-\\mu) = (1+\\theta_1B+\\theta_2B^2)\\epsilon_t\\]\n\n\nForcast\n\nShort Term ForcastLong term Forcast\n\n\n\n\nCode\nmyts %>%\n  Arima(order=c(0,1,0),include.drift = TRUE) %>%\n  forecast %>%\n  autoplot(main = \"Dow Jones Index Stock Prices Prediction\") +\n  ylab(\"stock prices\") + xlab(\"Year\")\n\n\n\n\n\n\n\n\n\nCode\nsarima.for(myts,182, 0,1,0, main='Dow Jones Index Stock Prices Prediction')\n\n\n\n\n\n$pred\nTime Series:\nStart = c(2023, 4) \nEnd = c(2023, 185) \nFrequency = 252 \n  [1] 33980.15 33987.28 33994.42 34001.55 34008.69 34015.82 34022.96 34030.09\n  [9] 34037.23 34044.36 34051.50 34058.63 34065.77 34072.90 34080.04 34087.17\n [17] 34094.31 34101.44 34108.58 34115.71 34122.85 34129.99 34137.12 34144.26\n [25] 34151.39 34158.53 34165.66 34172.80 34179.93 34187.07 34194.20 34201.34\n [33] 34208.47 34215.61 34222.74 34229.88 34237.01 34244.15 34251.28 34258.42\n [41] 34265.55 34272.69 34279.82 34286.96 34294.09 34301.23 34308.36 34315.50\n [49] 34322.63 34329.77 34336.90 34344.04 34351.18 34358.31 34365.45 34372.58\n [57] 34379.72 34386.85 34393.99 34401.12 34408.26 34415.39 34422.53 34429.66\n [65] 34436.80 34443.93 34451.07 34458.20 34465.34 34472.47 34479.61 34486.74\n [73] 34493.88 34501.01 34508.15 34515.28 34522.42 34529.55 34536.69 34543.82\n [81] 34550.96 34558.09 34565.23 34572.37 34579.50 34586.64 34593.77 34600.91\n [89] 34608.04 34615.18 34622.31 34629.45 34636.58 34643.72 34650.85 34657.99\n [97] 34665.12 34672.26 34679.39 34686.53 34693.66 34700.80 34707.93 34715.07\n[105] 34722.20 34729.34 34736.47 34743.61 34750.74 34757.88 34765.01 34772.15\n[113] 34779.28 34786.42 34793.56 34800.69 34807.83 34814.96 34822.10 34829.23\n[121] 34836.37 34843.50 34850.64 34857.77 34864.91 34872.04 34879.18 34886.31\n[129] 34893.45 34900.58 34907.72 34914.85 34921.99 34929.12 34936.26 34943.39\n[137] 34950.53 34957.66 34964.80 34971.93 34979.07 34986.20 34993.34 35000.47\n[145] 35007.61 35014.74 35021.88 35029.02 35036.15 35043.29 35050.42 35057.56\n[153] 35064.69 35071.83 35078.96 35086.10 35093.23 35100.37 35107.50 35114.64\n[161] 35121.77 35128.91 35136.04 35143.18 35150.31 35157.45 35164.58 35171.72\n[169] 35178.85 35185.99 35193.12 35200.26 35207.39 35214.53 35221.66 35228.80\n[177] 35235.93 35243.07 35250.21 35257.34 35264.48 35271.61\n\n$se\nTime Series:\nStart = c(2023, 4) \nEnd = c(2023, 185) \nFrequency = 252 \n  [1]  244.4498  345.7043  423.3995  488.8996  546.6064  598.7773  646.7534\n  [8]  691.4085  733.3495  773.0182  810.7483  846.7990  881.3764  914.6475\n [15]  946.7501  977.7993 1007.8924 1037.1128 1065.5321 1093.2128 1120.2098\n [22] 1146.5713 1172.3402 1197.5547 1222.2491 1246.4544 1270.1985 1293.5069\n [29] 1316.4026 1338.9068 1361.0390 1382.8170 1404.2573 1425.3752 1446.1847\n [36] 1466.6989 1486.9302 1506.8899 1526.5887 1546.0364 1565.2426 1584.2159\n [43] 1602.9647 1621.4967 1639.8193 1657.9394 1675.8636 1693.5981 1711.1488\n [50] 1728.5213 1745.7209 1762.7527 1779.6216 1796.3320 1812.8884 1829.2950\n [57] 1845.5557 1861.6744 1877.6547 1893.5002 1909.2142 1924.7998 1940.2603\n [64] 1955.5986 1970.8175 1985.9198 2000.9080 2015.7849 2030.5527 2045.2140\n [71] 2059.7708 2074.2255 2088.5802 2102.8369 2116.9976 2131.0642 2145.0385\n [78] 2158.9224 2172.7176 2186.4257 2200.0484 2213.5873 2227.0439 2240.4196\n [85] 2253.7160 2266.9344 2280.0762 2293.1426 2306.1350 2319.0546 2331.9027\n [92] 2344.6803 2357.3887 2370.0290 2382.6022 2395.1093 2407.5515 2419.9298\n [99] 2432.2450 2444.4982 2456.6903 2468.8222 2480.8948 2492.9088 2504.8653\n[106] 2516.7650 2528.6086 2540.3971 2552.1311 2563.8114 2575.4387 2587.0138\n[113] 2598.5373 2610.0099 2621.4323 2632.8052 2644.1291 2655.4048 2666.6327\n[120] 2677.8136 2688.9481 2700.0365 2711.0797 2722.0780 2733.0321 2743.9425\n[127] 2754.8096 2765.6340 2776.4163 2787.1568 2797.8561 2808.5146 2819.1329\n[134] 2829.7113 2840.2503 2850.7503 2861.2118 2871.6352 2882.0209 2892.3693\n[141] 2902.6808 2912.9558 2923.1947 2933.3979 2943.5657 2953.6985 2963.7966\n[148] 2973.8605 2983.8904 2993.8867 3003.8497 3013.7798 3023.6773 3033.5425\n[155] 3043.3758 3053.1773 3062.9475 3072.6866 3082.3950 3092.0729 3101.7205\n[162] 3111.3383 3120.9264 3130.4852 3140.0148 3149.5157 3158.9879 3168.4318\n[169] 3177.8477 3187.2358 3196.5962 3205.9294 3215.2354 3224.5146 3233.7672\n[176] 3242.9934 3252.1934 3261.3674 3270.5157 3279.6385 3288.7360 3297.8084\n\n\n\n\n\nThe two figures above show an idea of the short term and the long-term forecasted stock price. The forecasted values is be based on the estimated parameters of the model, which were obtained by fitting it to the historical data. One way to assess the accuracy of the forecasts is to use cross-validation, which involves splitting the data into training and testing sets and comparing the predicted values to the actual values in the test set. This can help to identify any potential problems with the model and to fine-tune the model parameters.\n\n12 Step ahead Cross Validation1 Step ahead Cross Validation\n\n\n\n\nCode\n#a seasonal cross validation using 12 steps ahead forecasts\nfarima1 <- function(x, h){forecast(Arima(x, order=c(0,1,0)), h=h)}\n\n# Compute cross-validated errors for up to 12 steps ahead\ne <- tsCV(myts, forecastfunction = farima1, h = 12)\n\n# Compute the MSE values and remove missing values\nmse <- colMeans(e^2, na.rm = TRUE)\n\n# Plot the MSE values against the forecast horizon\ndata.frame(h = 1:12, MSE = mse) %>%\n  ggplot(aes(x = h, y = MSE)) + geom_point()+geom_line()+ggtitle(\"12 Step Ahead Cross Validation\")\n\n\n\n\n\n\n\n\n\nCode\n#a seasonal cross validation using 1 steps ahead forecasts\nfarima1 <- function(x, h){forecast(Arima(x, order=c(0,1,0)),h=h)}\n\n# Compute cross-validated errors for up to 1 steps ahead\ne <- tsCV(myts, forecastfunction = farima1, h = 1)\nmse <-abs(mean(e,na.rm=TRUE))\n\n# Plot the MSE values against the forecast horizon\ndata.frame(h = 1:12, MSE = mse) %>%\n  ggplot(aes(x = h, y = MSE)) + geom_point()+geom_line()+ggtitle(\"1 Step Ahead Cross Validation\")\n\n\n\n\n\n\n\n\nThe one-step-ahead forecasting plot shows that the MSE seems to be stable. The MSE is tend to rise by 12 steps at each step of cross-validation. Lastly, the above-mentioned ARIMA model should be compared to benchmark methods before being used for a long time.\n\n\nModel Comparison\n\nPlotModel Error Table\n\n\n\n\nCode\nfit <- Arima(myts, order=c(0,1,0))\nautoplot(myts) +\n  autolayer(meanf(myts, h=182),\n            series=\"Mean\", PI=FALSE) +\n  autolayer(naive(myts, h=182),\n            series=\"Naïve\", PI=FALSE) +\n  autolayer(snaive(myts, h=182),\n            series=\"SNaïve\", PI=FALSE)+\n  autolayer(rwf(myts, h=182, drift=TRUE),\n            series=\"Drift\", PI=FALSE)+\n  autolayer(forecast(fit,182), \n            series=\"fit\",PI=FALSE) +\n  guides(colour=guide_legend(title=\"Forecast\"))\n\n\n\n\n\n\n\n\n\nCode\nsummary <- summary(fit)\nsnaive <- snaive(myts, h=182)\naccuracy(snaive)\n\n\n                   ME     RMSE      MAE      MPE     MAPE MASE      ACF1\nTraining set 1844.923 3197.208 2427.248 8.453577 10.71957    1 0.9912204\n\n\nCode\nsummary\n\n\nSeries: myts \nARIMA(0,1,0) \n\nsigma^2 = 59807:  log likelihood = -22678.43\nAIC=45358.86   AICc=45358.86   BIC=45364.96\n\nTraining set error measures:\n                   ME     RMSE      MAE        MPE      MAPE       MASE\nTraining set 7.136211 244.5167 148.0226 0.02962254 0.7027494 0.06098373\n                   ACF1\nTraining set -0.1147373\n\n\n\n\n\nError\nModel\nSnavie\n\n\n\n\nME\n1.285039\n515.1907\n\n\nRMSE\n385.9824\n7308.222\n\n\nMAE\n134.981\n4427.736\n\n\nMPE\n-0.01604944\n-5.254495\n\n\nMAPE\n0.7299033\n29.22506\n\n\nMASE\n0.03048532\n1.0000000\n\n\nACF1\n-0.02473038\n0.9969796\n\n\n\n\n\n\nThe ARIMA fitted forecast and snaive tracks the actual data points very closely when compared to other model. The other forecast methods are less accurate when compared. From the table, Model error measurements of fit are much lower than snaive method.We can conclude that the fitted model is good."
  },
  {
    "objectID": "arima_index_nasdaq.html",
    "href": "arima_index_nasdaq.html",
    "title": "ARIMA Model for NASDAQ Composite index",
    "section": "",
    "text": "During exploratory data analysis (EDA) of NADSAQ Composite Index, it was observed that the raw data is often non-stationary, exhibiting trends, seasonality, and other complex patterns that can make modeling and forecasting difficult. To address this issue, the data is often differentiated to achieve stationarity, which is a key assumption of the ARIMA model.\nDifferencing involves calculating the difference between each data point and the previous data point, which can help to remove trends and seasonality from the data. The goal is to create a new series of data that is stationary, meaning that the statistical properties of the data do not change over time.\n\nDifferentiated Time Series Plot\n\nPlotACFPACFADF Test\n\n\n\n\nCode\n#import the data\ndf <- read.csv(\"DATA/CLEANED DATA/nasdaq_raw_data.csv\")\n#convert to time series data\nmyts<-ts(df$IXIC.Adjusted,frequency=252,start=c(2010,1,4), end = c(2023,3,1)) \n#First order differentiation\ndf1 <- diff(myts)\n# Plot \nmyts  %>% diff() %>% ggtsdisplay(main = \"First order differentiation\") \n\n\n\n\n\n\n\n\n\nCode\n#ACF plot \nggAcf(df1,60,main=\"ACF Plot: First order differentiation\") \n\n\n\n\n\n\n\n\n\nCode\nggPacf(df1,60,main=\"PACF Plot: First order differentiation\") \n\n\n\n\n\n\n\n\n\nCode\ntseries::adf.test(df1)\n\n\n\n    Augmented Dickey-Fuller Test\n\ndata:  df1\nDickey-Fuller = -15.135, Lag order = 14, p-value = 0.01\nalternative hypothesis: stationary\n\n\n\n\n\nAfter analyzing the ACF and PACF plots, it can be observed that most of the bar lines lie between the blue lines, indicating that the time series is stationary. This has been further confirmed through an Augmented Dickey-Fuller test as the p-value is less than 0.05. Once the data is stationary, the next step is to model it using ARIMA. The combination of ACF and PACF plots can help determine the appropriate values for p, d, and q in the ARIMA model. The order of differencing required to achieve stationarity gives the d value. The p value is determined by examining significant spikes at various lags in the PACF plot, while the q value is determined through significant spikes in the ACF plot. It is important to consider other methods such as grid search and information criteria to ensure the most appropriate model is selected.\nHere the parameters are d = 1 p = 0,7 (PACF Plot) q = 0,7 (ACF Plot)\n\n\nModel Selection\n\nARIMA ResultAuto Arima Model\n\n\n\n\nCode\n######################## Check for different combinations ########\nd=1\ni=1\ntemp = data.frame()\nls=matrix(rep(NA,6*8),nrow=8) #nrow = 7x1x1\n\n\nfor (p in 1:8)# p=0,1,2,3,4,5,6,7 :8\n{\n  for(q in 1)# q=0 :1\n  {\n    for(d in 1)# d=1 :1\n    {\n      \n      if(p-1+d+q-1<=8)\n      {\n        \n        model<- Arima(myts,order=c(p-1,d,q-1),include.drift=TRUE) \n        ls[i,]= c(p-1,d,q-1,model$aic,model$bic,model$aicc)\n        i=i+1\n        \n      }\n      \n    }\n  }\n}\n\nmodel = as.data.frame(ls)\nnames(model)= c(\"p\",\"d\",\"q\",\"AIC\",\"BIC\",\"AICc\")\n\n#temp\nknitr::kable(model)\n\n\n\n\n\np\nd\nq\nAIC\nBIC\nAICc\n\n\n\n\n0\n1\n0\n40013.93\n40026.12\n40013.93\n\n\n1\n1\n0\n39988.60\n40006.89\n39988.61\n\n\n2\n1\n0\n39986.51\n40010.89\n39986.52\n\n\n3\n1\n0\n39986.37\n40016.85\n39986.39\n\n\n4\n1\n0\n39984.84\n40021.41\n39984.86\n\n\n5\n1\n0\n39985.77\n40028.44\n39985.81\n\n\n6\n1\n0\n39977.33\n40026.08\n39977.37\n\n\n7\n1\n0\n39963.60\n40018.45\n39963.65\n\n\n\n\n\n\n\n\n\nCode\nauto.arima(myts)\n\n\nSeries: myts \nARIMA(1,1,1) with drift \n\nCoefficients:\n          ar1     ma1   drift\n      -0.4058  0.3156  2.6027\ns.e.   0.1098  0.1135  1.7599\n\nsigma^2 = 11603:  log likelihood = -19989.15\nAIC=39986.31   AICc=39986.32   BIC=40010.69\n\n\n\n\n\nThe given table provides different ARIMA models with their corresponding AIC and BIC values. By sorting the table according to each criterion, we can identify the models with the lowest values. The smallest AIC value corresponds to an ARIMA (1,1,0) model, while the ARIMA (0,1,0) model has the lowest BIC value. Additionally, the auto.arima function in R suggests an ARIMA (0,1,0) model as the best fit for the data. Since there are different models to choose from, it is important to perform model diagnostics to determine the best model for the data. These diagnostics can include checking for residual normality, checking for autocorrelation and partial autocorrelation in the residuals, and comparing the forecasted values to the actual values. By conducting these tests, we can determine which model provides the best fit for the data and is therefore the most appropriate to use for forecasting future values.\n\n\nModel Diagnostic\n\nModel 1 PlotModel 1Model 2 PlotModel 2\n\n\n\n\nCode\nmodel_output <- capture.output(sarima(myts,1,1,0))\n\n\n\n\n\n\n\n\n\nCode\ncat(model_output[11:41], model_output[length(model_output)], sep = \"\\n\") \n\n\n$fit\n\nCall:\narima(x = xdata, order = c(p, d, q), seasonal = list(order = c(P, D, Q), period = S), \n    xreg = constant, transform.pars = trans, fixed = fixed, optim.control = list(trace = trc, \n        REPORT = 1, reltol = tol))\n\nCoefficients:\n          ar1  constant\n      -0.0911    2.6303\ns.e.   0.0174    1.7246\n\nsigma^2 estimated as 11607:  log likelihood = -19991.3,  aic = 39988.6\n\n$degrees_of_freedom\n[1] 3276\n\n$ttable\n         Estimate     SE t.value p.value\nar1       -0.0911 0.0174 -5.2380  0.0000\nconstant   2.6303 1.7246  1.5252  0.1273\n\n$AIC\n[1] 12.19909\n\n$AICc\n[1] 12.19909\n\n$BIC\n[1] 12.20466\n\n\n\n\n\n\nCode\nmodel_output <- capture.output(sarima(myts,0,1,0))\n\n\n\n\n\n\n\n\n\nCode\ncat(model_output[9:38], model_output[length(model_output)], sep = \"\\n\") \n\n\n$fit\n\nCall:\narima(x = xdata, order = c(p, d, q), seasonal = list(order = c(P, D, Q), period = S), \n    xreg = constant, transform.pars = trans, fixed = fixed, optim.control = list(trace = trc, \n        REPORT = 1, reltol = tol))\n\nCoefficients:\n      constant\n        2.6306\ns.e.    1.8896\n\nsigma^2 estimated as 11704:  log likelihood = -20004.97,  aic = 40013.93\n\n$degrees_of_freedom\n[1] 3277\n\n$ttable\n         Estimate     SE t.value p.value\nconstant   2.6306 1.8896  1.3922   0.164\n\n$AIC\n[1] 12.20681\n\n$AICc\n[1] 12.20681\n\n$BIC\n[1] 12.21053\n\n\n\n\n\nTo determine which model diagnosis is the best, we need to consider some key factors such as the coefficient estimates and standard errors, AIC, BIC, p-values, and the log-likelihood. Comparing the two models, model 2 is the best because it has the lowest BIC value when compared and the AIC value are almost the same, which indicate better goodness of fit. Also, all the coefficient estimates in the model are significant, as evidenced by the p-values are better than model 1, which implies that there is a strong correlation between the variables. The t-values are high, indicating that the estimates are highly reliable.\nTherefore, based on the AIC and BIC values, the significance of the coefficient estimates, and the reliability of the estimates, we can conclude that model 2 is the best model diagnosis. Therefore, (0,1,0) is the best model for this time series.\nThe best model identified is ARIMA(0,1,0). By analyzing the standardized residuals plot, it can be observed that the mean is close to 0 and the variance is slightly higher than 1. Deviations from these values could indicate poor model fit. However, in this case, the model seems to be well-fitted. The ACF plot of residuals shows very few significant lags, which is a positive sign for the model. The qq-plot also suggests normality in the residuals. Additionally, the p-values for the Ljung-Box test are greater than 0.05, indicating that the residuals are independent, which is favorable for the model’s accuracy.\nThe equation for this model: \\[(1-\\phi_1B-\\phi_2B^2)(1-B)(Y_t-\\mu) = (1+\\theta_1B+\\theta_2B^2)\\epsilon_t\\]\n\n\nForcast\n\nShort Term ForcastLong term Forcast\n\n\n\n\nCode\nmyts %>%\n  Arima(order=c(0,1,0),include.drift = TRUE) %>%\n  forecast %>%\n  autoplot(main = \"NASDAQ Composite Index Stock Prices Prediction\") +\n  ylab(\"stock prices\") + xlab(\"Year\")\n\n\n\n\n\n\n\n\n\nCode\nsarima.for(myts,182, 0,1,0, main='NASDAQ Composite Index Stock Prices Prediction')\n\n\n\n\n\n$pred\nTime Series:\nStart = c(2023, 4) \nEnd = c(2023, 185) \nFrequency = 252 \n  [1] 10934.30 10936.93 10939.56 10942.19 10944.82 10947.45 10950.08 10952.72\n  [9] 10955.35 10957.98 10960.61 10963.24 10965.87 10968.50 10971.13 10973.76\n [17] 10976.39 10979.02 10981.65 10984.28 10986.91 10989.54 10992.17 10994.81\n [25] 10997.44 11000.07 11002.70 11005.33 11007.96 11010.59 11013.22 11015.85\n [33] 11018.48 11021.11 11023.74 11026.37 11029.00 11031.63 11034.27 11036.90\n [41] 11039.53 11042.16 11044.79 11047.42 11050.05 11052.68 11055.31 11057.94\n [49] 11060.57 11063.20 11065.83 11068.46 11071.09 11073.72 11076.36 11078.99\n [57] 11081.62 11084.25 11086.88 11089.51 11092.14 11094.77 11097.40 11100.03\n [65] 11102.66 11105.29 11107.92 11110.55 11113.18 11115.81 11118.45 11121.08\n [73] 11123.71 11126.34 11128.97 11131.60 11134.23 11136.86 11139.49 11142.12\n [81] 11144.75 11147.38 11150.01 11152.64 11155.27 11157.91 11160.54 11163.17\n [89] 11165.80 11168.43 11171.06 11173.69 11176.32 11178.95 11181.58 11184.21\n [97] 11186.84 11189.47 11192.10 11194.73 11197.36 11200.00 11202.63 11205.26\n[105] 11207.89 11210.52 11213.15 11215.78 11218.41 11221.04 11223.67 11226.30\n[113] 11228.93 11231.56 11234.19 11236.82 11239.46 11242.09 11244.72 11247.35\n[121] 11249.98 11252.61 11255.24 11257.87 11260.50 11263.13 11265.76 11268.39\n[129] 11271.02 11273.65 11276.28 11278.91 11281.55 11284.18 11286.81 11289.44\n[137] 11292.07 11294.70 11297.33 11299.96 11302.59 11305.22 11307.85 11310.48\n[145] 11313.11 11315.74 11318.37 11321.01 11323.64 11326.27 11328.90 11331.53\n[153] 11334.16 11336.79 11339.42 11342.05 11344.68 11347.31 11349.94 11352.57\n[161] 11355.20 11357.83 11360.46 11363.10 11365.73 11368.36 11370.99 11373.62\n[169] 11376.25 11378.88 11381.51 11384.14 11386.77 11389.40 11392.03 11394.66\n[177] 11397.29 11399.92 11402.56 11405.19 11407.82 11410.45\n\n$se\nTime Series:\nStart = c(2023, 4) \nEnd = c(2023, 185) \nFrequency = 252 \n  [1]  108.1866  152.9989  187.3847  216.3732  241.9126  265.0019  286.2348\n  [8]  305.9979  324.5598  342.1160  358.8143  374.7693  390.0723  404.7972\n [15]  419.0049  432.7464  446.0647  458.9968  471.5744  483.8251  495.7732\n [22]  507.4401  518.8447  530.0039  540.9330  551.6455  562.1540  572.4696\n [29]  582.6026  592.5624  602.3574  611.9958  621.4847  630.8308  640.0405\n [36]  649.1195  658.0733  666.9069  675.6250  684.2321  692.7322  701.1292\n [43]  709.4269  717.6287  725.7377  733.7572  741.6899  749.5387  757.3061\n [50]  764.9947  772.6068  780.1446  787.6103  795.0058  802.3332  809.5943\n [57]  816.7909  823.9245  830.9970  838.0097  844.9643  851.8621  858.7044\n [64]  865.4927  872.2282  878.9120  885.5454  892.1295  898.6653  905.1540\n [71]  911.5964  917.9937  924.3466  930.6562  936.9234  943.1488  949.3335\n [78]  955.4781  961.5835  967.6503  973.6793  979.6713  985.6268  991.5465\n [85]  997.4311 1003.2812 1009.0973 1014.8802 1020.6303 1026.3481 1032.0343\n [92] 1037.6893 1043.3137 1048.9079 1054.4725 1060.0078 1065.5144 1070.9926\n [99] 1076.4430 1081.8659 1087.2618 1092.6310 1097.9740 1103.2911 1108.5827\n[106] 1113.8491 1119.0908 1124.3080 1129.5012 1134.6705 1139.8165 1144.9393\n[113] 1150.0392 1155.1167 1160.1719 1165.2053 1170.2169 1175.2072 1180.1764\n[120] 1185.1247 1190.0525 1194.9600 1199.8473 1204.7149 1209.5629 1214.3915\n[127] 1219.2010 1223.9916 1228.7635 1233.5169 1238.2521 1242.9693 1247.6686\n[134] 1252.3503 1257.0146 1261.6616 1266.2916 1270.9047 1275.5011 1280.0810\n[141] 1284.6446 1289.1920 1293.7235 1298.2391 1302.7391 1307.2236 1311.6927\n[148] 1316.1467 1320.5856 1325.0097 1329.4191 1333.8139 1338.1942 1342.5603\n[155] 1346.9122 1351.2501 1355.5741 1359.8844 1364.1810 1368.4642 1372.7340\n[162] 1376.9905 1381.2339 1385.4644 1389.6819 1393.8867 1398.0789 1402.2585\n[169] 1406.4257 1410.5806 1414.7233 1418.8538 1422.9724 1427.0792 1431.1741\n[176] 1435.2573 1439.3290 1443.3892 1447.4379 1451.4754 1455.5017 1459.5169\n\n\n\n\n\nThe two figures above show an idea of the short term and the long-term forecasted stock price. The forecasted values is be based on the estimated parameters of the model, which were obtained by fitting it to the historical data. One way to assess the accuracy of the forecasts is to use cross-validation, which involves splitting the data into training and testing sets and comparing the predicted values to the actual values in the test set. This can help to identify any potential problems with the model and to fine-tune the model parameters.\n\n12 Step ahead Cross Validation1 Step ahead Cross Validation\n\n\n\n\nCode\n#a seasonal cross validation using 12 steps ahead forecasts\nfarima1 <- function(x, h){forecast(Arima(x, order=c(0,1,0)), h=h)}\n\n# Compute cross-validated errors for up to 12 steps ahead\ne <- tsCV(myts, forecastfunction = farima1, h = 12)\n\n# Compute the MSE values and remove missing values\nmse <- colMeans(e^2, na.rm = TRUE)\n\n# Plot the MSE values against the forecast horizon\ndata.frame(h = 1:12, MSE = mse) %>%\n  ggplot(aes(x = h, y = MSE)) + geom_point()+geom_line()+ggtitle(\"12 Step Ahead Cross Validation\")\n\n\n\n\n\n\n\n\n\nCode\n#a seasonal cross validation using 1 steps ahead forecasts\nfarima1 <- function(x, h){forecast(Arima(x, order=c(0,1,0)),h=h)}\n\n# Compute cross-validated errors for up to 1 steps ahead\ne <- tsCV(myts, forecastfunction = farima1, h = 1)\nmse <-abs(mean(e,na.rm=TRUE))\n\n# Plot the MSE values against the forecast horizon\ndata.frame(h = 1:12, MSE = mse) %>%\n  ggplot(aes(x = h, y = MSE)) + geom_point()+geom_line()+ggtitle(\"1 Step Ahead Cross Validation\")\n\n\n\n\n\n\n\n\nThe one-step-ahead forecasting plot shows that the MSE seems to be stable. The MSE is tend to rise by 12 steps at each step of cross-validation. Lastly, the above-mentioned ARIMA model should be compared to benchmark methods before being used for a long time.\n\n\nModel Comparison\n\nPlotModel Error Table\n\n\n\n\nCode\nfit <- Arima(myts, order=c(0,1,0))\nautoplot(myts) +\n  autolayer(meanf(myts, h=182),\n            series=\"Mean\", PI=FALSE) +\n  autolayer(naive(myts, h=182),\n            series=\"Naïve\", PI=FALSE) +\n  autolayer(snaive(myts, h=182),\n            series=\"SNaïve\", PI=FALSE)+\n  autolayer(rwf(myts, h=182, drift=TRUE),\n            series=\"Drift\", PI=FALSE)+\n  autolayer(forecast(fit,182), \n            series=\"fit\",PI=FALSE) +\n  guides(colour=guide_legend(title=\"Forecast\"))\n\n\n\n\n\n\n\n\n\nCode\nsummary <- summary(fit)\nsnaive <- snaive(myts, h=182)\naccuracy(snaive)\n\n\n                   ME     RMSE      MAE      MPE     MAPE MASE      ACF1\nTraining set 812.8214 1820.728 1250.681 11.65299 15.87608    1 0.9945471\n\n\nCode\nsummary\n\n\nSeries: myts \nARIMA(0,1,0) \n\nsigma^2 = 11711:  log likelihood = -20005.93\nAIC=40013.87   AICc=40013.87   BIC=40019.96\n\nTraining set error measures:\n                   ME     RMSE    MAE        MPE      MAPE       MASE\nTraining set 2.630545 108.2021 62.269 0.03895396 0.8925344 0.04978808\n                    ACF1\nTraining set -0.09108687\n\n\n\n\n\nError\nModel\nSnavie\n\n\n\n\nME\n0.5450376\n183.6083\n\n\nRMSE\n162.141\n3564.003\n\n\nMAE\n53.77502\n2124.839\n\n\nMPE\n-0.04171079\n-19.15607\n\n\nMAPE\n0.9503091\n53.5699\n\n\nMASE\n0.02530781\n1.0000000\n\n\nACF1\n-0.02640682\n0.9977458\n\n\n\n\n\n\nThe ARIMA forecast tracks the actual data points very closely when compared to other model. The other forecast methods are less accurate when compared. From the table, Model error measurements of fit are much lower than snaive method.We can conclude that the fitted model is good."
  },
  {
    "objectID": "arima_index_sp500.html",
    "href": "arima_index_sp500.html",
    "title": "ARIMA Model for S&P 500 Index",
    "section": "",
    "text": "During exploratory data analysis (EDA) of S&P 500 Index, it was observed that the raw data is often non-stationary, exhibiting trends, seasonality, and other complex patterns that can make modeling and forecasting difficult. To address this issue, the data is often differentiated to achieve stationarity, which is a key assumption of the ARIMA model.\nDifferencing involves calculating the difference between each data point and the previous data point, which can help to remove trends and seasonality from the data. The goal is to create a new series of data that is stationary, meaning that the statistical properties of the data do not change over time.\n\nDifferentiated Time Series Plot\n\nPlotACFPACFADF Test\n\n\n\n\nCode\n#import the data\ndf <- read.csv(\"DATA/CLEANED DATA/sp500_raw_data.csv\")\n#convert to time series data\nmyts<-ts(df$GSPC.Adjusted,frequency=252,start=c(2010,1,4), end = c(2023,3,1)) \n#First order differentiation\ndf1 <- diff(myts)\n# Plot \nmyts  %>% diff() %>% ggtsdisplay(main = \"First order differentiation\") \n\n\n\n\n\n\n\n\n\nCode\n#ACF plot \nggAcf(df1,60,main=\"ACF Plot: First order differentiation\") \n\n\nWarning in ggplot2::geom_segment(lineend = \"butt\", ...): Ignoring unknown\nparameters: `main`\n\n\n\n\n\n\n\n\n\nCode\nggPacf(df1,60,main=\"PACF Plot: First order differentiation\") \n\n\n\n\n\n\n\n\n\nCode\ntseries::adf.test(df1)\n\n\n\n    Augmented Dickey-Fuller Test\n\ndata:  df1\nDickey-Fuller = -15.353, Lag order = 14, p-value = 0.01\nalternative hypothesis: stationary\n\n\n\n\n\nAfter analyzing the ACF and PACF plots, it can be observed that most of the bar lines lie between the blue lines, indicating that the time series is stationary. This has been further confirmed through an Augmented Dickey-Fuller test as the p-value is less than 0.05. Once the data is stationary, the next step is to model it using ARIMA. The combination of ACF and PACF plots can help determine the appropriate values for p, d, and q in the ARIMA model. The order of differencing required to achieve stationarity gives the d value. The p value is determined by examining significant spikes at various lags in the PACF plot, while the q value is determined through significant spikes in the ACF plot. It is important to consider other methods such as grid search and information criteria to ensure the most appropriate model is selected.\nHere the parameters are d = 1 p = 0,7 (PACF Plot) q = 0,7 (ACF Plot)\n\n\nModel Selection\n\nARIMA ResultAuto Arima Model\n\n\n\n\nCode\n######################## Check for different combinations ########\nd=1\ni=1\ntemp = data.frame()\nls=matrix(rep(NA,6*8),nrow=8) #nrow = 8x1x1\n\n\nfor (p in 1:8)# p=01,2,3,4,5,6,7 :7\n{\n  for(q in 1)# q=0 :1\n  {\n    for(d in 1)# d=1 :1\n    {\n      \n      if(p-1+d+q-1<=8)\n      {\n        \n        model<- Arima(myts,order=c(p-1,d,q-1),include.drift=TRUE) \n        ls[i,]= c(p-1,d,q-1,model$aic,model$bic,model$aicc)\n        i=i+1\n        \n      }\n      \n    }\n  }\n}\n\nmodel = as.data.frame(ls)\nnames(model)= c(\"p\",\"d\",\"q\",\"AIC\",\"BIC\",\"AICc\")\n\n#temp\nknitr::kable(model)\n\n\n\n\n\np\nd\nq\nAIC\nBIC\nAICc\n\n\n\n\n0\n1\n0\n31635.53\n31647.72\n31635.53\n\n\n1\n1\n0\n31599.12\n31617.40\n31599.13\n\n\n2\n1\n0\n31592.41\n31616.79\n31592.42\n\n\n3\n1\n0\n31594.22\n31624.70\n31594.24\n\n\n4\n1\n0\n31587.46\n31624.03\n31587.48\n\n\n5\n1\n0\n31587.80\n31630.47\n31587.83\n\n\n6\n1\n0\n31570.03\n31618.79\n31570.07\n\n\n7\n1\n0\n31543.39\n31598.25\n31543.45\n\n\n\n\n\n\n\n\n\nCode\nauto.arima(myts)\n\n\nSeries: myts \nARIMA(2,1,0) with drift \n\nCoefficients:\n          ar1     ar2   drift\n      -0.1024  0.0515  0.8652\ns.e.   0.0174  0.0174  0.4974\n\nsigma^2 = 896.3:  log likelihood = -15792.21\nAIC=31592.41   AICc=31592.42   BIC=31616.79\n\n\n\n\n\nThe given table provides different ARIMA models with their corresponding AIC and BIC values. By sorting the table according to each criterion, we can identify the models with the lowest values. The smallest AIC value corresponds to an ARIMA (1,1,0) model, while the ARIMA (0,1,0) model has the lowest BIC value. Additionally, the auto.arima function in R suggests an ARIMA (0,1,0) model as the best fit for the data. Since there are different models to choose from, it is important to perform model diagnostics to determine the best model for the data. These diagnostics can include checking for residual normality, checking for autocorrelation and partial autocorrelation in the residuals, and comparing the forecasted values to the actual values. By conducting these tests, we can determine which model provides the best fit for the data and is therefore the most appropriate to use for forecasting future values.\n\n\nModel Diagnostic\n\nModel 1 PlotModel 1Model 2 PlotModel 2\n\n\n\n\nCode\nmodel_output <- capture.output(sarima(myts,1,1,0))\n\n\n\n\n\n\n\n\n\nCode\ncat(model_output[11:41], model_output[length(model_output)], sep = \"\\n\") \n\n\n$fit\n\nCall:\narima(x = xdata, order = c(p, d, q), seasonal = list(order = c(P, D, Q), period = S), \n    xreg = constant, transform.pars = trans, fixed = fixed, optim.control = list(trace = trc, \n        REPORT = 1, reltol = tol))\n\nCoefficients:\n          ar1  constant\n      -0.1080    0.8652\ns.e.   0.0174    0.4724\n\nsigma^2 estimated as 897.9:  log likelihood = -15796.56,  aic = 31599.12\n\n$degrees_of_freedom\n[1] 3276\n\n$ttable\n         Estimate     SE t.value p.value\nar1       -0.1080 0.0174 -6.2151  0.0000\nconstant   0.8652 0.4724  1.8315  0.0671\n\n$AIC\n[1] 9.639756\n\n$AICc\n[1] 9.639757\n\n$BIC\n[1] 9.645334\n\n\n\n\n\n\nCode\nmodel_output <- capture.output(sarima(myts,0,1,0))\n\n\n\n\n\n\n\n\n\nCode\ncat(model_output[9:38], model_output[length(model_output)], sep = \"\\n\") \n\n\n$fit\n\nCall:\narima(x = xdata, order = c(p, d, q), seasonal = list(order = c(P, D, Q), period = S), \n    xreg = constant, transform.pars = trans, fixed = fixed, optim.control = list(trace = trc, \n        REPORT = 1, reltol = tol))\n\nCoefficients:\n      constant\n        0.8654\ns.e.    0.5264\n\nsigma^2 estimated as 908.5:  log likelihood = -15815.76,  aic = 31635.53\n\n$degrees_of_freedom\n[1] 3277\n\n$ttable\n         Estimate     SE t.value p.value\nconstant   0.8654 0.5264  1.6438  0.1003\n\n$AIC\n[1] 9.650863\n\n$AICc\n[1] 9.650863\n\n$BIC\n[1] 9.654582\n\n\n\n\n\nTo determine which model diagnosis is the best, we need to consider some key factors such as the coefficient estimates and standard errors, AIC, BIC, p-values, and the log-likelihood. Comparing the two models, model 2 is the best because it has the lowest BIC value when compared and the AIC value are almost the same, which indicate better goodness of fit. Also, all the coefficient estimates in the model are significant, as evidenced by the p-values are better than model 1, which implies that there is a strong correlation between the variables. The t-values are high, indicating that the estimates are highly reliable.\nTherefore, based on the AIC and BIC values, the significance of the coefficient estimates, and the reliability of the estimates, we can conclude that model 2 is the best model diagnosis. Therefore, (0,1,0) is the best model for this time series.\nThe best model identified is ARIMA(0,1,0). By analyzing the standardized residuals plot, it can be observed that the mean is close to 0 and the variance is slightly higher than 1. Deviations from these values could indicate poor model fit. However, in this case, the model seems to be well-fitted. The ACF plot of residuals shows very few significant lags, which is a positive sign for the model. The qq-plot also suggests normality in the residuals. Additionally, the p-values for the Ljung-Box test are greater than 0.05, indicating that the residuals are independent, which is favorable for the model’s accuracy.\nThe equation for this model: \\[(1-\\phi_1B-\\phi_2B^2)(1-B)(Y_t-\\mu) = (1+\\theta_1B+\\theta_2B^2)\\epsilon_t\\]\n\n\nForcast\n\nShort Term ForcastLong term Forcast\n\n\n\n\nCode\nmyts %>%\n  Arima(order=c(0,1,0),include.drift = TRUE) %>%\n  forecast %>%\n  autoplot(main = \"S&P 500 Index Stock Prices Prediction\") +\n  ylab(\"stock prices\") + xlab(\"Year\")\n\n\n\n\n\n\n\n\n\nCode\nsarima.for(myts,182, 0,1,0, main='S&P 500 Index Stock Prices Prediction')\n\n\n\n\n\n$pred\nTime Series:\nStart = c(2023, 4) \nEnd = c(2023, 185) \nFrequency = 252 \n  [1] 3970.475 3971.341 3972.206 3973.072 3973.937 3974.802 3975.668 3976.533\n  [9] 3977.398 3978.264 3979.129 3979.994 3980.860 3981.725 3982.590 3983.456\n [17] 3984.321 3985.186 3986.052 3986.917 3987.782 3988.648 3989.513 3990.379\n [25] 3991.244 3992.109 3992.975 3993.840 3994.705 3995.571 3996.436 3997.301\n [33] 3998.167 3999.032 3999.897 4000.763 4001.628 4002.493 4003.359 4004.224\n [41] 4005.089 4005.955 4006.820 4007.686 4008.551 4009.416 4010.282 4011.147\n [49] 4012.012 4012.878 4013.743 4014.608 4015.474 4016.339 4017.204 4018.070\n [57] 4018.935 4019.800 4020.666 4021.531 4022.397 4023.262 4024.127 4024.993\n [65] 4025.858 4026.723 4027.589 4028.454 4029.319 4030.185 4031.050 4031.915\n [73] 4032.781 4033.646 4034.511 4035.377 4036.242 4037.107 4037.973 4038.838\n [81] 4039.704 4040.569 4041.434 4042.300 4043.165 4044.030 4044.896 4045.761\n [89] 4046.626 4047.492 4048.357 4049.222 4050.088 4050.953 4051.818 4052.684\n [97] 4053.549 4054.414 4055.280 4056.145 4057.011 4057.876 4058.741 4059.607\n[105] 4060.472 4061.337 4062.203 4063.068 4063.933 4064.799 4065.664 4066.529\n[113] 4067.395 4068.260 4069.125 4069.991 4070.856 4071.722 4072.587 4073.452\n[121] 4074.318 4075.183 4076.048 4076.914 4077.779 4078.644 4079.510 4080.375\n[129] 4081.240 4082.106 4082.971 4083.836 4084.702 4085.567 4086.432 4087.298\n[137] 4088.163 4089.029 4089.894 4090.759 4091.625 4092.490 4093.355 4094.221\n[145] 4095.086 4095.951 4096.817 4097.682 4098.547 4099.413 4100.278 4101.143\n[153] 4102.009 4102.874 4103.739 4104.605 4105.470 4106.336 4107.201 4108.066\n[161] 4108.932 4109.797 4110.662 4111.528 4112.393 4113.258 4114.124 4114.989\n[169] 4115.854 4116.720 4117.585 4118.450 4119.316 4120.181 4121.047 4121.912\n[177] 4122.777 4123.643 4124.508 4125.373 4126.239 4127.104\n\n$se\nTime Series:\nStart = c(2023, 4) \nEnd = c(2023, 185) \nFrequency = 252 \n  [1]  30.14089  42.62566  52.20555  60.28178  67.39708  73.82980  79.74530\n  [8]  85.25131  90.42267  95.31387  99.96603 104.41111 108.67453 112.77689\n [15] 116.73517 120.56356 124.27408 127.87697 131.38110 134.79416 138.12291\n [22] 141.37331 144.55063 147.65961 150.70445 153.68899 156.61666 159.49060\n [29] 162.31366 165.08846 167.81738 170.50263 173.14624 175.75008 178.31591\n [36] 180.84534 183.33988 185.80093 188.22980 190.62773 192.99587 195.33530\n [43] 197.64704 199.93205 202.19124 204.42547 206.63554 208.82222 210.98624\n [50] 213.12828 215.24901 217.34905 219.42900 221.48941 223.53083 225.55377\n [57] 227.55874 229.54619 231.51658 233.47034 235.40788 237.32961 239.23590\n [64] 241.12713 243.00363 244.86575 246.71382 248.54815 250.36904 252.17679\n [71] 253.97166 255.75394 257.52388 259.28175 261.02777 262.76219 264.48524\n [78] 266.19714 267.89810 269.58832 271.26802 272.93737 274.59658 276.24583\n [85] 277.88528 279.51512 281.13551 282.74662 284.34860 285.94160 287.52577\n [92] 289.10127 290.66822 292.22678 293.77706 295.31921 296.85335 298.37960\n [99] 299.89808 301.40891 302.91220 304.40808 305.89663 307.37798 308.85222\n[106] 310.31946 311.77980 313.23333 314.68014 316.12033 317.55399 318.98121\n[113] 320.40206 321.81665 323.22504 324.62733 326.02358 327.41388 328.79830\n[120] 330.17692 331.54980 332.91702 334.27865 335.63476 336.98540 338.33066\n[127] 339.67059 341.00525 342.33471 343.65903 344.97826 346.29247 347.60171\n[134] 348.90604 350.20550 351.50017 352.79008 354.07530 355.35586 356.63183\n[141] 357.90325 359.17017 360.43263 361.69069 362.94439 364.19377 365.43888\n[148] 366.67976 367.91646 369.14901 370.37747 371.60186 372.82223 374.03862\n[155] 375.25106 376.45961 377.66428 378.86512 380.06217 381.25546 382.44503\n[162] 383.63091 384.81313 385.99174 387.16675 388.33821 389.50615 390.67060\n[169] 391.83158 392.98914 394.14329 395.29408 396.44152 397.58566 398.72651\n[176] 399.86410 400.99847 402.12964 403.25764 404.38248 405.50421 406.62285\n\n\n\n\n\nThe two figures above show an idea of the short term and the long-term forecasted stock price. The forecasted values is be based on the estimated parameters of the model, which were obtained by fitting it to the historical data. One way to assess the accuracy of the forecasts is to use cross-validation, which involves splitting the data into training and testing sets and comparing the predicted values to the actual values in the test set. This can help to identify any potential problems with the model and to fine-tune the model parameters.\n\n12 Step ahead Cross Validation1 Step ahead Cross Validation\n\n\n\n\nCode\n#a seasonal cross validation using 12 steps ahead forecasts\nfarima1 <- function(x, h){forecast(Arima(x, order=c(0,1,0)), h=h)}\n\n# Compute cross-validated errors for up to 12 steps ahead\ne <- tsCV(myts, forecastfunction = farima1, h = 12)\n\n# Compute the MSE values and remove missing values\nmse <- colMeans(e^2, na.rm = TRUE)\n\n# Plot the MSE values against the forecast horizon\ndata.frame(h = 1:12, MSE = mse) %>%\n  ggplot(aes(x = h, y = MSE)) + geom_point()+geom_line()+ggtitle(\"12 Step Ahead Cross Validation\")\n\n\n\n\n\n\n\n\n\nCode\n#a seasonal cross validation using 1 steps ahead forecasts\nfarima1 <- function(x, h){forecast(Arima(x, order=c(0,1,0)),h=h)}\n\n# Compute cross-validated errors for up to 1 steps ahead\ne <- tsCV(myts, forecastfunction = farima1, h = 1)\nmse <-abs(mean(e,na.rm=TRUE))\n\n# Plot the MSE values against the forecast horizon\ndata.frame(h = 1:12, MSE = mse) %>%\n  ggplot(aes(x = h, y = MSE)) + geom_point()+geom_line()+ggtitle(\"1 Step Ahead Cross Validation\")\n\n\n\n\n\n\n\n\nThe one-step-ahead forecasting plot shows that the MSE seems to be stable. The MSE is tend to rise by 12 steps at each step of cross-validation. Lastly, the above-mentioned ARIMA model should be compared to benchmark methods before being used for a long time.\n\n\nModel Comparison\n\nPlotModel Error Table\n\n\n\n\nCode\nfit <- Arima(myts, order=c(0,1,0))\nautoplot(myts) +\n  autolayer(meanf(myts, h=182),\n            series=\"Mean\", PI=FALSE) +\n  autolayer(naive(myts, h=182),\n            series=\"Naïve\", PI=FALSE) +\n  autolayer(snaive(myts, h=182),\n            series=\"SNaïve\", PI=FALSE)+\n  autolayer(rwf(myts, h=182, drift=TRUE),\n            series=\"Drift\", PI=FALSE)+\n  autolayer(forecast(fit,182), \n            series=\"fit\",PI=FALSE) +\n  guides(colour=guide_legend(title=\"Forecast\"))\n\n\n\n\n\n\n\n\n\nCode\nsummary <- summary(fit)\nsnaive <- snaive(myts, h=182)\naccuracy(snaive)\n\n\n                 ME     RMSE      MAE      MPE     MAPE MASE      ACF1\nTraining set 244.56 433.3665 321.6811 9.518344 11.83997    1 0.9922785\n\n\nCode\nsummary\n\n\nSeries: myts \nARIMA(0,1,0) \n\nsigma^2 = 909.2:  log likelihood = -15817.11\nAIC=31636.23   AICc=31636.23   BIC=31642.32\n\nTraining set error measures:\n                    ME     RMSE      MAE        MPE      MAPE       MASE\nTraining set 0.8654325 30.14872 18.13196 0.03191457 0.7380538 0.05636626\n                   ACF1\nTraining set -0.1079017\n\n\n\n\n\nError\nModel\nSnavie\n\n\n\n\nME\n0.1807799\n68.54409\n\n\nRMSE\n48.95665\n988.3694\n\n\nMAE\n16.46753\n593.4131\n\n\nMPE\n-0.0203944\n-7.952293\n\n\nMAPE\n0.7793433\n35.18675\n\n\nMASE\n0.02775052\n1.0000000\n\n\nACF1\n-0.02620846\n0.9973481\n\n\n\n\n\n\nThe ARIMA forecast tracks the actual data points very closely when compared to other model. The other forecast methods are less accurate when compared. From the table, Model error measurements of fit are much lower than snaive method.We can conclude that the fitted model is good."
  },
  {
    "objectID": "arima_macroeconomic_factors.html",
    "href": "arima_macroeconomic_factors.html",
    "title": "ARIMA/SARIMA Model for Macroeconomic Factors",
    "section": "",
    "text": "Macroeconomic factors, such as inflation rates, gross domestic product (GDP),interest rate and unemployment rates, play a critical role in the performance of the global economy. Accurately forecasting these factors is essential for policymakers, investors, and financial institutions seeking to make informed decisions.\nThe ARIMA and SARIMA models are commonly used time series analysis techniques for modeling and forecasting macroeconomic factors. The ARIMA model is a flexible and widely applicable model that can handle a wide range of data patterns, while the SARIMA model is a variant of the ARIMA model that incorporates seasonality into the analysis.\nThe choice between ARIMA and SARIMA models depends on the presence or absence of seasonality in the data. If the data exhibits seasonality, the SARIMA model may be more appropriate, as it allows for the modeling of both seasonal and non-seasonal patterns in the data. However, if the data does not exhibit seasonality, the ARIMA model may be sufficient for modeling and forecasting the macroeconomic factor of interest.\nOverall, the ARIMA and SARIMA models are powerful tools for modeling and forecasting macroeconomic factors, allowing analysts to identify unique patterns and trends and make more accurate predictions about future performance. However, as with any forecasting model, it is important to carefully consider the underlying assumptions and limitations of the model and incorporate relevant exogenous variables where appropriate.\nClick to view SARIMA Page for GDP Growth Rate\nClick to view SARIMA Page for Interest Rate\nClick to view SARIMA Page for Inflation Rate\nClick to view ARIMA Page for Unemployment Rate"
  },
  {
    "objectID": "arima_macroeconomic_gdp.html",
    "href": "arima_macroeconomic_gdp.html",
    "title": "SARIMA Model for GDP Growth Rate",
    "section": "",
    "text": "From the exploratory data analysis (EDA), it was observed that the raw data for GDP Growth Rate is already stationary, without the need for differencing. Therefore, an ARIMA or SARIMA model can be fitted directly to the raw data without the need for differencing.\n\nStationary Time Series\n\nPlotACFPACFADF TestSeasonal ACF PlotSeasonal PACF Ploy\n\n\n\n\nCode\n#import the data\ndf <- read.csv(\"DATA/CLEANED DATA/gdp_clean_data.csv\")\n#convert to time series data\nmyts<-ts(df$value,frequency=4,start=c(2010/1/1))\n# Plot \nmyts  %>% ggtsdisplay(main = \"GDP Groth Rate\") \n\n\n\n\n\n\n\n\n\nCode\n#ACF plot \nggAcf(myts,main=\"ACF Plot\") \n\n\nWarning in ggplot2::geom_segment(lineend = \"butt\", ...): Ignoring unknown\nparameters: `main`\n\n\n\n\n\n\n\n\n\nCode\nggPacf(myts,main=\"PACF Plot\") \n\n\n\n\n\n\n\n\n\nCode\ntseries::adf.test(myts)\n\n\n\n    Augmented Dickey-Fuller Test\n\ndata:  myts\nDickey-Fuller = -5.768, Lag order = 3, p-value = 0.01\nalternative hypothesis: stationary\n\n\n\n\n\n\nCode\n# Seasonal differencing (period = 4)\nggAcf(myts,lag = 4,main=\"ACF Plot: Seasonality\")\n\n\n\n\n\n\n\n\n\nCode\n# Seasonal differencing (period = 4)\nggPacf(myts,lag = 4,main=\"PACF Plot: Seasonality\")\n\n\n\n\n\n\n\n\nUpon analyzing the ACF and PACF plots, it was observed that most of the bar lines lie between the blue lines, indicating stationarity of the time series. This has been further confirmed by the Augmented Dickey-Fuller test, as evidenced by a p-value of less than 0.05. Given that the data is already stationary but there is presence of seasonality, an SARIMA model is preferred over ARIMA. The combination of ACF and PACF plots can help determine the appropriate values for p and q in the ARMA model. The p value is determined by examining significant spikes at various lags in the PACF plot, while the q value is determined through significant spikes in the ACF plot. P and Q are determined similarly, from seasonal plot.\nHere the parameters are d = 0 D = 0 p = 0,1 (PACF Plot) q = 0,1 (ACF Plot) P = 0,1 (PACF Seasonality Plot) Q = 0,1 (ACF Seasonality Plot)\n\n\nModel Selection\n\nSARIMA ResultAuto Arima Model\n\n\n\n\nCode\n######################## Check for different combinations ########\n\n\n#write a funtion\nSARIMA.c=function(p1,p2,q1,q2,P1,P2,Q1,Q2,data){\n  \n  #K=(p2+1)*(q2+1)*(P2+1)*(Q2+1)\n  \n  temp=c()\n  d=0\n  D=0\n  s=4\n  \n  i=1\n  temp= data.frame()\n  ls=matrix(rep(NA,9*16),nrow=16)\n  \n  \n  for (p in p1:p2)\n  {\n    for(q in q1:q2)\n    {\n      for(P in P1:P2)\n      {\n        for(Q in Q1:Q2)\n        {\n          if(p+d+q+P+D+Q<=9)\n          {\n            \n            model<- Arima(data,order=c(p-1,d,q-1),seasonal=c(P-1,D,Q-1))\n            ls[i,]= c(p-1,d,q-1,P-1,D,Q-1,model$aic,model$bic,model$aicc)\n            i=i+1\n            #print(i)\n            \n          }\n          \n        }\n      }\n    }\n    \n  }\n\n  \n  temp= as.data.frame(ls)\n  names(temp)= c(\"p\",\"d\",\"q\",\"P\",\"D\",\"Q\",\"AIC\",\"BIC\",\"AICc\")\n  \n  temp\n  \n}\n# q=0,1,; Q=0,1 and PACF plot: p=0,1; P=0,1, D=0 and d=O\noutput=SARIMA.c(p1=1,p2=2,q1=1,q2=2,P1=1,P2=2,Q1=1,Q2=2,data=myts)\n#output\n\nknitr::kable(output)\n\n\n\n\n\np\nd\nq\nP\nD\nQ\nAIC\nBIC\nAICc\n\n\n\n\n0\n0\n0\n0\n0\n0\n241.5679\n245.4703\n241.8128\n\n\n0\n0\n0\n0\n0\n1\n207.2709\n213.1247\n207.7709\n\n\n0\n0\n0\n1\n0\n0\n228.4538\n234.3075\n228.9538\n\n\n0\n0\n0\n1\n0\n1\n207.6683\n215.4733\n208.5194\n\n\n0\n0\n1\n0\n0\n0\n235.1650\n241.0187\n235.6650\n\n\n0\n0\n1\n0\n0\n1\n201.5710\n209.3760\n202.4221\n\n\n0\n0\n1\n1\n0\n0\n221.6803\n229.4853\n222.5314\n\n\n0\n0\n1\n1\n0\n1\n202.2159\n211.9721\n203.5202\n\n\n1\n0\n0\n0\n0\n0\n234.0196\n239.8734\n234.5196\n\n\n1\n0\n0\n0\n0\n1\n199.8027\n207.6077\n200.6538\n\n\n1\n0\n0\n1\n0\n0\n219.0909\n226.8959\n219.9419\n\n\n1\n0\n0\n1\n0\n1\n200.3070\n210.0632\n201.6113\n\n\n1\n0\n1\n0\n0\n0\n236.0128\n243.8178\n236.8639\n\n\n1\n0\n1\n0\n0\n1\n201.7720\n211.5282\n203.0763\n\n\n1\n0\n1\n1\n0\n0\n221.0125\n230.7687\n222.3169\n\n\n1\n0\n1\n1\n0\n1\n202.2245\n213.9320\n204.0912\n\n\n\n\n\n\n\n\n\nCode\nauto.arima(myts)\n\n\nSeries: myts \nARIMA(1,0,0)(2,0,1)[4] with non-zero mean \n\nCoefficients:\n         ar1     sar1     sar2     sma1    mean\n      0.4006  -0.3997  -0.3431  -0.7034  2.0673\ns.e.  0.1292   0.2515   0.2512   0.2626  0.0741\n\nsigma^2 = 2.105:  log likelihood = -94.11\nAIC=200.22   AICc=202.09   BIC=211.93\n\n\n\n\n\nSorting the table reveals that the AIC and BIC with the parameters (1,0,0) and seasonal parameters are the least (0,0,1). There is also a function that enables the automatic selection of an ARIMA model. According to R’s auto.arima technique, the model’s parameters should be (1,0,0) and seasonal parameters (2,0,1).\nSince there are different models to choose from, it is important to perform model diagnostics to determine the best model for the data. These diagnostics can include checking for residual normality, checking for autocorrelation and partial autocorrelation in the residuals, and comparing the forecasted values to the actual values. By conducting these tests, we can determine which model provides the best fit for the data and is therefore the most appropriate to use for forecasting future values.\n\n\nModel Diagnostic\n\nModel 1 PlotModel 1Model 2 PlotModel 2\n\n\n\n\nCode\nmodel_output <- capture.output(sarima(myts, 1,1,0,0,0,1,4))\n\n\n\n\n\n\n\n\n\nCode\ncat(model_output[26:57], model_output[length(model_output)], sep = \"\\n\") \n\n\n$fit\n\nCall:\narima(x = xdata, order = c(p, d, q), seasonal = list(order = c(P, D, Q), period = S), \n    xreg = constant, transform.pars = trans, fixed = fixed, optim.control = list(trace = trc, \n        REPORT = 1, reltol = tol))\n\nCoefficients:\n          ar1     sma1  constant\n      -0.3051  -1.0000    0.0012\ns.e.   0.1329   0.1524    0.0417\n\nsigma^2 estimated as 2.492:  log likelihood = -100.93,  aic = 209.87\n\n$degrees_of_freedom\n[1] 48\n\n$ttable\n         Estimate     SE t.value p.value\nar1       -0.3051 0.1329 -2.2954  0.0261\nsma1      -1.0000 0.1524 -6.5608  0.0000\nconstant   0.0012 0.0417  0.0290  0.9770\n\n$AIC\n[1] 4.115092\n\n$AICc\n[1] 4.125105\n\n$BIC\n[1] 4.266608\n\n\n\n\n\n\nCode\nmodel_output <- capture.output(sarima(myts, 1,0,0,2,0,1,4))\n\n\n\n\n\n\n\n\n\nCode\ncat(model_output[38:70], model_output[length(model_output)], sep = \"\\n\") \n\n\n$fit\n\nCall:\narima(x = xdata, order = c(p, d, q), seasonal = list(order = c(P, D, Q), period = S), \n    xreg = xmean, include.mean = FALSE, transform.pars = trans, fixed = fixed, \n    optim.control = list(trace = trc, REPORT = 1, reltol = tol))\n\nCoefficients:\n         ar1     sar1     sar2     sma1   xmean\n      0.4006  -0.3997  -0.3431  -0.7034  2.0673\ns.e.  0.1292   0.2515   0.2512   0.2626  0.0741\n\nsigma^2 estimated as 1.903:  log likelihood = -94.11,  aic = 200.22\n\n$degrees_of_freedom\n[1] 47\n\n$ttable\n      Estimate     SE t.value p.value\nar1     0.4006 0.1292  3.1000  0.0033\nsar1   -0.3997 0.2515 -1.5893  0.1187\nsar2   -0.3431 0.2512 -1.3658  0.1785\nsma1   -0.7034 0.2626 -2.6781  0.0102\nxmean   2.0673 0.0741 27.9061  0.0000\n\n$AIC\n[1] 3.850466\n\n$AICc\n[1] 3.87555\n\n$BIC\n[1] 4.07561\n\n\n\n\n\nIn the first model, the p-values for both ar1 and sma1 are less than 0.05, indicating that both coefficients are statistically significant. In the second model, the p-value for ar1 is less than 0.05, indicating that it is statistically significant, while the p-values for the seasonal coefficients (sar1 and sar2) are greater than 0.05, indicating that they are not statistically significant. The p-value for sma1 in the second model is less than 0.05, indicating that it is statistically significant. Based on the p-values, it appears that the first model (SARIMA(1,1,0)(0,1,1)[4]) has a statistically significant AR and MA component, while the second model (SARIMA(1,1,1)(0,0,2)[4]) has a statistically significant.\nBy analyzing the standardized residuals plot for the model (SARIMA(1,1,0)(0,1,1)[4]), it can be observed that the mean is close to 0 and the variance is slightly higher than 1. Deviations from these values could indicate poor model fit. However, in this case, the model seems to be well-fitted. The ACF plot of residuals shows no significant lags, which is a high sign for the model. The qq-plot also suggests high normality in the residuals. Additionally, the p-values for the Ljung-Box test are near than 0.05, indicating that the residuals are independent, which is favorable for the model’s accuracy.\nThe equation for this model: \\[ (1-\\phi_1B)(1-B^4)(1-\\Phi_1B^4)y_t = \\epsilon_t \\]\n\n\nForcast\n\nShort Term ForcastLong term Forcast\n\n\n\n\nCode\nmyts %>%\n  Arima(order=c(1,1,0),seasonal = c(0,1,1), include.drift = TRUE) %>%\n  forecast %>%\n  autoplot(main = \"GDP Growth Rate Prediction\") +\n  ylab(\"GDP growth\") + xlab(\"Year\")\n\n\n\n\n\n\n\n\n\nCode\nsarima.for(myts,12, 1,1,0,0,1,1,4, main='GDP Growth Rate Prediction')\n\n\n\n\n\n$pred\n          Qtr1      Qtr2      Qtr3      Qtr4\n2023 1.0795445 1.0756510 1.0580210 1.0348134\n2024 0.9436877 1.0147972 0.9763838 0.9589353\n2025 0.8662137 0.9377654 0.8992295 0.8818150\n\n$se\n         Qtr1     Qtr2     Qtr3     Qtr4\n2023 2.699600 3.327957 3.963720 4.483195\n2024 5.048923 5.523866 5.967367 6.377804\n2025 6.838954 7.240686 7.626250 7.991102\n\n\n\n\n\nThe two figures above show an idea of the short term and the long-term forecasted GDP Growth Rate. The forecasted values is be based on the estimated parameters of the model, which were obtained by fitting it to the historical data. One way to assess the accuracy of the forecasts is to use cross-validation, which involves splitting the data into training and testing sets and comparing the predicted values to the actual values in the test set. This can help to identify any potential problems with the model and to fine-tune the model parameters.\n\n12 Step ahead Cross Validation1 Step ahead Cross Validation\n\n\n\n\nCode\n#a seasonal cross validation using 12 steps ahead forecasts\nfarima1 <- function(x, h){forecast(Arima(x, order=c(1,1,0),seasonal = c(0,1,1)), h=h)}\n\n# Compute cross-validated errors for up to 12 steps ahead\ne <- tsCV(myts, forecastfunction = farima1, h = 12)\n\n# Compute the MSE values and remove missing values\nmse <- colMeans(e^2, na.rm = TRUE)\n\n# Plot the MSE values against the forecast horizon\ndata.frame(h = 1:12, MSE = mse) %>%\n  ggplot(aes(x = h, y = MSE)) + geom_point()+geom_line()+ggtitle(\"12 Step Ahead Cross Validation\")\n\n\n\n\n\n\n\n\n\nCode\n#a seasonal cross validation using 1 steps ahead forecasts\nfarima1 <- function(x, h){forecast(Arima(x, order=c(1,1,0),seasonal = c(0,1,1)),h=h)}\n\n# Compute cross-validated errors for up to 1 steps ahead\ne <- tsCV(myts, forecastfunction = farima1, h = 1)\nmse <-abs(mean(e,na.rm=TRUE))\n\n# Plot the MSE values against the forecast horizon\ndata.frame(h = 1:12, MSE = mse) %>%\n  ggplot(aes(x = h, y = MSE)) + geom_point()+geom_line()+ggtitle(\"1 Step Ahead Cross Validation\")\n\n\n\n\n\n\n\n\nThe one-step-ahead forecasting plot shows that the MSE seems to be stable. The MSE is tend to rise tell 10 step but there is sudden drop at 10th step of cross-validation. Lastly, the above-mentioned ARIMA model should be compared to benchmark methods before being used for a long time.\n\n\nModel Comparison\n\nPlotModel Error Table\n\n\n\n\nCode\nfit <- Arima(myts, order=c(1,1,0),seasonal = c(0,1,1), lambda = 4)\nautoplot(myts) +\n  autolayer(meanf(myts, h=12),\n            series=\"Mean\", PI=FALSE) +\n  autolayer(naive(myts, h=12),\n            series=\"Naïve\", PI=FALSE) +\n  autolayer(snaive(myts, h=12),\n            series=\"SNaïve\", PI=FALSE)+\n  autolayer(rwf(myts, h=12, drift=TRUE),\n            series=\"Drift\", PI=FALSE)+\n  autolayer(forecast(fit,12), \n            series=\"fit\",PI=FALSE) +\n  guides(colour=guide_legend(title=\"Forecast\"))\n\n\n\n\n\n\n\n\n\nCode\nsummary <- summary(fit)\nsnaive <- snaive(myts, h=12)\n#accuracy(snaive)\n\n\n\n\n\nError\nModel\nSnavie\n\n\n\n\nME\n-0.03237449\n-0.05208333\n\n\nRMSE\n2.466369\n4.285368\n\n\nMAE\n1.191784\n2.285417\n\n\nMPE\n-13.70472\n-18.72979\n\n\nMAPE\n45.10017\n89.98744\n\n\nMASE\n0.5214732\n1.0000000\n\n\nACF1\n-0.02438127\n0.3889634\n\n\n\n\n\n\nThe ARIMA forecast doesn’t track the actual data points very closely. The other forecast methods are less accurate when compared to other model.The trend is visible in the fitted model, which indicates it is a good fit. From the table, Model error measurements of fit are much lower than snaive method.We can conclude that the fitted model is good.\n\n\nStationary Time Series\n\nACFPACFADF TestSeasonal ACF PlotSeasonal PACF Ploy\n\n\n\n\nCode\n#import the data\ndf <- read.csv(\"DATA/CLEANED DATA/gdp_clean_data.csv\")\n#convert to time series data\nmyts<-ts(df$value,frequency=4,start=c(2010/1/1))\n#ACF plot \nggAcf(myts,main=\"ACF Plot\") \n\n\n\n\n\n\n\n\n\nCode\nggPacf(myts,main=\"PACF Plot\") \n\n\n\n\n\n\n\n\n\nCode\ntseries::adf.test(myts)\n\n\n\n    Augmented Dickey-Fuller Test\n\ndata:  myts\nDickey-Fuller = -5.768, Lag order = 3, p-value = 0.01\nalternative hypothesis: stationary\n\n\n\n\n\n\nCode\n# Seasonal differencing (period = 4)\nggAcf(myts,lag = 4,main=\"ACF Plot: Seasonality\")\n\n\n\n\n\n\n\n\n\nCode\n# Seasonal differencing (period = 4)\nggPacf(myts,lag = 4,main=\"PACF Plot: Seasonality\")\n\n\n\n\n\n\n\n\nUpon analyzing the ACF and PACF plots, it was observed that most of the bar lines lie between the blue lines, indicating stationarity of the time series. This has been further confirmed by the Augmented Dickey-Fuller test, as evidenced by a p-value of less than 0.05. Given that the data is already stationary but there is presence of seasonality, an SARIMA model is preferred over ARIMA. The combination of ACF and PACF plots can help determine the appropriate values for p and q in the ARMA model. The p value is determined by examining significant spikes at various lags in the PACF plot, while the q value is determined through significant spikes in the ACF plot. P and Q are determined similarly, from seasonal plot.\nHere the parameters are d = 0 D = 0 p = 0,1 (PACF Plot) q = 0,1 (ACF Plot) P = 0,1 (PACF Seasonality Plot) Q = 0,1 (ACF Seasonality Plot)\n\n\nModel Selection\n\nSARIMA ResultAuto Arima Model\n\n\n\n\nCode\n######################## Check for different combinations ########\n\n\n#write a funtion\nSARIMA.c=function(p1,p2,q1,q2,P1,P2,Q1,Q2,data){\n  \n  #K=(p2+1)*(q2+1)*(P2+1)*(Q2+1)\n  \n  temp=c()\n  d=0\n  D=0\n  s=4\n  \n  i=1\n  temp= data.frame()\n  ls=matrix(rep(NA,9*16),nrow=16)\n  \n  \n  for (p in p1:p2)\n  {\n    for(q in q1:q2)\n    {\n      for(P in P1:P2)\n      {\n        for(Q in Q1:Q2)\n        {\n          if(p+d+q+P+D+Q<=9)\n          {\n            \n            model<- Arima(data,order=c(p-1,d,q-1),seasonal=c(P-1,D,Q-1))\n            ls[i,]= c(p-1,d,q-1,P-1,D,Q-1,model$aic,model$bic,model$aicc)\n            i=i+1\n            #print(i)\n            \n          }\n          \n        }\n      }\n    }\n    \n  }\n\n  \n  temp= as.data.frame(ls)\n  names(temp)= c(\"p\",\"d\",\"q\",\"P\",\"D\",\"Q\",\"AIC\",\"BIC\",\"AICc\")\n  \n  temp\n  \n}\n# q=0,1,; Q=0,1 and PACF plot: p=0,1; P=0,1, D=0 and d=O\noutput=SARIMA.c(p1=1,p2=2,q1=1,q2=2,P1=1,P2=2,Q1=1,Q2=2,data=myts)\n#output\n\nknitr::kable(output)\n\n\n\n\n\np\nd\nq\nP\nD\nQ\nAIC\nBIC\nAICc\n\n\n\n\n0\n0\n0\n0\n0\n0\n241.5679\n245.4703\n241.8128\n\n\n0\n0\n0\n0\n0\n1\n207.2709\n213.1247\n207.7709\n\n\n0\n0\n0\n1\n0\n0\n228.4538\n234.3075\n228.9538\n\n\n0\n0\n0\n1\n0\n1\n207.6683\n215.4733\n208.5194\n\n\n0\n0\n1\n0\n0\n0\n235.1650\n241.0187\n235.6650\n\n\n0\n0\n1\n0\n0\n1\n201.5710\n209.3760\n202.4221\n\n\n0\n0\n1\n1\n0\n0\n221.6803\n229.4853\n222.5314\n\n\n0\n0\n1\n1\n0\n1\n202.2159\n211.9721\n203.5202\n\n\n1\n0\n0\n0\n0\n0\n234.0196\n239.8734\n234.5196\n\n\n1\n0\n0\n0\n0\n1\n199.8027\n207.6077\n200.6538\n\n\n1\n0\n0\n1\n0\n0\n219.0909\n226.8959\n219.9419\n\n\n1\n0\n0\n1\n0\n1\n200.3070\n210.0632\n201.6113\n\n\n1\n0\n1\n0\n0\n0\n236.0128\n243.8178\n236.8639\n\n\n1\n0\n1\n0\n0\n1\n201.7720\n211.5282\n203.0763\n\n\n1\n0\n1\n1\n0\n0\n221.0125\n230.7687\n222.3169\n\n\n1\n0\n1\n1\n0\n1\n202.2245\n213.9320\n204.0912\n\n\n\n\n\n\n\n\n\nCode\nauto.arima(myts)\n\n\nSeries: myts \nARIMA(1,0,0)(2,0,1)[4] with non-zero mean \n\nCoefficients:\n         ar1     sar1     sar2     sma1    mean\n      0.4006  -0.3997  -0.3431  -0.7034  2.0673\ns.e.  0.1292   0.2515   0.2512   0.2626  0.0741\n\nsigma^2 = 2.105:  log likelihood = -94.11\nAIC=200.22   AICc=202.09   BIC=211.93\n\n\n\n\n\nSorting the table reveals that the AIC and BIC with the parameters (1,0,0) and seasonal parameters are the least (0,0,1). There is also a function that enables the automatic selection of an ARIMA model. According to R’s auto.arima technique, the model’s parameters should be (1,0,0) and seasonal parameters (2,0,1).\nSince there are different models to choose from, it is important to perform model diagnostics to determine the best model for the data. These diagnostics can include checking for residual normality, checking for autocorrelation and partial autocorrelation in the residuals, and comparing the forecasted values to the actual values. By conducting these tests, we can determine which model provides the best fit for the data and is therefore the most appropriate to use for forecasting future values.\n\n\nModel Diagnostic\n\nModel 1 PlotModel 1Model 2 PlotModel 2\n\n\n\n\nCode\nmodel_output <- capture.output(sarima(myts, 1,1,0,0,0,1,4))\n\n\n\n\n\n\n\n\n\nCode\ncat(model_output[26:57], model_output[length(model_output)], sep = \"\\n\") \n\n\n$fit\n\nCall:\narima(x = xdata, order = c(p, d, q), seasonal = list(order = c(P, D, Q), period = S), \n    xreg = constant, transform.pars = trans, fixed = fixed, optim.control = list(trace = trc, \n        REPORT = 1, reltol = tol))\n\nCoefficients:\n          ar1     sma1  constant\n      -0.3051  -1.0000    0.0012\ns.e.   0.1329   0.1524    0.0417\n\nsigma^2 estimated as 2.492:  log likelihood = -100.93,  aic = 209.87\n\n$degrees_of_freedom\n[1] 48\n\n$ttable\n         Estimate     SE t.value p.value\nar1       -0.3051 0.1329 -2.2954  0.0261\nsma1      -1.0000 0.1524 -6.5608  0.0000\nconstant   0.0012 0.0417  0.0290  0.9770\n\n$AIC\n[1] 4.115092\n\n$AICc\n[1] 4.125105\n\n$BIC\n[1] 4.266608\n\n\n\n\n\n\nCode\nmodel_output <- capture.output(sarima(myts, 1,0,0,2,0,1,4))\n\n\n\n\n\n\n\n\n\nCode\ncat(model_output[38:70], model_output[length(model_output)], sep = \"\\n\") \n\n\n$fit\n\nCall:\narima(x = xdata, order = c(p, d, q), seasonal = list(order = c(P, D, Q), period = S), \n    xreg = xmean, include.mean = FALSE, transform.pars = trans, fixed = fixed, \n    optim.control = list(trace = trc, REPORT = 1, reltol = tol))\n\nCoefficients:\n         ar1     sar1     sar2     sma1   xmean\n      0.4006  -0.3997  -0.3431  -0.7034  2.0673\ns.e.  0.1292   0.2515   0.2512   0.2626  0.0741\n\nsigma^2 estimated as 1.903:  log likelihood = -94.11,  aic = 200.22\n\n$degrees_of_freedom\n[1] 47\n\n$ttable\n      Estimate     SE t.value p.value\nar1     0.4006 0.1292  3.1000  0.0033\nsar1   -0.3997 0.2515 -1.5893  0.1187\nsar2   -0.3431 0.2512 -1.3658  0.1785\nsma1   -0.7034 0.2626 -2.6781  0.0102\nxmean   2.0673 0.0741 27.9061  0.0000\n\n$AIC\n[1] 3.850466\n\n$AICc\n[1] 3.87555\n\n$BIC\n[1] 4.07561\n\n\n\n\n\nIn the first model, the p-values for both ar1 and sma1 are less than 0.05, indicating that both coefficients are statistically significant. In the second model, the p-value for ar1 is less than 0.05, indicating that it is statistically significant, while the p-values for the seasonal coefficients (sar1 and sar2) are greater than 0.05, indicating that they are not statistically significant. The p-value for sma1 in the second model is less than 0.05, indicating that it is statistically significant. Based on the p-values, it appears that the first model (SARIMA(1,1,0)(0,1,1)[4]) has a statistically significant AR and MA component, while the second model (SARIMA(1,1,1)(0,0,2)[4]) has a statistically significant.\nBy analyzing the standardized residuals plot for the model (SARIMA(1,1,0)(0,1,1)[4]), it can be observed that the mean is close to 0 and the variance is slightly higher than 1. Deviations from these values could indicate poor model fit. However, in this case, the model seems to be well-fitted. The ACF plot of residuals shows no significant lags, which is a high sign for the model. The qq-plot also suggests high normality in the residuals. Additionally, the p-values for the Ljung-Box test are near than 0.05, indicating that the residuals are independent, which is favorable for the model’s accuracy.\nThe equation for this model: \\[ (1-\\phi_1B)(1-B^4)(1-\\Phi_1B^4)y_t = \\epsilon_t \\]\n\n\nForcast\n\nShort Term ForcastLong term Forcast\n\n\n\n\nCode\nmyts %>%\n  Arima(order=c(1,1,0),seasonal = c(0,1,1), include.drift = TRUE) %>%\n  forecast %>%\n  autoplot(main = \"GDP Growth Rate Prediction\") +\n  ylab(\"GDP growth\") + xlab(\"Year\")\n\n\n\n\n\n\n\n\n\nCode\nsarima.for(myts,12, 1,1,0,0,1,1,4, main='GDP Growth Rate Prediction')\n\n\n\n\n\n$pred\n          Qtr1      Qtr2      Qtr3      Qtr4\n2023 1.0795445 1.0756510 1.0580210 1.0348134\n2024 0.9436877 1.0147972 0.9763838 0.9589353\n2025 0.8662137 0.9377654 0.8992295 0.8818150\n\n$se\n         Qtr1     Qtr2     Qtr3     Qtr4\n2023 2.699600 3.327957 3.963720 4.483195\n2024 5.048923 5.523866 5.967367 6.377804\n2025 6.838954 7.240686 7.626250 7.991102\n\n\n\n\n\nThe two figures above show an idea of the short term and the long-term forecasted GDP Growth Rate. The forecasted values is be based on the estimated parameters of the model, which were obtained by fitting it to the historical data. One way to assess the accuracy of the forecasts is to use cross-validation, which involves splitting the data into training and testing sets and comparing the predicted values to the actual values in the test set. This can help to identify any potential problems with the model and to fine-tune the model parameters.\n\n12 Step ahead Cross Validation1 Step ahead Cross Validation\n\n\n\n\nCode\n#a seasonal cross validation using 12 steps ahead forecasts\nfarima1 <- function(x, h){forecast(Arima(x, order=c(1,1,0),seasonal = c(0,1,1)), h=h)}\n\n# Compute cross-validated errors for up to 12 steps ahead\ne <- tsCV(myts, forecastfunction = farima1, h = 12)\n\n# Compute the MSE values and remove missing values\nmse <- colMeans(e^2, na.rm = TRUE)\n\n# Plot the MSE values against the forecast horizon\ndata.frame(h = 1:12, MSE = mse) %>%\n  ggplot(aes(x = h, y = MSE)) + geom_point()+geom_line()+ggtitle(\"12 Step Ahead Cross Validation\")\n\n\n\n\n\n\n\n\n\nCode\n#a seasonal cross validation using 1 steps ahead forecasts\nfarima1 <- function(x, h){forecast(Arima(x, order=c(1,1,0),seasonal = c(0,1,1)),h=h)}\n\n# Compute cross-validated errors for up to 1 steps ahead\ne <- tsCV(myts, forecastfunction = farima1, h = 1)\nmse <-abs(mean(e,na.rm=TRUE))\n\n# Plot the MSE values against the forecast horizon\ndata.frame(h = 1:12, MSE = mse) %>%\n  ggplot(aes(x = h, y = MSE)) + geom_point()+geom_line()+ggtitle(\"1 Step Ahead Cross Validation\")\n\n\n\n\n\n\n\n\nThe one-step-ahead forecasting plot shows that the MSE seems to be stable. The MSE is tend to rise tell 10 step but there is sudden drop at 10th step of cross-validation. Lastly, the above-mentioned ARIMA model should be compared to benchmark methods before being used for a long time.\n\n\nModel Comparison\n\nPlotModel Error Table\n\n\n\n\nCode\nfit <- Arima(myts, order=c(1,1,0),seasonal = c(0,1,1), lambda = 4)\nautoplot(myts) +\n  autolayer(meanf(myts, h=12),\n            series=\"Mean\", PI=FALSE) +\n  autolayer(naive(myts, h=12),\n            series=\"Naïve\", PI=FALSE) +\n  autolayer(snaive(myts, h=12),\n            series=\"SNaïve\", PI=FALSE)+\n  autolayer(rwf(myts, h=12, drift=TRUE),\n            series=\"Drift\", PI=FALSE)+\n  autolayer(forecast(fit,12), \n            series=\"fit\",PI=FALSE) +\n  guides(colour=guide_legend(title=\"Forecast\"))\n\n\n\n\n\n\n\n\n\nCode\nsummary <- summary(fit)\nsnaive <- snaive(myts, h=12)\n#accuracy(snaive)\n\n\n\n\n\nError\nModel\nSnavie\n\n\n\n\nME\n-0.03237449\n-0.05208333\n\n\nRMSE\n2.466369\n4.285368\n\n\nMAE\n1.191784\n2.285417\n\n\nMPE\n-13.70472\n-18.72979\n\n\nMAPE\n45.10017\n89.98744\n\n\nMASE\n0.5214732\n1.0000000\n\n\nACF1\n-0.02438127\n0.3889634\n\n\n\n\n\n\nThe ARIMA forecast doesn’t track the actual data points very closely. The other forecast methods are less accurate when compared to other model.The trend is visible in the fitted model, which indicates it is a good fit. From the table, Model error measurements of fit are much lower than snaive method.We can conclude that the fitted model is good."
  },
  {
    "objectID": "arima_macroeconomic_inflation.html",
    "href": "arima_macroeconomic_inflation.html",
    "title": "SARIMA Model for Inflation Rate",
    "section": "",
    "text": "From the exploratory data analysis (EDA), it may be observed that the raw data for certain macroeconomic factors, such as interest rates or stock prices, is non-stationary and requires differencing to achieve stationarity. By differencing the data, we remove the trend and other non-stationary components, making it easier to model and forecast accurately.\n\nDifferentiated Time Series Plot\n\nPlotACFPACFADF TestSeasonal ACF PlotSeasonal PACF Ploy\n\n\n\n\nCode\n#import the data\ndf <- read.csv(\"DATA/CLEANED DATA/inflation_rate_clean_data.csv\")\n#convert the data to time series data\nmyts <- ts(as.vector(t(as.matrix(df))), start=c(2010,1), end=c(2023,2), frequency=12)\n#First order differentiation\ndf1 <- diff(myts)\n# Plot \nmyts  %>% diff() %>% ggtsdisplay(main = \"First order differentiation\") \n\n\n\n\n\n\n\n\n\nCode\n#ACF plot \nggAcf(df1,36,main=\"ACF Plot: First order differentiation\") \n\n\n\n\n\n\n\n\n\nCode\nggPacf(df1,36,main=\"PACF Plot: First order differentiation\") \n\n\n\n\n\n\n\n\n\nCode\ntseries::adf.test(df1)\n\n\n\n    Augmented Dickey-Fuller Test\n\ndata:  df1\nDickey-Fuller = -3.8576, Lag order = 5, p-value = 0.01802\nalternative hypothesis: stationary\n\n\n\n\n\n\nCode\n# Seasonal differencing (period = 12)\ndf1_seasonal = myts %>%diff(differences = 1, lag = 12)\nggAcf(df1_seasonal,main=\"ACF Plot: Seasonality\")\n\n\n\n\n\n\n\n\n\nCode\n# Seasonal differencing (period = 12)\nggPacf(df1_seasonal,main=\"PACF Plot: Seasonality\")\n\n\n\n\n\n\n\n\nAfter analyzing the ACF and PACF plots, it can be observed that most of the bar lines lie between the blue lines, indicating that the time series is stationary. This has been further confirmed through an Augmented Dickey-Fuller test as the p-value is less than 0.05. Once the data is stationary and there is presence of seasonality, the next step is to model it using SARIMA. The combination of ACF and PACF plots can help determine the appropriate values for p, d, and q in the ARIMA model. The order of differencing required to achieve stationarity gives the d value. The p value is determined by examining significant spikes at various lags in the PACF plot, while the q value is determined through significant spikes in the ACF plot. P and Q are determined similarly, from seasonal plot.\nHere the parameters are d = 1, p = 0,1 (PACF Plot), q = 0,1 (ACF Plot), P = 1,2 (PACF Seasonality Plot), Q = 1,2,3,4 (ACF Seasonality Plot) D = 1\n\n\nModel Selection\n\nSARIMA ResultAuto Arima Model\n\n\n\n\nCode\n######################## Check for different combinations ########\n\n\n#write a funtion\nSARIMA.c=function(p1,q1,P1,P2,Q1,Q2,data){\n  \n  #K=(p2+1)*(q2+1)*(P2+1)*(Q2+1)\n  \n  temp=c()\n  d=1\n  D=1\n  s=12\n  \n  i=1\n  temp= data.frame()\n  ls=matrix(rep(NA,9*3),nrow=3)\n  \n  \n  for (p in p1)\n  {\n    for(q in q1)\n    {\n      for(P in P1:P2)\n      {\n        for(Q in Q1:Q2)\n        {\n          if(p+d+q+P+D+Q<=9)\n          {\n            \n            model<- Arima(data,order=c(p-1,d,q-1),seasonal=c(P-1,D,Q-1))\n            ls[i,]= c(p-1,d,q-1,P-1,D,Q-1,model$aic,model$bic,model$aicc)\n            i=i+1\n            #print(i)\n            \n          }\n          \n        }\n      }\n    }\n    \n  }\n\n  \n  temp= as.data.frame(ls)\n  names(temp)= c(\"p\",\"d\",\"q\",\"P\",\"D\",\"Q\",\"AIC\",\"BIC\",\"AICc\")\n  \n  temp\n  \n}\n# q=0,1,; Q=0,1,2,3 and PACF plot: p=0,1; P=0,1,2, D=1 and d=1\noutput=SARIMA.c(p1=2,q1=2,P1=1,P2=3,Q1=1,Q2=3,data=myts)\n#output\nknitr::kable(output)\n\n\n\n\n\np\nd\nq\nP\nD\nQ\nAIC\nBIC\nAICc\n\n\n\n\n1\n1\n1\n0\n1\n0\n86.02072\n94.950919\n86.19093\n\n\n1\n1\n1\n0\n1\n1\n-26.80019\n-14.893256\n-26.51448\n\n\n1\n1\n1\n1\n1\n0\n-16.22925\n-4.322316\n-15.94354\n\n\n\n\n\n\n\n\n\nCode\nauto.arima(myts)\n\n\nSeries: myts \nARIMA(4,1,1)(0,0,1)[12] with drift \n\nCoefficients:\n         ar1      ar2      ar3     ar4      ma1     sma1   drift\n      1.1757  -0.2352  -0.3924  0.3603  -0.6966  -0.5922  0.0215\ns.e.  0.1166   0.1251   0.1162  0.0757   0.1037   0.0807  0.0173\n\nsigma^2 = 0.02376:  log likelihood = 71.61\nAIC=-127.23   AICc=-126.25   BIC=-102.78\n\n\n\n\n\nSorting the table reveals that the AIC and BIC with the parameters (1,1,1) and seasonal parameters are the least (1,1,0). There is also a function that enables the automatic selection of an ARIMA model. According to R’s auto.arima technique, the model’s parameters should be (4,1,1) and (0,0,1).\nSince there are different models to choose from, it is important to perform model diagnostics to determine the best model for the data. These diagnostics can include checking for residual normality, checking for autocorrelation and partial autocorrelation in the residuals, and comparing the forecasted values to the actual values. By conducting these tests, we can determine which model provides the best fit for the data and is therefore the most appropriate to use for forecasting future values.\n\n\nModel Diagnostic\n\nModel 1 PlotModel 1Model 2 PlotModel 2\n\n\n\n\nCode\nmodel_output <- capture.output(sarima(myts, 1,1,1,1,1,0,12))\n\n\n\n\n\n\n\n\n\nCode\ncat(model_output[22:52], model_output[length(model_output)], sep = \"\\n\") \n\n\niter  11 value -1.502488\niter  11 value -1.502488\niter  11 value -1.502488\nfinal  value -1.502488 \nconverged\n$fit\n\nCall:\narima(x = xdata, order = c(p, d, q), seasonal = list(order = c(P, D, Q), period = S), \n    include.mean = !no.constant, transform.pars = trans, fixed = fixed, optim.control = list(trace = trc, \n        REPORT = 1, reltol = tol))\n\nCoefficients:\n         ar1     ma1     sar1\n      0.3616  0.2288  -0.8036\ns.e.  0.1242  0.1212   0.0530\n\nsigma^2 estimated as 0.04535:  log likelihood = 12.11,  aic = -16.23\n\n$degrees_of_freedom\n[1] 142\n\n$ttable\n     Estimate     SE  t.value p.value\nar1    0.3616 0.1242   2.9123  0.0042\nma1    0.2288 0.1212   1.8883  0.0610\nsar1  -0.8036 0.0530 -15.1732  0.0000\n\n$AIC\n[1] -0.1119259\n\n\n\n\n\n\nCode\nmodel_output <- capture.output(sarima(myts, 4,1,1,0,0,1,12))\n\n\n\n\n\n\n\n\n\nCode\ncat(model_output[46:81], model_output[length(model_output)], sep = \"\\n\") \n\n\n$fit\n\nCall:\narima(x = xdata, order = c(p, d, q), seasonal = list(order = c(P, D, Q), period = S), \n    xreg = constant, transform.pars = trans, fixed = fixed, optim.control = list(trace = trc, \n        REPORT = 1, reltol = tol))\n\nCoefficients:\n         ar1      ar2      ar3     ar4      ma1     sma1  constant\n      1.1757  -0.2352  -0.3924  0.3603  -0.6966  -0.5922    0.0215\ns.e.  0.1166   0.1251   0.1162  0.0757   0.1037   0.0807    0.0173\n\nsigma^2 estimated as 0.0227:  log likelihood = 71.61,  aic = -127.23\n\n$degrees_of_freedom\n[1] 150\n\n$ttable\n         Estimate     SE t.value p.value\nar1        1.1757 0.1166 10.0868  0.0000\nar2       -0.2352 0.1251 -1.8803  0.0620\nar3       -0.3924 0.1162 -3.3768  0.0009\nar4        0.3603 0.0757  4.7621  0.0000\nma1       -0.6966 0.1037 -6.7199  0.0000\nsma1      -0.5922 0.0807 -7.3380  0.0000\nconstant   0.0215 0.0173  1.2380  0.2177\n\n$AIC\n[1] -0.8103599\n\n$AICc\n[1] -0.8055721\n\n$BIC\n[1] -0.6546276\n\n\n\n\n\nA smaller p-value indicates that a coefficient is more statistically significant. Looking at the output provided, the second model SARMIA (4,1,1)(0,0,1)[12] appears to have lower AIC and BIC value and when comparing p-values for all of the coefficients, which suggests that it is a better model in terms of statistical significance.\nBy analyzing the standardized residuals plot for the model (SARIMA(4,1,1)(0,0,1)[12]), it can be observed that the mean is close to 0 and the variance is slightly higher than 1. Deviations from these values could indicate poor model fit. However, in this case, the model seems to be well-fitted. The ACF plot of residuals shows very low significant lags, which is a high sign for the model. The qq-plot also suggests high normality in the residuals. Additionally, the p-values for the Ljung-Box test are near than 0.05, indicating that the residuals are independent, which is favorable for the model’s accuracy.\nThe equation for this model: \\[\\Delta Y_t = \\phi_1 \\Delta Y_{t-1} + \\phi_2 \\Delta Y_{t-2} + \\phi_3 \\Delta Y_{t-3} + \\phi_4 \\Delta Y_{t-4} + \\theta_1 \\epsilon_{t-1} + \\epsilon_t + \\theta_{12} \\epsilon_{t-12}\\]\n\n\nForcast\n\nShort Term ForcastLong term Forcast\n\n\n\n\nCode\nmyts %>%\n  Arima(order=c(4,1,1),seasonal = c(0,0,1),include.drift = TRUE) %>%\n  forecast %>%\n  autoplot(main = \"Inflation Rate Prediction\") +\n  ylab(\"Inflation Rate\") + xlab(\"Year\")\n\n\n\n\n\n\n\n\n\nCode\nsarima.for(myts,36, 4,1,1,0,0,1,12, main = \"Inflation Rate Prediction\")\n\n\n\n\n\n$pred\n          Jan      Feb      Mar      Apr      May      Jun      Jul      Aug\n2023                   5.495776 5.389748 5.344802 5.205688 5.249631 5.164470\n2024 5.283954 5.283987 5.290514 5.314089 5.337350 5.358570 5.373114 5.386553\n2025 5.476534 5.494819 5.513071 5.531901 5.551331 5.571139 5.591010 5.610839\n2026 5.711529 5.732022                                                      \n          Sep      Oct      Nov      Dec\n2023 5.064247 5.138831 5.273497 5.297473\n2024 5.400955 5.418630 5.437955 5.457675\n2025 5.630673 5.650633 5.670779 5.691099\n2026                                    \n\n$se\n           Jan       Feb       Mar       Apr       May       Jun       Jul\n2023                     0.1506640 0.2690050 0.3827598 0.4595446 0.5219644\n2024 0.9329297 1.0004244 1.0394805 1.0695190 1.0948899 1.1240493 1.1549882\n2025 1.3042817 1.3255537 1.3460871 1.3656904 1.3844121 1.4024993 1.4201440\n2026 1.5180451 1.5331652                                                  \n           Aug       Sep       Oct       Nov       Dec\n2023 0.5789069 0.6431488 0.7142951 0.7894332 0.8627722\n2024 1.1860724 1.2138042 1.2384755 1.2609465 1.2827317\n2025 1.4374323 1.4543438 1.4708448 1.4869326 1.5026483\n2026                                                  \n\n\n\n\n\nThe two figures above show an idea of the short term and the long-term forecasted Inflation Rate. The forecasted values is be based on the estimated parameters of the model, which were obtained by fitting it to the historical data. One way to assess the accuracy of the forecasts is to use cross-validation, which involves splitting the data into training and testing sets and comparing the predicted values to the actual values in the test set. This can help to identify any potential problems with the model and to fine-tune the model parameters.\n\n12 Step ahead Cross Validation1 Step ahead Cross Validation\n\n\n\n\nCode\n#a seasonal cross validation using 12 steps ahead forecasts\nfarima1 <- function(x, h){forecast(Arima(x, order=c(4,1,1),seasonal = c(0,0,1)), h=h)}\n\n# Compute cross-validated errors for up to 12 steps ahead\ne <- tsCV(myts, forecastfunction = farima1, h = 12)\n\n# Compute the MSE values and remove missing values\nmse <- colMeans(e^2, na.rm = TRUE)\n\n# Plot the MSE values against the forecast horizon\ndata.frame(h = 1:12, MSE = mse) %>%\n  ggplot(aes(x = h, y = MSE)) + geom_point()+geom_line()+ggtitle(\"12 Step Ahead Cross Validation\")\n\n\n\n\n\n\n\n\n\nCode\n#a seasonal cross validation using 1 steps ahead forecasts\nfarima1 <- function(x, h){forecast(Arima(x, order=c(4,1,1),seasonal = c(0,0,1)),h=h)}\n\n# Compute cross-validated errors for up to 1 steps ahead\ne <- tsCV(myts, forecastfunction = farima1, h = 1)\nmse <-abs(mean(e,na.rm=TRUE))\n\n# Plot the MSE values against the forecast horizon\ndata.frame(h = 1:12, MSE = mse) %>%\n  ggplot(aes(x = h, y = MSE)) + geom_point()+geom_line()+ggtitle(\"1 Step Ahead Cross Validation\")\n\n\n\n\n\n\n\n\nThe one-step-ahead forecasting plot shows that the MSE seems to be stable. The MSE is tend to rise by 12 steps at each step of cross-validation. Lastly, the above-mentioned ARIMA model should be compared to benchmark methods before being used for a long time.\n\n\nModel Comparison\n\nPlotModel Error Table\n\n\n\n\nCode\nfit <- Arima(myts, order=c(4,1,1),seasonal = c(0,0,1))\nautoplot(myts) +\n  autolayer(meanf(myts, h=36),\n            series=\"Mean\", PI=FALSE) +\n  autolayer(naive(myts, h=36),\n            series=\"Naïve\", PI=FALSE) +\n  autolayer(snaive(myts, h=36),\n            series=\"SNaïve\", PI=FALSE)+\n  autolayer(rwf(myts, h=36, drift=TRUE),\n            series=\"Drift\", PI=FALSE)+\n  autolayer(forecast(fit,36), \n            series=\"fit\",PI=FALSE) +\n  guides(colour=guide_legend(title=\"Forecast\"))\n\n\n\n\n\n\n\n\n\nCode\nsummary <- summary(fit)\nsnaive <- snaive(myts, h=36)\n#accuracy(snaive)\n\n\n\n\n\nError\nModel\nSnavie\n\n\n\n\nME\n0.01203582\n0.4178082\n\n\nRMSE\n0.1508245\n1.215031\n\n\nMAE\n0.1068308\n0.7410959\n\n\nMPE\n0.3265318\n6.891819\n\n\nMAPE\n5.342487\n26.54941\n\n\nMASE\n0.1441525\n1.0000000\n\n\nACF1\n-0.0333594\n0.9389579\n\n\n\n\n\n\nThe ARIMA forecast tracks the actual data points very closely when compared to other model. The other forecast methods are less accurate when compared to other model.The trend is visible in the fitted model, which indicates it is a good fit. From the table, Model error measurements of fit are much lower than snaive method.We can conclude that the fitted model is good."
  },
  {
    "objectID": "arima_macroeconomic_interest.html",
    "href": "arima_macroeconomic_interest.html",
    "title": "SARIMA Model for Interest Rate",
    "section": "",
    "text": "From the exploratory data analysis (EDA), it may be observed that the raw data for certain macroeconomic factors, such as interest rates or stock prices, is non-stationary and requires differencing to achieve stationarity. By differencing the data, we remove the trend and other non-stationary components, making it easier to model and forecast accurately.\n\nDifferentiated Time Series Plot\n\nPlotACFPACFADF TestSeasonal ACF PlotSeasonal PACF Ploy\n\n\n\n\nCode\n#import the data\ndf <- read.csv(\"DATA/CLEANED DATA/interest_rate_clean_data.csv\")\n#convert to ts data\nmyts<-ts(df$value,frequency=12,start=c(2010/1/1))\n#First order differentiation\ndf1 <- diff(myts)\n# Plot \nmyts  %>% diff() %>% ggtsdisplay(main = \"First order differentiation\") \n\n\n\n\n\n\n\n\n\nCode\n#ACF plot \nggAcf(df1,36,main=\"ACF Plot: First order differentiation\") \n\n\n\n\n\n\n\n\n\nCode\nggPacf(df1,60,main=\"PACF Plot: First order differentiation\") \n\n\n\n\n\n\n\n\n\nCode\ntseries::adf.test(df1)\n\n\n\n    Augmented Dickey-Fuller Test\n\ndata:  df1\nDickey-Fuller = -4.1249, Lag order = 5, p-value = 0.01\nalternative hypothesis: stationary\n\n\n\n\n\n\nCode\n# Seasonal differencing (period = 12)\ndf1_seasonal = myts %>%diff(differences = 1, lag = 12)\nggAcf(df1_seasonal,main=\"ACF Plot: Seasonality\")\n\n\n\n\n\n\n\n\n\nCode\n# Seasonal differencing (period = 12)\nggPacf(df1_seasonal,main=\"PACF Plot: Seasonality\")\n\n\n\n\n\n\n\n\nAfter analyzing the ACF and PACF plots, it can be observed that most of the bar lines lie between the blue lines, indicating that the time series is stationary. This has been further confirmed through an Augmented Dickey-Fuller test as the p-value is less than 0.05. Once the data is stationary and there is presence of seasonality, the next step is to model it using SARIMA. The combination of ACF and PACF plots can help determine the appropriate values for p, d, and q in the ARIMA model. The order of differencing required to achieve stationarity gives the d value. The p value is determined by examining significant spikes at various lags in the PACF plot, while the q value is determined through significant spikes in the ACF plot. P and Q are determined similarly, from seasonal plot.\nHere the parameters are d = 1 p = 0 (PACF Plot) q = 0 (ACF Plot) P = 1 (PACF Seasonality Plot) Q = 1,2,3,4 (ACF Seasonality Plot) D = 1\n\n\nModel Selection\n\nSARIMA ResultAuto Arima Model\n\n\n\n\nCode\n######################## Check for different combinations ########\n\n\n#write a funtion\nSARIMA.c=function(p1,q1,P1,P2,Q1,Q2,data){\n  \n  #K=(p2+1)*(q2+1)*(P2+1)*(Q2+1)\n  \n  temp=c()\n  d=1\n  D=1\n  s=12\n  \n  i=1\n  temp= data.frame()\n  ls=matrix(rep(NA,9*7),nrow=7)\n  \n  \n  for (p in p1)\n  {\n    for(q in q1)\n    {\n      for(P in P1:P2)\n      {\n        for(Q in Q1:Q2)\n        {\n          if(p+d+q+P+D+Q<=9)\n          {\n            \n            model<- Arima(data,order=c(p-1,d,q-1),seasonal=c(P-1,D,Q-1))\n            ls[i,]= c(p-1,d,q-1,P-1,D,Q-1,model$aic,model$bic,model$aicc)\n            i=i+1\n            #print(i)\n            \n          }\n          \n        }\n      }\n    }\n    \n  }\n\n  \n  temp= as.data.frame(ls)\n  names(temp)= c(\"p\",\"d\",\"q\",\"P\",\"D\",\"Q\",\"AIC\",\"BIC\",\"AICc\")\n  \n  temp\n  \n}\n# q=0,; Q=0,1,2,3,4 and PACF plot: p=0; P=0,1, D=0 and d=O\noutput=SARIMA.c(p1=1,q1=1,P1=1,P2=2,Q1=1,Q2=5,data=myts)\n#output\nknitr::kable(output)\n\n\n\n\n\np\nd\nq\nP\nD\nQ\nAIC\nBIC\nAICc\n\n\n\n\n0\n1\n0\n0\n1\n0\n-38.60554\n-35.62193\n-38.57776\n\n\n0\n1\n0\n0\n1\n1\n-61.11557\n-55.14836\n-61.03165\n\n\n0\n1\n0\n0\n1\n2\n-59.78470\n-50.83388\n-59.61569\n\n\n0\n1\n0\n0\n1\n3\n-60.70802\n-48.77359\n-60.42433\n\n\n0\n1\n0\n1\n1\n0\n-57.49539\n-51.52817\n-57.41147\n\n\n0\n1\n0\n1\n1\n1\n-60.79010\n-51.83928\n-60.62109\n\n\n0\n1\n0\n1\n1\n2\n-60.05246\n-48.11803\n-59.76877\n\n\n\n\n\n\n\n\n\nCode\nauto.arima(myts)\n\n\nSeries: myts \nARIMA(1,0,0)(1,0,0)[12] with non-zero mean \n\nCoefficients:\n         ar1    sar1    mean\n      0.9548  0.2623  0.8646\ns.e.  0.0267  0.0940  0.3954\n\nsigma^2 = 0.03157:  log likelihood = 48.82\nAIC=-89.64   AICc=-89.38   BIC=-77.36\n\n\n\n\n\nSorting the table reveals that the AIC and BIC with the parameters (1,1,0) and seasonal parameters are the least (0,1,1). There is also a function that enables the automatic selection of an ARIMA model. According to R’s auto.arima technique, the model’s parameters should be (1,0,0) and (1,0,0).\nSince there are different models to choose from, it is important to perform model diagnostics to determine the best model for the data. These diagnostics can include checking for residual normality, checking for autocorrelation and partial autocorrelation in the residuals, and comparing the forecasted values to the actual values. By conducting these tests, we can determine which model provides the best fit for the data and is therefore the most appropriate to use for forecasting future values.\n\n\nModel Diagnostic\n\nModel 1 PlotModel 1Model 2 PlotModel 2\n\n\n\n\nCode\nmodel_output <- capture.output(sarima(myts, 1,1,0,0,1,1,12))\n\n\n\n\n\n\n\n\n\nCode\ncat(model_output[18:48], model_output[length(model_output)], sep = \"\\n\") \n\n\n$fit\n\nCall:\narima(x = xdata, order = c(p, d, q), seasonal = list(order = c(P, D, Q), period = S), \n    include.mean = !no.constant, transform.pars = trans, fixed = fixed, optim.control = list(trace = trc, \n        REPORT = 1, reltol = tol))\n\nCoefficients:\n          ar1     sma1\n      -0.0786  -0.5590\ns.e.   0.0869   0.1293\n\nsigma^2 estimated as 0.03614:  log likelihood = 32.97,  aic = -59.93\n\n$degrees_of_freedom\n[1] 144\n\n$ttable\n     Estimate     SE t.value p.value\nar1   -0.0786 0.0869 -0.9045  0.3673\nsma1  -0.5590 0.1293 -4.3227  0.0000\n\n$AIC\n[1] -0.4104909\n\n$AICc\n[1] -0.4099161\n\n$BIC\n[1] -0.3491839\n\n\n\n\n\n\nCode\nmodel_output <- capture.output(sarima(myts, 1,0,0,1,0,0,12))\n\n\n\n\n\n\n\n\n\nCode\ncat(model_output[60:91], model_output[length(model_output)], sep = \"\\n\") \n\n\n$fit\n\nCall:\narima(x = xdata, order = c(p, d, q), seasonal = list(order = c(P, D, Q), period = S), \n    xreg = xmean, include.mean = FALSE, transform.pars = trans, fixed = fixed, \n    optim.control = list(trace = trc, REPORT = 1, reltol = tol))\n\nCoefficients:\n         ar1    sar1   xmean\n      0.9548  0.2623  0.8646\ns.e.  0.0267  0.0940  0.3954\n\nsigma^2 estimated as 0.03097:  log likelihood = 48.82,  aic = -89.64\n\n$degrees_of_freedom\n[1] 156\n\n$ttable\n      Estimate     SE t.value p.value\nar1     0.9548 0.0267 35.7690  0.0000\nsar1    0.2623 0.0940  2.7898  0.0059\nxmean   0.8646 0.3954  2.1866  0.0303\n\n$AIC\n[1] -0.5637746\n\n$AICc\n[1] -0.5628008\n\n$BIC\n[1] -0.4865695\n\n\n\n\n\nLooking at the p-values of the coefficients in the two models, we can see that all the coefficients in the second model have p-values less than 0.05, indicating that they are statistically significant at the 5% level. In contrast, the p-value of the AR(1) coefficient in the first model is 0.3673, indicating that it is not statistically significant at the 5% level.Therefore, based on the p-values alone, the first model with SARIMA(1,1,0)(0,1,1)[12] appears to be the better model.\nBy analyzing the standardized residuals plot for the model (SARIMA(1,1,0)(0,1,1)[12]), it can be observed that the mean is close to 0 and the variance is higher than 1. Deviations from these values could indicate poor model fit. However, in this case, the model seems to be well-fitted. The ACF plot of residuals shows no significant lags, which is a high sign for the model. The qq-plot also suggests high normality in the residuals. Additionally, the p-values for the Ljung-Box test are near than 0.05, indicating that the residuals are independent, which is favorable for the model’s accuracy.\nThe equation for this model: \\[ (1-\\phi_1B)(1-B^{12})(1-\\Phi_1B^{12}))y_t = \\epsilon_t \\]\n\n\nForcast\n\nShort Term ForcastLong term Forcast\n\n\n\n\nCode\nmyts %>%\n  Arima(order=c(1,1,0), seasonal = c(0,1,1),include.drift = TRUE) %>%\n  forecast %>%\n  autoplot(main = \"Interest Rate Prediction\") +\n  ylab(\"Interest Rate\") + xlab(\"Year\")\n\n\n\n\n\n\n\n\n\nCode\nsarima.for(myts,36, 1,1,0,0,1,1,12, main = \"Interest Rate Prediction\")\n\n\n\n\n\n$pred\n          Jan      Feb      Mar      Apr      May      Jun      Jul      Aug\n2023                            2.282059 2.409382 2.295342 2.289331 2.117888\n2024 2.704908 2.565338 2.802065 3.054416 3.179239 3.065396 3.059369 2.887927\n2025 3.474948 3.335377 3.572105 3.824456 3.949279 3.835436 3.829409 3.657967\n2026 4.244988 4.105417 4.342144                                             \n          Sep      Oct      Nov      Dec\n2023 2.273762 2.634956 2.731161 2.557040\n2024 3.043801 3.404995 3.501201 3.327079\n2025 3.813841 4.175035 4.271240 4.097119\n2026                                    \n\n$se\n           Jan       Feb       Mar       Apr       May       Jun       Jul\n2023                               0.1901105 0.2585052 0.3129230 0.3591436\n2024 0.5615857 0.5885951 0.6144183 0.6672023 0.7137387 0.7575948 0.7990346\n2025 1.0126712 1.0440358 1.0744853 1.1263407 1.1740351 1.2200070 1.2642976\n2026 1.5028805 1.5390527 1.5743941                                        \n           Aug       Sep       Oct       Nov       Dec\n2023 0.4000625 0.4371678 0.4713612 0.5032366 0.5332099\n2024 0.8384296 0.8760547 0.9121292 0.9468302 0.9803037\n2025 1.3070892 1.3485235 1.3887222 1.4277895 1.4658159\n2026                                                  \n\n\n\n\n\nThe two figures above show an idea of the short term and the long-term forecasted Interest Rate. The forecasted values is be based on the estimated parameters of the model, which were obtained by fitting it to the historical data. One way to assess the accuracy of the forecasts is to use cross-validation, which involves splitting the data into training and testing sets and comparing the predicted values to the actual values in the test set. This can help to identify any potential problems with the model and to fine-tune the model parameters.\n\n12 Step ahead Cross Validation1 Step ahead Cross Validation\n\n\n\n\nCode\n#a seasonal cross validation using 12 steps ahead forecasts\nfarima1 <- function(x, h){forecast(Arima(x, order=c(1,1,0),seasonal = c(0,1,1)),lambda = 12, h=h)}\n\n# Compute cross-validated errors for up to 12 steps ahead\ne <- tsCV(myts, forecastfunction = farima1, h = 12)\n\n# Compute the MSE values and remove missing values\nmse <- colMeans(e^2, na.rm = TRUE)\n\n# Plot the MSE values against the forecast horizon\ndata.frame(h = 1:12, MSE = mse) %>%\n  ggplot(aes(x = h, y = MSE)) + geom_point()+geom_line()+ggtitle(\"12 Step Ahead Cross Validation\")\n\n\n\n\n\n\n\n\n\nCode\n#a seasonal cross validation using 1 steps ahead forecasts\nfarima1 <- function(x, h){forecast(Arima(x, order=c(1,1,0),seasonal = c(0,1,1)),lambda = 12,h=h)}\n\n# Compute cross-validated errors for up to 1 steps ahead\ne <- tsCV(myts, forecastfunction = farima1, h = 1)\nmse <-abs(mean(e,na.rm=TRUE))\n\n# Plot the MSE values against the forecast horizon\ndata.frame(h = 1:12, MSE = mse) %>%\n  ggplot(aes(x = h, y = MSE)) + geom_point()+geom_line()+ggtitle(\"1 Step Ahead Cross Validation\")\n\n\n\n\n\n\n\n\nThe one-step-ahead forecasting plot shows that the MSE seems to be stable. The MSE is tend to rise by 12 steps at each step of cross-validation. Lastly, the above-mentioned ARIMA model should be compared to benchmark methods before being used for a long time.\n\n\nModel Comparison\n\nPlotModel Error Table\n\n\n\n\nCode\nfit <- Arima(myts, order=c(1,1,0),seasonal = c(0,1,1),lambda = 12)\nautoplot(myts) +\n  autolayer(meanf(myts, h=36),\n            series=\"Mean\", PI=FALSE) +\n  autolayer(naive(myts, h=36),\n            series=\"Naïve\", PI=FALSE) +\n  autolayer(snaive(myts, h=36),\n            series=\"SNaïve\", PI=FALSE)+\n  autolayer(rwf(myts, h=36, drift=TRUE),\n            series=\"Drift\", PI=FALSE)+\n  autolayer(forecast(fit,36), \n            series=\"fit\",PI=FALSE) +\n  guides(colour=guide_legend(title=\"Forecast\"))\n\n\n\n\n\n\n\n\n\nCode\nsummary <- summary(fit)\nsnaive <- snaive(myts, h=36)\n#accuracy(snaive)\n\n\n\n\n\nError\nModel\nSnavie\n\n\n\n\nME\n0.01857939\n0.04380615\n\n\nRMSE\n0.1821734\n0.584916\n\n\nMAE\n0.1288216\n0.4637065\n\n\nMPE\n8.065525\n54.48896\n\n\nMAPE\n101.6013\n232.775\n\n\nMASE\n0.2778084\n1.0000000\n\n\nACF1\n-0.01083644\n0.9050999\n\n\n\n\n\n\nThe ARIMA forecast doesn’t track the actual data points very closely. The other forecast methods are less accurate when compared to other model.The trend is visible in the fitted model, which indicates it is a good fit. From the table, Model error measurements of fit are much lower than snaive method.We can conclude that the fitted model is good."
  },
  {
    "objectID": "arima_macroeconomic_unemployment.html",
    "href": "arima_macroeconomic_unemployment.html",
    "title": "ARIMA Model for Unemployment Rate",
    "section": "",
    "text": "During exploratory data analysis (EDA), it was observed that the raw data is often non-stationary, exhibiting trends, seasonality, and other complex patterns that can make modeling and forecasting difficult. To address this issue, the data is often differentiated to achieve stationarity, which is a key assumption of the ARIMA model.\nDifferencing involves calculating the difference between each data point and the previous data point, which can help to remove trends and seasonality from the data. The goal is to create a new series of data that is stationary, meaning that the statistical properties of the data do not change over time.\n\nDifferentiated Time Series Plot\n\nPlotACFPACFADF Test\n\n\n\n\nCode\n#import the data\ndf <- read.csv(\"DATA/CLEANED DATA/unemployment_rate_clean_data.csv\")\n#convert the data to ts data\nmyts<-ts(df$Value,frequency=12,start=c(2010/1/1))\n#First order differentiation\ndf1 <- diff(myts)\n# Plot \nmyts  %>% diff() %>% ggtsdisplay(main = \"First order differentiation\") \n\n\n\n\n\n\n\n\n\nCode\n#ACF plot \nggAcf(df1,36,main=\"ACF Plot: First order differentiation\") \n\n\n\n\n\n\n\n\n\nCode\nggPacf(df1,36,main=\"PACF Plot: First order differentiation\") \n\n\n\n\n\n\n\n\n\nCode\ntseries::adf.test(df1)\n\n\n\n    Augmented Dickey-Fuller Test\n\ndata:  df1\nDickey-Fuller = -6.457, Lag order = 5, p-value = 0.01\nalternative hypothesis: stationary\n\n\n\n\n\nAfter analyzing the ACF and PACF plots, it can be observed that most of the bar lines lie between the blue lines, indicating that the time series is stationary. This has been further confirmed through an Augmented Dickey-Fuller test as the p-value is less than 0.05. Once the data is stationary, the next step is to model it using ARIMA. The combination of ACF and PACF plots can help determine the appropriate values for p, d, and q in the ARIMA model. The order of differencing required to achieve stationarity gives the d value. The p value is determined by examining significant spikes at various lags in the PACF plot, while the q value is determined through significant spikes in the ACF plot. It is important to consider other methods such as grid search and information criteria to ensure the most appropriate model is selected.\nHere the parameters are d = 1 p = 0 (PACF Plot) q = 0 (ACF Plot)\n\n\nModel Selection\n\nARIMA ResultAuto Arima Model\n\n\n\n\nCode\nmodel <- arima(myts,c(0,1,0))\nmodel\n\n\n\nCall:\narima(x = myts, order = c(0, 1, 0))\n\n\nsigma^2 estimated as 0.7845:  log likelihood = -203.72,  aic = 409.43\n\n\n\n\n\n\nCode\nauto.arima(myts)\n\n\nSeries: myts \nARIMA(0,1,0) \n\nsigma^2 = 0.7845:  log likelihood = -203.72\nAIC=409.43   AICc=409.46   BIC=412.49\n\n\n\n\n\nThe given table provides different ARIMA models with their corresponding AIC and BIC values. By sorting the table according to each criterion, we can identify the models with the lowest values. The smallest AIC value and BIC value corresponds to an ARIMA (0,1,0) model. Additionally, the auto.arima function in R suggests an ARIMA (0,1,0) model as the best fit for the data. Since the model parameter are the same, we can proceed with model diagnostic for the parameters.\n\n\nModel Diagnostic\n\nModel PlotModel 1Residual\n\n\n\n\nCode\nmodel_output <- capture.output(sarima(myts, 1,0,1))\n\n\n\n\n\n\n\n\n\nCode\ncat(model_output[62:93], model_output[length(model_output)], sep = \"\\n\") \n\n\n$fit\n\nCall:\narima(x = xdata, order = c(p, d, q), seasonal = list(order = c(P, D, Q), period = S), \n    xreg = xmean, include.mean = FALSE, transform.pars = trans, fixed = fixed, \n    optim.control = list(trace = trc, REPORT = 1, reltol = tol))\n\nCoefficients:\n         ar1     ma1   xmean\n      0.9156  0.0771  6.1460\ns.e.  0.0355  0.0950  0.8277\n\nsigma^2 estimated as 0.7549:  log likelihood = -202.96,  aic = 413.93\n\n$degrees_of_freedom\n[1] 155\n\n$ttable\n      Estimate     SE t.value p.value\nar1     0.9156 0.0355 25.7721  0.0000\nma1     0.0771 0.0950  0.8118  0.4182\nxmean   6.1460 0.8277  7.4254  0.0000\n\n$AIC\n[1] 2.619783\n\n$AICc\n[1] 2.620769\n\n$BIC\n[1] 2.697317\n\n\n\n\n\n\nCode\narima <- auto.arima(myts)\ncheckresiduals(arima)\n\n\n\n\n\n\n    Ljung-Box test\n\ndata:  Residuals from ARIMA(0,1,0)\nQ* = 7.2289, df = 24, p-value = 0.9996\n\nModel df: 0.   Total lags used: 24\n\n\n\n\n\nThe best model is ARIMA(1,0,1). By analyzing the standardized residuals plot, it can be observed that the mean is close to 0 and the variance is slightly higher than 1. Deviations from these values could indicate poor model fit. However, in this case, the model seems to be well-fitted. The ACF plot of residuals shows no significant lags, which is a positive sign for the model. The qq-plot also suggests high normality in the residuals. Additionally, the p-values for the Ljung-Box test are near than 0, indicating that the residuals are independent, which is favorable for the model’s accuracy.\nThe equation for this model: \\[\\Delta Y_t = \\phi_1 \\Delta Y_{t-1} + \\theta_1 \\epsilon_{t-1} + \\epsilon_t\\]\n\n\nForcast\n\nShort Term ForcastLong term Forcast\n\n\n\n\nCode\nmyts %>%\n  Arima(order=c(1,0,1),include.drift = TRUE) %>%\n  forecast %>%\n  autoplot(main = \"Unemployment Rate Prediction\") +\n  ylab(\"Unemployment Rate\") + xlab(\"Year\")\n\n\n\n\n\n\n\n\n\nCode\nsarima.for(myts,36, 1,0,1, main = \"Unemployment Rate Prediction\")\n\n\n\n\n\n$pred\n          Jan      Feb      Mar      Apr      May      Jun      Jul      Aug\n2023                   3.814151 4.010889 4.191028 4.355968 4.506993 4.645275\n2024 5.180167 5.261654 5.336266 5.404583 5.467136 5.524412 5.576855 5.624873\n2025 5.810614 5.838911 5.864819 5.888542 5.910264 5.930153 5.948364 5.965038\n2026 6.029536 6.039362                                                      \n          Sep      Oct      Nov      Dec\n2023 4.771891 4.887824 4.993976 5.091172\n2024 5.668841 5.709098 5.745959 5.779711\n2025 5.980306 5.994285 6.007085 6.018805\n2026                                    \n\n$se\n           Jan       Feb       Mar       Apr       May       Jun       Jul\n2023                     0.8688527 1.2243105 1.4569544 1.6265546 1.7561668\n2024 2.1374499 2.1671027 2.1916538 2.2120269 2.2289639 2.2430649 2.2548189\n2025 2.2941710 2.2975230 2.3003295 2.3026798 2.3046483 2.3062974 2.3076791\n2026 2.3123513 2.3127525                                                  \n           Aug       Sep       Oct       Nov       Dec\n2023 1.8578756 1.9390393 2.0045542 2.0578738 2.1015336\n2024 2.2646262 2.2728158 2.2796591 2.2853807 2.2901664\n2025 2.3088368 2.3098070 2.3106200 2.3113014 2.3118726\n2026                                                  \n\n\n\n\n\nThe two figures above show an idea of the short term and the long-term forecasted unemployment rate. The forecasted values is be based on the estimated parameters of the model, which were obtained by fitting it to the historical data. One way to assess the accuracy of the forecasts is to use cross-validation, which involves splitting the data into training and testing sets and comparing the predicted values to the actual values in the test set. This can help to identify any potential problems with the model and to fine-tune the model parameters.\n\n12 Step ahead Cross Validation1 Step ahead Cross Validation\n\n\n\n\nCode\n#a seasonal cross validation using 12 steps ahead forecasts\nfarima1 <- function(x, h){forecast(Arima(x, order=c(1,0,1)), h=h)}\n\n# Compute cross-validated errors for up to 12 steps ahead\ne <- tsCV(myts, forecastfunction = farima1, h = 12)\n\n# Compute the MSE values and remove missing values\nmse <- colMeans(e^2, na.rm = TRUE)\n\n# Plot the MSE values against the forecast horizon\ndata.frame(h = 1:12, MSE = mse) %>%\n  ggplot(aes(x = h, y = MSE)) + geom_point()+geom_line()+ggtitle(\"12 Step Ahead Cross Validation\")\n\n\n\n\n\n\n\n\n\nCode\n#a seasonal cross validation using 1 steps ahead forecasts\nfarima1 <- function(x, h){forecast(Arima(x, order=c(1,0,1)),h=h)}\n\n# Compute cross-validated errors for up to 1 steps ahead\ne <- tsCV(myts, forecastfunction = farima1, h = 1)\nmse <-abs(mean(e,na.rm=TRUE))\n\n# Plot the MSE values against the forecast horizon\ndata.frame(h = 1:12, MSE = mse) %>%\n  ggplot(aes(x = h, y = MSE)) + geom_point()+geom_line()+ggtitle(\"1 Step Ahead Cross Validation\")\n\n\n\n\n\n\n\n\nThe one-step-ahead forecasting plot shows that the MSE seems to be stable. The MSE is tend to rise by 12 steps at each step of cross-validation. Lastly, the above-mentioned ARIMA model should be compared to benchmark methods before being used for a long time.\n\n\nModel Comparison\n\nPlotModel Error Table\n\n\n\n\nCode\nfit <- Arima(myts, order=c(1,0,1))\nautoplot(myts) +\n  autolayer(meanf(myts, h=36),\n            series=\"Mean\", PI=FALSE) +\n  autolayer(naive(myts, h=36),\n            series=\"Naïve\", PI=FALSE) +\n  autolayer(snaive(myts, h=36),\n            series=\"SNaïve\", PI=FALSE)+\n  autolayer(rwf(myts, h=36, drift=TRUE),\n            series=\"Drift\", PI=FALSE)+\n  autolayer(forecast(fit,36), \n            series=\"fit\",PI=FALSE) +\n  guides(colour=guide_legend(title=\"Forecast\"))\n\n\n\n\n\n\n\n\n\nCode\nsummary <- summary(fit)\nsnaive <- snaive(myts, h=36)\n#accuracy(snaive)\n\n\n\n\n\nError\nModel\nSnavie\n\n\n\n\nME\n-0.03281026\n0.4958904\n\n\nRMSE\n0.8688527\n2.221162\n\n\nMAE\n0.2861976\n1.330137\n\n\nMPE\n-1.977255\n-13.26146\n\n\nMAPE\n4.412742\n22.30239\n\n\nMASE\n0.215164\n1.0000000\n\n\nACF1\n-0.0139776\n0.819433\n\n\n\n\n\n\nThe benchmark models are all close to each other and showing different trend. From the table, Model error measurements of fit are much lower than snaive method which indicates that it is better fit then snaive method, but further analysis should be done to check on the prediction."
  },
  {
    "objectID": "arima_sector_communication_services.html",
    "href": "arima_sector_communication_services.html",
    "title": "ARIMA Model for Communication Services Sector Fund",
    "section": "",
    "text": "During exploratory data analysis (EDA), it was observed that the raw data is often non-stationary, exhibiting trends, seasonality, and other complex patterns that can make modeling and forecasting difficult. To address this issue, the data is often differentiated to achieve stationarity, which is a key assumption of the ARIMA model.\nDifferencing involves calculating the difference between each data point and the previous data point, which can help to remove trends and seasonality from the data. The goal is to create a new series of data that is stationary, meaning that the statistical properties of the data do not change over time.\n\nDifferentiated Time Series Plot\n::: panel-tabset ##### Plot\n\n\nCode\n#import the data\ndf <- read.csv(\"DATA/CLEANED DATA/XLC_raw_data.csv\")\n#convert to time series data\nmyts<-ts(df$XLC.Adjusted,frequency=252,start=c(2018,6,19), end = c(2023,3,1)) \n#First order differentiation\ndf1 <- diff(myts)\n# Plot \nmyts  %>% diff() %>% ggtsdisplay(main = \"First order differentiation\") \n\n\n\n\n\n\nACFPACFADF Test\n\n\n\n\nCode\n#ACF plot \nggAcf(df1,60,main=\"ACF Plot: First order differentiation\") \n\n\n\n\n\n\n\n\n\nCode\nggPacf(df1,60,main=\"PACF Plot: First order differentiation\") \n\n\n\n\n\n\n\n\n\nCode\ntseries::adf.test(df1)\n\n\n\n    Augmented Dickey-Fuller Test\n\ndata:  df1\nDickey-Fuller = -10.487, Lag order = 10, p-value = 0.01\nalternative hypothesis: stationary\n\n\n\n\n\nAfter analyzing the ACF and PACF plots, it can be observed that most of the bar lines lie between the blue lines, indicating that the time series is stationary. This has been further confirmed through an Augmented Dickey-Fuller test as the p-value is less than 0.05. Once the data is stationary, the next step is to model it using ARIMA. The combination of ACF and PACF plots can help determine the appropriate values for p, d, and q in the ARIMA model. The order of differencing required to achieve stationarity gives the d value. The p value is determined by examining significant spikes at various lags in the PACF plot, while the q value is determined through significant spikes in the ACF plot. It is important to consider other methods such as grid search and information criteria to ensure the most appropriate model is selected.\nHere the parameters are d = 1 p = 0,1,2 (PACF Plot) q = 0,1,2 (ACF Plot)\n\n\nModel Selection\n\nARIMA ResultAuto Arima Model\n\n\n\n\nCode\n######################## Check for different combinations ########\nd=1\ni=1\ntemp = data.frame()\nls=matrix(rep(NA,6*9),nrow=9) #nrow = 3x3x1\n\n\nfor (p in 1:3)# p=0,1,2 :3\n{\n  for(q in 1:3)# q=0,1,2 :3\n  {\n    for(d in 1)# d=1 :1\n    {\n      \n      if(p-1+d+q-1<=8)\n      {\n        \n        model<- Arima(myts,order=c(p-1,d,q-1),include.drift=TRUE) \n        ls[i,]= c(p-1,d,q-1,model$aic,model$bic,model$aicc)\n        i=i+1\n        \n      }\n      \n    }\n  }\n}\n\nmodel = as.data.frame(ls)\nnames(model)= c(\"p\",\"d\",\"q\",\"AIC\",\"BIC\",\"AICc\")\n\n#temp\nknitr::kable(model)\n\n\n\n\n\np\nd\nq\nAIC\nBIC\nAICc\n\n\n\n\n0\n1\n0\n3187.981\n3198.254\n3187.991\n\n\n0\n1\n1\n3180.249\n3195.659\n3180.268\n\n\n0\n1\n2\n3180.520\n3201.066\n3180.552\n\n\n1\n1\n0\n3179.556\n3194.965\n3179.575\n\n\n1\n1\n1\n3179.750\n3200.295\n3179.781\n\n\n1\n1\n2\n3181.616\n3207.299\n3181.664\n\n\n2\n1\n0\n3180.036\n3200.582\n3180.068\n\n\n2\n1\n1\n3181.642\n3207.325\n3181.690\n\n\n2\n1\n2\n3183.141\n3213.960\n3183.209\n\n\n\n\n\n\n\n\n\nCode\nauto.arima(myts)\n\n\nSeries: myts \nARIMA(1,1,0) \n\nCoefficients:\n          ar1\n      -0.0909\ns.e.   0.0281\n\nsigma^2 = 0.7317:  log likelihood = -1586.78\nAIC=3177.56   AICc=3177.57   BIC=3187.83\n\n\n\n\n\nThe given table provides different ARIMA models with their corresponding AIC and BIC values. By sorting the table according to each criterion, we can identify the models with the lowest values. The smallest AIC and BIC value corresponds to an ARIMA (2,1,2) mode. Additionally, the auto.arima function in R suggests an ARIMA (2,1,2) model as the best fit for the data. We can proceed with the further model diagnostic for the chosen best model i.e. (2,1,2), to look into the details of the model.\n\n\nModel Diagnostic\n\nModelModel\n\n\n\n\nCode\nmodel_output <- capture.output(sarima(myts,2,1,2))\n\n\n\n\n\n\n\n\n\nCode\ncat(model_output[201:234], model_output[length(model_output)], sep = \"\\n\") \n\n\nNA\nNA\nNA\nNA\nNA\nNA\nNA\nNA\nNA\nNA\nNA\nNA\nNA\nNA\nNA\nNA\nNA\nNA\nNA\nNA\nNA\nNA\nNA\nNA\nNA\nNA\nNA\nNA\nNA\nNA\nNA\nNA\nNA\nNA\n\n\n\n\n\nModel diagnostic is done for ARIMA(2,1,2). By analyzing the standardized residuals plot, it can be observed that the mean is close to 0 and the variance is slightly higher than 1. Deviations from these values could indicate poor model fit. However, in this case, the model seems to be well-fitted. The ACF plot of residuals shows very few significant lags, which is a positive sign for the model. The qq-plot also suggests normality in the residuals. Additionally, the p-values for the Ljung-Box test are greater than 0.05, indicating that the residuals are independent, which is favorable for the model’s accuracy.\nThe equation for this model: \\[(1-\\phi_1B-\\phi_2B^2)(1-B)(Yt - \\mu) = (1+\\theta_1B+\\theta_2B^2)\\epsilon_t\\]\n\n\nForcast\n\nShort Term ForcastLong term Forcast\n\n\n\n\nCode\nmyts %>%\n  Arima(order=c(2,1,2),include.drift = TRUE) %>%\n  forecast %>%\n  autoplot(main = \"Communication Services Sector Fund Stock Prices Prediction\") +\n  ylab(\"stock prices\") + xlab(\"Year\")\n\n\n\n\n\n\n\n\n\nCode\nsarima.for(myts,182, 2,1,2, main='Communication Services Sector Fund Stock Prices Prediction')\n\n\n\n\n\n$pred\nTime Series:\nStart = c(2023, 4) \nEnd = c(2023, 185) \nFrequency = 252 \n  [1] 46.45071 46.42804 46.44383 46.44007 46.44404 46.44344 46.44422 46.44381\n  [9] 46.44353 46.44287 46.44216 46.44130 46.44037 46.43937 46.43834 46.43726\n [17] 46.43616 46.43504 46.43391 46.43277 46.43161 46.43045 46.42929 46.42812\n [25] 46.42695 46.42578 46.42461 46.42344 46.42226 46.42109 46.41991 46.41873\n [33] 46.41756 46.41638 46.41520 46.41403 46.41285 46.41167 46.41050 46.40932\n [41] 46.40814 46.40696 46.40579 46.40461 46.40343 46.40226 46.40108 46.39990\n [49] 46.39872 46.39755 46.39637 46.39519 46.39401 46.39284 46.39166 46.39048\n [57] 46.38931 46.38813 46.38695 46.38577 46.38460 46.38342 46.38224 46.38107\n [65] 46.37989 46.37871 46.37753 46.37636 46.37518 46.37400 46.37282 46.37165\n [73] 46.37047 46.36929 46.36812 46.36694 46.36576 46.36458 46.36341 46.36223\n [81] 46.36105 46.35987 46.35870 46.35752 46.35634 46.35517 46.35399 46.35281\n [89] 46.35163 46.35046 46.34928 46.34810 46.34692 46.34575 46.34457 46.34339\n [97] 46.34222 46.34104 46.33986 46.33868 46.33751 46.33633 46.33515 46.33397\n[105] 46.33280 46.33162 46.33044 46.32927 46.32809 46.32691 46.32573 46.32456\n[113] 46.32338 46.32220 46.32102 46.31985 46.31867 46.31749 46.31632 46.31514\n[121] 46.31396 46.31278 46.31161 46.31043 46.30925 46.30808 46.30690 46.30572\n[129] 46.30454 46.30337 46.30219 46.30101 46.29983 46.29866 46.29748 46.29630\n[137] 46.29513 46.29395 46.29277 46.29159 46.29042 46.28924 46.28806 46.28688\n[145] 46.28571 46.28453 46.28335 46.28218 46.28100 46.27982 46.27864 46.27747\n[153] 46.27629 46.27511 46.27393 46.27276 46.27158 46.27040 46.26923 46.26805\n[161] 46.26687 46.26569 46.26452 46.26334 46.26216 46.26098 46.25981 46.25863\n[169] 46.25745 46.25628 46.25510 46.25392 46.25274 46.25157 46.25039 46.24921\n[177] 46.24804 46.24686 46.24568 46.24450 46.24333 46.24215\n\n$se\nTime Series:\nStart = c(2023, 4) \nEnd = c(2023, 185) \nFrequency = 252 \n  [1]  0.8542337  1.1524548  1.3997523  1.5987426  1.7759709  1.9341177\n  [7]  2.0796142  2.2145163  2.3411693  2.4608502  2.5746931  2.6834864\n [13]  2.7878856  2.8883993  2.9854499  3.0793836  3.1704922  3.2590232\n [19]  3.3451889  3.4291732  3.5111369  3.5912213  3.6695517  3.7462397\n [25]  3.8213855  3.8950791  3.9674021  4.0384287  4.1082265  4.1768573\n [31]  4.2443779  4.3108406  4.3762937  4.4407820  4.5043470  4.5670272\n [37]  4.6288586  4.6898748  4.7501073  4.8095855  4.8683370  4.9263879\n [43]  4.9837627  5.0404844  5.0965748  5.1520546  5.2069434  5.2612595\n [49]  5.3150205  5.3682432  5.4209434  5.4731362  5.5248359  5.5760563\n [55]  5.6268104  5.6771108  5.7269695  5.7763978  5.8254067  5.8740068\n [61]  5.9222080  5.9700201  6.0174522  6.0645135  6.1112123  6.1575569\n [67]  6.2035554  6.2492153  6.2945439  6.3395485  6.3842359  6.4286126\n [73]  6.4726850  6.5164594  6.5599417  6.6031377  6.6460529  6.6886928\n [79]  6.7310626  6.7731673  6.8150119  6.8566012  6.8979396  6.9390319\n [85]  6.9798822  7.0204948  7.0608738  7.1010233  7.1409470  7.1806487\n [91]  7.2201321  7.2594008  7.2984581  7.3373076  7.3759525  7.4143960\n [97]  7.4526411  7.4906910  7.5285486  7.5662167  7.6036983  7.6409960\n[103]  7.6781125  7.7150504  7.7518124  7.7884008  7.8248182  7.8610668\n[109]  7.8971491  7.9330672  7.9688235  8.0044200  8.0398589  8.0751423\n[115]  8.1102722  8.1452506  8.1800794  8.2147606  8.2492959  8.2836873\n[121]  8.3179365  8.3520452  8.3860152  8.4198482  8.4535457  8.4871095\n[127]  8.5205410  8.5538419  8.5870136  8.6200577  8.6529756  8.6857688\n[133]  8.7184386  8.7509864  8.7834136  8.8157216  8.8479116  8.8799849\n[139]  8.9119427  8.9437864  8.9755171  9.0071360  9.0386443  9.0700432\n[145]  9.1013337  9.1325170  9.1635942  9.1945664  9.2254346  9.2561998\n[151]  9.2868631  9.3174256  9.3478881  9.3782516  9.4085172  9.4386857\n[157]  9.4687580  9.4987352  9.5286181  9.5584076  9.5881045  9.6177097\n[163]  9.6472240  9.6766483  9.7059835  9.7352302  9.7643893  9.7934616\n[169]  9.8224479  9.8513489  9.8801653  9.9088980  9.9375475  9.9661147\n[175]  9.9946003 10.0230049 10.0513292 10.0795739 10.1077397 10.1358273\n[181] 10.1638372 10.1917702\n\n\n\n\n\nThe two figures above show an idea of the short term and the long-term forecasted stock price. The forecasted values is be based on the estimated parameters of the model, which were obtained by fitting it to the historical data. One way to assess the accuracy of the forecasts is to use cross-validation, which involves splitting the data into training and testing sets and comparing the predicted values to the actual values in the test set. This can help to identify any potential problems with the model and to fine-tune the model parameters.\n\n12 Step ahead Cross Validation1 Step ahead Cross Validation\n\n\n\n\nCode\n#a seasonal cross validation using 12 steps ahead forecasts\nfarima1 <- function(x, h){forecast(Arima(x, order=c(2,1,2)), h=h)}\n\n# Compute cross-validated errors for up to 12 steps ahead\ne <- tsCV(myts, forecastfunction = farima1, h = 12)\n\n# Compute the MSE values and remove missing values\nmse <- colMeans(e^2, na.rm = TRUE)\n\n# Plot the MSE values against the forecast horizon\ndata.frame(h = 1:12, MSE = mse) %>%\n  ggplot(aes(x = h, y = MSE)) + geom_point()+geom_line()+ggtitle(\"12 Step Ahead Cross Validation\")\n\n\n\n\n\n\n\n\n\nCode\n#a seasonal cross validation using 1 steps ahead forecasts\nfarima1 <- function(x, h){forecast(Arima(x, order=c(2,1,2)),h=h)}\n\n# Compute cross-validated errors for up to 1 steps ahead\ne <- tsCV(myts, forecastfunction = farima1, h = 1)\nmse <-abs(mean(e,na.rm=TRUE))\n\n# Plot the MSE values against the forecast horizon\ndata.frame(h = 1:12, MSE = mse) %>%\n  ggplot(aes(x = h, y = MSE)) + geom_point()+geom_line()+ggtitle(\"1 Step Ahead Cross Validation\")\n\n\n\n\n\n\n\n\nThe one-step-ahead forecasting plot shows that the MSE seems to be stable. The MSE is tend to rise by 12 steps at each step of cross-validation. Lastly, the above-mentioned ARIMA model should be compared to benchmark methods before being used for a long time.\n\n\nModel Comparison\n\nPlotModel Error Table\n\n\n\n\nCode\nfit <- Arima(myts, order=c(2,1,2))\nautoplot(myts) +\n  autolayer(meanf(myts, h=182),\n            series=\"Mean\", PI=FALSE) +\n  autolayer(naive(myts, h=182),\n            series=\"Naïve\", PI=FALSE) +\n  autolayer(snaive(myts, h=182),\n            series=\"SNaïve\", PI=FALSE)+\n  autolayer(rwf(myts, h=182, drift=TRUE),\n            series=\"Drift\", PI=FALSE)+\n  autolayer(forecast(fit,182), \n            series=\"fit\",PI=FALSE) +\n  guides(colour=guide_legend(title=\"Forecast\"))\n\n\n\n\n\n\n\n\n\nCode\nsummary <- summary(fit)\nsnaive <- snaive(myts, h=182)\naccuracy(snaive)\n\n\n                   ME     RMSE    MAE       MPE    MAPE MASE      ACF1\nTraining set 1.365266 17.45982 14.785 -1.229349 24.7035    1 0.9972269\n\n\nCode\nsummary\n\n\nSeries: myts \nARIMA(2,1,2) \n\nCoefficients:\n         ar1     ar2      ma1      ma2\n      0.2866  0.3470  -0.3810  -0.2955\ns.e.  0.3425  0.1725   0.3425   0.1641\n\nsigma^2 = 0.732:  log likelihood = -1585.57\nAIC=3181.14   AICc=3181.19   BIC=3206.83\n\nTraining set error measures:\n                      ME      RMSE       MAE         MPE     MAPE       MASE\nTraining set -0.00123788 0.8538962 0.6031582 -0.01655991 1.087715 0.04079528\n                    ACF1\nTraining set 0.006462849\n\n\n\n\n\nError\nModel\nSnavie\n\n\n\n\nME\n0.01010937\n1.761967\n\n\nRMSE\n0.8192804\n18.67951\n\n\nMAE\n0.5816893\n15.71119\n\n\nMPE\n0.005100481\n-1.616813\n\n\nMAPE\n1.084491\n27.61046\n\n\nMASE\n0.03702389\n1.0000000\n\n\nACF1\n-0.001399621\n0.9976193\n\n\n\n\n\n\nThe ARIMA forecast tracks the actual data points very closely when compared to other model. The other forecast methods are less accurate when compared. From the table, Model error measurements of fit are much lower than snaive method.We can conclude that the fitted model is good."
  },
  {
    "objectID": "arima_sector_consumer_discretionary.html",
    "href": "arima_sector_consumer_discretionary.html",
    "title": "ARIMA Model for Consumer Discretionary Sector Fund",
    "section": "",
    "text": "During exploratory data analysis (EDA), it was observed that the raw data is often non-stationary, exhibiting trends, seasonality, and other complex patterns that can make modeling and forecasting difficult. To address this issue, the data is often differentiated to achieve stationarity, which is a key assumption of the ARIMA model.\nDifferencing involves calculating the difference between each data point and the previous data point, which can help to remove trends and seasonality from the data. The goal is to create a new series of data that is stationary, meaning that the statistical properties of the data do not change over time.\n\nDifferentiated Time Series Plot\n\nPlotACFPACFADF Test\n\n\n\n\nCode\n#import the data\ndf <- read.csv(\"DATA/CLEANED DATA/XLY_raw_data.csv\")\n#convert to time series data\nmyts<-ts(df$XLY.Adjusted,frequency=252,start=c(2010,1,1), end = c(2023,3,1)) \n#First order differentiation\ndf1 <- diff(myts)\n# Plot \nmyts  %>% diff() %>% ggtsdisplay(main = \"First order differentiation\") \n\n\n\n\n\n\n\n\n\nCode\n#ACF plot \nggAcf(df1,60,main=\"ACF Plot: First order differentiation\") \n\n\n\n\n\n\n\n\n\nCode\nggPacf(df1,60,main=\"PACF Plot: First order differentiation\") \n\n\n\n\n\n\n\n\n\nCode\ntseries::adf.test(df1)\n\n\n\n    Augmented Dickey-Fuller Test\n\ndata:  df1\nDickey-Fuller = -14.88, Lag order = 14, p-value = 0.01\nalternative hypothesis: stationary\n\n\n\n\n\nAfter analyzing the ACF and PACF plots, it can be observed that most of the bar lines lie between the blue lines, indicating that the time series is stationary. This has been further confirmed through an Augmented Dickey-Fuller test as the p-value is less than 0.05. Once the data is stationary, the next step is to model it using ARIMA. The combination of ACF and PACF plots can help determine the appropriate values for p, d, and q in the ARIMA model. The order of differencing required to achieve stationarity gives the d value. The p value is determined by examining significant spikes at various lags in the PACF plot, while the q value is determined through significant spikes in the ACF plot. It is important to consider other methods such as grid search and information criteria to ensure the most appropriate model is selected.\nHere the parameters are d = 1 p = 0,10 (PACF Plot) q = 0,10 (ACF Plot)\n\n\nModel Selection\n\nARIMA(0,1,0) ResultARIMA(10,1,0) ResultARIMA(0,1,10) ResultARIMA(10,1,10) ResultAuto Arima Model\n\n\n\n\nCode\nArima(myts,order=c(0,1,0),include.drift=TRUE) \n\n\nSeries: myts \nARIMA(0,1,0) with drift \n\nCoefficients:\n       drift\n      0.0343\ns.e.  0.0245\n\nsigma^2 = 1.97:  log likelihood = -5761.98\nAIC=11527.97   AICc=11527.97   BIC=11540.16\n\n\n\n\n\n\nCode\nArima(myts,order=c(10,1,0),include.drift=TRUE) \n\n\nSeries: myts \nARIMA(10,1,0) with drift \n\nCoefficients:\n          ar1     ar2      ar3      ar4     ar5      ar6     ar7      ar8\n      -0.0082  0.0387  -0.0221  -0.0175  0.0059  -0.0478  0.0702  -0.0725\ns.e.   0.0175  0.0174   0.0174   0.0173  0.0173   0.0173  0.0173   0.0174\n         ar9     ar10   drift\n      0.0895  -0.0419  0.0343\ns.e.  0.0174   0.0175  0.0240\n\nsigma^2 = 1.917:  log likelihood = -5712.53\nAIC=11449.06   AICc=11449.16   BIC=11522.2\n\n\n\n\n\n\nCode\nArima(myts,order=c(0,1,10),include.drift=TRUE) \n\n\nSeries: myts \nARIMA(0,1,10) with drift \n\nCoefficients:\n          ma1     ma2      ma3      ma4     ma5      ma6     ma7      ma8\n      -0.0100  0.0346  -0.0145  -0.0274  0.0106  -0.0478  0.0692  -0.0614\ns.e.   0.0175  0.0175   0.0173   0.0173  0.0176   0.0180  0.0176   0.0177\n         ma9     ma10   drift\n      0.0789  -0.0362  0.0343\ns.e.  0.0167   0.0184  0.0241\n\nsigma^2 = 1.926:  log likelihood = -5720.2\nAIC=11464.39   AICc=11464.49   BIC=11537.53\n\n\n\n\n\n\nCode\nArima(myts,order=c(10,1,10),include.drift=TRUE) \n\n\nSeries: myts \nARIMA(10,1,10) with drift \n\nCoefficients:\n          ar1     ar2      ar3     ar4     ar5      ar6      ar7      ar8\n      -0.3161  0.3045  -0.4546  -0.177  0.9096  -0.2794  -0.7284  -0.0868\ns.e.      NaN     NaN   0.1251     NaN  0.0291   0.0516   0.0280      NaN\n          ar9     ar10     ma1      ma2     ma3     ma4      ma5     ma6\n      -0.2641  -0.4473  0.2977  -0.2779  0.4461  0.1271  -0.8973  0.2617\ns.e.      NaN   0.1207     NaN      NaN  0.1209     NaN   0.0378  0.0486\n         ma7     ma8     ma9    ma10   drift\n      0.7258  0.0718  0.3319  0.4081  0.0341\ns.e.     NaN     NaN     NaN  0.1193  0.0235\n\nsigma^2 = 1.883:  log likelihood = -5681.36\nAIC=11406.72   AICc=11407.04   BIC=11540.81\n\n\n\n\n\n\nCode\nauto.arima(myts)\n\n\nSeries: myts \nARIMA(2,1,0) \n\nCoefficients:\n          ar1     ar2\n      -0.0305  0.0546\ns.e.   0.0175  0.0175\n\nsigma^2 = 1.964:  log likelihood = -5756.36\nAIC=11518.72   AICc=11518.73   BIC=11537.01\n\n\n\n\n\nIn the Model selection, when comparing the AIC, AICc and BIC values of all the 4 models, model (0,1,0) has the least BIC and AIc value. The model generated by auto.arima is also (0,1,0). We can proceed with the further model diagnostic for the chosen best model i.e. (0,1,0), to look into the details of the model.\n\n\nModel Diagnostic\n\nModel PlotModel\n\n\n\n\nCode\nmodel_output <- capture.output(sarima(myts,0,1,0))\n\n\n\n\n\n\n\n\n\nCode\ncat(model_output[9:38], model_output[length(model_output)], sep = \"\\n\") \n\n\n$fit\n\nCall:\narima(x = xdata, order = c(p, d, q), seasonal = list(order = c(P, D, Q), period = S), \n    xreg = constant, transform.pars = trans, fixed = fixed, optim.control = list(trace = trc, \n        REPORT = 1, reltol = tol))\n\nCoefficients:\n      constant\n        0.0343\ns.e.    0.0245\n\nsigma^2 estimated as 1.969:  log likelihood = -5761.98,  aic = 11527.97\n\n$degrees_of_freedom\n[1] 3277\n\n$ttable\n         Estimate     SE t.value p.value\nconstant   0.0343 0.0245  1.4005  0.1615\n\n$AIC\n[1] 3.516768\n\n$AICc\n[1] 3.516769\n\n$BIC\n[1] 3.520487\n\n\n\n\n\nThe model diagnostic is done for ARIMA(0,1,0). By analyzing the standardized residuals plot, it can be observed that the mean is close to 0 and the variance is slightly higher than 1. Deviations from these values could indicate poor model fit. However, in this case, the model seems to be well-fitted. The ACF plot of residuals shows very few significant lags, which is a positive sign for the model. The qq-plot also suggests normality in the residuals. Additionally, the p-values for the Ljung-Box test are greater than 0.05, indicating that the residuals are independent, which is favorable for the model’s accuracy.\nThe equation for this model: \\[(1-\\phi_1B-\\phi_2B^2)(1-B)(Y_t-\\mu) = (1+\\theta_1B+\\theta_2B^2)\\epsilon_t\\]\n\n\nForcast\n\nShort Term ForcastLong term Forcast\n\n\n\n\nCode\nmyts %>%\n  Arima(order=c(0,1,0),include.drift = TRUE) %>%\n  forecast %>%\n  autoplot(main = \"Consumer Discretionary Sector Fund Stock Prices Prediction\") +\n  ylab(\"stock prices\") + xlab(\"Year\")\n\n\n\n\n\n\n\n\n\nCode\nsarima.for(myts,182, 0,1,0, main='Consumer Discretionary Sector Fund Stock Prices Prediction')\n\n\n\n\n\n$pred\nTime Series:\nStart = c(2023, 4) \nEnd = c(2023, 185) \nFrequency = 252 \n  [1] 137.7840 137.8183 137.8526 137.8870 137.9213 137.9556 137.9899 138.0243\n  [9] 138.0586 138.0929 138.1272 138.1616 138.1959 138.2302 138.2645 138.2989\n [17] 138.3332 138.3675 138.4019 138.4362 138.4705 138.5048 138.5392 138.5735\n [25] 138.6078 138.6421 138.6765 138.7108 138.7451 138.7794 138.8138 138.8481\n [33] 138.8824 138.9168 138.9511 138.9854 139.0197 139.0541 139.0884 139.1227\n [41] 139.1570 139.1914 139.2257 139.2600 139.2943 139.3287 139.3630 139.3973\n [49] 139.4317 139.4660 139.5003 139.5346 139.5690 139.6033 139.6376 139.6719\n [57] 139.7063 139.7406 139.7749 139.8092 139.8436 139.8779 139.9122 139.9465\n [65] 139.9809 140.0152 140.0495 140.0839 140.1182 140.1525 140.1868 140.2212\n [73] 140.2555 140.2898 140.3241 140.3585 140.3928 140.4271 140.4614 140.4958\n [81] 140.5301 140.5644 140.5988 140.6331 140.6674 140.7017 140.7361 140.7704\n [89] 140.8047 140.8390 140.8734 140.9077 140.9420 140.9763 141.0107 141.0450\n [97] 141.0793 141.1137 141.1480 141.1823 141.2166 141.2510 141.2853 141.3196\n[105] 141.3539 141.3883 141.4226 141.4569 141.4912 141.5256 141.5599 141.5942\n[113] 141.6286 141.6629 141.6972 141.7315 141.7659 141.8002 141.8345 141.8688\n[121] 141.9032 141.9375 141.9718 142.0061 142.0405 142.0748 142.1091 142.1434\n[129] 142.1778 142.2121 142.2464 142.2808 142.3151 142.3494 142.3837 142.4181\n[137] 142.4524 142.4867 142.5210 142.5554 142.5897 142.6240 142.6583 142.6927\n[145] 142.7270 142.7613 142.7957 142.8300 142.8643 142.8986 142.9330 142.9673\n[153] 143.0016 143.0359 143.0703 143.1046 143.1389 143.1732 143.2076 143.2419\n[161] 143.2762 143.3106 143.3449 143.3792 143.4135 143.4479 143.4822 143.5165\n[169] 143.5508 143.5852 143.6195 143.6538 143.6881 143.7225 143.7568 143.7911\n[177] 143.8255 143.8598 143.8941 143.9284 143.9628 143.9971\n\n$se\nTime Series:\nStart = c(2023, 4) \nEnd = c(2023, 185) \nFrequency = 252 \n  [1]  1.403312  1.984584  2.430608  2.806625  3.137902  3.437399  3.712816\n  [8]  3.969167  4.209937  4.437664  4.654261  4.861217  5.059715  5.250714\n [15]  5.435006  5.613250  5.786005  5.953751  6.116897  6.275804  6.430786\n [22]  6.582119  6.730050  6.874799  7.016562  7.155518  7.291825  7.425632\n [29]  7.557069  7.686259  7.813313  7.938334  8.061416  8.182647  8.302108\n [36]  8.419875  8.536016  8.650599  8.763683  8.875327  8.985584  9.094504\n [43]  9.202135  9.308522  9.413706  9.517728  9.620626  9.722434  9.823187\n [50]  9.922918 10.021655 10.119430 10.216269 10.312198 10.407244 10.501429\n [57] 10.594777 10.687309 10.779048 10.870012 10.960221 11.049693 11.138447\n [64] 11.226500 11.313867 11.400564 11.486608 11.572011 11.656789 11.740954\n [71] 11.824521 11.907501 11.989907 12.071750 12.153042 12.233794 12.314017\n [78] 12.393720 12.472914 12.551608 12.629812 12.707535 12.784785 12.861571\n [85] 12.937902 13.013784 13.089227 13.164238 13.238823 13.312991 13.386748\n [92] 13.460100 13.533055 13.605619 13.677798 13.749598 13.821025 13.892085\n [99] 13.962783 14.033125 14.103116 14.172761 14.242066 14.311035 14.379674\n[106] 14.447986 14.515977 14.583651 14.651012 14.718065 14.784814 14.851263\n[113] 14.917416 14.983277 15.048850 15.114138 15.179145 15.243875 15.308332\n[120] 15.372518 15.436437 15.500093 15.563488 15.626626 15.689510 15.752143\n[127] 15.814528 15.876668 15.938566 16.000224 16.061645 16.122833 16.183789\n[134] 16.244516 16.305017 16.365295 16.425351 16.485189 16.544810 16.604217\n[141] 16.663412 16.722398 16.781176 16.839750 16.898120 16.956289 17.014259\n[148] 17.072033 17.129612 17.186997 17.244192 17.301198 17.358016 17.414650\n[155] 17.471099 17.527367 17.583455 17.639364 17.695097 17.750655 17.806039\n[162] 17.861252 17.916294 17.971168 18.025875 18.080416 18.134794 18.189008\n[169] 18.243062 18.296956 18.350692 18.404270 18.457694 18.510963 18.564079\n[176] 18.617044 18.669858 18.722523 18.775041 18.827412 18.879638 18.931720\n\n\n\n\n\nThe two figures above show an idea of the short term and the long-term forecasted stock price. The forecasted values is be based on the estimated parameters of the model, which were obtained by fitting it to the historical data. One way to assess the accuracy of the forecasts is to use cross-validation, which involves splitting the data into training and testing sets and comparing the predicted values to the actual values in the test set. This can help to identify any potential problems with the model and to fine-tune the model parameters.\n\n12 Step ahead Cross Validation1 Step ahead Cross Validation\n\n\n\n\nCode\n#a seasonal cross validation using 12 steps ahead forecasts\nfarima1 <- function(x, h){forecast(Arima(x, order=c(0,1,0)), h=h)}\n\n# Compute cross-validated errors for up to 12 steps ahead\ne <- tsCV(myts, forecastfunction = farima1, h = 12)\n\n# Compute the MSE values and remove missing values\nmse <- colMeans(e^2, na.rm = TRUE)\n\n# Plot the MSE values against the forecast horizon\ndata.frame(h = 1:12, MSE = mse) %>%\n  ggplot(aes(x = h, y = MSE)) + geom_point()+geom_line()+ggtitle(\"12 Step Ahead Cross Validation\")\n\n\n\n\n\n\n\n\n\nCode\n#a seasonal cross validation using 1 steps ahead forecasts\nfarima1 <- function(x, h){forecast(Arima(x, order=c(0,1,0)),h=h)}\n\n# Compute cross-validated errors for up to 1 steps ahead\ne <- tsCV(myts, forecastfunction = farima1, h = 1)\nmse <-abs(mean(e,na.rm=TRUE))\n\n# Plot the MSE values against the forecast horizon\ndata.frame(h = 1:12, MSE = mse) %>%\n  ggplot(aes(x = h, y = MSE)) + geom_point()+geom_line()+ggtitle(\"1 Step Ahead Cross Validation\")\n\n\n\n\n\n\n\n\nThe one-step-ahead forecasting plot shows that the MSE seems to be stable. The MSE is tend to rise by 12 steps at each step of cross-validation. Lastly, the above-mentioned ARIMA model should be compared to benchmark methods before being used for a long time.\n\n\nModel Comparison\n\nPlotModel Error Table\n\n\n\n\nCode\nfit <- Arima(myts, order=c(0,1,0))\nautoplot(myts) +\n  autolayer(meanf(myts, h=182),\n            series=\"Mean\", PI=FALSE) +\n  autolayer(naive(myts, h=182),\n            series=\"Naïve\", PI=FALSE) +\n  autolayer(snaive(myts, h=182),\n            series=\"SNaïve\", PI=FALSE)+\n  autolayer(rwf(myts, h=182, drift=TRUE),\n            series=\"Drift\", PI=FALSE)+\n  autolayer(forecast(fit,182), \n            series=\"fit\",PI=FALSE) +\n  guides(colour=guide_legend(title=\"Forecast\"))\n\n\n\n\n\n\n\n\n\nCode\nsummary <- summary(fit)\nsnaive <- snaive(myts, h=182)\naccuracy(snaive)\n\n\n                   ME     RMSE      MAE    MPE     MAPE MASE      ACF1\nTraining set 10.67109 21.24869 15.42228 12.482 15.97199    1 0.9927812\n\n\nCode\nsummary\n\n\nSeries: myts \nARIMA(0,1,0) \n\nsigma^2 = 1.97:  log likelihood = -5762.96\nAIC=11527.93   AICc=11527.93   BIC=11534.02\n\nTraining set error measures:\n                     ME     RMSE       MAE        MPE      MAPE      MASE\nTraining set 0.03432379 1.403518 0.8077958 0.04362307 0.8758912 0.0523785\n                    ACF1\nTraining set -0.03291047\n\n\n\n\n\nError\nModel\nSnavie\n\n\n\n\nME\n0.009462746\n2.91434\n\n\nRMSE\n2.118625\n45.1313\n\n\nMAE\n0.6990421\n27.56965\n\n\nMPE\n-0.0492247\n-21.0712\n\n\nMAPE\n0.9543524\n58.49983\n\n\nMASE\n0.02535549\n1.0000000\n\n\nACF1\n-0.01001972\n0.9975806\n\n\n\n\n\n\nThe ARIMA forecast tracks the actual data points very closely when compared to other model. The other forecast methods are less accurate when compared. From the table, Model error measurements of fit are much lower than snaive method.We can conclude that the fitted model is good."
  },
  {
    "objectID": "arima_sector_energy.html",
    "href": "arima_sector_energy.html",
    "title": "ARIMA Model for Energy Sector Fund",
    "section": "",
    "text": "During exploratory data analysis (EDA), it was observed that the raw data is often non-stationary, exhibiting trends, seasonality, and other complex patterns that can make modeling and forecasting difficult. To address this issue, the data is often differentiated to achieve stationarity, which is a key assumption of the ARIMA model.\nDifferencing involves calculating the difference between each data point and the previous data point, which can help to remove trends and seasonality from the data. The goal is to create a new series of data that is stationary, meaning that the statistical properties of the data do not change over time.\n\nDifferentiated Time Series Plot\n\nPlotPACFADF Test\n\n\n\n\nCode\n#import the data\ndf <- read.csv(\"DATA/CLEANED DATA/XLE_raw_data.csv\")\n#convert to time series data\nmyts<-ts(df$XLE.Adjusted,frequency=252,start=c(2010,1,1), end = c(2023,3,1)) \n#First order differentiation\ndf1 <- diff(myts)\n# Plot \nmyts  %>% diff() %>% ggtsdisplay(main = \"First order differentiation\") \n\n\n\n\n\n\n\nCode\n#ACF plot \nggAcf(df1,60,main=\"ACF Plot: First order differentiation\") \n\n\n\n\n\n\n\n\n\nCode\nggPacf(df1,60,main=\"PACF Plot: First order differentiation\") \n\n\n\n\n\n\n\n\n\nCode\ntseries::adf.test(df1)\n\n\n\n    Augmented Dickey-Fuller Test\n\ndata:  df1\nDickey-Fuller = -14.575, Lag order = 14, p-value = 0.01\nalternative hypothesis: stationary\n\n\n\n\n\nAfter analyzing the ACF and PACF plots, it can be observed that most of the bar lines lie between the blue lines, indicating that the time series is stationary. This has been further confirmed through an Augmented Dickey-Fuller test as the p-value is less than 0.05. Once the data is stationary, the next step is to model it using ARIMA. The combination of ACF and PACF plots can help determine the appropriate values for p, d, and q in the ARIMA model. The order of differencing required to achieve stationarity gives the d value. The p value is determined by examining significant spikes at various lags in the PACF plot, while the q value is determined through significant spikes in the ACF plot. It is important to consider other methods such as grid search and information criteria to ensure the most appropriate model is selected.\nHere the parameters are d = 1 p = 0,7 (PACF Plot) q = 0,7 (ACF Plot)\n\n\nModel Selection\n\nARIMA(0,1,0) ResultARIMA(7,1,0) ResultARIMA(0,1,7) ResultARIMA(7,1,7) ResultAuto Arima Model\n\n\n\n\nCode\nArima(myts,order=c(0,1,0),include.drift=TRUE) \n\n\nSeries: myts \nARIMA(0,1,0) with drift \n\nCoefficients:\n       drift\n      0.0150\ns.e.  0.0148\n\nsigma^2 = 0.7171:  log likelihood = -4105.76\nAIC=8215.52   AICc=8215.53   BIC=8227.71\n\n\n\n\n\n\nCode\nArima(myts,order=c(7,1,0),include.drift=TRUE) \n\n\nSeries: myts \nARIMA(7,1,0) with drift \n\nCoefficients:\n         ar1     ar2      ar3     ar4      ar5      ar6     ar7   drift\n      0.0040  0.0241  -0.0245  0.0231  -0.0258  -0.0054  0.0363  0.0150\ns.e.  0.0175  0.0175   0.0174  0.0175   0.0175   0.0175  0.0175  0.0152\n\nsigma^2 = 0.716:  log likelihood = -4099.84\nAIC=8217.67   AICc=8217.73   BIC=8272.53\n\n\n\n\n\n\nCode\nArima(myts,order=c(0,1,7),include.drift=TRUE) \n\n\nSeries: myts \nARIMA(0,1,7) with drift \n\nCoefficients:\n         ma1     ma2      ma3     ma4      ma5      ma6     ma7   drift\n      0.0046  0.0238  -0.0247  0.0234  -0.0283  -0.0021  0.0345  0.0150\ns.e.  0.0175  0.0175   0.0174  0.0176   0.0175   0.0173  0.0177  0.0152\n\nsigma^2 = 0.716:  log likelihood = -4099.73\nAIC=8217.47   AICc=8217.52   BIC=8272.32\n\n\n\n\n\n\nCode\nArima(myts,order=c(7,1,7),include.drift=TRUE) \n\n\nSeries: myts \nARIMA(7,1,7) with drift \n\nCoefficients:\n          ar1     ar2     ar3     ar4     ar5      ar6      ar7     ma1\n      -0.3806  0.4362  0.4608  0.7702  0.3003  -0.5632  -0.6856  0.3729\ns.e.   0.1076  0.0706     NaN  0.0182  0.0922      NaN      NaN  0.1055\n          ma2      ma3      ma4      ma5   ma6    ma7   drift\n      -0.4355  -0.4869  -0.7525  -0.2837  0.55  0.728  0.0130\ns.e.   0.0567      NaN   0.0071   0.0897   NaN    NaN  0.0153\n\nsigma^2 = 0.7091:  log likelihood = -4082.04\nAIC=8196.08   AICc=8196.25   BIC=8293.6\n\n\n\n\n\n\nCode\nauto.arima(myts)\n\n\nSeries: myts \nARIMA(0,1,0) \n\nsigma^2 = 0.7171:  log likelihood = -4106.28\nAIC=8214.55   AICc=8214.55   BIC=8220.65\n\n\n\n\n\nIn the Model selection, when comparing the AIC, AICc and BIC values of all the 4 models, model (0,1,0) has the least BIC and AIc value. The model generated by auto.arima is also (0,1,0). We can proceed with the further model diagnostic for the chosen best model i.e. (0,1,0), to look into the details of the model.\n\n\nModel Diagnostic\n\nModel PlotModel\n\n\n\n\nCode\nmodel_output <- capture.output(sarima(myts,0,1,0))\n\n\n\n\n\n\n\n\n\nCode\ncat(model_output[9:38], model_output[length(model_output)], sep = \"\\n\") \n\n\n$fit\n\nCall:\narima(x = xdata, order = c(p, d, q), seasonal = list(order = c(P, D, Q), period = S), \n    xreg = constant, transform.pars = trans, fixed = fixed, optim.control = list(trace = trc, \n        REPORT = 1, reltol = tol))\n\nCoefficients:\n      constant\n        0.0150\ns.e.    0.0148\n\nsigma^2 estimated as 0.7169:  log likelihood = -4105.76,  aic = 8215.52\n\n$degrees_of_freedom\n[1] 3277\n\n$ttable\n         Estimate     SE t.value p.value\nconstant    0.015 0.0148  1.0139  0.3107\n\n$AIC\n[1] 2.506261\n\n$AICc\n[1] 2.506262\n\n$BIC\n[1] 2.50998\n\n\n\n\n\nThe model diagnostic is done for ARIMA(0,1,0). By analyzing the standardized residuals plot, it can be observed that the mean is close to 0 and the variance is slightly higher than 1. Deviations from these values could indicate poor model fit. However, in this case, the model seems to be well-fitted. The ACF plot of residuals shows very few significant lags, which is a positive sign for the model. The qq-plot also suggests normality in the residuals. Additionally, the p-values for the Ljung-Box test are greater than 0.05, indicating that the residuals are independent, which is favorable for the model’s accuracy.\nThe equation for this model: \\[(1-\\phi_1B-\\phi_2B^2)(1-B)(Y_t-\\mu) = (1+\\theta_1B+\\theta_2B^2)\\epsilon_t\\]\n\n\nForcast\n\nShort Term ForcastLong term Forcast\n\n\n\n\nCode\nmyts %>%\n  Arima(order=c(0,1,0),include.drift = TRUE) %>%\n  forecast %>%\n  autoplot(main = \"Energy Sector Fund Stock Prices Prediction\") +\n  ylab(\"stock prices\") + xlab(\"Year\")\n\n\n\n\n\n\n\n\n\nCode\nsarima.for(myts,182, 0,1,0, main='Energy Sector Fund Stock Prices Prediction')\n\n\n\n\n\n$pred\nTime Series:\nStart = c(2023, 4) \nEnd = c(2023, 185) \nFrequency = 252 \n  [1] 87.24462 87.25962 87.27461 87.28960 87.30460 87.31959 87.33459 87.34958\n  [9] 87.36457 87.37957 87.39456 87.40955 87.42455 87.43954 87.45454 87.46953\n [17] 87.48452 87.49952 87.51451 87.52950 87.54450 87.55949 87.57449 87.58948\n [25] 87.60447 87.61947 87.63446 87.64945 87.66445 87.67944 87.69444 87.70943\n [33] 87.72442 87.73942 87.75441 87.76940 87.78440 87.79939 87.81439 87.82938\n [41] 87.84437 87.85937 87.87436 87.88935 87.90435 87.91934 87.93434 87.94933\n [49] 87.96432 87.97932 87.99431 88.00930 88.02430 88.03929 88.05429 88.06928\n [57] 88.08427 88.09927 88.11426 88.12925 88.14425 88.15924 88.17424 88.18923\n [65] 88.20422 88.21922 88.23421 88.24920 88.26420 88.27919 88.29419 88.30918\n [73] 88.32417 88.33917 88.35416 88.36915 88.38415 88.39914 88.41414 88.42913\n [81] 88.44412 88.45912 88.47411 88.48910 88.50410 88.51909 88.53409 88.54908\n [89] 88.56407 88.57907 88.59406 88.60905 88.62405 88.63904 88.65403 88.66903\n [97] 88.68402 88.69902 88.71401 88.72900 88.74400 88.75899 88.77398 88.78898\n[105] 88.80397 88.81897 88.83396 88.84895 88.86395 88.87894 88.89393 88.90893\n[113] 88.92392 88.93892 88.95391 88.96890 88.98390 88.99889 89.01388 89.02888\n[121] 89.04387 89.05887 89.07386 89.08885 89.10385 89.11884 89.13383 89.14883\n[129] 89.16382 89.17882 89.19381 89.20880 89.22380 89.23879 89.25378 89.26878\n[137] 89.28377 89.29877 89.31376 89.32875 89.34375 89.35874 89.37373 89.38873\n[145] 89.40372 89.41872 89.43371 89.44870 89.46370 89.47869 89.49368 89.50868\n[153] 89.52367 89.53867 89.55366 89.56865 89.58365 89.59864 89.61363 89.62863\n[161] 89.64362 89.65862 89.67361 89.68860 89.70360 89.71859 89.73358 89.74858\n[169] 89.76357 89.77857 89.79356 89.80855 89.82355 89.83854 89.85353 89.86853\n[177] 89.88352 89.89852 89.91351 89.92850 89.94350 89.95849\n\n$se\nTime Series:\nStart = c(2023, 4) \nEnd = c(2023, 185) \nFrequency = 252 \n  [1]  0.8466922  1.1974036  1.4665140  1.6933845  1.8932614  2.0739639\n  [7]  2.2401371  2.3948073  2.5400767  2.6774759  2.8081604  2.9330279\n [13]  3.0527923  3.1680322  3.2792249  3.3867689  3.4910015  3.5922109\n [19]  3.6906459  3.7865228  3.8800312  3.9713386  4.0605933  4.1479279\n [25]  4.2334612  4.3173002  4.3995419  4.4802742  4.5595772  4.6375243\n [31]  4.7141828  4.7896145  4.8638766  4.9370217  5.0090988  5.0801534\n [37]  5.1502278  5.2193615  5.2875913  5.3549519  5.4214756  5.4871928\n [43]  5.5521323  5.6163209  5.6797842  5.7425461  5.8046295  5.8660559\n [49]  5.9268456  5.9870182  6.0465920  6.1055845  6.1640125  6.2218918\n [55]  6.2792376  6.3360645  6.3923862  6.4482159  6.5035664  6.5584498\n [61]  6.6128777  6.6668613  6.7204112  6.7735379  6.8262510  6.8785602\n [67]  6.9304746  6.9820030  7.0331539  7.0839355  7.1343556  7.1844218\n [73]  7.2341416  7.2835220  7.3325698  7.3812917  7.4296942  7.4777833\n [79]  7.5255652  7.5730455  7.6202301  7.6671243  7.7137333  7.7600625\n [85]  7.8061167  7.8519007  7.8974194  7.9426772  7.9876785  8.0324278\n [91]  8.0769291  8.1211866  8.1652042  8.2089858  8.2525351  8.2958557\n [97]  8.3389514  8.3818255  8.4244813  8.4669223  8.5091516  8.5511724\n[103]  8.5929876  8.6346004  8.6760136  8.7172301  8.7582525  8.7990838\n[109]  8.8397264  8.8801830  8.9204562  8.9605483  9.0004619  9.0401992\n[115]  9.0797626  9.1191544  9.1583768  9.1974319  9.2363218  9.2750487\n[121]  9.3136146  9.3520214  9.3902711  9.4283657  9.4663069  9.5040967\n[127]  9.5417369  9.5792291  9.6165752  9.6537768  9.6908355  9.7277531\n[133]  9.7645312  9.8011712  9.8376747  9.8740433  9.9102785  9.9463816\n[139]  9.9823542 10.0181976 10.0539132 10.0895024 10.1249665 10.1603068\n[145] 10.1955246 10.2306212 10.2655977 10.3004556 10.3351958 10.3698197\n[151] 10.4043283 10.4387229 10.4730045 10.5071743 10.5412333 10.5751826\n[157] 10.6090233 10.6427563 10.6763828 10.7099037 10.7433200 10.7766327\n[163] 10.8098428 10.8429511 10.8759586 10.9088663 10.9416750 10.9743856\n[169] 11.0069990 11.0395161 11.0719376 11.1042645 11.1364976 11.1686376\n[175] 11.2006854 11.2326418 11.2645075 11.2962833 11.3279700 11.3595683\n[181] 11.3910790 11.4225027\n\n\n\n\n\nThe two figures above show an idea of the short term and the long-term forecasted stock price. The forecasted values is be based on the estimated parameters of the model, which were obtained by fitting it to the historical data. One way to assess the accuracy of the forecasts is to use cross-validation, which involves splitting the data into training and testing sets and comparing the predicted values to the actual values in the test set. This can help to identify any potential problems with the model and to fine-tune the model parameters.\n\n12 Step ahead Cross Validation1 Step ahead Cross Validation\n\n\n\n\nCode\n#a seasonal cross validation using 12 steps ahead forecasts\nfarima1 <- function(x, h){forecast(Arima(x, order=c(0,1,0)), h=h)}\n\n# Compute cross-validated errors for up to 12 steps ahead\ne <- tsCV(myts, forecastfunction = farima1, h = 12)\n\n# Compute the MSE values and remove missing values\nmse <- colMeans(e^2, na.rm = TRUE)\n\n# Plot the MSE values against the forecast horizon\ndata.frame(h = 1:12, MSE = mse) %>%\n  ggplot(aes(x = h, y = MSE)) + geom_point()+geom_line()+ggtitle(\"12 Step Ahead Cross Validation\")\n\n\n\n\n\n\n\n\n\nCode\n#a seasonal cross validation using 1 steps ahead forecasts\nfarima1 <- function(x, h){forecast(Arima(x, order=c(0,1,0)),h=h)}\n\n# Compute cross-validated errors for up to 1 steps ahead\ne <- tsCV(myts, forecastfunction = farima1, h = 1)\nmse <-abs(mean(e,na.rm=TRUE))\n\n# Plot the MSE values against the forecast horizon\ndata.frame(h = 1:12, MSE = mse) %>%\n  ggplot(aes(x = h, y = MSE)) + geom_point()+geom_line()+ggtitle(\"1 Step Ahead Cross Validation\")\n\n\n\n\n\n\n\n\nThe one-step-ahead forecasting plot shows that the MSE seems to be stable. The MSE is tend to rise by 12 steps at each step of cross-validation. Lastly, the above-mentioned ARIMA model should be compared to benchmark methods before being used for a long time.\n\n\nModel Comparison\n\nPlotModel Error Table\n\n\n\n\nCode\nfit <- Arima(myts, order=c(0,1,0))\nautoplot(myts) +\n  autolayer(meanf(myts, h=182),\n            series=\"Mean\", PI=FALSE) +\n  autolayer(naive(myts, h=182),\n            series=\"Naïve\", PI=FALSE) +\n  autolayer(snaive(myts, h=182),\n            series=\"SNaïve\", PI=FALSE)+\n  autolayer(rwf(myts, h=182, drift=TRUE),\n            series=\"Drift\", PI=FALSE)+\n  autolayer(forecast(fit,182), \n            series=\"fit\",PI=FALSE) +\n  guides(colour=guide_legend(title=\"Forecast\"))\n\n\n\n\n\n\n\n\n\nCode\nsummary <- summary(fit)\nsnaive <- snaive(myts, h=182)\naccuracy(snaive)\n\n\n                   ME    RMSE      MAE      MPE    MAPE MASE      ACF1\nTraining set 3.226711 13.4245 10.63302 2.192255 21.1083    1 0.9957227\n\n\nCode\nsummary\n\n\nSeries: myts \nARIMA(0,1,0) \n\nsigma^2 = 0.7171:  log likelihood = -4106.28\nAIC=8214.55   AICc=8214.55   BIC=8220.65\n\nTraining set error measures:\n                     ME      RMSE       MAE         MPE     MAPE       MASE\nTraining set 0.01500078 0.8466961 0.5954274 0.009107056 1.210244 0.05599796\n                    ACF1\nTraining set 0.002311381\n\n\n\n\n\nError\nModel\nSnavie\n\n\n\n\nME\n0.001858803\n1.488227\n\n\nRMSE\n1.032129\n16.98363\n\n\nMAE\n0.5766939\n12.5794\n\n\nMPE\n-0.01822702\n-2.493712\n\n\nMAPE\n1.173334\n25.42997\n\n\nMASE\n0.0458443\n1.0000000\n\n\nACF1\n0.0093721\n0.9959312\n\n\n\n\n\n\nThe ARIMA forecast tracks the actual data points very closely when compared to other model. The other forecast methods are less accurate when compared. From the table, Model error measurements of fit are much lower than snaive method.We can conclude that the fitted model is good."
  },
  {
    "objectID": "arima_sector_financial.html",
    "href": "arima_sector_financial.html",
    "title": "ARIMA Model for Financial Sector Fund",
    "section": "",
    "text": "During exploratory data analysis (EDA), it was observed that the raw data is often non-stationary, exhibiting trends, seasonality, and other complex patterns that can make modeling and forecasting difficult. To address this issue, the data is often differentiated to achieve stationarity, which is a key assumption of the ARIMA model.\nDifferencing involves calculating the difference between each data point and the previous data point, which can help to remove trends and seasonality from the data. The goal is to create a new series of data that is stationary, meaning that the statistical properties of the data do not change over time.\n\nDifferentiated Time Series Plot\n\nPlotACFPACFADF Test\n\n\n\n\nCode\n#import the data\ndf <- read.csv(\"DATA/CLEANED DATA/XLF_raw_data.csv\")\n#convert to time series data\nmyts<-ts(df$XLF.Adjusted,frequency=252,start=c(2010,1,1), end = c(2023,3,1)) \n#First order differentiation\ndf1 <- diff(myts)\n# Plot \nmyts  %>% diff() %>% ggtsdisplay(main = \"First order differentiation\") \n\n\n\n\n\n\n\n\n\nCode\n#ACF plot \nggAcf(df1,60,main=\"ACF Plot: First order differentiation\") \n\n\n\n\n\n\n\n\n\nCode\nggPacf(df1,60,main=\"PACF Plot: First order differentiation\") \n\n\n\n\n\n\n\n\n\nCode\ntseries::adf.test(df1)\n\n\n\n    Augmented Dickey-Fuller Test\n\ndata:  df1\nDickey-Fuller = -15.915, Lag order = 14, p-value = 0.01\nalternative hypothesis: stationary\n\n\n\n\n\nAfter analyzing the ACF and PACF plots, it can be observed that most of the bar lines lie between the blue lines, indicating that the time series is stationary. This has been further confirmed through an Augmented Dickey-Fuller test as the p-value is less than 0.05. Once the data is stationary, the next step is to model it using ARIMA. The combination of ACF and PACF plots can help determine the appropriate values for p, d, and q in the ARIMA model. The order of differencing required to achieve stationarity gives the d value. The p value is determined by examining significant spikes at various lags in the PACF plot, while the q value is determined through significant spikes in the ACF plot. It is important to consider other methods such as grid search and information criteria to ensure the most appropriate model is selected.\nHere the parameters are d = 1 p = 0,1 (PACF Plot) q = 0,1 (ACF Plot)\n\n\nModel Selection\n\nARIMA ResultAuto Arima Model\n\n\n\n\nCode\n######################## Check for different combinations ########\nd=1\ni=1\ntemp = data.frame()\nls=matrix(rep(NA,6*4),nrow=4) #nrow = 2x2x1\n\n\nfor (p in 1:2)# p=0,1 :2\n{\n  for(q in 1:2)# q=0,1 :2\n  {\n    for(d in 1)# d=1 :1\n    {\n      \n      if(p-1+d+q-1<=8)\n      {\n        \n        model<- Arima(myts,order=c(p-1,d,q-1),include.drift=TRUE) \n        ls[i,]= c(p-1,d,q-1,model$aic,model$bic,model$aicc)\n        i=i+1\n        \n      }\n      \n    }\n  }\n}\n\nmodel = as.data.frame(ls)\nnames(model)= c(\"p\",\"d\",\"q\",\"AIC\",\"BIC\",\"AICc\")\n\n#temp\nknitr::kable(model)\n\n\n\n\n\np\nd\nq\nAIC\nBIC\nAICc\n\n\n\n\n0\n1\n0\n1432.313\n1444.503\n1432.317\n\n\n0\n1\n1\n1418.253\n1436.538\n1418.261\n\n\n1\n1\n0\n1416.456\n1434.741\n1416.463\n\n\n1\n1\n1\n1413.432\n1437.812\n1413.444\n\n\n\n\n\n\n\n\n\nCode\nauto.arima(myts)\n\n\nSeries: myts \nARIMA(4,1,1) with drift \n\nCoefficients:\n          ar1      ar2     ar3      ar4     ma1   drift\n      -0.9078  -0.0018  0.0490  -0.0531  0.8485  0.0079\ns.e.   0.0409   0.0237  0.0237   0.0187  0.0373  0.0050\n\nsigma^2 = 0.08882:  log likelihood = -680.08\nAIC=1374.15   AICc=1374.18   BIC=1416.82\n\n\n\n\n\nThe given table provides different ARIMA models with their corresponding AIC and BIC values. By sorting the table according to each criterion, we can identify the models with the lowest values. The smallest AIC value corresponds to an ARIMA (1,1,0) model, while the ARIMA (0,1,0) model has the lowest BIC value. Additionally, the auto.arima function in R suggests an ARIMA (0,1,0) model as the best fit for the data. Since there are different models to choose from, it is important to perform model diagnostics to determine the best model for the data. These diagnostics can include checking for residual normality, checking for autocorrelation and partial autocorrelation in the residuals, and comparing the forecasted values to the actual values. By conducting these tests, we can determine which model provides the best fit for the data and is therefore the most appropriate to use for forecasting future values.\n\n\nModel Diagnostic\n\nModel 1 PlotModel 1Model 2 PlotModel 2\n\n\n\n\nCode\nmodel_output <- capture.output(sarima(myts,1,1,0))\n\n\n\n\n\n\n\n\n\nCode\ncat(model_output[11:41], model_output[length(model_output)], sep = \"\\n\") \n\n\n$fit\n\nCall:\narima(x = xdata, order = c(p, d, q), seasonal = list(order = c(P, D, Q), period = S), \n    xreg = constant, transform.pars = trans, fixed = fixed, optim.control = list(trace = trc, \n        REPORT = 1, reltol = tol))\n\nCoefficients:\n          ar1  constant\n      -0.0737    0.0080\ns.e.   0.0174    0.0049\n\nsigma^2 estimated as 0.09003:  log likelihood = -705.23,  aic = 1416.46\n\n$degrees_of_freedom\n[1] 3276\n\n$ttable\n         Estimate     SE t.value p.value\nar1       -0.0737 0.0174 -4.2313  0.0000\nconstant   0.0080 0.0049  1.6375  0.1016\n\n$AIC\n[1] 0.4321097\n\n$AICc\n[1] 0.4321108\n\n$BIC\n[1] 0.4376878\n\n\n\n\n\n\nCode\nmodel_output <- capture.output(sarima(myts,0,1,0))\n\n\n\n\n\n\n\n\n\nCode\ncat(model_output[9:38], model_output[length(model_output)], sep = \"\\n\") \n\n\n$fit\n\nCall:\narima(x = xdata, order = c(p, d, q), seasonal = list(order = c(P, D, Q), period = S), \n    xreg = constant, transform.pars = trans, fixed = fixed, optim.control = list(trace = trc, \n        REPORT = 1, reltol = tol))\n\nCoefficients:\n      constant\n        0.0080\ns.e.    0.0053\n\nsigma^2 estimated as 0.09052:  log likelihood = -714.16,  aic = 1432.31\n\n$degrees_of_freedom\n[1] 3277\n\n$ttable\n         Estimate     SE t.value p.value\nconstant    0.008 0.0053   1.522  0.1281\n\n$AIC\n[1] 0.4369472\n\n$AICc\n[1] 0.4369476\n\n$BIC\n[1] 0.4406659\n\n\n\n\n\nTo determine which model diagnosis is the best, we need to consider some key factors such as the coefficient estimates and standard errors, AIC, BIC, p-values, and the log-likelihood. Comparing the two models, model 2 is the best because it has the lowest BIC value when compared and the AIC value are almost the same, which indicate better goodness of fit. Also, all the coefficient estimates in the model are significant, as evidenced by the p-values are better than model 1, which implies that there is a strong correlation between the variables. The t-values are high, indicating that the estimates are highly reliable.\nTherefore, based on the AIC and BIC values, the significance of the coefficient estimates, and the reliability of the estimates, we can conclude that model 2 is the best model diagnosis. Therefore, (0,1,0) is the best model for this time series.\nThe best model identified is ARIMA(0,1,0). By analyzing the standardized residuals plot, it can be observed that the mean is close to 0 and the variance is slightly higher than 1. Deviations from these values could indicate poor model fit. However, in this case, the model seems to be well-fitted. The ACF plot of residuals shows very few significant lags, which is a positive sign for the model. The qq-plot also suggests normality in the residuals. Additionally, the p-values for the Ljung-Box test are greater than 0.05, indicating that the residuals are independent, which is favorable for the model’s accuracy.\nThe equation for this model: \\[(1-\\phi_1B-\\phi_2B^2)(1-B)(Y_t-\\mu) = (1+\\theta_1B+\\theta_2B^2)\\epsilon_t\\]\n\n\nForcast\n\nShort Term ForcastLong term Forcast\n\n\n\n\nCode\nmyts %>%\n  Arima(order=c(0,1,0),include.drift = TRUE) %>%\n  forecast %>%\n  autoplot(main = \"Financial Sector Fund Stock Prices Prediction\") +\n  ylab(\"stock prices\") + xlab(\"Year\")\n\n\n\n\n\n\n\n\n\nCode\nsarima.for(myts,182, 0,1,0, main='Financial Sector Fund Stock Prices Prediction')\n\n\n\n\n\n$pred\nTime Series:\nStart = c(2023, 4) \nEnd = c(2023, 185) \nFrequency = 252 \n  [1] 35.61129 35.61929 35.62729 35.63529 35.64328 35.65128 35.65928 35.66728\n  [9] 35.67528 35.68327 35.69127 35.69927 35.70727 35.71527 35.72326 35.73126\n [17] 35.73926 35.74726 35.75526 35.76325 35.77125 35.77925 35.78725 35.79525\n [25] 35.80324 35.81124 35.81924 35.82724 35.83524 35.84323 35.85123 35.85923\n [33] 35.86723 35.87523 35.88322 35.89122 35.89922 35.90722 35.91521 35.92321\n [41] 35.93121 35.93921 35.94721 35.95520 35.96320 35.97120 35.97920 35.98720\n [49] 35.99519 36.00319 36.01119 36.01919 36.02719 36.03518 36.04318 36.05118\n [57] 36.05918 36.06718 36.07517 36.08317 36.09117 36.09917 36.10717 36.11516\n [65] 36.12316 36.13116 36.13916 36.14716 36.15515 36.16315 36.17115 36.17915\n [73] 36.18715 36.19514 36.20314 36.21114 36.21914 36.22714 36.23513 36.24313\n [81] 36.25113 36.25913 36.26713 36.27512 36.28312 36.29112 36.29912 36.30712\n [89] 36.31511 36.32311 36.33111 36.33911 36.34711 36.35510 36.36310 36.37110\n [97] 36.37910 36.38710 36.39509 36.40309 36.41109 36.41909 36.42709 36.43508\n[105] 36.44308 36.45108 36.45908 36.46707 36.47507 36.48307 36.49107 36.49907\n[113] 36.50706 36.51506 36.52306 36.53106 36.53906 36.54705 36.55505 36.56305\n[121] 36.57105 36.57905 36.58704 36.59504 36.60304 36.61104 36.61904 36.62703\n[129] 36.63503 36.64303 36.65103 36.65903 36.66702 36.67502 36.68302 36.69102\n[137] 36.69902 36.70701 36.71501 36.72301 36.73101 36.73901 36.74700 36.75500\n[145] 36.76300 36.77100 36.77900 36.78699 36.79499 36.80299 36.81099 36.81899\n[153] 36.82698 36.83498 36.84298 36.85098 36.85898 36.86697 36.87497 36.88297\n[161] 36.89097 36.89897 36.90696 36.91496 36.92296 36.93096 36.93896 36.94695\n[169] 36.95495 36.96295 36.97095 36.97895 36.98694 36.99494 37.00294 37.01094\n[177] 37.01893 37.02693 37.03493 37.04293 37.05093 37.05892\n\n$se\nTime Series:\nStart = c(2023, 4) \nEnd = c(2023, 185) \nFrequency = 252 \n  [1] 0.3008706 0.4254953 0.5211231 0.6017412 0.6727671 0.7369794 0.7960288\n  [8] 0.8509905 0.9026118 0.9514363 0.9978748 1.0422463 1.0848043 1.1257547\n [15] 1.1652668 1.2034823 1.2405212 1.2764858 1.3114645 1.3455342 1.3787622\n [22] 1.4112081 1.4429246 1.4739588 1.5043529 1.5341450 1.5633694 1.5920575\n [29] 1.6202377 1.6479361 1.6751765 1.7019811 1.7283699 1.7543619 1.7799744\n [36] 1.8052235 1.8301243 1.8546909 1.8789362 1.9028727 1.9265118 1.9498643\n [43] 1.9729404 1.9957497 2.0183013 2.0406036 2.0626648 2.0844926 2.1060941\n [50] 2.1274763 2.1486458 2.1696087 2.1903709 2.2109383 2.2313160 2.2515093\n [57] 2.2715231 2.2913621 2.3110308 2.3305335 2.3498744 2.3690574 2.3880863\n [64] 2.4069647 2.4256962 2.4442842 2.4627319 2.4810424 2.4992188 2.5172639\n [71] 2.5351806 2.5529716 2.5706394 2.5881867 2.6056157 2.6229290 2.6401287\n [78] 2.6572171 2.6741963 2.6910683 2.7078353 2.7244990 2.7410615 2.7575245\n [85] 2.7738898 2.7901590 2.8063340 2.8224163 2.8384074 2.8543090 2.8701225\n [92] 2.8858493 2.9014909 2.9170486 2.9325237 2.9479177 2.9632316 2.9784669\n [99] 2.9936245 3.0087059 3.0237120 3.0386440 3.0535030 3.0682900 3.0830061\n[106] 3.0976523 3.1122296 3.1267389 3.1411812 3.1555573 3.1698683 3.1841150\n[113] 3.1982982 3.2124188 3.2264776 3.2404754 3.2544130 3.2682911 3.2821106\n[120] 3.2958722 3.3095765 3.3232243 3.3368162 3.3503531 3.3638354 3.3772640\n[127] 3.3906393 3.4039621 3.4172330 3.4304525 3.4436213 3.4567399 3.4698089\n[134] 3.4828288 3.4958003 3.5087238 3.5215999 3.5344291 3.5472119 3.5599488\n[141] 3.5726403 3.5852868 3.5978889 3.6104470 3.6229616 3.6354331 3.6478620\n[148] 3.6602487 3.6725936 3.6848971 3.6971597 3.7093817 3.7215636 3.7337058\n[155] 3.7458086 3.7578724 3.7698977 3.7818846 3.7938338 3.8057453 3.8176198\n[162] 3.8294574 3.8412585 3.8530235 3.8647527 3.8764464 3.8881049 3.8997285\n[169] 3.9113176 3.9228725 3.9343935 3.9458808 3.9573347 3.9687556 3.9801438\n[176] 3.9914994 4.0028228 4.0141143 4.0253741 4.0366025 4.0477998 4.0589661\n\n\n\n\n\nThe two figures above show an idea of the short term and the long-term forecasted stock price. The forecasted values is be based on the estimated parameters of the model, which were obtained by fitting it to the historical data. One way to assess the accuracy of the forecasts is to use cross-validation, which involves splitting the data into training and testing sets and comparing the predicted values to the actual values in the test set. This can help to identify any potential problems with the model and to fine-tune the model parameters.\n\n12 Step ahead Cross Validation1 Step ahead Cross Validation\n\n\n\n\nCode\n#a seasonal cross validation using 12 steps ahead forecasts\nfarima1 <- function(x, h){forecast(Arima(x, order=c(0,1,0)), h=h)}\n\n# Compute cross-validated errors for up to 12 steps ahead\ne <- tsCV(myts, forecastfunction = farima1, h = 12)\n\n# Compute the MSE values and remove missing values\nmse <- colMeans(e^2, na.rm = TRUE)\n\n# Plot the MSE values against the forecast horizon\ndata.frame(h = 1:12, MSE = mse) %>%\n  ggplot(aes(x = h, y = MSE)) + geom_point()+geom_line()+ggtitle(\"12 Step Ahead Cross Validation\")\n\n\n\n\n\n\n\n\n\nCode\n#a seasonal cross validation using 1 steps ahead forecasts\nfarima1 <- function(x, h){forecast(Arima(x, order=c(0,1,0)),h=h)}\n\n# Compute cross-validated errors for up to 1 steps ahead\ne <- tsCV(myts, forecastfunction = farima1, h = 1)\nmse <-abs(mean(e,na.rm=TRUE))\n\n# Plot the MSE values against the forecast horizon\ndata.frame(h = 1:12, MSE = mse) %>%\n  ggplot(aes(x = h, y = MSE)) + geom_point()+geom_line()+ggtitle(\"1 Step Ahead Cross Validation\")\n\n\n\n\n\n\n\n\nThe one-step-ahead forecasting plot shows that the MSE seems to be stable. The MSE is tend to rise by 12 steps at each step of cross-validation. Lastly, the above-mentioned ARIMA model should be compared to benchmark methods before being used for a long time.\n\n\nModel Comparison\n\nPlotModel Error Table\n\n\n\n\nCode\nfit <- Arima(myts, order=c(0,1,0))\nautoplot(myts) +\n  autolayer(meanf(myts, h=182),\n            series=\"Mean\", PI=FALSE) +\n  autolayer(naive(myts, h=182),\n            series=\"Naïve\", PI=FALSE) +\n  autolayer(snaive(myts, h=182),\n            series=\"SNaïve\", PI=FALSE)+\n  autolayer(rwf(myts, h=182, drift=TRUE),\n            series=\"Drift\", PI=FALSE)+\n  autolayer(forecast(fit,182), \n            series=\"fit\",PI=FALSE) +\n  guides(colour=guide_legend(title=\"Forecast\"))\n\n\n\n\n\n\n\n\n\nCode\nsummary <- summary(fit)\nsnaive <- snaive(myts, h=182)\naccuracy(snaive)\n\n\n                   ME     RMSE      MAE      MPE     MAPE MASE      ACF1\nTraining set 2.064605 4.656717 3.316979 8.718375 14.98144    1 0.9947578\n\n\nCode\nsummary\n\n\nSeries: myts \nARIMA(0,1,0) \n\nsigma^2 = 0.09059:  log likelihood = -715.31\nAIC=1432.63   AICc=1432.63   BIC=1438.72\n\nTraining set error measures:\n                      ME     RMSE       MAE        MPE      MAPE       MASE\nTraining set 0.007998394 0.300931 0.1914707 0.03010762 0.9793247 0.05772443\n                    ACF1\nTraining set -0.07370225\n\n\n\n\n\nError\nModel\nSnavie\n\n\n\n\nME\n0.001430649\n0.5394057\n\n\nRMSE\n0.4622021\n8.718076\n\n\nMAE\n0.17157\n5.297662\n\n\nMPE\n-0.02929688\n-9.401781\n\n\nMAPE\n1.02526\n37.78756\n\n\nMASE\n0.03238599\n1.0000000\n\n\nACF1\n-0.0292269\n0.996959\n\n\n\n\n\n\nThe ARIMA forecast tracks the actual data points very closely when compared to other model. The other forecast methods are less accurate when compared. From the table, Model error measurements of fit are much lower than snaive method.We can conclude that the fitted model is good."
  },
  {
    "objectID": "arima_sector_health_care.html",
    "href": "arima_sector_health_care.html",
    "title": "ARIMA Model for Health Care Sector Fund",
    "section": "",
    "text": "During exploratory data analysis (EDA), it was observed that the raw data is often non-stationary, exhibiting trends, seasonality, and other complex patterns that can make modeling and forecasting difficult. To address this issue, the data is often differentiated to achieve stationarity, which is a key assumption of the ARIMA model.\nDifferencing involves calculating the difference between each data point and the previous data point, which can help to remove trends and seasonality from the data. The goal is to create a new series of data that is stationary, meaning that the statistical properties of the data do not change over time.\n\nDifferentiated Time Series Plot\n\nPlotACFADF Test\n\n\n\n\nCode\n#import the data\ndf <- read.csv(\"DATA/CLEANED DATA/XLV_raw_data.csv\")\n#convert to time series data\nmyts<-ts(df$XLV.Adjusted,frequency=252,start=c(2010,1,1), end = c(2023,3,1)) \n#First order differentiation\ndf1 <- diff(myts)\n# Plot \nmyts  %>% diff() %>% ggtsdisplay(main = \"First order differentiation\") \n\n\n\n\n\n\n\n\n\nCode\n##### PACF\n#ACF plot \nggAcf(df1,60,main=\"ACF Plot: First order differentiation\") \n\n\n\n\n\n\n\nCode\nggPacf(df1,60,main=\"PACF Plot: First order differentiation\") \n\n\n\n\n\n\n\n\n\nCode\ntseries::adf.test(df1)\n\n\n\n    Augmented Dickey-Fuller Test\n\ndata:  df1\nDickey-Fuller = -16.669, Lag order = 14, p-value = 0.01\nalternative hypothesis: stationary\n\n\n\n\n\nAfter analyzing the ACF and PACF plots, it can be observed that most of the bar lines lie between the blue lines, indicating that the time series is stationary. This has been further confirmed through an Augmented Dickey-Fuller test as the p-value is less than 0.05. Once the data is stationary, the next step is to model it using ARIMA. The combination of ACF and PACF plots can help determine the appropriate values for p, d, and q in the ARIMA model. The order of differencing required to achieve stationarity gives the d value. The p value is determined by examining significant spikes at various lags in the PACF plot, while the q value is determined through significant spikes in the ACF plot. It is important to consider other methods such as grid search and information criteria to ensure the most appropriate model is selected.\nHere the parameters are d = 1 p = 0 (PACF Plot) q = 0 (ACF Plot)\n\n\nModel Selection\n\nARIMA ResultAuto Arima Model\n\n\n\n\nCode\nArima(myts,order=c(0,1,0),include.drift=TRUE) \n\n\nSeries: myts \nARIMA(0,1,0) with drift \n\nCoefficients:\n       drift\n      0.0334\ns.e.  0.0145\n\nsigma^2 = 0.6867:  log likelihood = -4034.79\nAIC=8073.57   AICc=8073.58   BIC=8085.76\n\n\n\n\n\n\nCode\nauto.arima(myts)\n\n\nError in polyroot(c(1, -testvec)) : root finding code failed\n\n\nSeries: myts \nARIMA(0,1,1) with drift \n\nCoefficients:\n          ma1   drift\n      -0.0684  0.0334\ns.e.   0.0165  0.0134\n\nsigma^2 = 0.6834:  log likelihood = -4026.26\nAIC=8058.53   AICc=8058.53   BIC=8076.81\n\n\n\n\n\nIn the Model selection, chosen model is the same as the auto.arima generated model. We can proceed with the further model diagnostic for the chosen best model i.e. (0,1,0), to look into the details of the model.\n\n\nModel Diagnostic\n\nModel PlotModel\n\n\n\n\nCode\nmodel_output <- capture.output(sarima(myts,0,1,0))\n\n\n\n\n\n\n\n\n\nCode\ncat(model_output[9:38], model_output[length(model_output)], sep = \"\\n\") \n\n\n$fit\n\nCall:\narima(x = xdata, order = c(p, d, q), seasonal = list(order = c(P, D, Q), period = S), \n    xreg = constant, transform.pars = trans, fixed = fixed, optim.control = list(trace = trc, \n        REPORT = 1, reltol = tol))\n\nCoefficients:\n      constant\n        0.0334\ns.e.    0.0145\n\nsigma^2 estimated as 0.6865:  log likelihood = -4034.79,  aic = 8073.57\n\n$degrees_of_freedom\n[1] 3277\n\n$ttable\n         Estimate     SE t.value p.value\nconstant   0.0334 0.0145  2.3091   0.021\n\n$AIC\n[1] 2.462957\n\n$AICc\n[1] 2.462957\n\n$BIC\n[1] 2.466676\n\n\n\n\n\nThe model diagnostic is done for ARIMA(0,1,0). By analyzing the standardized residuals plot, it can be observed that the mean is close to 0 and the variance is slightly higher than 1. Deviations from these values could indicate poor model fit. However, in this case, the model seems to be well-fitted. The ACF plot of residuals shows very few significant lags, which is a positive sign for the model. The qq-plot also suggests normality in the residuals. Additionally, the p-values for the Ljung-Box test are greater than 0.05, indicating that the residuals are independent, which is favorable for the model’s accuracy.\nThe equation for this model: \\[(1-\\phi_1B-\\phi_2B^2)(1-B)(Y_t-\\mu) = (1+\\theta_1B+\\theta_2B^2)\\epsilon_t\\]\n\n\nForcast\n\nShort Term ForcastLong term Forcast\n\n\n\n\nCode\nmyts %>%\n  Arima(order=c(0,1,0),include.drift = TRUE) %>%\n  forecast %>%\n  autoplot(main = \"Health Care Sector Fund Stock Prices Prediction\") +\n  ylab(\"stock prices\") + xlab(\"Year\")\n\n\n\n\n\n\n\n\n\nCode\nsarima.for(myts,182, 0,1,0, main='Health Care Sector Fund Stock Prices Prediction')\n\n\n\n\n\n$pred\nTime Series:\nStart = c(2023, 4) \nEnd = c(2023, 185) \nFrequency = 252 \n  [1] 134.7878 134.8212 134.8546 134.8881 134.9215 134.9549 134.9883 135.0217\n  [9] 135.0551 135.0886 135.1220 135.1554 135.1888 135.2222 135.2556 135.2891\n [17] 135.3225 135.3559 135.3893 135.4227 135.4562 135.4896 135.5230 135.5564\n [25] 135.5898 135.6232 135.6567 135.6901 135.7235 135.7569 135.7903 135.8237\n [33] 135.8572 135.8906 135.9240 135.9574 135.9908 136.0242 136.0577 136.0911\n [41] 136.1245 136.1579 136.1913 136.2247 136.2582 136.2916 136.3250 136.3584\n [49] 136.3918 136.4252 136.4587 136.4921 136.5255 136.5589 136.5923 136.6257\n [57] 136.6592 136.6926 136.7260 136.7594 136.7928 136.8262 136.8597 136.8931\n [65] 136.9265 136.9599 136.9933 137.0267 137.0602 137.0936 137.1270 137.1604\n [73] 137.1938 137.2272 137.2607 137.2941 137.3275 137.3609 137.3943 137.4278\n [81] 137.4612 137.4946 137.5280 137.5614 137.5948 137.6283 137.6617 137.6951\n [89] 137.7285 137.7619 137.7953 137.8288 137.8622 137.8956 137.9290 137.9624\n [97] 137.9958 138.0293 138.0627 138.0961 138.1295 138.1629 138.1963 138.2298\n[105] 138.2632 138.2966 138.3300 138.3634 138.3968 138.4303 138.4637 138.4971\n[113] 138.5305 138.5639 138.5973 138.6308 138.6642 138.6976 138.7310 138.7644\n[121] 138.7978 138.8313 138.8647 138.8981 138.9315 138.9649 138.9983 139.0318\n[129] 139.0652 139.0986 139.1320 139.1654 139.1988 139.2323 139.2657 139.2991\n[137] 139.3325 139.3659 139.3994 139.4328 139.4662 139.4996 139.5330 139.5664\n[145] 139.5999 139.6333 139.6667 139.7001 139.7335 139.7669 139.8004 139.8338\n[153] 139.8672 139.9006 139.9340 139.9674 140.0009 140.0343 140.0677 140.1011\n[161] 140.1345 140.1679 140.2014 140.2348 140.2682 140.3016 140.3350 140.3684\n[169] 140.4019 140.4353 140.4687 140.5021 140.5355 140.5689 140.6024 140.6358\n[177] 140.6692 140.7026 140.7360 140.7694 140.8029 140.8363\n\n$se\nTime Series:\nStart = c(2023, 4) \nEnd = c(2023, 185) \nFrequency = 252 \n  [1]  0.8285565  1.1717559  1.4351020  1.6571131  1.8527087  2.0295407\n  [7]  2.1921545  2.3435118  2.4856696  2.6201258  2.7480111  2.8702040\n [13]  2.9874031  3.1001747  3.2089857  3.3142261  3.4162261  3.5152677\n [19]  3.6115942  3.7054175  3.7969230  3.8862746  3.9736175  4.0590815\n [25]  4.1427827  4.2248259  4.3053060  4.3843091  4.4619135  4.5381910\n [31]  4.6132075  4.6870236  4.7596949  4.8312733  4.9018066  4.9713392\n [37]  5.0399126  5.1075655  5.1743339  5.2402516  5.3053504  5.3696601\n [43]  5.4332085  5.4960223  5.5581262  5.6195438  5.6802974  5.7404081\n [49]  5.7998957  5.8587794  5.9170772  5.9748061  6.0319826  6.0886222\n [55]  6.1447397  6.2003494  6.2554647  6.3100986  6.3642635  6.4179713\n [61]  6.4712334  6.5240607  6.5764636  6.6284523  6.6800363  6.7312251\n [67]  6.7820275  6.8324522  6.8825075  6.9322013  6.9815415  7.0305353\n [73]  7.0791901  7.1275128  7.1755101  7.2231884  7.2705541  7.3176132\n [79]  7.3643716  7.4108349  7.4570088  7.5028985  7.5485093  7.5938461\n [85]  7.6389138  7.6837172  7.7282609  7.7725493  7.8165867  7.8603775\n [91]  7.9039256  7.9472351  7.9903099  8.0331536  8.0757701  8.1181629\n [97]  8.1603355  8.2022912  8.2440334  8.2855653  8.3268901  8.3680108\n[103]  8.4089304  8.4496519  8.4901780  8.5305116  8.5706554  8.6106121\n[109]  8.6503842  8.6899742  8.7293848  8.7686182  8.8076768  8.8465630\n[115]  8.8852790  8.9238270  8.9622092  9.0004278  9.0384847  9.0763821\n[121]  9.1141219  9.1517060  9.1891365  9.2264151  9.2635437  9.3005240\n[127]  9.3373579  9.3740471  9.4105932  9.4469980  9.4832630  9.5193898\n[133]  9.5553801  9.5912353  9.6269570  9.6625466  9.6980056  9.7333354\n[139]  9.7685375  9.8036131  9.8385637  9.8733906  9.9080951  9.9426784\n[145]  9.9771419 10.0114867 10.0457141 10.0798253 10.1138214 10.1477037\n[151] 10.1814732 10.2151310 10.2486783 10.2821162 10.3154457 10.3486678\n[157] 10.3817836 10.4147941 10.4477004 10.4805033 10.5132038 10.5458030\n[163] 10.5783017 10.6107009 10.6430014 10.6752042 10.7073101 10.7393201\n[169] 10.7712350 10.8030555 10.8347826 10.8664171 10.8979597 10.9294113\n[175] 10.9607727 10.9920446 11.0232277 11.0543229 11.0853309 11.1162524\n[181] 11.1470881 11.1778388\n\n\n\n\n\nThe two figures above show an idea of the short term and the long-term forecasted stock price. The forecasted values is be based on the estimated parameters of the model, which were obtained by fitting it to the historical data. One way to assess the accuracy of the forecasts is to use cross-validation, which involves splitting the data into training and testing sets and comparing the predicted values to the actual values in the test set. This can help to identify any potential problems with the model and to fine-tune the model parameters.\n\n12 Step ahead Cross Validation1 Step ahead Cross Validation\n\n\n\n\nCode\n#a seasonal cross validation using 12 steps ahead forecasts\nfarima1 <- function(x, h){forecast(Arima(x, order=c(0,1,0)), h=h)}\n\n# Compute cross-validated errors for up to 12 steps ahead\ne <- tsCV(myts, forecastfunction = farima1, h = 12)\n\n# Compute the MSE values and remove missing values\nmse <- colMeans(e^2, na.rm = TRUE)\n\n# Plot the MSE values against the forecast horizon\ndata.frame(h = 1:12, MSE = mse) %>%\n  ggplot(aes(x = h, y = MSE)) + geom_point()+geom_line()+ggtitle(\"12 Step Ahead Cross Validation\")\n\n\n\n\n\n\n\n\n\nCode\n#a seasonal cross validation using 1 steps ahead forecasts\nfarima1 <- function(x, h){forecast(Arima(x, order=c(0,1,0)),h=h)}\n\n# Compute cross-validated errors for up to 1 steps ahead\ne <- tsCV(myts, forecastfunction = farima1, h = 1)\nmse <-abs(mean(e,na.rm=TRUE))\n\n# Plot the MSE values against the forecast horizon\ndata.frame(h = 1:12, MSE = mse) %>%\n  ggplot(aes(x = h, y = MSE)) + geom_point()+geom_line()+ggtitle(\"1 Step Ahead Cross Validation\")\n\n\n\n\n\n\n\n\nThe one-step-ahead forecasting plot shows that the MSE seems to be stable. The MSE is tend to rise by 12 steps at each step of cross-validation. Lastly, the above-mentioned ARIMA model should be compared to benchmark methods before being used for a long time.\n\n\nModel Comparison\n\nPlotModel Error Table\n\n\n\n\nCode\nfit <- Arima(myts, order=c(0,1,0))\nautoplot(myts) +\n  autolayer(meanf(myts, h=182),\n            series=\"Mean\", PI=FALSE) +\n  autolayer(naive(myts, h=182),\n            series=\"Naïve\", PI=FALSE) +\n  autolayer(snaive(myts, h=182),\n            series=\"SNaïve\", PI=FALSE)+\n  autolayer(rwf(myts, h=182, drift=TRUE),\n            series=\"Drift\", PI=FALSE)+\n  autolayer(forecast(fit,182), \n            series=\"fit\",PI=FALSE) +\n  guides(colour=guide_legend(title=\"Forecast\"))\n\n\n\n\n\n\n\n\n\nCode\nsummary <- summary(fit)\nsnaive <- snaive(myts, h=182)\naccuracy(snaive)\n\n\n                   ME     RMSE      MAE     MPE     MAPE MASE      ACF1\nTraining set 8.737679 11.45234 9.250671 12.5155 13.20282    1 0.9876419\n\n\nCode\nsummary\n\n\nSeries: myts \nARIMA(0,1,0) \n\nsigma^2 = 0.6876:  log likelihood = -4037.45\nAIC=8076.9   AICc=8076.9   BIC=8083\n\nTraining set error measures:\n                     ME      RMSE      MAE        MPE      MAPE       MASE\nTraining set 0.03341445 0.8291038 0.519495 0.04557335 0.7375729 0.05615755\n                    ACF1\nTraining set -0.07603354\n\n\n\n\n\nError\nModel\nSnavie\n\n\n\n\nME\n0.008081029\n2.821097\n\n\nRMSE\n1.64206\n33.39996\n\n\nMAE\n0.4678367\n20.38998\n\n\nMPE\n-0.03649654\n-16.47367\n\n\nMAPE\n0.8072921\n52.59869\n\n\nMASE\n0.02294444\n1.0000000\n\n\nACF1\n-0.003904598\n0.9973734\n\n\n\n\n\n\nThe ARIMA forecast tracks the actual data points very closely when compared to other model. The other forecast methods are less accurate when compared. From the table, Model error measurements of fit are much lower than snaive method.We can conclude that the fitted model is good."
  },
  {
    "objectID": "arima_sector_industrial.html",
    "href": "arima_sector_industrial.html",
    "title": "ARIMA Model for Industrial Sector Fund",
    "section": "",
    "text": "During exploratory data analysis (EDA), it was observed that the raw data is often non-stationary, exhibiting trends, seasonality, and other complex patterns that can make modeling and forecasting difficult. To address this issue, the data is often differentiated to achieve stationarity, which is a key assumption of the ARIMA model.\nDifferencing involves calculating the difference between each data point and the previous data point, which can help to remove trends and seasonality from the data. The goal is to create a new series of data that is stationary, meaning that the statistical properties of the data do not change over time.\n\nDifferentiated Time Series Plot\n\nPlotACFPACFADF Test\n\n\n\n\nCode\n#import the data\ndf <- read.csv(\"DATA/CLEANED DATA/XLI_raw_data.csv\")\n#convert to time series data\nmyts<-ts(df$XLI.Adjusted,frequency=252,start=c(2010,1,1), end = c(2023,3,1)) \n#First order differentiation\ndf1 <- diff(myts)\n# Plot \nmyts  %>% diff() %>% ggtsdisplay(main = \"First order differentiation\") \n\n\n\n\n\n\n\n\n\nCode\n#ACF plot \nggAcf(df1,60,main=\"ACF Plot: First order differentiation\") \n\n\n\n\n\n\n\n\n\nCode\nggPacf(df1,60,main=\"PACF Plot: First order differentiation\") \n\n\n\n\n\n\n\n\n\nCode\ntseries::adf.test(df1)\n\n\n\n    Augmented Dickey-Fuller Test\n\ndata:  df1\nDickey-Fuller = -14.662, Lag order = 14, p-value = 0.01\nalternative hypothesis: stationary\n\n\n\n\n\nAfter analyzing the ACF and PACF plots, it can be observed that most of the bar lines lie between the blue lines, indicating that the time series is stationary. This has been further confirmed through an Augmented Dickey-Fuller test as the p-value is less than 0.05. Once the data is stationary, the next step is to model it using ARIMA. The combination of ACF and PACF plots can help determine the appropriate values for p, d, and q in the ARIMA model. The order of differencing required to achieve stationarity gives the d value. The p value is determined by examining significant spikes at various lags in the PACF plot, while the q value is determined through significant spikes in the ACF plot. It is important to consider other methods such as grid search and information criteria to ensure the most appropriate model is selected.\nHere the parameters are d = 1 p = 0 (PACF Plot) q = 0 (ACF Plot)\n\n\nModel Selection\n\nARIMA ResultAuto Arima Model\n\n\n\n\nCode\nArima(myts,order=c(0,1,0),include.drift=TRUE) \n\n\nSeries: myts \nARIMA(0,1,0) with drift \n\nCoefficients:\n       drift\n      0.0243\ns.e.  0.0130\n\nsigma^2 = 0.5578:  log likelihood = -3693.98\nAIC=7391.96   AICc=7391.96   BIC=7404.15\n\n\n\n\n\n\nCode\nauto.arima(myts)\n\n\nSeries: myts \nARIMA(2,1,0) with drift \n\nCoefficients:\n          ar1     ar2   drift\n      -0.0363  0.0503  0.0243\ns.e.   0.0174  0.0174  0.0132\n\nsigma^2 = 0.5559:  log likelihood = -3687.43\nAIC=7382.87   AICc=7382.88   BIC=7407.25\n\n\n\n\n\nIn the Model selection, chosen model is the same as the auto.arima generated model. We can proceed with the further model diagnostic for the chosen best model i.e. (0,1,0), to look into the details of the model.\n\n\nModel Diagnostic\n\nModel PlotModel\n\n\n\n\nCode\nmodel_output <- capture.output(sarima(myts,0,1,0))\n\n\n\n\n\n\n\n\n\nCode\ncat(model_output[9:38], model_output[length(model_output)], sep = \"\\n\") \n\n\n$fit\n\nCall:\narima(x = xdata, order = c(p, d, q), seasonal = list(order = c(P, D, Q), period = S), \n    xreg = constant, transform.pars = trans, fixed = fixed, optim.control = list(trace = trc, \n        REPORT = 1, reltol = tol))\n\nCoefficients:\n      constant\n        0.0243\ns.e.    0.0130\n\nsigma^2 estimated as 0.5576:  log likelihood = -3693.98,  aic = 7391.96\n\n$degrees_of_freedom\n[1] 3277\n\n$ttable\n         Estimate    SE t.value p.value\nconstant   0.0243 0.013  1.8638  0.0624\n\n$AIC\n[1] 2.25502\n\n$AICc\n[1] 2.25502\n\n$BIC\n[1] 2.258739\n\n\n\n\n\nThe model diagnostic is done for ARIMA(0,1,0). By analyzing the standardized residuals plot, it can be observed that the mean is close to 0 and the variance is slightly higher than 1. Deviations from these values could indicate poor model fit. However, in this case, the model seems to be well-fitted. The ACF plot of residuals shows very few significant lags, which is a positive sign for the model. The qq-plot also suggests normality in the residuals. Additionally, the p-values for the Ljung-Box test are greater than 0.05, indicating that the residuals are independent, which is favorable for the model’s accuracy.\nThe equation for this model: \\[(1-\\phi_1B-\\phi_2B^2)(1-B)(Y_t-\\mu) = (1+\\theta_1B+\\theta_2B^2)\\epsilon_t\\]\n\n\nForcast\n\nShort Term ForcastLong term Forcast\n\n\n\n\nCode\nmyts %>%\n  Arima(order=c(0,1,0),include.drift = TRUE) %>%\n  forecast %>%\n  autoplot(main = \"Industrial Sector Fund Stock Prices Prediction\") +\n  ylab(\"stock prices\") + xlab(\"Year\")\n\n\n\n\n\n\n\n\n\nCode\nsarima.for(myts,182, 0,1,0, main='Industrial Sector Fund Stock Prices Prediction')\n\n\n\n\n\n$pred\nTime Series:\nStart = c(2023, 4) \nEnd = c(2023, 185) \nFrequency = 252 \n  [1] 101.5876 101.6119 101.6362 101.6605 101.6849 101.7092 101.7335 101.7578\n  [9] 101.7821 101.8064 101.8307 101.8550 101.8793 101.9036 101.9279 101.9522\n [17] 101.9766 102.0009 102.0252 102.0495 102.0738 102.0981 102.1224 102.1467\n [25] 102.1710 102.1953 102.2196 102.2439 102.2683 102.2926 102.3169 102.3412\n [33] 102.3655 102.3898 102.4141 102.4384 102.4627 102.4870 102.5113 102.5356\n [41] 102.5600 102.5843 102.6086 102.6329 102.6572 102.6815 102.7058 102.7301\n [49] 102.7544 102.7787 102.8030 102.8273 102.8517 102.8760 102.9003 102.9246\n [57] 102.9489 102.9732 102.9975 103.0218 103.0461 103.0704 103.0947 103.1191\n [65] 103.1434 103.1677 103.1920 103.2163 103.2406 103.2649 103.2892 103.3135\n [73] 103.3378 103.3621 103.3864 103.4108 103.4351 103.4594 103.4837 103.5080\n [81] 103.5323 103.5566 103.5809 103.6052 103.6295 103.6538 103.6781 103.7025\n [89] 103.7268 103.7511 103.7754 103.7997 103.8240 103.8483 103.8726 103.8969\n [97] 103.9212 103.9455 103.9698 103.9942 104.0185 104.0428 104.0671 104.0914\n[105] 104.1157 104.1400 104.1643 104.1886 104.2129 104.2372 104.2615 104.2859\n[113] 104.3102 104.3345 104.3588 104.3831 104.4074 104.4317 104.4560 104.4803\n[121] 104.5046 104.5289 104.5532 104.5776 104.6019 104.6262 104.6505 104.6748\n[129] 104.6991 104.7234 104.7477 104.7720 104.7963 104.8206 104.8450 104.8693\n[137] 104.8936 104.9179 104.9422 104.9665 104.9908 105.0151 105.0394 105.0637\n[145] 105.0880 105.1123 105.1367 105.1610 105.1853 105.2096 105.2339 105.2582\n[153] 105.2825 105.3068 105.3311 105.3554 105.3797 105.4040 105.4284 105.4527\n[161] 105.4770 105.5013 105.5256 105.5499 105.5742 105.5985 105.6228 105.6471\n[169] 105.6714 105.6957 105.7201 105.7444 105.7687 105.7930 105.8173 105.8416\n[177] 105.8659 105.8902 105.9145 105.9388 105.9631 105.9874\n\n$se\nTime Series:\nStart = c(2023, 4) \nEnd = c(2023, 185) \nFrequency = 252 \n  [1]  0.7467396  1.0560493  1.2933910  1.4934793  1.6697606  1.8291311\n  [7]  1.9756874  2.1120987  2.2402189  2.3613981  2.4766552  2.5867820\n [13]  2.6924081  2.7940439  2.8921102  2.9869586  3.0788864  3.1681480\n [19]  3.2549627  3.3395212  3.4219910  3.5025194  3.5812376  3.6582622\n [25]  3.7336982  3.8076400  3.8801730  3.9513748  4.0213161  4.0900615\n [31]  4.1576704  4.2241974  4.2896927  4.3542030  4.4177713  4.4804379\n [37]  4.5422400  4.6032124  4.6633876  4.7227962  4.7814667  4.8394260\n [43]  4.8966993  4.9533105  5.0092819  5.0646347  5.1193891  5.1735641\n [49]  5.2271775  5.2802467  5.3327878  5.3848162  5.4363467  5.4873933\n [55]  5.5379695  5.5880879  5.6377607  5.6869997  5.7358161  5.7842205\n [61]  5.8322231  5.8798339  5.9270622  5.9739172  6.0204075  6.0665416\n [67]  6.1123275  6.1577729  6.2028854  6.2476721  6.2921401  6.3362960\n [73]  6.3801464  6.4236974  6.4669551  6.5099253  6.5526138  6.5950260\n [79]  6.6371672  6.6790425  6.7206568  6.7620151  6.8031220  6.8439819\n [85]  6.8845994  6.9249786  6.9651238  7.0050388  7.0447278  7.0841943\n [91]  7.1234422  7.1624751  7.2012964  7.2399095  7.2783178  7.3165244\n [97]  7.3545326  7.3923454  7.4299657  7.4673965  7.5046406  7.5417008\n[103]  7.5785797  7.6152801  7.6518044  7.6881552  7.7243350  7.7603461\n[109]  7.7961908  7.8318715  7.8673904  7.9027496  7.9379514  7.9729977\n[115]  8.0078906  8.0426322  8.0772243  8.1116689  8.1459679  8.1801230\n[121]  8.2141361  8.2480090  8.2817433  8.3153408  8.3488031  8.3821318\n[127]  8.4153285  8.4483947  8.4813321  8.5141420  8.5468259  8.5793854\n[133]  8.6118217  8.6441364  8.6763307  8.7084059  8.7403635  8.7722046\n[139]  8.8039306  8.8355427  8.8670420  8.8984299  8.9297074  8.9608758\n[145]  8.9919361  9.0228895  9.0537371  9.0844799  9.1151191  9.1456556\n[151]  9.1760904  9.2064247  9.2366593  9.2667953  9.2968337  9.3267752\n[157]  9.3566210  9.3863718  9.4160287  9.4455924  9.4750639  9.5044441\n[163]  9.5337336  9.5629335  9.5920445  9.6210674  9.6500030  9.6788521\n[169]  9.7076154  9.7362938  9.7648880  9.7933987  9.8218266  9.8501725\n[175]  9.8784370  9.9066209  9.9347249  9.9627495  9.9906956 10.0185637\n[181] 10.0463545 10.0740686\n\n\n\n\n\nThe two figures above show an idea of the short term and the long-term forecasted stock price. The forecasted values is be based on the estimated parameters of the model, which were obtained by fitting it to the historical data. One way to assess the accuracy of the forecasts is to use cross-validation, which involves splitting the data into training and testing sets and comparing the predicted values to the actual values in the test set. This can help to identify any potential problems with the model and to fine-tune the model parameters.\n\n12 Step ahead Cross Validation1 Step ahead Cross Validation\n\n\n\n\nCode\n#a seasonal cross validation using 12 steps ahead forecasts\nfarima1 <- function(x, h){forecast(Arima(x, order=c(0,1,0)), h=h)}\n\n# Compute cross-validated errors for up to 12 steps ahead\ne <- tsCV(myts, forecastfunction = farima1, h = 12)\n\n# Compute the MSE values and remove missing values\nmse <- colMeans(e^2, na.rm = TRUE)\n\n# Plot the MSE values against the forecast horizon\ndata.frame(h = 1:12, MSE = mse) %>%\n  ggplot(aes(x = h, y = MSE)) + geom_point()+geom_line()+ggtitle(\"12 Step Ahead Cross Validation\")\n\n\n\n\n\n\n\n\n\nCode\n#a seasonal cross validation using 1 steps ahead forecasts\nfarima1 <- function(x, h){forecast(Arima(x, order=c(0,1,0)),h=h)}\n\n# Compute cross-validated errors for up to 1 steps ahead\ne <- tsCV(myts, forecastfunction = farima1, h = 1)\nmse <-abs(mean(e,na.rm=TRUE))\n\n# Plot the MSE values against the forecast horizon\ndata.frame(h = 1:12, MSE = mse) %>%\n  ggplot(aes(x = h, y = MSE)) + geom_point()+geom_line()+ggtitle(\"1 Step Ahead Cross Validation\")\n\n\n\n\n\n\n\n\nThe one-step-ahead forecasting plot shows that the MSE seems to be stable. The MSE is tend to rise by 12 steps at each step of cross-validation. Lastly, the above-mentioned ARIMA model should be compared to benchmark methods before being used for a long time.\n\n\nModel Comparison\n\nPlotModel Error Table\n\n\n\n\nCode\nfit <- Arima(myts, order=c(0,1,0))\nautoplot(myts) +\n  autolayer(meanf(myts, h=182),\n            series=\"Mean\", PI=FALSE) +\n  autolayer(naive(myts, h=182),\n            series=\"Naïve\", PI=FALSE) +\n  autolayer(snaive(myts, h=182),\n            series=\"SNaïve\", PI=FALSE)+\n  autolayer(rwf(myts, h=182, drift=TRUE),\n            series=\"Drift\", PI=FALSE)+\n  autolayer(forecast(fit,182), \n            series=\"fit\",PI=FALSE) +\n  guides(colour=guide_legend(title=\"Forecast\"))\n\n\n\n\n\n\n\n\n\nCode\nsummary <- summary(fit)\nsnaive <- snaive(myts, h=182)\naccuracy(snaive)\n\n\n                   ME     RMSE      MAE      MPE     MAPE MASE      ACF1\nTraining set 5.859939 10.92158 7.954268 9.956243 13.12541    1 0.9937622\n\n\nCode\nsummary\n\n\nSeries: myts \nARIMA(0,1,0) \n\nsigma^2 = 0.5582:  log likelihood = -3695.71\nAIC=7393.43   AICc=7393.43   BIC=7399.52\n\nTraining set error measures:\n                     ME      RMSE       MAE        MPE      MAPE       MASE\nTraining set 0.02430772 0.7470214 0.4768783 0.03874199 0.8629878 0.05995251\n                    ACF1\nTraining set -0.03821481\n\n\n\n\n\nError\nModel\nSnavie\n\n\n\n\nME\n0.004904666\n1.759821\n\n\nRMSE\n1.317807\n23.47883\n\n\nMAE\n0.4339241\n14.15936\n\n\nMPE\n-0.03607689\n-9.529885\n\n\nMAPE\n0.931558\n38.60253\n\n\nMASE\n0.03064573\n1.0000000\n\n\nACF1\n-0.006843027\n0.9965928\n\n\n\n\n\n\nThe ARIMA fitted forecast and snaive tracks the actual data points very closely when compared to other model. The other forecast methods are less accurate when compared. From the table, Model error measurements of fit are much lower than snaive method.We can conclude that the fitted model is good."
  },
  {
    "objectID": "arima_sector_market.html",
    "href": "arima_sector_market.html",
    "title": "ARIMA Model for Sector Market",
    "section": "",
    "text": "The ARIMA model is a commonly used time series analysis technique for forecasting the performance of sector markets. However, the raw data is often non-stationary, requiring differentiation to achieve stationarity, a key assumption of the model.\nWith the appropriate order of differencing and seasonality identified using statistical tests, the ARIMA model can be fitted to the data to identify the appropriate AR, I, and MA parameters. It may also be necessary to incorporate relevant exogenous variables into the model to improve forecast accuracy. Overall, the ARIMA model is a powerful tool for modeling and forecasting sector markets, allowing analysts to identify unique patterns and trends and make more accurate predictions about future performance.\nClick to view ARIMA Page for Consumer Staples Sector Fund\nClick to view ARIMA Page for Utilities Sector Fund\nClick to view ARIMA Page for Health Care Sector Fund\nClick to view ARIMA Page for Industrial Sector Fund\nClick to view ARIMA Page for Financial Sector Fund\nClick to view ARIMA Page for Consumer Discretionary Sector Fund\nClick to view ARIMA Page for Communication Services Sector Fund\nClick to view ARIMA Page for Real Estate Sector Fund\nClick to view ARIMA Page for Materials Sector Fund\nClick to view ARIMA Page for Technology Sector Fund\nClick to view ARIMA Page for Energy Sector Fund"
  },
  {
    "objectID": "arima_sector_materials_sector.html",
    "href": "arima_sector_materials_sector.html",
    "title": "ARIMA Model for Materials Sector Fund",
    "section": "",
    "text": "During exploratory data analysis (EDA), it was observed that the raw data is often non-stationary, exhibiting trends, seasonality, and other complex patterns that can make modeling and forecasting difficult. To address this issue, the data is often differentiated to achieve stationarity, which is a key assumption of the ARIMA model.\nDifferencing involves calculating the difference between each data point and the previous data point, which can help to remove trends and seasonality from the data. The goal is to create a new series of data that is stationary, meaning that the statistical properties of the data do not change over time.\n\nDifferentiated Time Series Plot\n\nPlotACFPACFADF Test\n\n\n\n\nCode\n#import the data\ndf <- read.csv(\"DATA/CLEANED DATA/XLB_raw_data.csv\")\n#convert to time series data\nmyts<-ts(df$XLB.Adjusted,frequency=252,start=c(2010,1,1), end = c(2023,3,1)) \n#First order differentiation\ndf1 <- diff(myts)\n# Plot \nmyts  %>% diff() %>% ggtsdisplay(main = \"First order differentiation\") \n\n\n\n\n\n\n\n\n\nCode\n#ACF plot \nggAcf(df1,60,main=\"ACF Plot: First order differentiation\") \n\n\n\n\n\n\n\n\n\nCode\nggPacf(df1,60,main=\"PACF Plot: First order differentiation\") \n\n\n\n\n\n\n\n\n\nCode\ntseries::adf.test(df1)\n\n\n\n    Augmented Dickey-Fuller Test\n\ndata:  df1\nDickey-Fuller = -15.19, Lag order = 14, p-value = 0.01\nalternative hypothesis: stationary\n\n\n\n\n\nAfter analyzing the ACF and PACF plots, it can be observed that most of the bar lines lie between the blue lines, indicating that the time series is stationary. This has been further confirmed through an Augmented Dickey-Fuller test as the p-value is less than 0.05. Once the data is stationary, the next step is to model it using ARIMA. The combination of ACF and PACF plots can help determine the appropriate values for p, d, and q in the ARIMA model. The order of differencing required to achieve stationarity gives the d value. The p value is determined by examining significant spikes at various lags in the PACF plot, while the q value is determined through significant spikes in the ACF plot. It is important to consider other methods such as grid search and information criteria to ensure the most appropriate model is selected.\nHere the parameters are d = 1 p = 0(PACF Plot) q = 0 (ACF Plot)\n\n\nModel Selection\n\nARIMA(0,1,0) ResultAuto Arima Model\n\n\n\n\nCode\nArima(myts,order=c(0,1,0),include.drift=TRUE) \n\n\nSeries: myts \nARIMA(0,1,0) with drift \n\nCoefficients:\n       drift\n      0.0174\ns.e.  0.0114\n\nsigma^2 = 0.423:  log likelihood = -3240.44\nAIC=6484.88   AICc=6484.89   BIC=6497.07\n\n\n\n\n\n\nCode\nauto.arima(myts)\n\n\nSeries: myts \nARIMA(0,1,0) with drift \n\nCoefficients:\n       drift\n      0.0174\ns.e.  0.0114\n\nsigma^2 = 0.423:  log likelihood = -3240.44\nAIC=6484.88   AICc=6484.89   BIC=6497.07\n\n\n\n\n\nIn the Model selection, the chosen model and the suto.arima generated model are the same i.e.(0,1,0). So this is the best model for th series. We can proceed with the further model diagnostic for the chosen best model i.e. (0,1,0), to look into the details of the model.\n\n\nModel Diagnostic\n\nModel PlotModel\n\n\n\n\nCode\nmodel_output <- capture.output(sarima(myts,0,1,0))\n\n\n\n\n\n\n\n\n\nCode\ncat(model_output[9:38], model_output[length(model_output)], sep = \"\\n\") \n\n\n$fit\n\nCall:\narima(x = xdata, order = c(p, d, q), seasonal = list(order = c(P, D, Q), period = S), \n    xreg = constant, transform.pars = trans, fixed = fixed, optim.control = list(trace = trc, \n        REPORT = 1, reltol = tol))\n\nCoefficients:\n      constant\n        0.0174\ns.e.    0.0114\n\nsigma^2 estimated as 0.4228:  log likelihood = -3240.44,  aic = 6484.88\n\n$degrees_of_freedom\n[1] 3277\n\n$ttable\n         Estimate     SE t.value p.value\nconstant   0.0174 0.0114  1.5343   0.125\n\n$AIC\n[1] 1.978305\n\n$AICc\n[1] 1.978305\n\n$BIC\n[1] 1.982023\n\n\n\n\n\nThe model diagnostic is done for ARIMA(0,1,0). By analyzing the standardized residuals plot, it can be observed that the mean is close to 0 and the variance is slightly higher than 1. Deviations from these values could indicate poor model fit. However, in this case, the model seems to be well-fitted. The ACF plot of residuals shows very few significant lags, which is a positive sign for the model. The qq-plot also suggests normality in the residuals. Additionally, the p-values for the Ljung-Box test are greater than 0.05, indicating that the residuals are independent, which is favorable for the model’s accuracy.\nThe equation for this model: \\[(1-\\phi_1B-\\phi_2B^2)(1-B)(Y_t-\\mu) = (1+\\theta_1B+\\theta_2B^2)\\epsilon_t\\]\n\n\nForcast\n\nShort Term ForcastLong term Forcast\n\n\n\n\nCode\nmyts %>%\n  Arima(order=c(0,1,0),include.drift = TRUE) %>%\n  forecast %>%\n  autoplot(main = \"Materials Sector Fund Stock Prices Prediction\") +\n  ylab(\"stock prices\") + xlab(\"Year\")\n\n\n\n\n\n\n\n\n\nCode\nsarima.for(myts,182, 0,1,0, main='Materials Sector Fund Stock Prices Prediction')\n\n\n\n\n\n$pred\nTime Series:\nStart = c(2023, 4) \nEnd = c(2023, 185) \nFrequency = 252 \n  [1] 82.62812 82.64555 82.66297 82.68040 82.69782 82.71525 82.73268 82.75010\n  [9] 82.76753 82.78495 82.80238 82.81980 82.83723 82.85466 82.87208 82.88951\n [17] 82.90693 82.92436 82.94179 82.95921 82.97664 82.99406 83.01149 83.02891\n [25] 83.04634 83.06377 83.08119 83.09862 83.11604 83.13347 83.15089 83.16832\n [33] 83.18575 83.20317 83.22060 83.23802 83.25545 83.27288 83.29030 83.30773\n [41] 83.32515 83.34258 83.36000 83.37743 83.39486 83.41228 83.42971 83.44713\n [49] 83.46456 83.48198 83.49941 83.51684 83.53426 83.55169 83.56911 83.58654\n [57] 83.60397 83.62139 83.63882 83.65624 83.67367 83.69109 83.70852 83.72595\n [65] 83.74337 83.76080 83.77822 83.79565 83.81308 83.83050 83.84793 83.86535\n [73] 83.88278 83.90020 83.91763 83.93506 83.95248 83.96991 83.98733 84.00476\n [81] 84.02218 84.03961 84.05704 84.07446 84.09189 84.10931 84.12674 84.14417\n [89] 84.16159 84.17902 84.19644 84.21387 84.23129 84.24872 84.26615 84.28357\n [97] 84.30100 84.31842 84.33585 84.35327 84.37070 84.38813 84.40555 84.42298\n[105] 84.44040 84.45783 84.47526 84.49268 84.51011 84.52753 84.54496 84.56238\n[113] 84.57981 84.59724 84.61466 84.63209 84.64951 84.66694 84.68437 84.70179\n[121] 84.71922 84.73664 84.75407 84.77149 84.78892 84.80635 84.82377 84.84120\n[129] 84.85862 84.87605 84.89347 84.91090 84.92833 84.94575 84.96318 84.98060\n[137] 84.99803 85.01546 85.03288 85.05031 85.06773 85.08516 85.10258 85.12001\n[145] 85.13744 85.15486 85.17229 85.18971 85.20714 85.22456 85.24199 85.25942\n[153] 85.27684 85.29427 85.31169 85.32912 85.34655 85.36397 85.38140 85.39882\n[161] 85.41625 85.43367 85.45110 85.46853 85.48595 85.50338 85.52080 85.53823\n[169] 85.55565 85.57308 85.59051 85.60793 85.62536 85.64278 85.66021 85.67764\n[177] 85.69506 85.71249 85.72991 85.74734 85.76476 85.78219\n\n$se\nTime Series:\nStart = c(2023, 4) \nEnd = c(2023, 185) \nFrequency = 252 \n  [1] 0.6502513 0.9195943 1.1262684 1.3005027 1.4540062 1.5927840 1.7204033\n  [8] 1.8391885 1.9507540 2.0562753 2.1566397 2.2525367 2.3445146 2.4330177\n [15] 2.5184126 2.6010054 2.6810550 2.7587828 2.8343799 2.9080124 2.9798260\n [22] 3.0499491 3.1184959 3.1855680 3.2512567 3.3156443 3.3788051 3.4408067\n [29] 3.5017107 3.5615733 3.6204463 3.6783771 3.7354096 3.7915843 3.8469388\n [36] 3.9015081 3.9553245 4.0084185 4.0608183 4.1125506 4.1636401 4.2141103\n [43] 4.2639832 4.3132795 4.3620186 4.4102192 4.4578986 4.5050735 4.5517594\n [50] 4.5979713 4.6437234 4.6890291 4.7339012 4.7783520 4.8223930 4.8660355\n [57] 4.9092900 4.9521667 4.9946753 5.0368252 5.0786253 5.1200842 5.1612100\n [64] 5.2020107 5.2424939 5.2826669 5.3225366 5.3621099 5.4013933 5.4403931\n [71] 5.4791152 5.5175656 5.5557499 5.5936736 5.6313418 5.6687598 5.7059324\n [78] 5.7428644 5.7795604 5.8160248 5.8522621 5.8882764 5.9240717 5.9596520\n [85] 5.9950212 6.0301829 6.0651408 6.0998983 6.1344589 6.1688259 6.2030025\n [92] 6.2369918 6.2707969 6.3044207 6.3378661 6.3711360 6.4042330 6.4371599\n [99] 6.4699192 6.5025134 6.5349451 6.5672167 6.5993304 6.6312886 6.6630935\n[106] 6.6947473 6.7262522 6.7576102 6.7888233 6.8198936 6.8508230 6.8816134\n[113] 6.9122666 6.9427845 6.9731688 7.0034213 7.0335437 7.0635376 7.0934047\n[120] 7.1231466 7.1527648 7.1822608 7.2116363 7.2408925 7.2700310 7.2990532\n[127] 7.3279605 7.3567542 7.3854356 7.4140060 7.4424668 7.4708192 7.4990643\n[134] 7.5272035 7.5552379 7.5831686 7.6109968 7.6387237 7.6663503 7.6938777\n[141] 7.7213069 7.7486390 7.7758751 7.8030161 7.8300631 7.8570169 7.8838786\n[148] 7.9106490 7.9373292 7.9639200 7.9904223 8.0168370 8.0431649 8.0694070\n[155] 8.0955639 8.1216367 8.1476260 8.1735326 8.1993575 8.2251012 8.2507646\n[162] 8.2763484 8.3018534 8.3272803 8.3526297 8.3779025 8.4030993 8.4282207\n[169] 8.4532675 8.4782402 8.5031397 8.5279664 8.5527211 8.5774043 8.6020167\n[176] 8.6265589 8.6510315 8.6754350 8.6997701 8.7240372 8.7482371 8.7723702\n\n\n\n\n\nThe two figures above show an idea of the short term and the long-term forecasted stock price. The forecasted values is be based on the estimated parameters of the model, which were obtained by fitting it to the historical data. One way to assess the accuracy of the forecasts is to use cross-validation, which involves splitting the data into training and testing sets and comparing the predicted values to the actual values in the test set. This can help to identify any potential problems with the model and to fine-tune the model parameters.\n\n12 Step ahead Cross Validation1 Step ahead Cross Validation\n\n\n\n\nCode\n#a seasonal cross validation using 12 steps ahead forecasts\nfarima1 <- function(x, h){forecast(Arima(x, order=c(0,1,0)), h=h)}\n\n# Compute cross-validated errors for up to 12 steps ahead\ne <- tsCV(myts, forecastfunction = farima1, h = 12)\n\n# Compute the MSE values and remove missing values\nmse <- colMeans(e^2, na.rm = TRUE)\n\n# Plot the MSE values against the forecast horizon\ndata.frame(h = 1:12, MSE = mse) %>%\n  ggplot(aes(x = h, y = MSE)) + geom_point()+geom_line()+ggtitle(\"12 Step Ahead Cross Validation\")\n\n\n\n\n\n\n\n\n\nCode\n#a seasonal cross validation using 1 steps ahead forecasts\nfarima1 <- function(x, h){forecast(Arima(x, order=c(0,1,0)),h=h)}\n\n# Compute cross-validated errors for up to 1 steps ahead\ne <- tsCV(myts, forecastfunction = farima1, h = 1)\nmse <-abs(mean(e,na.rm=TRUE))\n\n# Plot the MSE values against the forecast horizon\ndata.frame(h = 1:12, MSE = mse) %>%\n  ggplot(aes(x = h, y = MSE)) + geom_point()+geom_line()+ggtitle(\"1 Step Ahead Cross Validation\")\n\n\n\n\n\n\n\n\nThe one-step-ahead forecasting plot shows that the MSE seems to be stable. The MSE is tend to rise by 12 steps at each step of cross-validation. Lastly, the above-mentioned ARIMA model should be compared to benchmark methods before being used for a long time.\n\n\nModel Comparison\n\nPlotModel Error Table\n\n\n\n\nCode\nfit <- Arima(myts, order=c(0,1,0))\nautoplot(myts) +\n  autolayer(meanf(myts, h=182),\n            series=\"Mean\", PI=FALSE) +\n  autolayer(naive(myts, h=182),\n            series=\"Naïve\", PI=FALSE) +\n  autolayer(snaive(myts, h=182),\n            series=\"SNaïve\", PI=FALSE)+\n  autolayer(rwf(myts, h=182, drift=TRUE),\n            series=\"Drift\", PI=FALSE)+\n  autolayer(forecast(fit,182), \n            series=\"fit\",PI=FALSE) +\n  guides(colour=guide_legend(title=\"Forecast\"))\n\n\n\n\n\n\n\n\n\nCode\nsummary <- summary(fit)\nsnaive <- snaive(myts, h=182)\naccuracy(snaive)\n\n\n                   ME     RMSE      MAE      MPE     MAPE MASE      ACF1\nTraining set 4.453465 9.051279 6.697518 8.128289 13.12814    1 0.9936968\n\n\nCode\nsummary\n\n\nSeries: myts \nARIMA(0,1,0) \n\nsigma^2 = 0.4231:  log likelihood = -3241.62\nAIC=6485.24   AICc=6485.24   BIC=6491.33\n\nTraining set error measures:\n                     ME      RMSE       MAE        MPE      MAPE       MASE\nTraining set 0.01742826 0.6503857 0.4410917 0.02656753 0.9608813 0.06585898\n                    ACF1\nTraining set -0.02017692\n\n\n\n\n\nError\nModel\nSnavie\n\n\n\n\nME\n0.002368649\n1.198017\n\n\nRMSE\n1.001375\n18.26109\n\n\nMAE\n0.4089937\n11.13495\n\n\nMPE\n-0.02319761\n-5.976606\n\n\nMAPE\n1.002138\n30.78121\n\n\nMASE\n0.03673064\n1.0000000\n\n\nACF1\n-0.01059568\n0.9967933\n\n\n\n\n\n\nThe ARIMA forecast tracks the actual data points very closely when compared to other model. The other forecast methods are less accurate when compared. From the table, Model error measurements of fit are much lower than snaive method.We can conclude that the fitted model is good."
  },
  {
    "objectID": "arima_sector_real_estate.html",
    "href": "arima_sector_real_estate.html",
    "title": "ARIMA Model for Real Estate Sector Fund",
    "section": "",
    "text": "During exploratory data analysis (EDA), it was observed that the raw data is often non-stationary, exhibiting trends, seasonality, and other complex patterns that can make modeling and forecasting difficult. To address this issue, the data is often differentiated to achieve stationarity, which is a key assumption of the ARIMA model.\nDifferencing involves calculating the difference between each data point and the previous data point, which can help to remove trends and seasonality from the data. The goal is to create a new series of data that is stationary, meaning that the statistical properties of the data do not change over time.\n\nDifferentiated Time Series Plot\n\nPlotACFPACFADF Test\n\n\n\n\nCode\n#import the data\ndf <- read.csv(\"DATA/CLEANED DATA/XLRE_raw_data.csv\")\n#convert to time series data\nmyts<-ts(df$XLRE.Adjusted,frequency=252,start=c(2016,1,1), end = c(2023,3,1)) \n#First order differentiation\ndf1 <- diff(myts)\n# Plot \nmyts  %>% diff() %>% ggtsdisplay(main = \"First order differentiation\") \n\n\n\n\n\n\n\n\n\nCode\n#ACF plot \nggAcf(df1,60,main=\"ACF Plot: First order differentiation\") \n\n\n\n\n\n\n\n\n\nCode\nggPacf(df1,60,main=\"PACF Plot: First order differentiation\") \n\n\n\n\n\n\n\n\n\nCode\ntseries::adf.test(df1)\n\n\n\n    Augmented Dickey-Fuller Test\n\ndata:  df1\nDickey-Fuller = -11.999, Lag order = 12, p-value = 0.01\nalternative hypothesis: stationary\n\n\n\n\n\nAfter analyzing the ACF and PACF plots, it can be observed that most of the bar lines lie between the blue lines, indicating that the time series is stationary. This has been further confirmed through an Augmented Dickey-Fuller test as the p-value is less than 0.05. Once the data is stationary, the next step is to model it using ARIMA. The combination of ACF and PACF plots can help determine the appropriate values for p, d, and q in the ARIMA model. The order of differencing required to achieve stationarity gives the d value. The p value is determined by examining significant spikes at various lags in the PACF plot, while the q value is determined through significant spikes in the ACF plot. It is important to consider other methods such as grid search and information criteria to ensure the most appropriate model is selected.\nHere the parameters are d = 1 p = 0,1,2,3,4 (PACF Plot) q = 0,1,2,3 (ACF Plot)\n\n\nModel Selection\n\nARIMA ResultAuto Arima Model\n\n\n\n\nCode\n######################## Check for different combinations ########\nd=1\ni=1\ntemp = data.frame()\nls=matrix(rep(NA,6*20),nrow=20) #nrow = 5x4x1\n\n\nfor (p in 1:5)# p=0,1,2,3,4 :5\n{\n  for(q in 1:4)# q=0,1,2,3 :4\n  {\n    for(d in 1)# d=1 :1\n    {\n      \n      if(p-1+d+q-1<=8)\n      {\n        \n        model<- Arima(myts,order=c(p-1,d,q-1),include.drift=TRUE) \n        ls[i,]= c(p-1,d,q-1,model$aic,model$bic,model$aicc)\n        i=i+1\n        \n      }\n      \n    }\n  }\n}\n\nmodel = as.data.frame(ls)\nnames(model)= c(\"p\",\"d\",\"q\",\"AIC\",\"BIC\",\"AICc\")\n\n#temp\nknitr::kable(model)\n\n\n\n\n\np\nd\nq\nAIC\nBIC\nAICc\n\n\n\n\n0\n1\n0\n2194.858\n2205.811\n2194.865\n\n\n0\n1\n1\n2190.233\n2206.663\n2190.247\n\n\n0\n1\n2\n2181.028\n2202.933\n2181.050\n\n\n0\n1\n3\n2182.855\n2210.237\n2182.889\n\n\n1\n1\n0\n2189.336\n2205.766\n2189.350\n\n\n1\n1\n1\n2189.154\n2211.060\n2189.177\n\n\n1\n1\n2\n2182.979\n2210.362\n2183.013\n\n\n1\n1\n3\n2142.234\n2175.093\n2142.282\n\n\n2\n1\n0\n2184.102\n2206.008\n2184.125\n\n\n2\n1\n1\n2185.362\n2212.744\n2185.396\n\n\n2\n1\n2\n2162.848\n2195.707\n2162.896\n\n\n2\n1\n3\n2133.509\n2171.845\n2133.573\n\n\n3\n1\n0\n2182.933\n2210.316\n2182.967\n\n\n3\n1\n1\n2136.175\n2169.034\n2136.223\n\n\n3\n1\n2\n2164.716\n2203.051\n2164.780\n\n\n3\n1\n3\n2166.574\n2210.386\n2166.656\n\n\n4\n1\n0\n2161.678\n2194.537\n2161.726\n\n\n4\n1\n1\n2132.067\n2170.403\n2132.131\n\n\n4\n1\n2\n2133.372\n2177.183\n2133.453\n\n\n4\n1\n3\n2134.925\n2184.213\n2135.027\n\n\n\n\n\n\n\n\n\nCode\nauto.arima(myts)\n\n\nError in polyroot(c(1, testvec)) : root finding code failed\n\n\nSeries: myts \nARIMA(4,1,1) \n\nCoefficients:\n          ar1     ar2     ar3      ar4     ma1\n      -0.8638  0.0224  0.0905  -0.0666  0.8301\ns.e.   0.0477  0.0315  0.0315   0.0267  0.0422\n\nsigma^2 = 0.1949:  log likelihood = -1059.31\nAIC=2130.62   AICc=2130.67   BIC=2163.48\n\n\n\n\n\nThe given table provides different ARIMA models with their corresponding AIC and BIC values. By sorting the table according to each criterion, we can identify the models with the lowest values. The smallest AIC and BIC value corresponds to an ARIMA (3,1,1) mode. Additionally, the auto.arima function in R suggests an ARIMA (4,1,1) model as the best fit for the data. Since there are different models to choose from, it is important to perform model diagnostics to determine the best model for the data. These diagnostics can include checking for residual normality, checking for autocorrelation and partial autocorrelation in the residuals, and comparing the forecasted values to the actual values. By conducting these tests, we can determine which model provides the best fit for the data and is therefore the most appropriate to use for forecasting future values.\n\n\nModel Diagnostic\n\nModel 1 PlotModel 1Model 2 PlotModel 2\n\n\n\n\nCode\nmodel_output <- capture.output(sarima(myts,3,1,1))\n\n\n\n\n\n\n\n\n\nCode\ncat(model_output[46:79], model_output[length(model_output)], sep = \"\\n\") \n\n\nconverged\n$fit\n\nCall:\narima(x = xdata, order = c(p, d, q), seasonal = list(order = c(P, D, Q), period = S), \n    xreg = constant, transform.pars = trans, fixed = fixed, optim.control = list(trace = trc, \n        REPORT = 1, reltol = tol))\n\nCoefficients:\n          ar1     ar2     ar3     ma1  constant\n      -0.9175  0.0169  0.1407  0.8763    0.0077\ns.e.   0.0376  0.0322  0.0246  0.0307    0.0112\n\nsigma^2 estimated as 0.1949:  log likelihood = -1062.09,  aic = 2136.18\n\n$degrees_of_freedom\n[1] 1761\n\n$ttable\n         Estimate     SE  t.value p.value\nar1       -0.9175 0.0376 -24.4338  0.0000\nar2        0.0169 0.0322   0.5249  0.5997\nar3        0.1407 0.0246   5.7192  0.0000\nma1        0.8763 0.0307  28.5716  0.0000\nconstant   0.0077 0.0112   0.6904  0.4900\n\n$AIC\n[1] 1.209612\n\n$AICc\n[1] 1.209632\n\n$BIC\n[1] 1.228219\n\n\n\n\n\n\nCode\nmodel_output <- capture.output(sarima(myts,4,1,1))\n\n\n\n\n\n\n\n\n\nCode\ncat(model_output[53:86], model_output[length(model_output)], sep = \"\\n\") \n\n\ns.e.   0.0478  0.0315  0.0315   0.0267  0.0422    0.0105\n\nsigma^2 estimated as 0.1942:  log likelihood = -1059.03,  aic = 2132.07\n\n$degrees_of_freedom\n[1] 1760\n\n$ttable\n         Estimate     SE  t.value p.value\nar1       -0.8639 0.0478 -18.0922  0.0000\nar2        0.0219 0.0315   0.6930  0.4884\nar3        0.0899 0.0315   2.8502  0.0044\nar4       -0.0670 0.0267  -2.5066  0.0123\nma1        0.8299 0.0422  19.6470  0.0000\nconstant   0.0079 0.0105   0.7463  0.4556\n\n$AIC\n[1] 1.207286\n\n$AICc\n[1] 1.207313\n\n$BIC\n[1] 1.228994\n\nNA\nNA\nNA\nNA\nNA\nNA\nNA\nNA\nNA\n\n\n\n\n\nTo determine which model diagnosis is the best, we need to consider some key factors such as the coefficient estimates and standard errors, AIC, BIC, p-values, and the log-likelihood. Comparing the two models, model 1 is the best because it has the lowest BIC and AIC value when compared and the AIC value are almost the same, which indicate better goodness of fit. Also, all the coefficient estimates in the model are significant, as evidenced by the p-values are better than model 2, which implies that there is a strong correlation between the variables. The t-values are high, indicating that the estimates are highly reliable.\nTherefore, based on the AIC and BIC values, the significance of the coefficient estimates, and the reliability of the estimates, we can conclude that model 1 is the best model diagnosis. Therefore, (3,1,1) is the best model for this time series.\nThe best model identified is ARIMA(3,1,1). By analyzing the standardized residuals plot, it can be observed that the mean is close to 0 and the variance is slightly higher than 1. Deviations from these values could indicate poor model fit. However, in this case, the model seems to be well-fitted. The ACF plot of residuals shows very few significant lags, which is a positive sign for the model. The qq-plot also suggests normality in the residuals. Additionally, the p-values for the Ljung-Box test are greater than 0.05, indicating that the residuals are independent, which is favorable for the model’s accuracy.\nThe equation for this model: \\[(1-\\phi_1B-\\phi_2B^2-\\phi_3B^3)(Yt - \\mu) = (1+\\theta_1B)\\epsilon_t\\]\n\n\nForcast\n\nShort Term ForcastLong term Forcast\n\n\n\n\nCode\nmyts %>%\n  Arima(order=c(3,1,1),include.drift = TRUE) %>%\n  forecast %>%\n  autoplot(main = \"Real Estate Sector Fund Stock Prices Prediction\") +\n  ylab(\"stock prices\") + xlab(\"Year\")\n\n\n\n\n\n\n\n\n\nCode\nsarima.for(myts,182, 3,1,1, main='Real Estate Sector Fund Stock Prices Prediction')\n\n\n\n\n\n$pred\nTime Series:\nStart = c(2023, 4) \nEnd = c(2023, 185) \nFrequency = 252 \n  [1] 37.53849 37.75075 37.56591 37.74510 37.62103 37.72551 37.66637 37.71855\n  [9] 37.69797 37.72303 37.72064 37.73397 37.73883 37.74787 37.75514 37.76291\n [17] 37.77079 37.77833 37.78624 37.79382 37.80167 37.80932 37.81711 37.82481\n [25] 37.83256 37.84028 37.84802 37.85575 37.86349 37.87122 37.87895 37.88668\n [33] 37.89442 37.90215 37.90988 37.91761 37.92535 37.93308 37.94081 37.94854\n [41] 37.95628 37.96401 37.97174 37.97947 37.98721 37.99494 38.00267 38.01040\n [49] 38.01814 38.02587 38.03360 38.04133 38.04907 38.05680 38.06453 38.07226\n [57] 38.08000 38.08773 38.09546 38.10320 38.11093 38.11866 38.12639 38.13413\n [65] 38.14186 38.14959 38.15732 38.16506 38.17279 38.18052 38.18825 38.19599\n [73] 38.20372 38.21145 38.21918 38.22692 38.23465 38.24238 38.25011 38.25785\n [81] 38.26558 38.27331 38.28104 38.28878 38.29651 38.30424 38.31198 38.31971\n [89] 38.32744 38.33517 38.34291 38.35064 38.35837 38.36610 38.37384 38.38157\n [97] 38.38930 38.39703 38.40477 38.41250 38.42023 38.42796 38.43570 38.44343\n[105] 38.45116 38.45889 38.46663 38.47436 38.48209 38.48982 38.49756 38.50529\n[113] 38.51302 38.52076 38.52849 38.53622 38.54395 38.55169 38.55942 38.56715\n[121] 38.57488 38.58262 38.59035 38.59808 38.60581 38.61355 38.62128 38.62901\n[129] 38.63674 38.64448 38.65221 38.65994 38.66767 38.67541 38.68314 38.69087\n[137] 38.69860 38.70634 38.71407 38.72180 38.72954 38.73727 38.74500 38.75273\n[145] 38.76047 38.76820 38.77593 38.78366 38.79140 38.79913 38.80686 38.81459\n[153] 38.82233 38.83006 38.83779 38.84552 38.85326 38.86099 38.86872 38.87645\n[161] 38.88419 38.89192 38.89965 38.90738 38.91512 38.92285 38.93058 38.93832\n[169] 38.94605 38.95378 38.96151 38.96925 38.97698 38.98471 38.99244 39.00018\n[177] 39.00791 39.01564 39.02337 39.03111 39.03884 39.04657\n\n$se\nTime Series:\nStart = c(2023, 4) \nEnd = c(2023, 185) \nFrequency = 252 \n  [1] 0.4414966 0.6116593 0.7578616 0.9009080 1.0064143 1.1184884 1.2081614\n  [8] 1.3002696 1.3804965 1.4600240 1.5331314 1.6042862 1.6716223 1.7367851\n [15] 1.7993642 1.8599457 1.9185721 1.9754706 2.0307787 2.0846117 2.1370978\n [22] 2.1883179 2.2383721 2.2873270 2.3352588 2.3822244 2.4282830 2.4734834\n [29] 2.5178729 2.5614931 2.6043829 2.6465776 2.6881102 2.7290107 2.7693072\n [36] 2.8090258 2.8481904 2.8868238 2.9249470 2.9625796 2.9997402 3.0364460\n [43] 3.0727134 3.1085577 3.1439933 3.1790340 3.2136926 3.2479814 3.2819120\n [50] 3.3154954 3.3487419 3.3816617 3.4142640 3.4465580 3.4785522 3.5102547\n [57] 3.5416735 3.5728160 3.6036894 3.6343006 3.6646560 3.6947621 3.7246248\n [64] 3.7542500 3.7836433 3.8128099 3.8417551 3.8704839 3.8990010 3.9273110\n [71] 3.9554184 3.9833275 4.0110424 4.0385671 4.0659055 4.0930613 4.1200381\n [78] 4.1468394 4.1734686 4.1999290 4.2262237 4.2523558 4.2783283 4.3041440\n [85] 4.3298059 4.3553165 4.3806786 4.4058947 4.4309673 4.4558988 4.4806916\n [92] 4.5053480 4.5298702 4.5542603 4.5785205 4.6026529 4.6266593 4.6505419\n [99] 4.6743024 4.6979427 4.7214647 4.7448701 4.7681606 4.7913378 4.8144035\n[106] 4.8373593 4.8602065 4.8829469 4.9055819 4.9281129 4.9505414 4.9728687\n[113] 4.9950962 5.0172253 5.0392572 5.0611931 5.0830344 5.1047823 5.1264379\n[120] 5.1480024 5.1694769 5.1908626 5.2121606 5.2333718 5.2544975 5.2755386\n[127] 5.2964960 5.3173709 5.3381641 5.3588767 5.3795095 5.4000635 5.4205395\n[134] 5.4409385 5.4612613 5.4815088 5.5016817 5.5217809 5.5418073 5.5617615\n[141] 5.5816444 5.6014567 5.6211992 5.6408726 5.6604776 5.6800149 5.6994853\n[148] 5.7188894 5.7382279 5.7575014 5.7767106 5.7958561 5.8149387 5.8339587\n[155] 5.8529170 5.8718141 5.8906506 5.9094270 5.9281439 5.9468019 5.9654016\n[162] 5.9839435 6.0024281 6.0208559 6.0392275 6.0575434 6.0758041 6.0940100\n[169] 6.1121618 6.1302598 6.1483045 6.1662964 6.1842359 6.2021236 6.2199598\n[176] 6.2377451 6.2554797 6.2731642 6.2907991 6.3083846 6.3259212 6.3434093\n\n\n\n\n\nThe two figures above show an idea of the short term and the long-term forecasted stock price. The forecasted values is be based on the estimated parameters of the model, which were obtained by fitting it to the historical data. One way to assess the accuracy of the forecasts is to use cross-validation, which involves splitting the data into training and testing sets and comparing the predicted values to the actual values in the test set. This can help to identify any potential problems with the model and to fine-tune the model parameters.\n\n12 Step ahead Cross Validation1 Step ahead Cross Validation\n\n\n\n\nCode\n#a seasonal cross validation using 12 steps ahead forecasts\nfarima1 <- function(x, h){forecast(Arima(x, order=c(3,1,1)), h=h)}\n\n# Compute cross-validated errors for up to 12 steps ahead\ne <- tsCV(myts, forecastfunction = farima1, h = 12)\n\n# Compute the MSE values and remove missing values\nmse <- colMeans(e^2, na.rm = TRUE)\n\n# Plot the MSE values against the forecast horizon\ndata.frame(h = 1:12, MSE = mse) %>%\n  ggplot(aes(x = h, y = MSE)) + geom_point()+geom_line()+ggtitle(\"12 Step Ahead Cross Validation\")\n\n\n\n\n\n\n\n\n\nCode\n#a seasonal cross validation using 1 steps ahead forecasts\nfarima1 <- function(x, h){forecast(Arima(x, order=c(3,1,1)),h=h)}\n\n# Compute cross-validated errors for up to 1 steps ahead\ne <- tsCV(myts, forecastfunction = farima1, h = 1)\nmse <-abs(mean(e,na.rm=TRUE))\n\n# Plot the MSE values against the forecast horizon\ndata.frame(h = 1:12, MSE = mse) %>%\n  ggplot(aes(x = h, y = MSE)) + geom_point()+geom_line()+ggtitle(\"1 Step Ahead Cross Validation\")\n\n\n\n\n\n\n\n\nThe one-step-ahead forecasting plot shows that the MSE seems to be stable. The MSE is tend to rise by 12 steps at each step of cross-validation. Lastly, the above-mentioned ARIMA model should be compared to benchmark methods before being used for a long time.\n\n\nModel Comparison\n\nPlotModel Error Table\n\n\n\n\nCode\nfit <- Arima(myts, order=c(3,1,1))\nautoplot(myts) +\n  autolayer(meanf(myts, h=182),\n            series=\"Mean\", PI=FALSE) +\n  autolayer(naive(myts, h=182),\n            series=\"Naïve\", PI=FALSE) +\n  autolayer(snaive(myts, h=182),\n            series=\"SNaïve\", PI=FALSE)+\n  autolayer(rwf(myts, h=182, drift=TRUE),\n            series=\"Drift\", PI=FALSE)+\n  autolayer(forecast(fit,182), \n            series=\"fit\",PI=FALSE) +\n  guides(colour=guide_legend(title=\"Forecast\"))\n\n\n\n\n\n\n\n\n\nCode\nsummary <- summary(fit)\nsnaive <- snaive(myts, h=182)\naccuracy(snaive)\n\n\n                   ME     RMSE      MAE      MPE     MAPE MASE      ACF1\nTraining set 2.675922 5.951611 4.417328 6.922133 11.96095    1 0.9911468\n\n\nCode\nsummary\n\n\nSeries: myts \nARIMA(3,1,1) \n\nCoefficients:\n          ar1     ar2     ar3     ma1\n      -0.9171  0.0175  0.1410  0.8762\ns.e.   0.0376  0.0322  0.0246  0.0307\n\nsigma^2 = 0.1954:  log likelihood = -1062.33\nAIC=2134.67   AICc=2134.7   BIC=2162.05\n\nTraining set error measures:\n                      ME      RMSE       MAE        MPE      MAPE       MASE\nTraining set 0.007363989 0.4414333 0.2958979 0.01592701 0.9017148 0.06698572\n                    ACF1\nTraining set 0.007050727\n\n\n\n\n\nError\nModel\nSnavie\n\n\n\n\nME\n0.001165962\n0.3348181\n\n\nRMSE\n0.4838699\n8.889729\n\n\nMAE\n0.2666752\n6.46935\n\n\nMPE\n-0.007797867\n-2.538989\n\n\nMAPE\n0.8598005\n21.31273\n\n\nMASE\n0.04122133\n1.0000000\n\n\nACF1\n-0.001108128\n0.9965303\n\n\n\n\n\n\nThe ARIMA fitted and snaive forecast tracks the actual data points very closely when compared to other model. The other forecast methods are less accurate when compared. From the table, Model error measurements of fit are much lower than snaive method.We can conclude that the fitted model is good."
  },
  {
    "objectID": "arima_sector_technology.html",
    "href": "arima_sector_technology.html",
    "title": "ARIMA Model for Technology Sector Fund",
    "section": "",
    "text": "During exploratory data analysis (EDA), it was observed that the raw data is often non-stationary, exhibiting trends, seasonality, and other complex patterns that can make modeling and forecasting difficult. To address this issue, the data is often differentiated to achieve stationarity, which is a key assumption of the ARIMA model.\nDifferencing involves calculating the difference between each data point and the previous data point, which can help to remove trends and seasonality from the data. The goal is to create a new series of data that is stationary, meaning that the statistical properties of the data do not change over time.\n\nDifferentiated Time Series Plot\n\nPlotACFPACFADF Test\n\n\n\n\nCode\n#import the data\ndf <- read.csv(\"DATA/CLEANED DATA/XLK_raw_data.csv\")\n#convert to time series data\nmyts<-ts(df$XLK.Adjusted,frequency=252,start=c(2010,1,1), end = c(2023,3,1)) \n#First order differentiation\ndf1 <- diff(myts)\n# Plot \nmyts  %>% diff() %>% ggtsdisplay(main = \"First order differentiation\") \n\n\n\n\n\n\n\n\n\nCode\n#ACF plot \nggAcf(df1,60,main=\"ACF Plot: First order differentiation\") \n\n\n\n\n\n\n\n\n\nCode\nggPacf(df1,60,main=\"PACF Plot: First order differentiation\") \n\n\n\n\n\n\n\n\n\nCode\ntseries::adf.test(df1)\n\n\n\n    Augmented Dickey-Fuller Test\n\ndata:  df1\nDickey-Fuller = -15.488, Lag order = 14, p-value = 0.01\nalternative hypothesis: stationary\n\n\n\n\n\nAfter analyzing the ACF and PACF plots, it can be observed that most of the bar lines lie between the blue lines, indicating that the time series is stationary. This has been further confirmed through an Augmented Dickey-Fuller test as the p-value is less than 0.05. Once the data is stationary, the next step is to model it using ARIMA. The combination of ACF and PACF plots can help determine the appropriate values for p, d, and q in the ARIMA model. The order of differencing required to achieve stationarity gives the d value. The p value is determined by examining significant spikes at various lags in the PACF plot, while the q value is determined through significant spikes in the ACF plot. It is important to consider other methods such as grid search and information criteria to ensure the most appropriate model is selected.\nHere the parameters are d = 1 p = 0,7 (PACF Plot) q = 0,7 (ACF Plot)\n\n\nModel Selection\n\nARIMA(0,1,0) ResultARIMA(7,1,0) ResultARIMA(0,1,7) ResultARIMA(7,1,7) ResultAuto Arima Model\n\n\n\n\nCode\nArima(myts,order=c(0,1,0),include.drift=TRUE) \n\n\nSeries: myts \nARIMA(0,1,0) with drift \n\nCoefficients:\n       drift\n      0.0335\ns.e.  0.0209\n\nsigma^2 = 1.428:  log likelihood = -5234.41\nAIC=10472.82   AICc=10472.82   BIC=10485.01\n\n\n\n\n\n\nCode\nArima(myts,order=c(7,1,0),include.drift=TRUE) \n\n\nSeries: myts \nARIMA(7,1,0) with drift \n\nCoefficients:\n          ar1      ar2      ar3      ar4     ar5      ar6     ar7   drift\n      -0.1023  -0.0056  -0.0268  -0.0353  0.0269  -0.0618  0.0702  0.0335\ns.e.   0.0174   0.0175   0.0175   0.0175  0.0175   0.0175  0.0175  0.0182\n\nsigma^2 = 1.395:  log likelihood = -5193.04\nAIC=10404.09   AICc=10404.14   BIC=10458.94\n\n\n\n\n\n\nCode\nArima(myts,order=c(0,1,7),include.drift=TRUE) \n\n\nSeries: myts \nARIMA(0,1,7) with drift \n\nCoefficients:\n          ma1      ma2     ma3      ma4     ma5      ma6     ma7   drift\n      -0.0918  -0.0186  0.0007  -0.0610  0.0541  -0.0752  0.0686  0.0334\ns.e.   0.0176   0.0177  0.0177   0.0191  0.0185   0.0174  0.0172  0.0181\n\nsigma^2 = 1.396:  log likelihood = -5194\nAIC=10405.99   AICc=10406.05   BIC=10460.85\n\n\n\n\n\n\nCode\nArima(myts,order=c(7,1,7),include.drift=TRUE) \n\n\nSeries: myts \nARIMA(7,1,7) with drift \n\nCoefficients:\n          ar1     ar2      ar3      ar4    ar5     ar6     ar7     ma1      ma2\n      -0.8243  0.0902  -0.2232  -0.1274  0.889  0.6473  0.1019  0.7435  -0.1717\ns.e.      NaN     NaN      NaN      NaN    NaN     NaN     NaN     NaN      NaN\n         ma3     ma4      ma5      ma6      ma7   drift\n      0.2258  0.0945  -0.9068  -0.6346  -0.0268  0.0329\ns.e.     NaN     NaN      NaN      NaN      NaN  0.0148\n\nsigma^2 = 1.36:  log likelihood = -5149.94\nAIC=10331.88   AICc=10332.05   BIC=10429.4\n\n\n\n\n\n\nCode\nauto.arima(myts)\n\n\nSeries: myts \nARIMA(0,1,1) with drift \n\nCoefficients:\n          ma1   drift\n      -0.1095  0.0335\ns.e.   0.0173  0.0185\n\nsigma^2 = 1.411:  log likelihood = -5214.66\nAIC=10435.31   AICc=10435.32   BIC=10453.6\n\n\n\n\n\nIn the Model selection, when comparing the AIC, AICc and BIC values of all the 4 models, model (0,1,0) has the least BIC and and (7,1,7) has the least AIC value. The model generated by auto.arima is also (0,1,0). We can proceed with the further model diagnostic comparison for both the model to find the best model.\n\n\nModel Diagnostic\n\nModel 1 PlotModel 1Model 2 PlotModel 2\n\n\n\n\nCode\nmodel_output <- capture.output(sarima(myts,7,1,7))\n\n\n\n\n\n\n\n\n\nCode\ncat(model_output[104:149], model_output[length(model_output)], sep = \"\\n\") \n\n\niter   2 value 0.152204\niter   3 value 0.152162\niter   4 value 0.152160\niter   5 value 0.152154\niter   6 value 0.152148\niter   7 value 0.152147\niter   8 value 0.152146\niter   9 value 0.152145\niter  10 value 0.152142\niter  11 value 0.152141\niter  12 value 0.152141\niter  13 value 0.152139\niter  14 value 0.152138\niter  15 value 0.152137\niter  16 value 0.152136\niter  17 value 0.152135\niter  18 value 0.152133\niter  19 value 0.152130\niter  20 value 0.152129\niter  21 value 0.152128\niter  22 value 0.152128\niter  23 value 0.152127\niter  24 value 0.152126\niter  25 value 0.152125\niter  26 value 0.152125\niter  27 value 0.152124\niter  28 value 0.152124\niter  29 value 0.152124\niter  30 value 0.152124\niter  31 value 0.152124\niter  32 value 0.152124\niter  33 value 0.152124\niter  33 value 0.152124\niter  33 value 0.152124\nfinal  value 0.152124 \nconverged\n$fit\n\nCall:\narima(x = xdata, order = c(p, d, q), seasonal = list(order = c(P, D, Q), period = S), \n    xreg = constant, transform.pars = trans, fixed = fixed, optim.control = list(trace = trc, \n        REPORT = 1, reltol = tol))\n\nCoefficients:\n          ar1     ar2      ar3      ar4    ar5     ar6     ar7     ma1      ma2\n      -0.8243  0.0902  -0.2232  -0.1274  0.889  0.6473  0.1019  0.7435  -0.1717\n\n\n\n\n\n\nCode\nmodel_output <- capture.output(sarima(myts,0,1,0))\n\n\n\n\n\n\n\n\n\nCode\ncat(model_output[9:38], model_output[length(model_output)], sep = \"\\n\") \n\n\n$fit\n\nCall:\narima(x = xdata, order = c(p, d, q), seasonal = list(order = c(P, D, Q), period = S), \n    xreg = constant, transform.pars = trans, fixed = fixed, optim.control = list(trace = trc, \n        REPORT = 1, reltol = tol))\n\nCoefficients:\n      constant\n        0.0335\ns.e.    0.0209\n\nsigma^2 estimated as 1.427:  log likelihood = -5234.41,  aic = 10472.82\n\n$degrees_of_freedom\n[1] 3277\n\n$ttable\n         Estimate     SE t.value p.value\nconstant   0.0335 0.0209  1.6041  0.1088\n\n$AIC\n[1] 3.19488\n\n$AICc\n[1] 3.19488\n\n$BIC\n[1] 3.198599\n\n\n\n\n\nTo determine which model diagnosis is the best, we need to consider some key factors such as the coefficient estimates and standard errors, AIC, BIC, p-values, and the log-likelihood. Comparing the two models, model 2 is the best because it has the lowest BIC value when compared and the AIC value are almost the same, which indicate better goodness of fit. Also, all the coefficient estimates in the model are significant, as evidenced by the p-values are better than model 1, which implies that there is a strong correlation between the variables. The t-values are high, indicating that the estimates are highly reliable.\nTherefore, based on the AIC and BIC values, the significance of the coefficient estimates, and the reliability of the estimates, we can conclude that model 2 is the best model diagnosis. Therefore, (0,1,0) is the best model for this time series.\nThe best model identified is ARIMA(0,1,0). By analyzing the standardized residuals plot, it can be observed that the mean is close to 0 and the variance is slightly higher than 1. Deviations from these values could indicate poor model fit. However, in this case, the model seems to be well-fitted. The ACF plot of residuals shows very few significant lags, which is a positive sign for the model. The qq-plot also suggests normality in the residuals. Additionally, the p-values for the Ljung-Box test are greater than 0.05, indicating that the residuals are independent, which is favorable for the model’s accuracy.\nThe equation for this model: \\[(1-\\phi_1B-\\phi_2B^2)(1-B)(Y_t-\\mu) = (1+\\theta_1B+\\theta_2B^2)\\epsilon_t\\]\n\n\nForcast\n\nShort Term ForcastLong term Forcast\n\n\n\n\nCode\nmyts %>%\n  Arima(order=c(0,1,0),include.drift = TRUE) %>%\n  forecast %>%\n  autoplot(main = \"Technology Sector Fund Stock Prices Prediction\") +\n  ylab(\"stock prices\") + xlab(\"Year\")\n\n\n\n\n\n\n\n\n\nCode\nsarima.for(myts,182, 0,1,0, main='Technology Sector Fund Stock Prices Prediction')\n\n\n\n\n\n$pred\nTime Series:\nStart = c(2023, 4) \nEnd = c(2023, 185) \nFrequency = 252 \n  [1] 128.9090 128.9425 128.9759 129.0094 129.0429 129.0763 129.1098 129.1433\n  [9] 129.1768 129.2102 129.2437 129.2772 129.3107 129.3441 129.3776 129.4111\n [17] 129.4445 129.4780 129.5115 129.5450 129.5784 129.6119 129.6454 129.6789\n [25] 129.7123 129.7458 129.7793 129.8128 129.8462 129.8797 129.9132 129.9466\n [33] 129.9801 130.0136 130.0471 130.0805 130.1140 130.1475 130.1810 130.2144\n [41] 130.2479 130.2814 130.3149 130.3483 130.3818 130.4153 130.4487 130.4822\n [49] 130.5157 130.5492 130.5826 130.6161 130.6496 130.6831 130.7165 130.7500\n [57] 130.7835 130.8170 130.8504 130.8839 130.9174 130.9508 130.9843 131.0178\n [65] 131.0513 131.0847 131.1182 131.1517 131.1852 131.2186 131.2521 131.2856\n [73] 131.3191 131.3525 131.3860 131.4195 131.4529 131.4864 131.5199 131.5534\n [81] 131.5868 131.6203 131.6538 131.6873 131.7207 131.7542 131.7877 131.8211\n [89] 131.8546 131.8881 131.9216 131.9550 131.9885 132.0220 132.0555 132.0889\n [97] 132.1224 132.1559 132.1894 132.2228 132.2563 132.2898 132.3232 132.3567\n[105] 132.3902 132.4237 132.4571 132.4906 132.5241 132.5576 132.5910 132.6245\n[113] 132.6580 132.6915 132.7249 132.7584 132.7919 132.8253 132.8588 132.8923\n[121] 132.9258 132.9592 132.9927 133.0262 133.0597 133.0931 133.1266 133.1601\n[129] 133.1936 133.2270 133.2605 133.2940 133.3274 133.3609 133.3944 133.4279\n[137] 133.4613 133.4948 133.5283 133.5618 133.5952 133.6287 133.6622 133.6957\n[145] 133.7291 133.7626 133.7961 133.8295 133.8630 133.8965 133.9300 133.9634\n[153] 133.9969 134.0304 134.0639 134.0973 134.1308 134.1643 134.1977 134.2312\n[161] 134.2647 134.2982 134.3316 134.3651 134.3986 134.4321 134.4655 134.4990\n[169] 134.5325 134.5660 134.5994 134.6329 134.6664 134.6998 134.7333 134.7668\n[177] 134.8003 134.8337 134.8672 134.9007 134.9342 134.9676\n\n$se\nTime Series:\nStart = c(2023, 4) \nEnd = c(2023, 185) \nFrequency = 252 \n  [1]  1.194695  1.689554  2.069273  2.389391  2.671420  2.926394  3.160867\n  [8]  3.379109  3.584086  3.777959  3.962356  4.138546  4.307536  4.470141\n [15]  4.627036  4.778782  4.925855  5.068663  5.207557  5.342840  5.474782\n [22]  5.603618  5.729558  5.852788  5.973477  6.091775  6.207820  6.321734\n [29]  6.433632  6.543616  6.651783  6.758218  6.863003  6.966212  7.067913\n [36]  7.168173  7.267049  7.364597  7.460871  7.555917  7.649783  7.742511\n [43]  7.834142  7.924713  8.014261  8.102819  8.190419  8.277093  8.362868\n [50]  8.447772  8.531832  8.615071  8.697514  8.779183  8.860098  8.940282\n [57]  9.019753  9.098529  9.176630  9.254071  9.330870  9.407041  9.482601\n [64]  9.557563  9.631942  9.705752  9.779004  9.851711  9.923886  9.995539\n [71] 10.066683 10.137327 10.207482 10.277159 10.346366 10.415113 10.483410\n [78] 10.551264 10.618685 10.685681 10.752259 10.818427 10.884193 10.949564\n [85] 11.014548 11.079150 11.143377 11.207237 11.270734 11.333876 11.396668\n [92] 11.459116 11.521225 11.583002 11.644451 11.705577 11.766385 11.826881\n [99] 11.887069 11.946954 12.006540 12.065832 12.124834 12.183551 12.241985\n[106] 12.300142 12.358026 12.415639 12.472986 12.530071 12.586897 12.643468\n[113] 12.699787 12.755857 12.811681 12.867264 12.922607 12.977714 13.032589\n[120] 13.087233 13.141650 13.195842 13.249813 13.303565 13.357101 13.410423\n[127] 13.463534 13.516436 13.569132 13.621624 13.673914 13.726005 13.777900\n[134] 13.829599 13.881107 13.932423 13.983552 14.034494 14.085251 14.135827\n[141] 14.186222 14.236439 14.286479 14.336345 14.386038 14.435560 14.484912\n[148] 14.534097 14.583116 14.631971 14.680663 14.729194 14.777566 14.825780\n[155] 14.873838 14.921741 14.969491 15.017089 15.064536 15.111835 15.158986\n[162] 15.205990 15.252850 15.299567 15.346141 15.392574 15.438867 15.485023\n[169] 15.531041 15.576923 15.622670 15.668284 15.713765 15.759115 15.804335\n[176] 15.849426 15.894389 15.939225 15.983935 16.028521 16.072983 16.117323\n\n\n\n\n\nThe two figures above show an idea of the short term and the long-term forecasted stock price. The forecasted values is be based on the estimated parameters of the model, which were obtained by fitting it to the historical data. One way to assess the accuracy of the forecasts is to use cross-validation, which involves splitting the data into training and testing sets and comparing the predicted values to the actual values in the test set. This can help to identify any potential problems with the model and to fine-tune the model parameters.\n\n12 Step ahead Cross Validation1 Step ahead Cross Validation\n\n\n\n\nCode\n#a seasonal cross validation using 12 steps ahead forecasts\nfarima1 <- function(x, h){forecast(Arima(x, order=c(0,1,0)), h=h)}\n\n# Compute cross-validated errors for up to 12 steps ahead\ne <- tsCV(myts, forecastfunction = farima1, h = 12)\n\n# Compute the MSE values and remove missing values\nmse <- colMeans(e^2, na.rm = TRUE)\n\n# Plot the MSE values against the forecast horizon\ndata.frame(h = 1:12, MSE = mse) %>%\n  ggplot(aes(x = h, y = MSE)) + geom_point()+geom_line()+ggtitle(\"12 Step Ahead Cross Validation\")\n\n\n\n\n\n\n\n\n\nCode\n#a seasonal cross validation using 1 steps ahead forecasts\nfarima1 <- function(x, h){forecast(Arima(x, order=c(0,1,0)),h=h)}\n\n# Compute cross-validated errors for up to 1 steps ahead\ne <- tsCV(myts, forecastfunction = farima1, h = 1)\nmse <-abs(mean(e,na.rm=TRUE))\n\n# Plot the MSE values against the forecast horizon\ndata.frame(h = 1:12, MSE = mse) %>%\n  ggplot(aes(x = h, y = MSE)) + geom_point()+geom_line()+ggtitle(\"1 Step Ahead Cross Validation\")\n\n\n\n\n\n\n\n\nThe one-step-ahead forecasting plot shows that the MSE seems to be stable. The MSE is tend to rise by 12 steps at each step of cross-validation. Lastly, the above-mentioned ARIMA model should be compared to benchmark methods before being used for a long time.\n\n\nModel Comparison\n\nPlotModel Error Table\n\n\n\n\nCode\nfit <- Arima(myts, order=c(0,1,0))\nautoplot(myts) +\n  autolayer(meanf(myts, h=182),\n            series=\"Mean\", PI=FALSE) +\n  autolayer(naive(myts, h=182),\n            series=\"Naïve\", PI=FALSE) +\n  autolayer(snaive(myts, h=182),\n            series=\"SNaïve\", PI=FALSE)+\n  autolayer(rwf(myts, h=182, drift=TRUE),\n            series=\"Drift\", PI=FALSE)+\n  autolayer(forecast(fit,182), \n            series=\"fit\",PI=FALSE) +\n  guides(colour=guide_legend(title=\"Forecast\"))\n\n\n\n\n\n\n\n\n\nCode\nsummary <- summary(fit)\nsnaive <- snaive(myts, h=182)\naccuracy(snaive)\n\n\n                   ME     RMSE      MAE      MPE     MAPE MASE      ACF1\nTraining set 9.887861 18.33357 12.50646 14.42715 16.56907    1 0.9931727\n\n\nCode\nsummary\n\n\nSeries: myts \nARIMA(0,1,0) \n\nsigma^2 = 1.428:  log likelihood = -5235.69\nAIC=10473.39   AICc=10473.39   BIC=10479.48\n\nTraining set error measures:\n                     ME     RMSE       MAE        MPE      MAPE      MASE\nTraining set 0.03346887 1.194982 0.6382573 0.04900745 0.9157251 0.0510342\n                   ACF1\nTraining set -0.1098026\n\n\n\n\n\nError\nModel\nSnavie\n\n\n\n\nME\n0.003719744\n1.368119\n\n\nRMSE\n1.980341\n40.85699\n\n\nMAE\n0.5360759\n22.68148\n\n\nMPE\n-0.08174304\n-34.13581\n\n\nMAPE\n0.9990605\n71.76659\n\n\nMASE\n0.02363496\n1.0000000\n\n\nACF1\n-0.02466494\n0.9974566\n\n\n\n\n\n\nThe ARIMA forecast tracks the actual data points very closely when compared to other model. The other forecast methods are less accurate when compared. From the table, Model error measurements of fit are much lower than snaive method.We can conclude that the fitted model is good."
  },
  {
    "objectID": "arima_sector_utilites.html",
    "href": "arima_sector_utilites.html",
    "title": "ARIMA Model for Utilities Sector Fund",
    "section": "",
    "text": "During exploratory data analysis (EDA), it was observed that the raw data is often non-stationary, exhibiting trends, seasonality, and other complex patterns that can make modeling and forecasting difficult. To address this issue, the data is often differentiated to achieve stationarity, which is a key assumption of the ARIMA model.\nDifferencing involves calculating the difference between each data point and the previous data point, which can help to remove trends and seasonality from the data. The goal is to create a new series of data that is stationary, meaning that the statistical properties of the data do not change over time.\n\nDifferentiated Time Series Plot\n\nPlotACFPACFADF Test\n\n\n\n\nCode\n#import the data\ndf <- read.csv(\"DATA/CLEANED DATA/XLU_raw_data.csv\")\n#convert to time series data\nmyts<-ts(df$XLU.Adjusted,frequency=252,start=c(2010,1,1), end = c(2023,3,1)) \n#First order differentiation\ndf1 <- diff(myts)\n# Plot \nmyts  %>% diff() %>% ggtsdisplay(main = \"First order differentiation\") \n\n\n\n\n\n\n\n\n\nCode\n#ACF plot \nggAcf(df1,60,main=\"ACF Plot: First order differentiation\") \n\n\n\n\n\n\n\n\n\nCode\nggPacf(df1,60,main=\"PACF Plot: First order differentiation\") \n\n\n\n\n\n\n\n\n\nCode\ntseries::adf.test(df1)\n\n\n\n    Augmented Dickey-Fuller Test\n\ndata:  df1\nDickey-Fuller = -15.702, Lag order = 14, p-value = 0.01\nalternative hypothesis: stationary\n\n\n\n\n\nAfter analyzing the ACF and PACF plots, it can be observed that most of the bar lines lie between the blue lines, indicating that the time series is stationary. This has been further confirmed through an Augmented Dickey-Fuller test as the p-value is less than 0.05. Once the data is stationary, the next step is to model it using ARIMA. The combination of ACF and PACF plots can help determine the appropriate values for p, d, and q in the ARIMA model. The order of differencing required to achieve stationarity gives the d value. The p value is determined by examining significant spikes at various lags in the PACF plot, while the q value is determined through significant spikes in the ACF plot. It is important to consider other methods such as grid search and information criteria to ensure the most appropriate model is selected.\nHere the parameters are d = 1 p = 0,9 (PACF Plot) q = 0,9 (ACF Plot)\n\n\nModel Selection\n\nARIMA(0,1,0) ResultARIMA(9,1,0) ResultARIMA(9,1,9) ResultARIMA(0,1,9) ResultAuto Arima Model\n\n\n\n\nCode\nArima(myts,order=c(0,1,0),include.drift=TRUE) \n\n\nSeries: myts \nARIMA(0,1,0) with drift \n\nCoefficients:\n       drift\n      0.0159\ns.e.  0.0093\n\nsigma^2 = 0.2808:  log likelihood = -2569.14\nAIC=5142.28   AICc=5142.28   BIC=5154.47\n\n\n\n\n\n\nCode\nArima(myts,order=c(9,1,0),include.drift=TRUE) \n\n\nSeries: myts \nARIMA(9,1,0) with drift \n\nCoefficients:\n          ar1     ar2     ar3      ar4     ar5      ar6     ar7      ar8\n      -0.0552  0.0417  0.0000  -0.0828  0.0718  -0.0601  0.0634  -0.0421\ns.e.   0.0173  0.0174  0.0173   0.0173  0.0173   0.0173  0.0174   0.0174\n         ar9   drift\n      0.1179  0.0159\ns.e.  0.0174  0.0095\n\nsigma^2 = 0.268:  log likelihood = -2488.45\nAIC=4998.9   AICc=4998.98   BIC=5065.94\n\n\n\n\n\n\nCode\nArima(myts,order=c(9,1,9),include.drift=TRUE) \n\n\nSeries: myts \nARIMA(9,1,9) with drift \n\nCoefficients:\n          ar1     ar2      ar3      ar4      ar5     ar6      ar7      ar8\n      -0.3212  0.4734  -0.4963  -1.3165  -0.2542  0.1611  -0.2822  -0.5719\ns.e.   0.1775     NaN   0.1409      NaN   0.0706     NaN      NaN   0.0903\n          ar9     ma1      ma2     ma3     ma4     ma5      ma6     ma7     ma8\n      -0.2258  0.2685  -0.4422  0.5556  1.2035  0.2592  -0.1124  0.3162  0.5160\ns.e.      NaN  0.1759      NaN  0.1439     NaN     NaN      NaN     NaN  0.1087\n         ma9  drift\n      0.2969  0.016\ns.e.     NaN  0.009\n\nsigma^2 = 0.265:  log likelihood = -2465.21\nAIC=4970.43   AICc=4970.68   BIC=5092.32\n\n\n\n\n\n\nCode\nArima(myts,order=c(0,1,9),include.drift=TRUE) \n\n\nSeries: myts \nARIMA(0,1,9) with drift \n\nCoefficients:\n          ma1     ma2     ma3      ma4     ma5      ma6     ma7      ma8\n      -0.0511  0.0451  0.0073  -0.0783  0.0864  -0.0794  0.0671  -0.0362\ns.e.   0.0174  0.0173  0.0173   0.0175  0.0180   0.0181  0.0174   0.0168\n         ma9   drift\n      0.0963  0.0159\ns.e.  0.0180  0.0096\n\nsigma^2 = 0.2693:  log likelihood = -2495.91\nAIC=5013.81   AICc=5013.89   BIC=5080.85\n\n\n\n\n\n\nCode\nauto.arima(myts)\n\n\nSeries: myts \nARIMA(0,1,0)(0,0,1)[252] with drift \n\nCoefficients:\n         sma1   drift\n      -0.0673  0.0159\ns.e.   0.0190  0.0087\n\nsigma^2 = 0.2797:  log likelihood = -2562.91\nAIC=5131.82   AICc=5131.83   BIC=5150.11\n\n\n\n\n\nIn the Model selection, when comparing the AIC, AICc and BIC values of all the 4 models, model (0,1,0) has the least BIC and AIc value. The model generated by auto.arima is also (0,1,0). We can proceed with the further model diagnostic for the chosen best model i.e. (0,1,0), to look into the details of the model.\n\n\nModel Diagnostic\n\nModel PlotModel\n\n\n\n\nCode\nmodel_output <- capture.output(sarima(myts,0,1,0))\n\n\n\n\n\n\n\n\n\nCode\ncat(model_output[9:38], model_output[length(model_output)], sep = \"\\n\") \n\n\n$fit\n\nCall:\narima(x = xdata, order = c(p, d, q), seasonal = list(order = c(P, D, Q), period = S), \n    xreg = constant, transform.pars = trans, fixed = fixed, optim.control = list(trace = trc, \n        REPORT = 1, reltol = tol))\n\nCoefficients:\n      constant\n        0.0159\ns.e.    0.0093\n\nsigma^2 estimated as 0.2807:  log likelihood = -2569.14,  aic = 5142.28\n\n$degrees_of_freedom\n[1] 3277\n\n$ttable\n         Estimate     SE t.value p.value\nconstant   0.0159 0.0093  1.7165  0.0862\n\n$AIC\n[1] 1.568724\n\n$AICc\n[1] 1.568724\n\n$BIC\n[1] 1.572442\n\n\n\n\n\nThe model diagnostic is done for ARIMA(0,1,0). By analyzing the standardized residuals plot, it can be observed that the mean is close to 0 and the variance is slightly higher than 1. Deviations from these values could indicate poor model fit. However, in this case, the model seems to be well-fitted. The ACF plot of residuals shows very few significant lags, which is a positive sign for the model. The qq-plot also suggests normality in the residuals. Additionally, the p-values for the Ljung-Box test are greater than 0.05, indicating that the residuals are independent, which is favorable for the model’s accuracy.\nThe equation for this model: \\[(1-\\phi_1B-\\phi_2B^2)(1-B)(Y_t-\\mu) = (1+\\theta_1B+\\theta_2B^2)\\epsilon_t\\]\n\n\nForcast\n\nShort Term ForcastLong term Forcast\n\n\n\n\nCode\nmyts %>%\n  Arima(order=c(0,1,0),include.drift = TRUE) %>%\n  forecast %>%\n  autoplot(main = \"Utilities Sector Fund Stock Prices Prediction\") +\n  ylab(\"stock prices\") + xlab(\"Year\")\n\n\n\n\n\n\n\n\n\nCode\nsarima.for(myts,182, 0,1,0, main='Utilities Sector Fund Stock Prices Prediction')\n\n\n\n\n\n$pred\nTime Series:\nStart = c(2023, 4) \nEnd = c(2023, 185) \nFrequency = 252 \n  [1] 71.56850 71.58438 71.60027 71.61615 71.63204 71.64792 71.66381 71.67969\n  [9] 71.69558 71.71146 71.72735 71.74323 71.75912 71.77500 71.79089 71.80677\n [17] 71.82266 71.83854 71.85443 71.87031 71.88619 71.90208 71.91796 71.93385\n [25] 71.94973 71.96562 71.98150 71.99739 72.01327 72.02916 72.04504 72.06093\n [33] 72.07681 72.09270 72.10858 72.12447 72.14035 72.15624 72.17212 72.18801\n [41] 72.20389 72.21978 72.23566 72.25155 72.26743 72.28332 72.29920 72.31509\n [49] 72.33097 72.34686 72.36274 72.37863 72.39451 72.41040 72.42628 72.44217\n [57] 72.45805 72.47394 72.48982 72.50571 72.52159 72.53748 72.55336 72.56925\n [65] 72.58513 72.60102 72.61690 72.63278 72.64867 72.66455 72.68044 72.69632\n [73] 72.71221 72.72809 72.74398 72.75986 72.77575 72.79163 72.80752 72.82340\n [81] 72.83929 72.85517 72.87106 72.88694 72.90283 72.91871 72.93460 72.95048\n [89] 72.96637 72.98225 72.99814 73.01402 73.02991 73.04579 73.06168 73.07756\n [97] 73.09345 73.10933 73.12522 73.14110 73.15699 73.17287 73.18876 73.20464\n[105] 73.22053 73.23641 73.25230 73.26818 73.28407 73.29995 73.31584 73.33172\n[113] 73.34761 73.36349 73.37938 73.39526 73.41114 73.42703 73.44291 73.45880\n[121] 73.47468 73.49057 73.50645 73.52234 73.53822 73.55411 73.56999 73.58588\n[129] 73.60176 73.61765 73.63353 73.64942 73.66530 73.68119 73.69707 73.71296\n[137] 73.72884 73.74473 73.76061 73.77650 73.79238 73.80827 73.82415 73.84004\n[145] 73.85592 73.87181 73.88769 73.90358 73.91946 73.93535 73.95123 73.96712\n[153] 73.98300 73.99889 74.01477 74.03066 74.04654 74.06243 74.07831 74.09420\n[161] 74.11008 74.12597 74.14185 74.15774 74.17362 74.18950 74.20539 74.22127\n[169] 74.23716 74.25304 74.26893 74.28481 74.30070 74.31658 74.33247 74.34835\n[177] 74.36424 74.38012 74.39601 74.41189 74.42778 74.44366\n\n$se\nTime Series:\nStart = c(2023, 4) \nEnd = c(2023, 185) \nFrequency = 252 \n  [1] 0.5298365 0.7493019 0.9177037 1.0596729 1.1847503 1.2978290 1.4018155\n  [8] 1.4986038 1.5895094 1.6754900 1.7572687 1.8354073 1.9103525 1.9824665\n [15] 2.0520478 2.1193458 2.1845717 2.2479057 2.3095036 2.3695007 2.4280157\n [22] 2.4851532 2.5410064 2.5956579 2.6491823 2.7016464 2.7531110 2.8036310\n [29] 2.8532566 2.9020338 2.9500045 2.9972076 3.0436787 3.0894509 3.1345547\n [36] 3.1790187 3.2228693 3.2661312 3.3088276 3.3509800 3.3926086 3.4337327\n [43] 3.4743700 3.5145374 3.5542510 3.5935257 3.6323757 3.6708146 3.7088552\n [50] 3.7465095 3.7837891 3.8207050 3.8572676 3.8934869 3.9293723 3.9649330\n [57] 4.0001775 4.0351142 4.0697510 4.1040955 4.1381550 4.1719364 4.2054465\n [64] 4.2386916 4.2716780 4.3044117 4.3368983 4.3691433 4.4011521 4.4329298\n [71] 4.4644813 4.4958114 4.5269246 4.5578255 4.5885183 4.6190071 4.6492960\n [78] 4.6793888 4.7092894 4.7390013 4.7685281 4.7978731 4.8270398 4.8560313\n [85] 4.8848507 4.9135011 4.9419854 4.9703065 4.9984671 5.0264699 5.0543176\n [92] 5.0820127 5.1095577 5.1369550 5.1642069 5.1913158 5.2182839 5.2451133\n [99] 5.2718061 5.2983645 5.3247905 5.3510859 5.3772527 5.4032928 5.4292080\n[106] 5.4550002 5.4806709 5.5062219 5.5316550 5.5569716 5.5821734 5.6072620\n[113] 5.6322388 5.6571053 5.6818630 5.7065132 5.7310575 5.7554971 5.7798333\n[120] 5.8040675 5.8282010 5.8522349 5.8761705 5.9000090 5.9237516 5.9473994\n[127] 5.9709536 5.9944152 6.0177853 6.0410650 6.0642554 6.0873574 6.1103721\n[134] 6.1333004 6.1561433 6.1789017 6.2015767 6.2241690 6.2466797 6.2691095\n[141] 6.2914593 6.3137300 6.3359225 6.3580374 6.3800758 6.4020382 6.4239256\n[148] 6.4457386 6.4674781 6.4891448 6.5107393 6.5322625 6.5537150 6.5750975\n[155] 6.5964106 6.6176552 6.6388317 6.6599409 6.6809835 6.7019599 6.7228709\n[162] 6.7437171 6.7644990 6.7852173 6.8058725 6.8264652 6.8469959 6.8674653\n[169] 6.8878739 6.9082222 6.9285107 6.9487399 6.9689105 6.9890228 7.0090774\n[176] 7.0290749 7.0490155 7.0689000 7.0887286 7.1085020 7.1282204 7.1478845\n\n\n\n\n\nThe two figures above show an idea of the short term and the long-term forecasted stock price. The forecasted values is be based on the estimated parameters of the model, which were obtained by fitting it to the historical data. One way to assess the accuracy of the forecasts is to use cross-validation, which involves splitting the data into training and testing sets and comparing the predicted values to the actual values in the test set. This can help to identify any potential problems with the model and to fine-tune the model parameters.\n\n12 Step ahead Cross Validation1 Step ahead Cross Validation\n\n\n\n\nCode\n#a seasonal cross validation using 12 steps ahead forecasts\nfarima1 <- function(x, h){forecast(Arima(x, order=c(0,1,0)), h=h)}\n\n# Compute cross-validated errors for up to 12 steps ahead\ne <- tsCV(myts, forecastfunction = farima1, h = 12)\n\n# Compute the MSE values and remove missing values\nmse <- colMeans(e^2, na.rm = TRUE)\n\n# Plot the MSE values against the forecast horizon\ndata.frame(h = 1:12, MSE = mse) %>%\n  ggplot(aes(x = h, y = MSE)) + geom_point()+geom_line()+ggtitle(\"12 Step Ahead Cross Validation\")\n\n\n\n\n\n\n\n\n\nCode\n#a seasonal cross validation using 1 steps ahead forecasts\nfarima1 <- function(x, h){forecast(Arima(x, order=c(0,1,0)),h=h)}\n\n# Compute cross-validated errors for up to 1 steps ahead\ne <- tsCV(myts, forecastfunction = farima1, h = 1)\nmse <-abs(mean(e,na.rm=TRUE))\n\n# Plot the MSE values against the forecast horizon\ndata.frame(h = 1:12, MSE = mse) %>%\n  ggplot(aes(x = h, y = MSE)) + geom_point()+geom_line()+ggtitle(\"1 Step Ahead Cross Validation\")\n\n\n\n\n\n\n\n\nThe one-step-ahead forecasting plot shows that the MSE seems to be stable. The MSE is tend to rise by 12 steps at each step of cross-validation. Lastly, the above-mentioned ARIMA model should be compared to benchmark methods before being used for a long time.\n\n\nModel Comparison\n\nPlotModel Error Table\n\n\n\n\nCode\nfit <- Arima(myts, order=c(0,1,0))\nautoplot(myts) +\n  autolayer(meanf(myts, h=182),\n            series=\"Mean\", PI=FALSE) +\n  autolayer(naive(myts, h=182),\n            series=\"Naïve\", PI=FALSE) +\n  autolayer(snaive(myts, h=182),\n            series=\"SNaïve\", PI=FALSE)+\n  autolayer(rwf(myts, h=182, drift=TRUE),\n            series=\"Drift\", PI=FALSE)+\n  autolayer(forecast(fit,182), \n            series=\"fit\",PI=FALSE) +\n  guides(colour=guide_legend(title=\"Forecast\"))\n\n\n\n\n\n\n\n\n\nCode\nsummary <- summary(fit)\nsnaive <- snaive(myts, h=182)\naccuracy(snaive)\n\n\n                   ME     RMSE      MAE      MPE     MAPE MASE      ACF1\nTraining set 4.136974 5.464734 4.441471 9.786299 10.44789    1 0.9775654\n\n\nCode\nsummary\n\n\nSeries: myts \nARIMA(0,1,0) \n\nsigma^2 = 0.281:  log likelihood = -2570.61\nAIC=5143.22   AICc=5143.22   BIC=5149.32\n\nTraining set error measures:\n                     ME      RMSE      MAE        MPE      MAPE       MASE\nTraining set 0.01588599 0.5299938 0.321907 0.03344792 0.7580015 0.07247757\n                    ACF1\nTraining set -0.08532626\n\n\n\n\n\nError\nModel\nSnavie\n\n\n\n\nME\n0.002908013\n1.118668\n\n\nRMSE\n0.8014312\n15.21741\n\n\nMAE\n0.2891511\n9.130549\n\n\nMPE\n-0.01774937\n-6.87212\n\n\nMAPE\n0.780873\n33.35526\n\n\nMASE\n0.03166853\n1.0000000\n\n\nACF1\n-0.006536676\n0.9970383\n\n\n\n\n\n\nThe ARIMA forecast tracks the actual data points very closely when compared to other model. The other forecast methods are less accurate when compared. From the table, Model error measurements of fit are much lower than snaive method.We can conclude that the fitted model is good."
  },
  {
    "objectID": "arima_stock_index.html",
    "href": "arima_stock_index.html",
    "title": "ARIMA Model for US Stock Indices",
    "section": "",
    "text": "The US stock market is one of the largest and most influential markets in the world, with the Dow Jones, Nasdaq, and S&P 500 indices serving as key indicators of market performance. To accurately forecast future values of these indices, it is essential to identify and model the complex patterns and trends that are often present in the data. The ARIMA model is a commonly used time series analysis technique that is well-suited for modeling and forecasting stock market indices, as it allows analysts to capture the underlying patterns and dynamics of the data. By fitting an ARIMA model to historical data, analysts can make more accurate predictions about future stock market performance, helping investors and financial institutions make informed decisions.\nClick to view the ARIMA Page for Dow Jones Index\nClick to view the ARIMA Page for NASDAQ Composite Index\nClick to view the ARIMA Page for S&P 500 Index"
  },
  {
    "objectID": "arimax_dow_jones_macroeconomic.html",
    "href": "arimax_dow_jones_macroeconomic.html",
    "title": "ARIMAX/SARIMAX Model for Dow Jones index and macroeconomic factors",
    "section": "",
    "text": "Endogenous variable: Dow Jones Index Exogenous variables: Macroeconomic indicators such as GDP, inflation rate, unemployment rate and interest rates\nBefore proceeding with model selection, it can be useful to examine the feature variables and select the appropriate ones. This can be done by creating cross-correlation function plots, which can serve as a verification of the correlation heatmap for the variables.\n\nCCF Plot for Dow Jones and Exogenous Variables\n\nGDPInterest RateInflation RateUnemployment Rate\n\n\n\n\nCode\npar(mfrow=c(1,1))\nccf_result <- ccf(normalized_index_factor_data[, c(\"DJI.Adjusted\")], normalized_index_factor_data[, c(\"gdp\")], \n    lag.max = 300,\n    main = \"Cros-Correlation Plot for Dow Jones Index and GDP Growth Rate \",\n    ylab = \"CCF\")\n\n\n\n\n\nCode\ncat(\"The sum of cross correlation function is\", sum(abs(ccf_result$acf)))\n\n\nThe sum of cross correlation function is 4.745225\n\n\n\n\n\n\nCode\npar(mfrow=c(1,1))\nccf_result <- ccf(normalized_index_factor_data[, c(\"DJI.Adjusted\")], normalized_index_factor_data[, c(\"interest\")], \n    lag.max = 300,\n    main = \"Cros-Correlation Plot for Dow Jones Index and Interest Rate\",\n    ylab = \"CCF\")\n\n\n\n\n\nCode\ncat(\"The sum of cross correlation function is\", sum(abs(ccf_result$acf)))\n\n\nThe sum of cross correlation function is 9.845121\n\n\n\n\n\n\nCode\npar(mfrow=c(1,1))\nccf_result <- ccf(normalized_index_factor_data[, c(\"DJI.Adjusted\")], normalized_index_factor_data[, c(\"inflation\")], \n    lag.max = 300,\n    main = \"Cros-Correlation Plot for Dow Jones Index and Inflation Rate\",\n    ylab = \"CCF\")\n\n\n\n\n\nCode\ncat(\"The sum of cross correlation function is\", sum(abs(ccf_result$acf)))\n\n\nThe sum of cross correlation function is 19.95814\n\n\n\n\n\n\nCode\npar(mfrow=c(1,1))\nccf_result <- ccf(normalized_index_factor_data[, c(\"DJI.Adjusted\")], normalized_index_factor_data[, c(\"unemployment\")], \n    lag.max = 300,\n    main = \"Cros-Correlation Plot for Dow Jones Index and Unemployment Rate\",\n    ylab = \"CCF\")\n\n\n\n\n\nCode\ncat(\"The sum of cross correlation function is\", sum(abs(ccf_result$acf)))\n\n\nThe sum of cross correlation function is 22.80055\n\n\n\n\n\nThe cross-correlation feature plots reaffirm the findings from the heatmap analysis, indicating that inflation and unemployment rate exhibit stronger correlations with the index compared to GDP and interest rates. The cross-correlation plots for GDP and interest rates show weaker and more scattered patterns, with strikes between the blue lines indicating lower correlation coefficients. This suggests that inflation and unemployment rate are more suitable feature variables for the ARIMAXmodel when predicting Dow Jones movements.\nFinal Exogenous variables: Macroeconomic indicators: Inflation rate and unemployment rate.\n\n\nEndogenous and Exogenous Variables Plot\n\n\nCode\nfinal_dj_factor_data <- index_factor_data %>%dplyr::select( Date,DJI.Adjusted, inflation,unemployment)\nnumeric_vars_dj_factor_data <- c(\"DJI.Adjusted\", \"inflation\", \"unemployment\")\nnumeric_dj_factor_data <- final_dj_factor_data[, numeric_vars_dj_factor_data]\nnormalized_dj_factor_data_numeric <- scale(numeric_dj_factor_data)\nnormalized_dj_factor_data_numeric_df <- data.frame(normalized_dj_factor_data_numeric)\nnormalized_dj_factor_data_ts <- ts(normalized_dj_factor_data_numeric, start = c(2010, 1), frequency = 4)\n\nautoplot(normalized_dj_factor_data_ts, facets=TRUE) +\n  xlab(\"Year\") + ylab(\"\") +\n  ggtitle(\"Dow Jones Index Stock Price, Inflation Rate and Unemployment Rate in USA 2010-2023\")\n\n\n\n\n\n\n\nCheck the stationarity\nBefore proceeding with further analysis, it’s important to check the stationarity of the time series data. If the series is not stationary, it needs to be differentiated before performing ARIMAX model. The stationarity of a multivariate time series data can be checked using the Phillips-Perron test, which is specifically designed for multivariate data. By performing the Phillips-Perron test on the multivariate time series data, we can determine if the data is stationary or if it requires differencing to make it suitable for ARIMAX modeling.\n\n\nCode\n# Convert your multivariate time series data to a matrix\nfinal_dj_factor_data_ts_multivariate <- as.matrix(normalized_dj_factor_data_ts)\n\n# Check for stationarity using Phillips-Perron test\nphillips_perron_test <- ur.pp(final_dj_factor_data_ts_multivariate)  \nsummary(phillips_perron_test)\n\n\n\n################################## \n# Phillips-Perron Unit Root Test # \n################################## \n\nTest regression with intercept \n\n\nCall:\nlm(formula = y ~ y.l1)\n\nResiduals:\n    Min      1Q  Median      3Q     Max \n-1.5621 -0.1561 -0.0549  0.0940  4.4921 \n\nCoefficients:\n            Estimate Std. Error t value Pr(>|t|)    \n(Intercept) 0.002691   0.040890   0.066    0.948    \ny.l1        0.857196   0.041305  20.753   <2e-16 ***\n---\nSignif. codes:  0 '***' 0.001 '**' 0.01 '*' 0.05 '.' 0.1 ' ' 1\n\nResidual standard error: 0.5091 on 153 degrees of freedom\nMultiple R-squared:  0.7379,    Adjusted R-squared:  0.7362 \nF-statistic: 430.7 on 1 and 153 DF,  p-value: < 2.2e-16\n\n\nValue of test-statistic, type: Z-alpha  is: -22.979 \n\n         aux. Z statistics\nZ-tau-mu            0.0656\n\n\nThe Phillips-Perron unit root test output suggests that the data is stationary. The test regression includes an intercept term and tests whether the coefficient on the lagged value is significantly different from one. The estimated coefficients indicate that the intercept is not significant (p-value > 0.05), while the coefficient on the lagged value is highly significant (p-value < 2.2e-16), suggesting that the data is stationary.\nAdditionally, the “Z-alpha” value is the test statistic, which is compared to critical values of the standard normal distribution to determine significance. In this case, the test statistic is -22.979, indicating strong evidence against the null hypothesis of a unit root. The “Z-tau-mu” value is an auxiliary test statistic used to correct for autocorrelation in the residuals, and it is close to zero, suggesting that there is no evidence of residual autocorrelation.\n\n\nFitting a ARIMAX model\nFitting an ARIMAX model will be a useful approach to modeling and predicting time series data when external factors are believed to have an impact on the response variable. In this case, we will be using an ARIMAX model to predict the DJI Index, taking into account the effects of inflation and unemployment on the Index. By including these exogenous variables in our model, we can improve the accuracy of our predictions and gain a better understanding of the underlying dynamics of the time series.\n\nAuto ARIMAResiduals\n\n\n\n\nCode\nxreg <- cbind(Inflation = normalized_dj_factor_data_ts[, \"inflation\"],\n              Unemployment = normalized_dj_factor_data_ts[, \"unemployment\"])\n\nfit <- auto.arima(normalized_dj_factor_data_ts[, \"DJI.Adjusted\"], xreg = xreg)\nsummary(fit)\n\n\nSeries: normalized_dj_factor_data_ts[, \"DJI.Adjusted\"] \nRegression with ARIMA(0,1,0) errors \n\nCoefficients:\n      Inflation  Unemployment\n         0.0999       -0.2337\ns.e.     0.0804        0.0365\n\nsigma^2 = 0.03424:  log likelihood = 14.7\nAIC=-23.4   AICc=-22.89   BIC=-17.6\n\nTraining set error measures:\n                     ME      RMSE       MAE      MPE     MAPE      MASE\nTraining set 0.03018628 0.1796281 0.1389231 4.691153 32.83951 0.3988319\n                    ACF1\nTraining set -0.07188426\n\n\n\n\n\n\nCode\ncheckresiduals(fit)\n\n\n\n\n\n\n    Ljung-Box test\n\ndata:  Residuals from Regression with ARIMA(0,1,0) errors\nQ* = 6.4181, df = 8, p-value = 0.6005\n\nModel df: 0.   Total lags used: 8\n\n\n\n\n\nThe auto.arima function reveals that the best-fit model is (0,1,0). Analysis of the residuals plot indicates that the fluctuates a lot and lies between -2000 to 2000.The ACF plot of the residuals shows significant lags, indicating that the model is performing well. The qq-plot also suggests normality in the residuals. The p-value greater than 0.05 actually suggests that there is no evidence of significant autocorrelation in the residuals, which means that the model is performing well in terms of capturing the temporal patterns in the data.\n\n\nFitting the model manually\n\nLinear ModelACF of ResidualsACF of ResidualsDifferentiated Residual\n\n\n\n\nCode\nnormalized_dj_factor_data_numeric_df$DJI.Adjusted<-ts(normalized_dj_factor_data_numeric_df$DJI.Adjusted,star=decimal_date(as.Date(\"2010-01-01\",format = \"%Y-%m-%d\")),frequency = 4)\nnormalized_dj_factor_data_numeric_df$inflation<-ts(normalized_dj_factor_data_numeric_df$inflation,star=decimal_date(as.Date(\"2010-01-01\",format = \"%Y-%m-%d\")),frequency = 4)\nnormalized_dj_factor_data_numeric_df$unemployment<-ts(normalized_dj_factor_data_numeric_df$unemployment,star=decimal_date(as.Date(\"2010-01-01\",format = \"%Y-%m-%d\")),frequency = 4)\n\n############# First fit the linear model##########\nfit.reg <- lm(DJI.Adjusted ~ inflation+unemployment, data=normalized_dj_factor_data_numeric_df)\nsummary(fit.reg)\n\n\n\nCall:\nlm(formula = DJI.Adjusted ~ inflation + unemployment, data = normalized_dj_factor_data_numeric_df)\n\nResiduals:\n    Min      1Q  Median      3Q     Max \n-0.8660 -0.4391 -0.2518  0.3505  1.6797 \n\nCoefficients:\n               Estimate Std. Error t value Pr(>|t|)    \n(Intercept)   1.180e-16  9.263e-02   0.000   1.0000    \ninflation     5.296e-01  1.067e-01   4.963 8.79e-06 ***\nunemployment -3.416e-01  1.067e-01  -3.202   0.0024 ** \n---\nSignif. codes:  0 '***' 0.001 '**' 0.01 '*' 0.05 '.' 0.1 ' ' 1\n\nResidual standard error: 0.668 on 49 degrees of freedom\nMultiple R-squared:  0.5713,    Adjusted R-squared:  0.5538 \nF-statistic: 32.65 on 2 and 49 DF,  p-value: 9.73e-10\n\n\n\n\n\n\nCode\nres.fit<-ts(residuals(fit.reg),star=decimal_date(as.Date(\"2010-01-01\",format = \"%Y-%m-%d\")),frequency = 4)\n############## Then look at the residuals ############\nacf(res.fit)\n\n\n\n\n\n\n\n\n\nCode\nPacf(res.fit)\n\n\n\n\n\n\n\n\n\nCode\nres.fit %>% diff() %>% ggtsdisplay()\n\n\n\n\n\n\n\n\nThe output displays the results of a linear regression model using two predictor variables, inflation and unemployment, to explain the variation in the response variable, DJI.Adjusted. The coefficients of inflation and unemployment have p-values of less than 0.05, indicating that both variables significantly impact DJI.Adjusted. The R-squared value of approximately 57% suggests that the model explains a moderate amount of the variation in the response variable. Examination of the ACF plot of residuals reveals some autocorrelation, and therefore, the series is differentiated to achieve stationarity. By analyzing the differentiated ACF and PACF plots, the parameters if the ARIMAX model are that p=4, q=1, and d=1.\n\n\nFinding the model parameters.\n\nARIMAX ResultModel 1 PlotModel 1Model 2 PlotModel 2\n\n\n\n\nCode\nARIMA.c=function(p1,p2,q1,q2,data){\ntemp=c()\nd=1\ni=1\ntemp= data.frame()\nls=matrix(rep(NA,6*30),nrow=30)\n\n\nfor (p in p1:p2)#\n{\n  for(q in q1:q2)#\n  {\n    for(d in 0:1)\n    {\n      \n      if(p+d+q<=6)\n      {\n        \n        model<- Arima(data,order=c(p,d,q))\n        ls[i,]= c(p,d,q,model$aic,model$bic,model$aicc)\n        i=i+1\n        #print(i)\n        \n      }\n      \n    }\n  }\n}\n\n\ntemp= as.data.frame(ls)\nnames(temp)= c(\"p\",\"d\",\"q\",\"AIC\",\"BIC\",\"AICc\")\n\ntemp\n}\n\noutput <- ARIMA.c(1,5,1,5,data=residuals(fit.reg))\n\noutput[which.min(output$AIC),] \n\n\n  p d q      AIC      BIC     AICc\n6 1 1 3 7.439979 17.09911 8.773313\n\n\nCode\noutput[which.min(output$BIC),]\n\n\n  p d q      AIC      BIC     AICc\n6 1 1 3 7.439979 17.09911 8.773313\n\n\nCode\noutput[which.min(output$AICc),]\n\n\n  p d q      AIC      BIC     AICc\n6 1 1 3 7.439979 17.09911 8.773313\n\n\n\n\n\n\nCode\nset.seed(1234)\n\nmodel_output_1 <- capture.output(sarima(res.fit, 1,1,3)) \n\n\n\n\n\n\n\n\n\nCode\ncat(model_output_1[45:78], model_output_1[length(model_output_1)], sep = \"\\n\")\n\n\n$fit\n\nCall:\narima(x = xdata, order = c(p, d, q), seasonal = list(order = c(P, D, Q), period = S), \n    xreg = constant, transform.pars = trans, fixed = fixed, optim.control = list(trace = trc, \n        REPORT = 1, reltol = tol))\n\nCoefficients:\n         ar1      ma1     ma2     ma3  constant\n      0.4565  -0.4496  0.1422  0.4319   -0.0068\ns.e.  0.2077   0.2094  0.1524  0.1736    0.0654\n\nsigma^2 estimated as 0.05442:  log likelihood = 1.29,  aic = 9.43\n\n$degrees_of_freedom\n[1] 46\n\n$ttable\n         Estimate     SE t.value p.value\nar1        0.4565 0.2077  2.1978  0.0330\nma1       -0.4496 0.2094 -2.1469  0.0371\nma2        0.1422 0.1524  0.9331  0.3557\nma3        0.4319 0.1736  2.4882  0.0165\nconstant  -0.0068 0.0654 -0.1040  0.9176\n\n$AIC\n[1] 0.1848859\n\n$AICc\n[1] 0.2110297\n\n$BIC\n[1] 0.4121595\n\n\n\n\n\n\nCode\nmodel_output_2 <- capture.output(sarima(res.fit,0,1,0)) \n\n\n\n\n\n\n\n\n\nCode\ncat(model_output_2[9:38], model_output_2[length(model_output_2)], sep = \"\\n\")\n\n\n$fit\n\nCall:\narima(x = xdata, order = c(p, d, q), seasonal = list(order = c(P, D, Q), period = S), \n    xreg = constant, transform.pars = trans, fixed = fixed, optim.control = list(trace = trc, \n        REPORT = 1, reltol = tol))\n\nCoefficients:\n      constant\n       -0.0052\ns.e.    0.0365\n\nsigma^2 estimated as 0.06791:  log likelihood = -3.78,  aic = 11.56\n\n$degrees_of_freedom\n[1] 50\n\n$ttable\n         Estimate     SE t.value p.value\nconstant  -0.0052 0.0365 -0.1417  0.8879\n\n$AIC\n[1] 0.2267275\n\n$AICc\n[1] 0.2283282\n\n$BIC\n[1] 0.3024854\n\n\n\n\n\nAfter manual fitting, we have identified that the ARIMAX model most suitable for our data is likely (1,1,3). A comparison of its AIC, AICc, and BIC values to those of other models indicates that it has the minimum values. We proceed to fit this model using both manual and auto.arima methods. After fitting, we observe that both models have p-values greater than 0, indicating that they are statistically significant. To determine the best fit model, we use cross-validation.\n\n\nCross Validation\n\nRMSE PlotRMSE Results\n\n\n\n\nCode\nn=length(res.fit)\nk= 51\n \n \nrmse1 <- matrix(NA, (n-k),4)\nrmse2 <- matrix(NA, (n-k),4)\n\n\nst <- tsp(res.fit)[1]+(k-5)/4 \n\nfor(i in 1:(n-k))\n{\n  xtrain <- window(res.fit, end=st + i/4)\n  xtest <- window(res.fit, start=st + (i+1)/4, end=st + (i+4)/4)\n  \n  #ARIMA(0,1,0) ARIMA(1,1,3)\n  \n  fit <- Arima(xtrain, order=c(1,1,3),\n                include.drift=TRUE, method=\"ML\")\n  fcast <- forecast(fit, h=4)\n  \n  fit2 <- Arima(xtrain, order=c(0,1,0),\n                include.drift=TRUE, method=\"ML\")\n  fcast2 <- forecast(fit2, h=4)\n\n  \n\n  rmse1[i,1:length(xtest)]   <- sqrt((fcast$mean-xtest)^2)\n  rmse2[i,1:length(xtest)] <- sqrt((fcast2$mean-xtest)^2)\n  \n}\n\nplot(1:4,colMeans(rmse1,na.rm=TRUE), type=\"l\",col=2, xlab=\"horizon\", ylab=\"RMSE\")\nlines(1:4, colMeans(rmse2,na.rm=TRUE), type=\"l\",col=3)\nlegend(\"topleft\",legend=c(\"fit1\",\"fit2\"),col=2:3,lty=1)\n\n\n\n\n\n\n\n\n\nCode\ncat(\"RMSE values for Model 1\\n\", colMeans(rmse1))\n\n\nRMSE values for Model 1\n 0.2600182 0.512597 0.8190045 1.189145\n\n\nCode\ncat(\"RMSE values for Model 2\\n\", colMeans(rmse2))\n\n\nRMSE values for Model 2\n 0.3674812 0.7675162 1.181429 1.556654\n\n\n\n\n\nThe RMSE cross-validation plot shows that the RMSE of Model 1 (1,1,3) is lower than that of Model 2 (0,1,0). This suggests that Model 1 (1,1,3) is a better fit for the data when compared to Model 2 (0,1,0).\n\n\nForecast\n\nForecast for DJI with feature variablesARIMA Model for InflationARIMA Model for EmploymentARIMA Model for DJI with feature variables\n\n\n\n\nCode\n#fiting an ARIMA model to the Inflation variable\ninflation_fit<-auto.arima(normalized_dj_factor_data_numeric_df$inflation) \nfinflation<-forecast(inflation_fit)\n\n#fitting an ARIMA model to the Unemployment variable\nunemployment_fit<-auto.arima(normalized_dj_factor_data_numeric_df$unemployment) \nfunemployment<-forecast(unemployment_fit)\n\n# best model fit for forcasting\nxreg <- cbind(Inflation = normalized_dj_factor_data_ts[, \"inflation\"],\n              Unemployment = normalized_dj_factor_data_ts[, \"unemployment\"])\n\nfit <- Arima(normalized_dj_factor_data_ts[, \"DJI.Adjusted\"],order=c(1,1,3),xreg=xreg)\n\n# Forcast the stock price using feature variables\nfxreg <- cbind(Inflation = finflation$mean,\n              Unemployment = funemployment$mean)\nfcast <- forecast(fit, xreg=fxreg) \nautoplot(fcast) + xlab(\"Year\") +\n  ylab(\"Normalised DJI.Adjusted\")\n\n\n\n\n\n\n\n\n\nCode\n#fiting an ARIMA model to the Inflation variable\ninflation_fit<-auto.arima(normalized_dj_factor_data_numeric_df$inflation) \nfinflation<-forecast(inflation_fit)\nsummary(inflation_fit)\n\n\nSeries: normalized_dj_factor_data_numeric_df$inflation \nARIMA(0,1,3)(0,0,1)[4] with drift \n\nCoefficients:\n         ma1     ma2     ma3     sma1   drift\n      0.2153  0.5354  0.4353  -0.3707  0.0567\ns.e.  0.1719  0.1915  0.1455   0.1942  0.0499\n\nsigma^2 = 0.0708:  log likelihood = -3.48\nAIC=18.96   AICc=20.87   BIC=30.55\n\nTraining set error measures:\n                      ME     RMSE       MAE       MPE    MAPE      MASE\nTraining set 0.002515126 0.250258 0.1802161 -17.11236 114.163 0.3180481\n                    ACF1\nTraining set -0.06141191\n\n\n\n\n\n\nCode\n#fitting an ARIMA model to the Unemployment variable\nunemployment_fit<-auto.arima(normalized_dj_factor_data_numeric_df$unemployment) \nfunemployment<-forecast(unemployment_fit)\nsummary(unemployment_fit)\n\n\nSeries: normalized_dj_factor_data_numeric_df$unemployment \nARIMA(0,1,2) \n\nCoefficients:\n          ma1      ma2\n      -0.3027  -0.2410\ns.e.   0.1376   0.1472\n\nsigma^2 = 0.4982:  log likelihood = -53.72\nAIC=113.45   AICc=113.96   BIC=119.24\n\nTraining set error measures:\n                     ME      RMSE      MAE      MPE     MAPE      MASE\nTraining set -0.1001611 0.6851533 0.281119 140.1191 219.8632 0.4565275\n                    ACF1\nTraining set -0.02079058\n\n\n\n\n\n\nCode\n# best model fit for forcasting\nxreg <- cbind(Inflation = normalized_dj_factor_data_ts[, \"inflation\"],\n              Unemployment = normalized_dj_factor_data_ts[, \"unemployment\"])\n\nfit <- Arima(normalized_dj_factor_data_ts[, \"DJI.Adjusted\"],order=c(1,1,3),xreg=xreg)\nsummary(fit)\n\n\nSeries: normalized_dj_factor_data_ts[, \"DJI.Adjusted\"] \nRegression with ARIMA(1,1,3) errors \n\nCoefficients:\n         ar1      ma1     ma2     ma3  Inflation  Unemployment\n      0.4207  -0.6167  0.3593  0.5494     0.2413       -0.2258\ns.e.  0.2317   0.2680  0.2682  0.2372     0.0521        0.0188\n\nsigma^2 = 0.02576:  log likelihood = 21.19\nAIC=-28.38   AICc=-25.77   BIC=-14.85\n\nTraining set error measures:\n                      ME      RMSE       MAE       MPE     MAPE      MASE\nTraining set 0.001852359 0.1492948 0.1197532 -2.903386 19.90147 0.3437973\n                    ACF1\nTraining set -0.02545389\n\n\n\n\n\nThe ARIMAX model may have incorporated exogenous variables such as macroeconomic factors, which could indicate the potential impact of external factors on the forecasted values. The findings suggest that inflation and unemployment rates significantly impact the DJI stock price, underscoring the importance of including macroeconomic factors in stock market forecasting. This is further supported by the comparison of RMSE, MAE, AIC and BIC values for forecasts with and without the feature variable. It is found that including the feature variable leads to a lower values, indicating that the feature variable makes a significant impact and should be considered in stock market forecasting."
  },
  {
    "objectID": "arimax_nasdaq_macroeconomic.html",
    "href": "arimax_nasdaq_macroeconomic.html",
    "title": "ARIMAX/SARIMAX Model for NASDAQ Composite index and macroeconomic factors",
    "section": "",
    "text": "Endogenous variable: NASDAQ Composite Index Exogenous variables: Macroeconomic indicators such as GDP, inflation rate, unemployment rate and interest rates\nBefore proceeding with model selection, it can be useful to examine the feature variables and select the appropriate ones. This can be done by creating cross-correlation function plots, which can serve as a verification of the correlation heatmap for the variables.\n\nCCF Plot for NASDAQ Composite and Exogenous Variables\n\nGDPInterest RateInflation RateUnemployment Rate\n\n\n\n\nCode\npar(mfrow=c(1,1))\nccf_result <- ccf(normalized_index_factor_data[, c(\"IXIC.Adjusted\")], normalized_index_factor_data[, c(\"gdp\")], \n    lag.max = 300,\n    main = \"Cros-Correlation Plot for NASDAQ Composite Index and GDP Growth Rate \",\n    ylab = \"CCF\")\n\n\n\n\n\nCode\ncat(\"The sum of cross correlation function is\", sum(abs(ccf_result$acf)))\n\n\nThe sum of cross correlation function is 4.754054\n\n\n\n\n\n\nCode\npar(mfrow=c(1,1))\nccf_result <- ccf(normalized_index_factor_data[, c(\"IXIC.Adjusted\")], normalized_index_factor_data[, c(\"interest\")], \n    lag.max = 300,\n    main = \"Cros-Correlation Plot for NASDAQ Composite Index and Interest Rate\",\n    ylab = \"CCF\")\n\n\n\n\n\nCode\ncat(\"The sum of cross correlation function is\", sum(abs(ccf_result$acf)))\n\n\nThe sum of cross correlation function is 10.60738\n\n\n\n\n\n\nCode\npar(mfrow=c(1,1))\nccf_result <- ccf(normalized_index_factor_data[, c(\"IXIC.Adjusted\")], normalized_index_factor_data[, c(\"inflation\")], \n    lag.max = 300,\n    main = \"Cros-Correlation Plot for NASDAQ Composite Index and Inflation Rate\",\n    ylab = \"CCF\")\n\n\n\n\n\nCode\ncat(\"The sum of cross correlation function is\", sum(abs(ccf_result$acf)))\n\n\nThe sum of cross correlation function is 18.798\n\n\n\n\n\n\nCode\npar(mfrow=c(1,1))\nccf_result <- ccf(normalized_index_factor_data[, c(\"IXIC.Adjusted\")], normalized_index_factor_data[, c(\"unemployment\")], \n    lag.max = 300,\n    main = \"Cros-Correlation Plot for NASDAQ Composite Index and Unemployment Rate\",\n    ylab = \"CCF\")\n\n\n\n\n\nCode\ncat(\"The sum of cross correlation function is\", sum(abs(ccf_result$acf)))\n\n\nThe sum of cross correlation function is 22.22389\n\n\n\n\n\nThe cross-correlation feature plots reaffirm the findings from the heatmap analysis, indicating that inflation and unemployment rate exhibit stronger correlations with the index compared to GDP and interest rates. The cross-correlation plots for GDP and interest rates show weaker and more scattered patterns, with strikes between the blue lines indicating lower correlation coefficients. This suggests that inflation and unemployment rate are more suitable feature variables for the ARIMAX model when predicting NASDAQ Composite movements.\nFinal Exogenous variables: Macroeconomic indicators: Inflation rate and unemployment rate.\n\n\nEndogenous and Exogenous Variables Plot\n\n\nCode\nfinal_nadsaq_factor_data <- index_factor_data %>%dplyr::select( Date,IXIC.Adjusted, inflation,unemployment)\nnumeric_vars_nadsaq_factor_data <- c(\"IXIC.Adjusted\", \"inflation\", \"unemployment\")\nnumeric_nadsaq_factor_data <- final_nadsaq_factor_data[, numeric_vars_nadsaq_factor_data]\nnormalized_nadsaq_factor_data_numeric <- scale(numeric_nadsaq_factor_data)\nnormalized_nadsaq_factor_data_numeric_df <- data.frame(normalized_nadsaq_factor_data_numeric)\nnormalized_nadsaq_factor_data_ts <- ts(normalized_nadsaq_factor_data_numeric, start = c(2010, 1), frequency = 4)\n\nautoplot(normalized_nadsaq_factor_data_ts, facets=TRUE) +\n  xlab(\"Year\") + ylab(\"\") +\n  ggtitle(\"NASDAQ Composite Index Stock Price, Inflation Rate and Unemployment Rate in USA 2010-2023\")\n\n\n\n\n\n\n\nCheck the stationarity\nBefore proceeding with further analysis, it’s important to check the stationarity of the time series data. If the series is not stationary, it needs to be differentiated before performing ARIMAX model. The stationarity of a multivariate time series data can be checked using the Phillips-Perron test, which is specifically designed for multivariate data. By performing the Phillips-Perron test on the multivariate time series data, we can determine if the data is stationary or if it requires differencing to make it suitable for ARIMAX modeling.\n\n\nCode\n# Convert your multivariate time series data to a matrix\nfinal_nadsaq_factor_data_ts_multivariate <- as.matrix(normalized_nadsaq_factor_data_ts)\n\n# Check for stationarity using Phillips-Perron test\nphillips_perron_test <- ur.pp(final_nadsaq_factor_data_ts_multivariate)  \nsummary(phillips_perron_test)\n\n\n\n################################## \n# Phillips-Perron Unit Root Test # \n################################## \n\nTest regression with intercept \n\n\nCall:\nlm(formula = y ~ y.l1)\n\nResiduals:\n    Min      1Q  Median      3Q     Max \n-1.5440 -0.1528 -0.0519  0.0850  4.4989 \n\nCoefficients:\n            Estimate Std. Error t value Pr(>|t|)    \n(Intercept) 0.001161   0.040544   0.029    0.977    \ny.l1        0.861900   0.040955  21.045   <2e-16 ***\n---\nSignif. codes:  0 '***' 0.001 '**' 0.01 '*' 0.05 '.' 0.1 ' ' 1\n\nResidual standard error: 0.5048 on 153 degrees of freedom\nMultiple R-squared:  0.7432,    Adjusted R-squared:  0.7416 \nF-statistic: 442.9 on 1 and 153 DF,  p-value: < 2.2e-16\n\n\nValue of test-statistic, type: Z-alpha  is: -23.0718 \n\n         aux. Z statistics\nZ-tau-mu            0.0293\n\n\nThe Phillips-Perron unit root test output suggests that the data is stationary. The test regression includes an intercept term and tests whether the coefficient on the lagged value is significantly different from one. The estimated coefficients indicate that the intercept is not significant (p-value > 0.05), while the coefficient on the lagged value is highly significant (p-value < 2e-16), suggesting that the data is stationary.\nAdditionally, the “Z-alpha” value is the test statistic, which is compared to critical values of the standard normal distribution to determine significance. In this case, the test statistic is -23.0718, indicating strong evidence against the null hypothesis of a unit root. The “Z-tau-mu” value is an auxiliary test statistic used to correct for autocorrelation in the residuals, and it is close to zero, suggesting that there is no evidence of residual autocorrelation.\n\n\nFitting a ARIMAX model\nFitting an ARIMAX model will be a useful approach to modeling and predicting time series data when external factors are believed to have an impact on the response variable. In this case, we will be using an ARIMAX model to predict the NASDAQ Composite Index, taking into account the effects of inflation and unemployment on the Index. By including these exogenous variables in our model, we can improve the accuracy of our predictions and gain a better understanding of the underlying dynamics of the time series.\n\nAuto ARIMAResiduals\n\n\n\n\nCode\nxreg <- cbind(Inflation = normalized_nadsaq_factor_data_ts[, \"inflation\"],\n              Unemployment = normalized_nadsaq_factor_data_ts[, \"unemployment\"])\n\nfit <- auto.arima(normalized_nadsaq_factor_data_ts[, \"IXIC.Adjusted\"], xreg = xreg)\nsummary(fit)\n\n\nSeries: normalized_nadsaq_factor_data_ts[, \"IXIC.Adjusted\"] \nRegression with ARIMA(0,1,0) errors \n\nCoefficients:\n      Inflation  Unemployment\n         0.1224       -0.1363\ns.e.     0.0863        0.0392\n\nsigma^2 = 0.0394:  log likelihood = 11.12\nAIC=-16.24   AICc=-15.73   BIC=-10.44\n\nTraining set error measures:\n                    ME      RMSE       MAE       MPE     MAPE      MASE\nTraining set 0.0284036 0.1926934 0.1243624 -4.567455 34.48622 0.3764756\n                  ACF1\nTraining set 0.2577338\n\n\n\n\n\n\nCode\ncheckresiduals(fit)\n\n\n\n\n\n\n    Ljung-Box test\n\ndata:  Residuals from Regression with ARIMA(0,1,0) errors\nQ* = 13.819, df = 8, p-value = 0.0866\n\nModel df: 0.   Total lags used: 8\n\n\n\n\n\nThe auto.arima function reveals that the best-fit model is (0,1,0). Analysis of the residuals plot indicates that the fluctuates a lot and lies between -2000 to 2000.The ACF plot of the residuals shows significant lags, indicating that the model is performing well. The qq-plot also suggests normality in the residuals. The p-value greater than 0.05 actually suggests that there is no evidence of significant autocorrelation in the residuals, which means that the model is performing well in terms of capturing the temporal patterns in the data.\n\n\nFitting the model manually\n\nLinear ModelACF of ResidualsACF of ResidualsDifferentiated Residual\n\n\n\n\nCode\nnormalized_nadsaq_factor_data_numeric_df$IXIC.Adjusted<-ts(normalized_nadsaq_factor_data_numeric_df$IXIC.Adjusted,star=decimal_date(as.Date(\"2010-01-01\",format = \"%Y-%m-%d\")),frequency = 4)\nnormalized_nadsaq_factor_data_numeric_df$inflation<-ts(normalized_nadsaq_factor_data_numeric_df$inflation,star=decimal_date(as.Date(\"2010-01-01\",format = \"%Y-%m-%d\")),frequency = 4)\nnormalized_nadsaq_factor_data_numeric_df$unemployment<-ts(normalized_nadsaq_factor_data_numeric_df$unemployment,star=decimal_date(as.Date(\"2010-01-01\",format = \"%Y-%m-%d\")),frequency = 4)\n\n############# First fit the linear model##########\nfit.reg <- lm(IXIC.Adjusted ~ inflation+unemployment, data=normalized_nadsaq_factor_data_numeric_df)\nsummary(fit.reg)\n\n\n\nCall:\nlm(formula = IXIC.Adjusted ~ inflation + unemployment, data = normalized_nadsaq_factor_data_numeric_df)\n\nResiduals:\n    Min      1Q  Median      3Q     Max \n-0.9537 -0.4333 -0.2053  0.2360  2.1227 \n\nCoefficients:\n               Estimate Std. Error t value Pr(>|t|)    \n(Intercept)   3.654e-17  9.763e-02   0.000     1.00    \ninflation     6.353e-01  1.125e-01   5.649  8.1e-07 ***\nunemployment -1.566e-01  1.125e-01  -1.392     0.17    \n---\nSignif. codes:  0 '***' 0.001 '**' 0.01 '*' 0.05 '.' 0.1 ' ' 1\n\nResidual standard error: 0.704 on 49 degrees of freedom\nMultiple R-squared:  0.5238,    Adjusted R-squared:  0.5044 \nF-statistic: 26.95 on 2 and 49 DF,  p-value: 1.276e-08\n\n\n\n\n\n\nCode\nres.fit<-ts(residuals(fit.reg),star=decimal_date(as.Date(\"2010-01-01\",format = \"%Y-%m-%d\")),frequency = 4)\n############## Then look at the residuals ############\nacf(res.fit)\n\n\n\n\n\n\n\n\n\nCode\nPacf(res.fit)\n\n\n\n\n\n\n\n\n\nCode\nres.fit %>% diff() %>% ggtsdisplay()\n\n\n\n\n\n\n\n\nThe output displays the results of a linear regression model using two predictor variables, inflation and unemployment, to explain the variation in the response variable, IXIC.Adjusted. The coefficients of inflation and unemployment have p-values of less than 0.05, indicating that both variables significantly impact IXIC.Adjusted. The R-squared value of approximately 52% suggests that the model explains a moderate amount of the variation in the response variable. Examination of the ACF plot of residuals reveals some autocorrelation, and therefore, the series is differentiated to achieve stationarity. By analyzing the differentiated ACF and PACF plots, the parameters if the ARIMAX model are that p=1, q=1,2, and d=1.\n\n\nFinding the model parameters.\n\nARIMAX ResultModel 1 PlotModel 1Model 2 PlotModel 2Model 3 PlotModel 3\n\n\n\n\nCode\nARIMA.c=function(p1,q1,q2,data){\ntemp=c()\nd=1\ni=1\ntemp= data.frame()\nls=matrix(rep(NA,6*30),nrow=30)\n\n\nfor (p in p1)#\n{\n  for(q in q1:q2)#\n  {\n    for(d in 0:1)\n    {\n      \n      if(p+d+q<=6)\n      {\n        \n        model<- Arima(data,order=c(p,d,q))\n        ls[i,]= c(p,d,q,model$aic,model$bic,model$aicc)\n        i=i+1\n        #print(i)\n        \n      }\n      \n    }\n  }\n}\n\n\ntemp= as.data.frame(ls)\nnames(temp)= c(\"p\",\"d\",\"q\",\"AIC\",\"BIC\",\"AICc\")\n\ntemp\n}\n\noutput <- ARIMA.c(1,1,2,data=residuals(fit.reg))\n\noutput[which.min(output$AIC),] \n\n\n  p d q      AIC      BIC     AICc\n4 1 1 2 5.193728 12.92103 6.063294\n\n\nCode\noutput[which.min(output$BIC),]\n\n\n  p d q     AIC      BIC     AICc\n2 1 1 1 5.89386 11.68934 6.404499\n\n\nCode\noutput[which.min(output$AICc),]\n\n\n  p d q      AIC      BIC     AICc\n4 1 1 2 5.193728 12.92103 6.063294\n\n\n\n\n\n\nCode\nset.seed(1234)\n\nmodel_output_1 <- capture.output(sarima(res.fit, 1,1,1)) \n\n\n\n\n\n\n\n\n\nCode\ncat(model_output_1[32:63], model_output_1[length(model_output_1)], sep = \"\\n\")\n\n\n$fit\n\nCall:\narima(x = xdata, order = c(p, d, q), seasonal = list(order = c(P, D, Q), period = S), \n    xreg = constant, transform.pars = trans, fixed = fixed, optim.control = list(trace = trc, \n        REPORT = 1, reltol = tol))\n\nCoefficients:\n         ar1      ma1  constant\n      0.7210  -0.3976   -0.0146\ns.e.  0.1802   0.2033    0.0708\n\nsigma^2 estimated as 0.05812:  log likelihood = 0.07,  aic = 7.85\n\n$degrees_of_freedom\n[1] 48\n\n$ttable\n         Estimate     SE t.value p.value\nar1        0.7210 0.1802  4.0008  0.0002\nma1       -0.3976 0.2033 -1.9557  0.0563\nconstant  -0.0146 0.0708 -0.2055  0.8381\n\n$AIC\n[1] 0.1539454\n\n$AICc\n[1] 0.1639579\n\n$BIC\n[1] 0.3054612\n\n\n\n\n\n\nCode\nmodel_output_2 <- capture.output(sarima(res.fit,1,1,2)) \n\n\n\n\n\n\n\n\n\nCode\ncat(model_output_2[31:63], model_output_2[length(model_output_2)], sep = \"\\n\")\n\n\n$fit\n\nCall:\narima(x = xdata, order = c(p, d, q), seasonal = list(order = c(P, D, Q), period = S), \n    xreg = constant, transform.pars = trans, fixed = fixed, optim.control = list(trace = trc, \n        REPORT = 1, reltol = tol))\n\nCoefficients:\n         ar1      ma1     ma2  constant\n      0.6080  -0.4268  0.3364   -0.0163\ns.e.  0.2276   0.2165  0.1899    0.0737\n\nsigma^2 estimated as 0.0548:  log likelihood = 1.43,  aic = 7.14\n\n$degrees_of_freedom\n[1] 47\n\n$ttable\n         Estimate     SE t.value p.value\nar1        0.6080 0.2276  2.6717  0.0103\nma1       -0.4268 0.2165 -1.9715  0.0546\nma2        0.3364 0.1899  1.7714  0.0830\nconstant  -0.0163 0.0737 -0.2218  0.8254\n\n$AIC\n[1] 0.1400875\n\n$AICc\n[1] 0.1571378\n\n$BIC\n[1] 0.3294822\n\n\n\n\n\n\nCode\nmodel_output_3 <- capture.output(sarima(res.fit,0,1,0)) \n\n\n\n\n\n\n\n\n\nCode\ncat(model_output_3[9:38], model_output_3[length(model_output_3)], sep = \"\\n\")\n\n\n$fit\n\nCall:\narima(x = xdata, order = c(p, d, q), seasonal = list(order = c(P, D, Q), period = S), \n    xreg = constant, transform.pars = trans, fixed = fixed, optim.control = list(trace = trc, \n        REPORT = 1, reltol = tol))\n\nCoefficients:\n      constant\n       -0.0086\ns.e.    0.0369\n\nsigma^2 estimated as 0.06942:  log likelihood = -4.34,  aic = 12.69\n\n$degrees_of_freedom\n[1] 50\n\n$ttable\n         Estimate     SE t.value p.value\nconstant  -0.0086 0.0369 -0.2318  0.8176\n\n$AIC\n[1] 0.2487808\n\n$AICc\n[1] 0.2503814\n\n$BIC\n[1] 0.3245387\n\n\n\n\n\nFollowing the manual fitting process, we identified that the ARIMAX model with the minimum AIC and AICc has the parameters (1,1,2), while the model with the minimum BIC has the parameters (1,1,1). We fitted both models using both the manual and auto.arima methods and observed that all the models have p-values greater than 0, indicating statistical significance. To determine the best fit model, we performed cross-validation.\n\n\nCross Validation\n\nRMSE PlotRMSE Results\n\n\n\n\nCode\nn=length(res.fit)\nk= 51\n \n \nrmse1 <- matrix(NA, (n-k),4)\nrmse2 <- matrix(NA, (n-k),4)\nrmse3 <- matrix(NA, (n-k),4)\n\n\nst <- tsp(res.fit)[1]+(k-5)/4 \n\nfor(i in 1:(n-k))\n{\n  xtrain <- window(res.fit, end=st + i/4)\n  xtest <- window(res.fit, start=st + (i+1)/4, end=st + (i+4)/4)\n  \n  #ARIMA(0,1,0) ARIMA(1,1,1) ARIMA(1,1,2)\n  \n  fit <- Arima(xtrain, order=c(1,1,2),\n                include.drift=TRUE, method=\"ML\")\n  fcast <- forecast(fit, h=4)\n  \n  fit2 <- Arima(xtrain, order=c(1,1,1),\n                include.drift=TRUE, method=\"ML\")\n  fcast2 <- forecast(fit2, h=4)\n\n  fit3 <- Arima(xtrain, order=c(0,1,0),\n                include.drift=TRUE, method=\"ML\")\n  fcast3 <- forecast(fit3, h=4)\n\n  rmse1[i,1:length(xtest)]   <- sqrt((fcast$mean-xtest)^2)\n  rmse2[i,1:length(xtest)] <- sqrt((fcast2$mean-xtest)^2)\n  rmse3[i,1:length(xtest)] <- sqrt((fcast3$mean-xtest)^2)\n}\n\nplot(1:4,colMeans(rmse1,na.rm=TRUE), type=\"l\",col=2, xlab=\"horizon\", ylab=\"RMSE\")\nlines(1:4, colMeans(rmse2,na.rm=TRUE), type=\"l\",col=3)\nlines(1:4, colMeans(rmse3,na.rm=TRUE), type=\"l\",col=4)\nlegend(\"topleft\",legend=c(\"fit1\",\"fit2\",\"fit3\"),col=2:4,lty=1)\n\n\n\n\n\n\n\n\n\nCode\ncat(\"RMSE values for Model 1\\n\", colMeans(rmse1))\n\n\nRMSE values for Model 1\n 0.02597127 0.5330269 1.236045 1.521616\n\n\nCode\ncat(\"RMSE values for Model 2\\n\", colMeans(rmse2))\n\n\nRMSE values for Model 2\n 0.3579776 0.9094041 1.62559 1.919108\n\n\nCode\ncat(\"RMSE values for Model 3\\n\", colMeans(rmse3))\n\n\nRMSE values for Model 3\n 0.4130706 0.9870188 1.712512 2.009977\n\n\n\n\n\nThe RMSE cross-validation plot shows that the RMSE of Model 1 (1,1,2) is lower than that of Model 2 (1,1,1) and Model 3 (0,1,0). This suggests that Model 1 (1,1,2) is a better fit for the data when compared other models.\n\n\nForecast\n\nForecast for NASDAQ Index with feature variablesARIMA Model for InflationARIMA Model for EmploymentARIMA Model for NASDAQ Composite Index with feature variables\n\n\n\n\nCode\n#fiting an ARIMA model to the Inflation variable\ninflation_fit<-auto.arima(normalized_nadsaq_factor_data_numeric_df$inflation) \nfinflation<-forecast(inflation_fit)\n\n#fitting an ARIMA model to the Unemployment variable\nunemployment_fit<-auto.arima(normalized_nadsaq_factor_data_numeric_df$unemployment) \nfunemployment<-forecast(unemployment_fit)\n\n# best model fit for forcasting\nxreg <- cbind(Inflation = normalized_nadsaq_factor_data_ts[, \"inflation\"],\n              Unemployment = normalized_nadsaq_factor_data_ts[, \"unemployment\"])\n\nfit <- Arima(normalized_nadsaq_factor_data_ts[, \"IXIC.Adjusted\"],order=c(1,1,3),xreg=xreg)\n\n# Forcast the stock price using feature variables\nfxreg <- cbind(Inflation = finflation$mean,\n              Unemployment = funemployment$mean)\nfcast <- forecast(fit, xreg=fxreg) \nautoplot(fcast) + xlab(\"Year\") +\n  ylab(\"Normalised IXIC.Adjusted\")\n\n\n\n\n\n\n\n\n\nCode\n#fiting an ARIMA model to the Inflation variable\ninflation_fit<-auto.arima(normalized_nadsaq_factor_data_numeric_df$inflation) \nfinflation<-forecast(inflation_fit)\nsummary(inflation_fit)\n\n\nSeries: normalized_nadsaq_factor_data_numeric_df$inflation \nARIMA(0,1,3)(0,0,1)[4] with drift \n\nCoefficients:\n         ma1     ma2     ma3     sma1   drift\n      0.2153  0.5354  0.4353  -0.3707  0.0567\ns.e.  0.1719  0.1915  0.1455   0.1942  0.0499\n\nsigma^2 = 0.0708:  log likelihood = -3.48\nAIC=18.96   AICc=20.87   BIC=30.55\n\nTraining set error measures:\n                      ME     RMSE       MAE       MPE    MAPE      MASE\nTraining set 0.002515126 0.250258 0.1802161 -17.11236 114.163 0.3180481\n                    ACF1\nTraining set -0.06141191\n\n\n\n\n\n\nCode\n#fitting an ARIMA model to the Unemployment variable\nunemployment_fit<-auto.arima(normalized_nadsaq_factor_data_numeric_df$unemployment) \nfunemployment<-forecast(unemployment_fit)\nsummary(unemployment_fit)\n\n\nSeries: normalized_nadsaq_factor_data_numeric_df$unemployment \nARIMA(0,1,2) \n\nCoefficients:\n          ma1      ma2\n      -0.3027  -0.2410\ns.e.   0.1376   0.1472\n\nsigma^2 = 0.4982:  log likelihood = -53.72\nAIC=113.45   AICc=113.96   BIC=119.24\n\nTraining set error measures:\n                     ME      RMSE      MAE      MPE     MAPE      MASE\nTraining set -0.1001611 0.6851533 0.281119 140.1191 219.8632 0.4565275\n                    ACF1\nTraining set -0.02079058\n\n\n\n\n\n\nCode\n# best model fit for forcasting\nxreg <- cbind(Inflation = normalized_nadsaq_factor_data_ts[, \"inflation\"],\n              Unemployment = normalized_nadsaq_factor_data_ts[, \"unemployment\"])\n\nfit <- Arima(normalized_nadsaq_factor_data_ts[, \"IXIC.Adjusted\"],order=c(1,1,3),xreg=xreg)\nsummary(fit)\n\n\nSeries: normalized_nadsaq_factor_data_ts[, \"IXIC.Adjusted\"] \nRegression with ARIMA(1,1,3) errors \n\nCoefficients:\n          ar1     ma1     ma2     ma3  Inflation  Unemployment\n      -0.2800  0.6402  0.3728  0.7326     0.0955       -0.1747\ns.e.   0.1965  0.1478  0.2092  0.2020     0.0880        0.0282\n\nsigma^2 = 0.03115:  log likelihood = 17.37\nAIC=-20.74   AICc=-18.13   BIC=-7.21\n\nTraining set error measures:\n                      ME      RMSE       MAE       MPE     MAPE      MASE\nTraining set 0.007283442 0.1641978 0.1086579 -5.307647 28.99016 0.3289342\n                     ACF1\nTraining set -0.004903313\n\n\n\n\n\nThe ARIMAX model may have incorporated exogenous variables such as macroeconomic factors, which could indicate the potential impact of external factors on the forecasted values. The findings suggest that inflation and unemployment rates significantly impact the NADSAQ Composite Index stock price, underscoring the importance of including macroeconomic factors in stock market forecasting. This is further supported by the comparison of RMSE, MAE, AIC and BIC values for forecasts with and without the feature variable. It is found that including the feature variable leads to a lower values, indicating that the feature variable makes a significant impact and should be considered in stock market forecasting."
  },
  {
    "objectID": "arimax_sector_communication_services_macroeconomic.html",
    "href": "arimax_sector_communication_services_macroeconomic.html",
    "title": "ARIMAX/SARIMAX Page for Communication Services Sector Fund and macroeconomic factors",
    "section": "",
    "text": "Endogenous variable: Communication Services Sector Fund Exogenous variables: Macroeconomic indicators such as GDP, inflation rate, unemployment rate and interest rates\nBefore proceeding with model selection, it can be useful to examine the feature variables and select the appropriate ones. This can be done by creating cross-correlation function plots, which can serve as a verification of the correlation heatmap for the variables.\n\nCCF Plot for Communication Services Sector Fund and Exogenous Variables\n\nGDPInterest RateInflation RateUnemployment Rate\n\n\n\n\nCode\npar(mfrow=c(1,1))\nccf_result <- ccf(normalized_sector_factor_data[, c(\"XLC.Adjusted\")], normalized_sector_factor_data[, c(\"gdp\")], \n    lag.max = 300,\n    main = \"Cros-Correlation Plot for Communication Services Sector Fund and GDP Growth Rate \",\n    ylab = \"CCF\")\n\n\n\n\n\nCode\ncat(\"The sum of cross correlation function is\", sum(abs(ccf_result$acf)))\n\n\nThe sum of cross correlation function is 5.453043\n\n\n\n\n\n\nCode\npar(mfrow=c(1,1))\nccf_result <- ccf(normalized_sector_factor_data[, c(\"XLC.Adjusted\")], normalized_sector_factor_data[, c(\"interest\")], \n    lag.max = 300,\n    main = \"Cros-Correlation Plot for Communication Services Sector Fund and Interest Rate\",\n    ylab = \"CCF\")\n\n\n\n\n\nCode\ncat(\"The sum of cross correlation function is\", sum(abs(ccf_result$acf)))\n\n\nThe sum of cross correlation function is 9.647901\n\n\n\n\n\n\nCode\npar(mfrow=c(1,1))\nccf_result <- ccf(normalized_sector_factor_data[, c(\"XLC.Adjusted\")], normalized_sector_factor_data[, c(\"inflation\")], \n    lag.max = 300,\n    main = \"Cros-Correlation Plot for Communication Services Sector Fund and Inflation Rate\",\n    ylab = \"CCF\")\n\n\n\n\n\nCode\ncat(\"The sum of cross correlation function is\", sum(abs(ccf_result$acf)))\n\n\nThe sum of cross correlation function is 9.872718\n\n\n\n\n\n\nCode\npar(mfrow=c(1,1))\nccf_result <- ccf(normalized_sector_factor_data[, c(\"XLC.Adjusted\")], normalized_sector_factor_data[, c(\"unemployment\")], \n    lag.max = 300,\n    main = \"Cros-Correlation Plot for Communication Services Sector Fund and Unemployment Rate\",\n    ylab = \"CCF\")\n\n\n\n\n\nCode\ncat(\"The sum of cross correlation function is\", sum(abs(ccf_result$acf)))\n\n\nThe sum of cross correlation function is 7.151748\n\n\n\n\n\nThe cross-correlation feature plots reaffirm the findings from the heatmap analysis, indicating that inflation rate exhibit stronger correlations with the index compared to GDP, interest rate and unemployment rate. The cross-correlation plots for GDP, unemployment and interest rates show weaker and more scattered patterns, with strikes between the blue lines indicating lower correlation coefficients. This suggests that inflation rate are more suitable feature variables for the ARIMAX model when predicting Communication Services Sector Fund movements.\nFinal Exogenous variables: Macroeconomic indicators: Inflation rate\n\n\nEndogenous and Exogenous Variables Plot\n\n\nCode\nfinal_XLC_factor_data <- sector_factor_data %>%dplyr::select( Date,XLC.Adjusted, inflation)\nnumeric_vars_XLC_factor_data <- c(\"XLC.Adjusted\", \"inflation\")\nnumeric_XLC_factor_data <- final_XLC_factor_data[, numeric_vars_XLC_factor_data]\nnormalized_XLC_factor_data_numeric <- scale(numeric_XLC_factor_data)\nnormalized_XLC_factor_data_numeric_df <- data.frame(normalized_XLC_factor_data_numeric)\nnormalized_XLC_factor_data_ts <- ts(normalized_XLC_factor_data_numeric, start = c(2018, 7), frequency = 4)\n\nautoplot(normalized_XLC_factor_data_ts, facets=TRUE) +\n  xlab(\"Year\") + ylab(\"\") +\n  ggtitle(\"Communication Services Sector Fund Stock Price, Inflation Rate and Unemployment Rate in USA 2018-2023\")\n\n\n\n\n\n\n\nCheck the stationarity\nBefore proceeding with further analysis, it’s important to check the stationarity of the time series data. If the series is not stationary, it needs to be differentiated before performing a ARIMAX model. The stationarity of a multivariate time series data can be checked using the Phillips-Perron test, which is specifically designed for multivariate data. By performing the Phillips-Perron test on the multivariate time series data, we can determine if the data is stationary or if it requires differencing to make it suitable for ARIMAX modeling.\n\n\nCode\n# Convert your multivariate time series data to a matrix\nfinal_XLC_factor_data_ts_multivariate <- as.matrix(normalized_XLC_factor_data_ts)\n\n# Check for stationarity using Phillips-Perron test\nphillips_perron_test <- ur.pp(final_XLC_factor_data_ts_multivariate)  \nsummary(phillips_perron_test)\n\n\n\n################################## \n# Phillips-Perron Unit Root Test # \n################################## \n\nTest regression with intercept \n\n\nCall:\nlm(formula = y ~ y.l1)\n\nResiduals:\n     Min       1Q   Median       3Q      Max \n-1.06019 -0.17368 -0.05236  0.26744  0.79058 \n\nCoefficients:\n            Estimate Std. Error t value Pr(>|t|)    \n(Intercept)  0.06560    0.07770   0.844    0.405    \ny.l1         0.92511    0.08246  11.218 8.43e-13 ***\n---\nSignif. codes:  0 '***' 0.001 '**' 0.01 '*' 0.05 '.' 0.1 ' ' 1\n\nResidual standard error: 0.4591 on 33 degrees of freedom\nMultiple R-squared:  0.7923,    Adjusted R-squared:  0.786 \nF-statistic: 125.8 on 1 and 33 DF,  p-value: 8.433e-13\n\n\nValue of test-statistic, type: Z-alpha  is: -5.1925 \n\n         aux. Z statistics\nZ-tau-mu            0.6613\n\n\nThe Phillips-Perron unit root test output suggests that the data is stationary. The test regression includes an intercept term and tests whether the coefficient on the lagged value is significantly different from one. The estimated coefficients indicate that the intercept is not significant (p-value > 0.05), while the coefficient on the lagged value is highly significant (p-value < 8.433e-13), suggesting that the data is stationary.\nAdditionally, the “Z-alpha” value is the test statistic, which is compared to critical values of the standard normal distribution to determine significance. In this case, the test statistic is -5.1925, indicating strong evidence against the null hypothesis of a unit root. The “Z-tau-mu” value is an auxiliary test statistic used to correct for autocorrelation in the residuals, and it is close to zero, suggesting that there is no evidence of residual autocorrelation.\n\n\nFitting a ARIMAX model\nFitting an ARIMAX model will be a useful approach to modeling and predicting time series data when external factors are believed to have an impact on the response variable. In this case, we will be using an ARIMAX model to predict the Communication Services Sector Fund Stock Price, taking into account the effects of inflation on the sector. By including these exogenous variables in our model, we can improve the accuracy of our predictions and gain a better understanding of the underlying dynamics of the time series.\n\nAuto ARIMAResiduals\n\n\n\n\nCode\nxreg <- cbind(Inflation = normalized_XLC_factor_data_ts[, \"inflation\"])\n\nfit <- auto.arima(normalized_XLC_factor_data_ts[, \"XLC.Adjusted\"], xreg = xreg)\nsummary(fit)\n\n\nSeries: normalized_XLC_factor_data_ts[, \"XLC.Adjusted\"] \nRegression with ARIMA(1,2,0) errors \n\nCoefficients:\n          ar1    xreg\n      -0.6425  0.7675\ns.e.   0.1844  0.2688\n\nsigma^2 = 0.2251:  log likelihood = -9.97\nAIC=25.94   AICc=27.94   BIC=28.26\n\nTraining set error measures:\n                    ME      RMSE       MAE       MPE     MAPE     MASE\nTraining set -0.063185 0.4183969 0.3514946 -14.76584 69.18403 0.328731\n                    ACF1\nTraining set -0.07403479\n\n\n\n\n\n\nCode\ncheckresiduals(fit)\n\n\n\n\n\n\n    Ljung-Box test\n\ndata:  Residuals from Regression with ARIMA(1,2,0) errors\nQ* = 0.22825, df = 3, p-value = 0.9729\n\nModel df: 1.   Total lags used: 4\n\n\n\n\n\nThe auto.arima function reveals that the best-fit model is (0,1,0). Analysis of the residuals plot indicates that the fluctuates a lot and lies between -0.6 to 0.6. The ACF plot of the residuals shows significant lags, indicating that the model is performing well. The qq-plot also suggests normality in the residuals. The p-value greater than 0.05 actually suggests that there is no evidence of significant autocorrelation in the residuals, which means that the model is performing well in terms of capturing the temporal patterns in the data.\n\n\nFitting the model manually\n\nLinear ModelACF of ResidualsACF of ResidualsDifferentiated Residual\n\n\n\n\nCode\nnormalized_XLC_factor_data_numeric_df$XLC.Adjusted<-ts(normalized_XLC_factor_data_numeric_df$XLC.Adjusted,star=decimal_date(as.Date(\"2018-07-01\",format = \"%Y-%m-%d\")),frequency = 4)\nnormalized_XLC_factor_data_numeric_df$inflation<-ts(normalized_XLC_factor_data_numeric_df$inflation,star=decimal_date(as.Date(\"2018-07-01\",format = \"%Y-%m-%d\")),frequency = 4)\n\n\n############# First fit the linear model##########\nfit.reg <- lm(XLC.Adjusted ~ inflation, data=normalized_XLC_factor_data_numeric_df)\nsummary(fit.reg)\n\n\n\nCall:\nlm(formula = XLC.Adjusted ~ inflation, data = normalized_XLC_factor_data_numeric_df)\n\nResiduals:\n    Min      1Q  Median      3Q     Max \n-1.4465 -0.5415 -0.2586  0.6946  1.4192 \n\nCoefficients:\n              Estimate Std. Error t value Pr(>|t|)  \n(Intercept) -5.759e-16  2.129e-01   0.000    1.000  \ninflation    4.817e-01  2.191e-01   2.199    0.043 *\n---\nSignif. codes:  0 '***' 0.001 '**' 0.01 '*' 0.05 '.' 0.1 ' ' 1\n\nResidual standard error: 0.9033 on 16 degrees of freedom\nMultiple R-squared:  0.232, Adjusted R-squared:  0.184 \nF-statistic: 4.834 on 1 and 16 DF,  p-value: 0.04295\n\n\n\n\n\n\nCode\nres.fit<-ts(residuals(fit.reg),star=decimal_date(as.Date(\"2018-01-01\",format = \"%Y-%m-%d\")),frequency = 4)\n############## Then look at the residuals ############\nacf(res.fit)\n\n\n\n\n\n\n\n\n\nCode\nPacf(res.fit)\n\n\n\n\n\n\n\n\n\nCode\nres.fit %>% diff() %>% ggtsdisplay()\n\n\n\n\n\n\n\n\nThe output displays the results of a linear regression model using two predictor variables, inflation and unemployment, to explain the variation in the response variable, XLC.Adjusted The coefficients of inflation have p-values of less than 0.05, indicating that both variables significantly impact XLC.Adjusted The R-squared value of approximately 23% suggests that the model explains a moderate amount of the variation in the response variable. Examination of the ACF plot of residuals reveals some autocorrelation, and therefore, the series is differentiated to achieve stationarity. By analyzing the differentiated ACF and PACF plots, the parameters if the ARIMAX modle are that p=0, q=0, and d=1.\n\n\nFitting Model.\n\nModel Plot 1Model 1Model Plot 2Model 2\n\n\n\n\nCode\nset.seed(1234)\n\nmodel_output <- capture.output(sarima(res.fit, 0,1,0)) \n\n\n\n\n\n\n\n\n\nCode\ncat(model_output[9:38], model_output[length(model_output)], sep = \"\\n\")\n\n\n$fit\n\nCall:\narima(x = xdata, order = c(p, d, q), seasonal = list(order = c(P, D, Q), period = S), \n    xreg = constant, transform.pars = trans, fixed = fixed, optim.control = list(trace = trc, \n        REPORT = 1, reltol = tol))\n\nCoefficients:\n      constant\n       -0.0560\ns.e.    0.1215\n\nsigma^2 estimated as 0.251:  log likelihood = -12.37,  aic = 28.74\n\n$degrees_of_freedom\n[1] 16\n\n$ttable\n         Estimate     SE t.value p.value\nconstant   -0.056 0.1215 -0.4606  0.6513\n\n$AIC\n[1] 1.690689\n\n$AICc\n[1] 1.706376\n\n$BIC\n[1] 1.788714\n\n\n\n\n\n\nCode\nset.seed(1234)\n\nmodel_output <- capture.output(sarima(res.fit, 1,2,0)) \n\n\n\n\n\n\n\n\n\nCode\ncat(model_output[15:43], model_output[length(model_output)], sep = \"\\n\")\n\n\n$fit\n\nCall:\narima(x = xdata, order = c(p, d, q), seasonal = list(order = c(P, D, Q), period = S), \n    include.mean = !no.constant, transform.pars = trans, fixed = fixed, optim.control = list(trace = trc, \n        REPORT = 1, reltol = tol))\n\nCoefficients:\n          ar1\n      -0.6352\ns.e.   0.1860\n\nsigma^2 estimated as 0.211:  log likelihood = -10.52,  aic = 25.03\n\n$degrees_of_freedom\n[1] 15\n\n$ttable\n    Estimate    SE t.value p.value\nar1  -0.6352 0.186 -3.4143  0.0038\n\n$AIC\n[1] 1.564486\n\n$AICc\n[1] 1.582343\n\n$BIC\n[1] 1.66106\n\n\n\n\n\nWe fitted both models using both the manual and auto.arima methods and observed that all the models have p-values greater than 0, indicating statistical significance. To determine the best fit model, we performed cross-validation.\n\n\nCross Validation\n\nRMSE PlotRMSE Results\n\n\n\n\nCode\nn=length(res.fit)\nk= 10\n \n \nrmse1 <- matrix(NA, (n-k),4)\nrmse2 <- matrix(NA, (n-k),4)\nrmse3 <- matrix(NA, (n-k),4)\n\nst <- tsp(res.fit)[1]+(k-5)/4 \n\nfor(i in 1:(n-k))\n{\n  xtrain <- window(res.fit, end=st + i/4)\n  xtest <- window(res.fit, start=st + (i+1)/4, end=st + (i+4)/4)\n\n  \n  fit <- Arima(xtrain, order=c(0,1,0),\n                include.drift=TRUE, method=\"ML\")\n  fcast <- forecast(fit, h=4)\n  \n  fit2 <- Arima(xtrain, order=c(1,2,0),\n                include.drift=TRUE, method=\"ML\")\n  fcast2 <- forecast(fit2, h=4)\n\n\n  rmse1[i,1:length(xtest)]   <- sqrt((fcast$mean-xtest)^2)\n  rmse2[i,1:length(xtest)] <- sqrt((fcast2$mean-xtest)^2)\n}\n\nplot(1:4,colMeans(rmse1,na.rm=TRUE), type=\"l\",col=2, xlab=\"horizon\", ylab=\"RMSE\")\nlines(1:4, colMeans(rmse2,na.rm=TRUE), type=\"l\",col=3)\nlegend(\"topleft\",legend=c(\"fit1\",\"fit2\"),col=2:4,lty=1)\n\n\n\n\n\n\n\n\n\nCode\ncat(\"RMSE values for Model 1\\n\", colMeans(rmse1))\n\n\nRMSE values for Model 1\n 0.4100462 0.6829554 1.168707 1.662351\n\n\nCode\ncat(\"RMSE values for Model 2\\n\", colMeans(rmse2))\n\n\nRMSE values for Model 2\n 0.4866858 0.7792442 1.380504 1.962777\n\n\n\n\n\nThe RMSE cross-validation plot shows that the RMSE of Model 2 (1,2,0) is lower than that of Model 1 (0,1,0). This suggests that Model 2 (1,2,0) is a better fit for the data when compared other model.\n\n\nForecast\n::: panel-tabset ### Forecast for Communication Services Sector Fund with feature variable\n\n\nCode\n#fiting an ARIMA model to the Inflation variable\ninflation_fit<-auto.arima(normalized_XLC_factor_data_numeric_df$inflation) \nfinflation<-forecast(inflation_fit)\n\n\n# best model fit for forcasting\nxreg <- cbind(Inflation = normalized_XLC_factor_data_ts[, \"inflation\"])\n\nfit <- Arima(normalized_XLC_factor_data_ts[, \"XLC.Adjusted\"],order=c(1,2,0),xreg=xreg)\n\n# Forcast the stock price using feature variables\nfxreg <- cbind(Inflation = finflation$mean)\nfcast <- forecast(fit, xreg=fxreg) \nautoplot(fcast) + xlab(\"Year\") +\n  ylab(\"Normalised XLC.Adjusted\")\n\n\n\n\n\n\n\nARIMA Model for Inflation\n\n\nCode\n#fiting an ARIMA model to the Inflation variable\ninflation_fit<-auto.arima(normalized_XLC_factor_data_numeric_df$inflation) \nfinflation<-forecast(inflation_fit)\nsummary(inflation_fit)\n\n\nSeries: normalized_XLC_factor_data_numeric_df$inflation \nARIMA(0,1,0) \n\nsigma^2 = 0.1378:  log likelihood = -7.28\nAIC=16.55   AICc=16.82   BIC=17.38\n\nTraining set error measures:\n                    ME      RMSE       MAE       MPE     MAPE      MASE\nTraining set 0.1201871 0.3607553 0.2312026 -26.91442 61.15158 0.2676132\n                  ACF1\nTraining set 0.2431721\n\n\n\n\nARIMA Model for Communication Services Sector Fund with feature variable\n\n\nCode\n# best model fit for forcasting\nxreg <- cbind(Inflation = normalized_XLC_factor_data_ts[, \"inflation\"])\n\nfit <- Arima(normalized_XLC_factor_data_ts[, \"XLC.Adjusted\"],order=c(1,2,0),xreg=xreg)\nsummary(fit)\n\n\nSeries: normalized_XLC_factor_data_ts[, \"XLC.Adjusted\"] \nRegression with ARIMA(1,2,0) errors \n\nCoefficients:\n          ar1    xreg\n      -0.6425  0.7675\ns.e.   0.1844  0.2688\n\nsigma^2 = 0.2251:  log likelihood = -9.97\nAIC=25.94   AICc=27.94   BIC=28.26\n\nTraining set error measures:\n                    ME      RMSE       MAE       MPE     MAPE     MASE\nTraining set -0.063185 0.4183969 0.3514946 -14.76584 69.18403 0.328731\n                    ACF1\nTraining set -0.07403479\n\n\nThe ARIMAX model may have incorporated exogenous variables such as macroeconomic factors, which could indicate the potential impact of external factors on the forecasted values. The findings suggest that inflation rates significantly impact the Communication Services Sector Fund stock price, underscoring the importance of including macroeconomic factors in stock market forecasting. This is further supported by the comparison of RMSE, MAE, AIC and BIC values for forecasts with and without the feature variable. It is found that including the feature variable leads to a lower values, indicating that the feature variable makes a significant impact and should be considered in stock market forecasting."
  },
  {
    "objectID": "arimax_sector_consumer_discretionary_macroeconomic.html",
    "href": "arimax_sector_consumer_discretionary_macroeconomic.html",
    "title": "ARIMAX/SARIMAX Page for Consumer Discretionary Sector Fund and macroeconomic factors",
    "section": "",
    "text": "Endogenous variable: Consumer Discretionary Sector Fund Exogenous variables: Macroeconomic indicators such as GDP, inflation rate, unemployment rate and interest rates\nBefore proceeding with model selection, it can be useful to examine the feature variables and select the appropriate ones. This can be done by creating cross-correlation function plots, which can serve as a verification of the correlation heatmap for the variables.\n\nCCF Plot for Consumer Discretionary Sector Fund and Exogenous Variables\n\nGDPInterest RateInflation RateUnemployment Rate\n\n\n\n\nCode\npar(mfrow=c(1,1))\nccf_result <- ccf(normalized_sector_factor_data[, c(\"XLY.Adjusted\")], normalized_sector_factor_data[, c(\"gdp\")], \n    lag.max = 300,\n    main = \"Cros-Correlation Plot for Consumer Discretionary Sector Fund and GDP Growth Rate \",\n    ylab = \"CCF\")\n\n\n\n\n\nCode\ncat(\"The sum of cross correlation function is\", sum(abs(ccf_result$acf)))\n\n\nThe sum of cross correlation function is 4.929662\n\n\n\n\n\n\nCode\npar(mfrow=c(1,1))\nccf_result <- ccf(normalized_sector_factor_data[, c(\"XLY.Adjusted\")], normalized_sector_factor_data[, c(\"interest\")], \n    lag.max = 300,\n    main = \"Cros-Correlation Plot for Consumer Discretionary Sector Fund and Interest Rate\",\n    ylab = \"CCF\")\n\n\n\n\n\nCode\ncat(\"The sum of cross correlation function is\", sum(abs(ccf_result$acf)))\n\n\nThe sum of cross correlation function is 9.474914\n\n\n\n\n\n\nCode\npar(mfrow=c(1,1))\nccf_result <- ccf(normalized_sector_factor_data[, c(\"XLY.Adjusted\")], normalized_sector_factor_data[, c(\"inflation\")], \n    lag.max = 300,\n    main = \"Cros-Correlation Plot for Consumer Discretionary Sector Fund and Inflation Rate\",\n    ylab = \"CCF\")\n\n\n\n\n\nCode\ncat(\"The sum of cross correlation function is\", sum(abs(ccf_result$acf)))\n\n\nThe sum of cross correlation function is 10.30298\n\n\n\n\n\n\nCode\npar(mfrow=c(1,1))\nccf_result <- ccf(normalized_sector_factor_data[, c(\"XLY.Adjusted\")], normalized_sector_factor_data[, c(\"unemployment\")], \n    lag.max = 300,\n    main = \"Cros-Correlation Plot for Consumer Discretionary Sector Fund and Unemployment Rate\",\n    ylab = \"CCF\")\n\n\n\n\n\nCode\ncat(\"The sum of cross correlation function is\", sum(abs(ccf_result$acf)))\n\n\nThe sum of cross correlation function is 6.993324\n\n\n\n\n\nThe cross-correlation feature plots reaffirm the findings from the heatmap analysis, indicating that inflation rate exhibit stronger correlations with the index compared to GDP, interest rate and unemployment rate. The cross-correlation plots for GDP, unemployment and interest rates show weaker and more scattered patterns, with strikes between the blue lines indicating lower correlation coefficients. This suggests that inflation rate are more suitable feature variables for the ARIMAX model when predicting Consumer Discretionary Sector Fund movements.\nFinal Exogenous variables: Macroeconomic indicators: Inflation rate\n\n\nEndogenous and Exogenous Variables Plot\n\n\nCode\nfinal_XLY_factor_data <- sector_factor_data %>%dplyr::select( Date,XLY.Adjusted, inflation)\nnumeric_vars_XLY_factor_data <- c(\"XLY.Adjusted\", \"inflation\")\nnumeric_XLY_factor_data <- final_XLY_factor_data[, numeric_vars_XLY_factor_data]\nnormalized_XLY_factor_data_numeric <- scale(numeric_XLY_factor_data)\nnormalized_XLY_factor_data_numeric_df <- data.frame(normalized_XLY_factor_data_numeric)\nnormalized_XLY_factor_data_ts <- ts(normalized_XLY_factor_data_numeric, start = c(2018, 7), frequency = 4)\n\nautoplot(normalized_XLY_factor_data_ts, facets=TRUE) +\n  xlab(\"Year\") + ylab(\"\") +\n  ggtitle(\"Consumer Discretionary Sector Fund Stock Price, Inflation Rate and Unemployment Rate in USA 2018-2023\")\n\n\n\n\n\n\n\nCheck the stationarity\nBefore proceeding with further analysis, it’s important to check the stationarity of the time series data. If the series is not stationary, it needs to be differentiated before performing a ARIMAX model. The stationarity of a multivariate time series data can be checked using the Phillips-Perron test, which is specifically designed for multivariate data. By performing the Phillips-Perron test on the multivariate time series data, we can determine if the data is stationary or if it requires differencing to make it suitable for ARIMAX modeling.\n\n\nCode\n# Convert your multivariate time series data to a matrix\nfinal_XLY_factor_data_ts_multivariate <- as.matrix(normalized_XLY_factor_data_ts)\n\n# Check for stationarity using Phillips-Perron test\nphillips_perron_test <- ur.pp(final_XLY_factor_data_ts_multivariate)  \nsummary(phillips_perron_test)\n\n\n\n################################## \n# Phillips-Perron Unit Root Test # \n################################## \n\nTest regression with intercept \n\n\nCall:\nlm(formula = y ~ y.l1)\n\nResiduals:\n    Min      1Q  Median      3Q     Max \n-1.2506 -0.1793 -0.0025  0.2765  0.9314 \n\nCoefficients:\n            Estimate Std. Error t value Pr(>|t|)    \n(Intercept)  0.07270    0.08474   0.858    0.397    \ny.l1         0.89206    0.08994   9.918 1.99e-11 ***\n---\nSignif. codes:  0 '***' 0.001 '**' 0.01 '*' 0.05 '.' 0.1 ' ' 1\n\nResidual standard error: 0.5007 on 33 degrees of freedom\nMultiple R-squared:  0.7488,    Adjusted R-squared:  0.7412 \nF-statistic: 98.38 on 1 and 33 DF,  p-value: 1.987e-11\n\n\nValue of test-statistic, type: Z-alpha  is: -5.3574 \n\n         aux. Z statistics\nZ-tau-mu            0.7495\n\n\nThe Phillips-Perron unit root test output suggests that the data is stationary. The test regression includes an intercept term and tests whether the coefficient on the lagged value is significantly different from one. The estimated coefficients indicate that the intercept is not significant (p-value > 0.05), while the coefficient on the lagged value is highly significant (p-value < 1.99e-11), suggesting that the data is stationary.\nAdditionally, the “Z-alpha” value is the test statistic, which is compared to critical values of the standard normal distribution to determine significance. In this case, the test statistic is -5.3574, indicating strong evidence against the null hypothesis of a unit root. The “Z-tau-mu” value is an auxiliary test statistic used to correct for autocorrelation in the residuals, and it is close to zero, suggesting that there is no evidence of residual autocorrelation.\n\n\nFitting a ARIMAX model\nFitting an ARIMAX model will be a useful approach to modeling and predicting time series data when external factors are believed to have an impact on the response variable. In this case, we will be using an ARIMAX model to predict the Consumer Discretionary Sector Fund Stock Price, taking into account the effects of inflation on the sector. By including these exogenous variables in our model, we can improve the accuracy of our predictions and gain a better understanding of the underlying dynamics of the time series.\n\nAuto ARIMAResiduals\n\n\n\n\nCode\nxreg <- cbind(Inflation = normalized_XLY_factor_data_ts[, \"inflation\"])\n\nfit <- auto.arima(normalized_XLY_factor_data_ts[, \"XLY.Adjusted\"], xreg = xreg)\nsummary(fit)\n\n\nSeries: normalized_XLY_factor_data_ts[, \"XLY.Adjusted\"] \nRegression with ARIMA(0,1,0) errors \n\nCoefficients:\n        xreg\n      0.7705\ns.e.  0.3474\n\nsigma^2 = 0.3005:  log likelihood = -13.39\nAIC=30.77   AICc=31.63   BIC=32.44\n\nTraining set error measures:\n                      ME      RMSE       MAE       MPE     MAPE      MASE\nTraining set -0.03054598 0.5168065 0.4073284 -466.6557 515.1414 0.4379855\n                  ACF1\nTraining set 0.1083258\n\n\n\n\n\n\nCode\ncheckresiduals(fit)\n\n\n\n\n\n\n    Ljung-Box test\n\ndata:  Residuals from Regression with ARIMA(0,1,0) errors\nQ* = 0.62692, df = 4, p-value = 0.96\n\nModel df: 0.   Total lags used: 4\n\n\n\n\n\nThe auto.arima function reveals that the best-fit model is (0,1,0). Analysis of the residuals plot indicates that the fluctuates a lot and lies between -1 to 1. The ACF plot of the residuals shows significant lags, indicating that the model is performing well. The qq-plot also suggests normality in the residuals. The p-value greater than 0.05 actually suggests that there is no evidence of significant autocorrelation in the residuals, which means that the model is performing well in terms of capturing the temporal patterns in the data.\n\n\nFitting the model manually\n\nLinear ModelACF of ResidualsACF of ResidualsDifferentiated Residual\n\n\n\n\nCode\nnormalized_XLY_factor_data_numeric_df$XLY.Adjusted<-ts(normalized_XLY_factor_data_numeric_df$XLY.Adjusted,star=decimal_date(as.Date(\"2018-07-01\",format = \"%Y-%m-%d\")),frequency = 4)\nnormalized_XLY_factor_data_numeric_df$inflation<-ts(normalized_XLY_factor_data_numeric_df$inflation,star=decimal_date(as.Date(\"2018-07-01\",format = \"%Y-%m-%d\")),frequency = 4)\n\n\n############# First fit the linear model##########\nfit.reg <- lm(XLY.Adjusted ~ inflation, data=normalized_XLY_factor_data_numeric_df)\nsummary(fit.reg)\n\n\n\nCall:\nlm(formula = XLY.Adjusted ~ inflation, data = normalized_XLY_factor_data_numeric_df)\n\nResiduals:\n    Min      1Q  Median      3Q     Max \n-1.0287 -0.6438 -0.2041  0.7331  1.2059 \n\nCoefficients:\n              Estimate Std. Error t value Pr(>|t|)   \n(Intercept) -2.129e-16  1.826e-01    0.00   1.0000   \ninflation    6.595e-01  1.879e-01    3.51   0.0029 **\n---\nSignif. codes:  0 '***' 0.001 '**' 0.01 '*' 0.05 '.' 0.1 ' ' 1\n\nResidual standard error: 0.7748 on 16 degrees of freedom\nMultiple R-squared:  0.435, Adjusted R-squared:  0.3997 \nF-statistic: 12.32 on 1 and 16 DF,  p-value: 0.002903\n\n\n\n\n\n\nCode\nres.fit<-ts(residuals(fit.reg),star=decimal_date(as.Date(\"2018-07-01\",format = \"%Y-%m-%d\")),frequency = 4)\n############## Then look at the residuals ############\nacf(res.fit)\n\n\n\n\n\n\n\n\n\nCode\nPacf(res.fit)\n\n\n\n\n\n\n\n\n\nCode\nres.fit %>% diff() %>% ggtsdisplay()\n\n\n\n\n\n\n\n\nThe output displays the results of a linear regression model using two predictor variables, inflation and unemployment, to explain the variation in the response variable, XLY.Adjusted The coefficients of inflation have p-values of less than 0.05, indicating that both variables significantly impact XLY.Adjusted The R-squared value of approximately 43% suggests that the model explains not good amount of the variation in the response variable. Examination of the ACF plot of residuals reveals some autocorrelation, and therefore, the series is differentiated to achieve stationarity. By analyzing the differentiated ACF and PACF plots, the parameters if the ARIMAX modle are that p=0, q=0, and d=1.\nAs both the model fitted manually and the auto.arima parameters are the same, i.e. p = 0, q = 0 and d = 1. We proceed with fitting the model for the parameters.\n\n\nModel Fitting\n\nModel PlotModel\n\n\n\n\nCode\nset.seed(1234)\n\nmodel_output <- capture.output(sarima(res.fit, 0,1,0)) \n\n\n\n\n\n\n\n\n\nCode\ncat(model_output[9:38], model_output[length(model_output)], sep = \"\\n\")\n\n\n$fit\n\nCall:\narima(x = xdata, order = c(p, d, q), seasonal = list(order = c(P, D, Q), period = S), \n    xreg = constant, transform.pars = trans, fixed = fixed, optim.control = list(trace = trc, \n        REPORT = 1, reltol = tol))\n\nCoefficients:\n      constant\n       -0.0182\ns.e.    0.1293\n\nsigma^2 estimated as 0.2842:  log likelihood = -13.43,  aic = 30.85\n\n$degrees_of_freedom\n[1] 16\n\n$ttable\n         Estimate     SE t.value p.value\nconstant  -0.0182 0.1293 -0.1406  0.8899\n\n$AIC\n[1] 1.814973\n\n$AICc\n[1] 1.830659\n\n$BIC\n[1] 1.912998\n\n\n\n\n\nThe best model identified is ARIMA(0,1,0). By analyzing the standardized residuals plot, it can be observed that the mean is close to 0. Deviations from these values could indicate poor model fit. However, in this case, the model seems to be well-fitted. The ACF plot of residuals shows very few significant lags, which is a positive sign for the model. The qq-plot also suggests normality in the residuals. Additionally, the p-values for the Ljung-Box test are greater than 0.05, indicating that the residuals are independent, which is favorable for the model’s accuracy.\n\n\nForecast\n::: panel-tabset ### Forecast for Consumer Discretionary Sector Fund with feature variable\n\n\nCode\n#fiting an ARIMA model to the Inflation variable\ninflation_fit<-auto.arima(normalized_XLY_factor_data_numeric_df$inflation) \nfinflation<-forecast(inflation_fit)\n\n\n# best model fit for forcasting\nxreg <- cbind(Inflation = normalized_XLY_factor_data_ts[, \"inflation\"])\n\nfit <- Arima(normalized_XLY_factor_data_ts[, \"XLY.Adjusted\"],order=c(0,1,0),xreg=xreg)\n\n# Forcast the stock price using feature variables\nfxreg <- cbind(Inflation = finflation$mean)\nfcast <- forecast(fit, xreg=fxreg) \nautoplot(fcast) + xlab(\"Year\") +\n  ylab(\"Normalised XLY.Adjusted\")\n\n\n\n\n\n\n\nARIMA Model for Inflation\n\n\nCode\n#fiting an ARIMA model to the Inflation variable\ninflation_fit<-auto.arima(normalized_XLY_factor_data_numeric_df$inflation) \nfinflation<-forecast(inflation_fit)\nsummary(inflation_fit)\n\n\nSeries: normalized_XLY_factor_data_numeric_df$inflation \nARIMA(0,1,0) \n\nsigma^2 = 0.1378:  log likelihood = -7.28\nAIC=16.55   AICc=16.82   BIC=17.38\n\nTraining set error measures:\n                    ME      RMSE       MAE       MPE     MAPE      MASE\nTraining set 0.1201871 0.3607553 0.2312026 -26.91442 61.15158 0.2676132\n                  ACF1\nTraining set 0.2431721\n\n\n\n\nARIMA Model for Consumer Discretionary Sector Fund with feature variable\n\n\nCode\n# best model fit for forcasting\nxreg <- cbind(Inflation = normalized_XLY_factor_data_ts[, \"inflation\"])\n\nfit <- Arima(normalized_XLY_factor_data_ts[, \"XLY.Adjusted\"],order=c(0,1,0),xreg=xreg)\nsummary(fit)\n\n\nSeries: normalized_XLY_factor_data_ts[, \"XLY.Adjusted\"] \nRegression with ARIMA(0,1,0) errors \n\nCoefficients:\n        xreg\n      0.7705\ns.e.  0.3474\n\nsigma^2 = 0.3005:  log likelihood = -13.39\nAIC=30.77   AICc=31.63   BIC=32.44\n\nTraining set error measures:\n                      ME      RMSE       MAE       MPE     MAPE      MASE\nTraining set -0.03054598 0.5168065 0.4073284 -466.6557 515.1414 0.4379855\n                  ACF1\nTraining set 0.1083258\n\n\nThe ARIMAX model may have incorporated exogenous variables such as macroeconomic factors, which could indicate the potential impact of external factors on the forecasted values. The findings suggest that inflation rates significantly impact the Consumer Discretionary Sector Fund stock price, underscoring the importance of including macroeconomic factors in stock market forecasting. This is further supported by the comparison of RMSE, MAE, AIC and BIC values for forecasts with and without the feature variable. It is found that including the feature variable leads to a lower values, indicating that the feature variable makes a significant impact and should be considered in stock market forecasting."
  },
  {
    "objectID": "arimax_sector_consumer_staples_macroeconomic.html",
    "href": "arimax_sector_consumer_staples_macroeconomic.html",
    "title": "ARIMAX/SARIMAX for Consumer Staples Sector Fund and macroeconomic factors",
    "section": "",
    "text": "Endogenous variable: Consumer Staples Sector Fund Exogenous variables: Macroeconomic indicators such as GDP, inflation rate, unemployment rate and interest rates\nBefore proceeding with model selection, it can be useful to examine the feature variables and select the appropriate ones. This can be done by creating cross-correlation function plots, which can serve as a verification of the correlation heatmap for the variables.\n\nCCF Plot for Consumer Staples Sector Fund and Exogenous Variables\n\nGDPInterest RateInflation RateUnemployment Rate\n\n\n\n\nCode\npar(mfrow=c(1,1))\nccf_result <- ccf(normalized_sector_factor_data[, c(\"XLP.Adjusted\")], normalized_sector_factor_data[, c(\"gdp\")], \n    lag.max = 300,\n    main = \"Cros-Correlation Plot for Consumer Staples Sector Fund and GDP Growth Rate \",\n    ylab = \"CCF\")\n\n\n\n\n\nCode\ncat(\"The sum of cross correlation function is\", sum(abs(ccf_result$acf)))\n\n\nThe sum of cross correlation function is 5.056662\n\n\n\n\n\n\nCode\npar(mfrow=c(1,1))\nccf_result <- ccf(normalized_sector_factor_data[, c(\"XLP.Adjusted\")], normalized_sector_factor_data[, c(\"interest\")], \n    lag.max = 300,\n    main = \"Cros-Correlation Plot for Consumer Staples Sector Fund and Interest Rate\",\n    ylab = \"CCF\")\n\n\n\n\n\nCode\ncat(\"The sum of cross correlation function is\", sum(abs(ccf_result$acf)))\n\n\nThe sum of cross correlation function is 9.343761\n\n\n\n\n\n\nCode\npar(mfrow=c(1,1))\nccf_result <- ccf(normalized_sector_factor_data[, c(\"XLP.Adjusted\")], normalized_sector_factor_data[, c(\"inflation\")], \n    lag.max = 300,\n    main = \"Cros-Correlation Plot for Consumer Staples Sector Fund and Inflation Rate\",\n    ylab = \"CCF\")\n\n\n\n\n\nCode\ncat(\"The sum of cross correlation function is\", sum(abs(ccf_result$acf)))\n\n\nThe sum of cross correlation function is 11.02386\n\n\n\n\n\n\nCode\npar(mfrow=c(1,1))\nccf_result <- ccf(normalized_sector_factor_data[, c(\"XLP.Adjusted\")], normalized_sector_factor_data[, c(\"unemployment\")], \n    lag.max = 300,\n    main = \"Cros-Correlation Plot for Consumer Staples Sector Fund and Unemployment Rate\",\n    ylab = \"CCF\")\n\n\n\n\n\nCode\ncat(\"The sum of cross correlation function is\", sum(abs(ccf_result$acf)))\n\n\nThe sum of cross correlation function is 6.397481\n\n\n\n\n\nThe cross-correlation feature plots reaffirm the findings from the heatmap analysis, indicating that inflation rate exhibit stronger correlations with the index compared to GDP, interest rate and unemployment rate. The cross-correlation plots for GDP, unemployment and interest rates show weaker and more scattered patterns, with strikes between the blue lines indicating lower correlation coefficients. This suggests that inflation rate are more suitable feature variables for the ARIMAX model when predicting Consumer Staples Sector Fund movements.\nFinal Exogenous variables: Macroeconomic indicators: Inflation rate\n\n\nEndogenous and Exogenous Variables Plot\n\n\nCode\nfinal_XLP_factor_data <- sector_factor_data %>%dplyr::select( Date,XLP.Adjusted, inflation)\nnumeric_vars_XLP_factor_data <- c(\"XLP.Adjusted\", \"inflation\")\nnumeric_XLP_factor_data <- final_XLP_factor_data[, numeric_vars_XLP_factor_data]\nnormalized_XLP_factor_data_numeric <- scale(numeric_XLP_factor_data)\nnormalized_XLP_factor_data_numeric_df <- data.frame(normalized_XLP_factor_data_numeric)\nnormalized_XLP_factor_data_ts <- ts(normalized_XLP_factor_data_numeric, start = c(2018, 7), frequency = 4)\n\nautoplot(normalized_XLP_factor_data_ts, facets=TRUE) +\n  xlab(\"Year\") + ylab(\"\") +\n  ggtitle(\"Consumer Staples Sector Fund Stock Price, Inflation Rate and Unemployment Rate in USA 2018-2023\")\n\n\n\n\n\n\n\nCheck the stationarity\nBefore proceeding with further analysis, it’s important to check the stationarity of the time series data. If the series is not stationary, it needs to be differentiated before performing a ARIMAX model. The stationarity of a multivariate time series data can be checked using the Phillips-Perron test, which is specifically designed for multivariate data. By performing the Phillips-Perron test on the multivariate time series data, we can determine if the data is stationary or if it requires differencing to make it suitable for ARIMAX modeling.\n\n\nCode\n# Convert your multivariate time series data to a matrix\nfinal_XLP_factor_data_ts_multivariate <- as.matrix(normalized_XLP_factor_data_ts)\n\n# Check for stationarity using Phillips-Perron test\nphillips_perron_test <- ur.pp(final_XLP_factor_data_ts_multivariate)  \nsummary(phillips_perron_test)\n\n\n\n################################## \n# Phillips-Perron Unit Root Test # \n################################## \n\nTest regression with intercept \n\n\nCall:\nlm(formula = y ~ y.l1)\n\nResiduals:\n     Min       1Q   Median       3Q      Max \n-1.24887 -0.18245  0.04207  0.18828  0.82983 \n\nCoefficients:\n            Estimate Std. Error t value Pr(>|t|)    \n(Intercept)  0.08589    0.07449   1.153    0.257    \ny.l1         0.90580    0.07905  11.458 4.82e-13 ***\n---\nSignif. codes:  0 '***' 0.001 '**' 0.01 '*' 0.05 '.' 0.1 ' ' 1\n\nResidual standard error: 0.4401 on 33 degrees of freedom\nMultiple R-squared:  0.7991,    Adjusted R-squared:  0.793 \nF-statistic: 131.3 on 1 and 33 DF,  p-value: 4.819e-13\n\n\nValue of test-statistic, type: Z-alpha  is: -5.2349 \n\n         aux. Z statistics\nZ-tau-mu            0.9514\n\n\nThe Phillips-Perron unit root test output suggests that the data is stationary. The test regression includes an intercept term and tests whether the coefficient on the lagged value is significantly different from one. The estimated coefficients indicate that the intercept is not significant (p-value > 0.05), while the coefficient on the lagged value is highly significant (p-value < 4.82e-13), suggesting that the data is stationary.\nAdditionally, the “Z-alpha” value is the test statistic, which is compared to critical values of the standard normal distribution to determine significance. In this case, the test statistic is -5.2349, indicating strong evidence against the null hypothesis of a unit root. The “Z-tau-mu” value is an auxiliary test statistic used to correct for autocorrelation in the residuals, and it is close to zero, suggesting that there is no evidence of residual autocorrelation.\n\n\nFitting a ARIMAX model\nFitting an ARIMAX model will be a useful approach to modeling and predicting time series data when external factors are believed to have an impact on the response variable. In this case, we will be using an ARIMAX model to predict the Consumer Staples Sector Fund Stock Price, taking into account the effects of inflation on the sector. By including these exogenous variables in our model, we can improve the accuracy of our predictions and gain a better understanding of the underlying dynamics of the time series.\n\nAuto ARIMAResiduals\n\n\n\n\nCode\nxreg <- cbind(Inflation = normalized_XLP_factor_data_ts[, \"inflation\"])\n\nfit <- auto.arima(normalized_XLP_factor_data_ts[, \"XLP.Adjusted\"], xreg = xreg)\nsummary(fit)\n\n\nSeries: normalized_XLP_factor_data_ts[, \"XLP.Adjusted\"] \nRegression with ARIMA(0,1,0) errors \n\nCoefficients:\n        xreg\n      0.5390\ns.e.  0.2455\n\nsigma^2 = 0.15:  log likelihood = -7.48\nAIC=18.96   AICc=19.82   BIC=20.63\n\nTraining set error measures:\n                     ME     RMSE       MAE      MPE    MAPE      MASE\nTraining set 0.05960426 0.365138 0.3186148 7.758641 72.3286 0.4301663\n                    ACF1\nTraining set 0.005736426\n\n\n\n\n\n\nCode\ncheckresiduals(fit)\n\n\n\n\n\n\n    Ljung-Box test\n\ndata:  Residuals from Regression with ARIMA(0,1,0) errors\nQ* = 3.4594, df = 4, p-value = 0.4841\n\nModel df: 0.   Total lags used: 4\n\n\n\n\n\nThe auto.arima function reveals that the best-fit model is (0,1,0). Analysis of the residuals plot indicates that the fluctuates a lot and lies between -0.6 to 0.6. The ACF plot of the residuals shows significant lags, indicating that the model is performing well. The qq-plot also suggests normality in the residuals. The p-value greater than 0.05 actually suggests that there is no evidence of significant autocorrelation in the residuals, which means that the model is performing well in terms of capturing the temporal patterns in the data.\n\n\nFitting the model manually\n\nLinear ModelACF of ResidualsACF of ResidualsDifferentiated Residual\n\n\n\n\nCode\nnormalized_XLP_factor_data_numeric_df$XLP.Adjusted<-ts(normalized_XLP_factor_data_numeric_df$XLP.Adjusted,star=decimal_date(as.Date(\"2018-07-01\",format = \"%Y-%m-%d\")),frequency = 4)\nnormalized_XLP_factor_data_numeric_df$inflation<-ts(normalized_XLP_factor_data_numeric_df$inflation,star=decimal_date(as.Date(\"2018-07-01\",format = \"%Y-%m-%d\")),frequency = 4)\n\n\n############# First fit the linear model##########\nfit.reg <- lm(XLP.Adjusted ~ inflation, data=normalized_XLP_factor_data_numeric_df)\nsummary(fit.reg)\n\n\n\nCall:\nlm(formula = XLP.Adjusted ~ inflation, data = normalized_XLP_factor_data_numeric_df)\n\nResiduals:\n    Min      1Q  Median      3Q     Max \n-1.0945 -0.3724  0.0981  0.2660  1.1861 \n\nCoefficients:\n             Estimate Std. Error t value Pr(>|t|)    \n(Intercept) 3.255e-16  1.460e-01    0.00        1    \ninflation   7.993e-01  1.502e-01    5.32  6.9e-05 ***\n---\nSignif. codes:  0 '***' 0.001 '**' 0.01 '*' 0.05 '.' 0.1 ' ' 1\n\nResidual standard error: 0.6194 on 16 degrees of freedom\nMultiple R-squared:  0.6389,    Adjusted R-squared:  0.6163 \nF-statistic: 28.31 on 1 and 16 DF,  p-value: 6.9e-05\n\n\n\n\n\n\nCode\nres.fit<-ts(residuals(fit.reg),star=decimal_date(as.Date(\"2018-01-01\",format = \"%Y-%m-%d\")),frequency = 4)\n############## Then look at the residuals ############\nacf(res.fit)\n\n\n\n\n\n\n\n\n\nCode\nPacf(res.fit)\n\n\n\n\n\n\n\n\n\nCode\nres.fit %>% diff() %>% ggtsdisplay()\n\n\n\n\n\n\n\n\nThe output displays the results of a linear regression model using two predictor variables, inflation and unemployment, to explain the variation in the response variable, XLP.Adjusted The coefficients of inflation have p-values of less than 0.05, indicating that both variables significantly impact XLP.Adjusted The R-squared value of approximately 64% suggests that the model explains a moderate amount of the variation in the response variable. Examination of the ACF plot of residuals reveals some autocorrelation, and therefore, the series is differentiated to achieve stationarity. By analyzing the differentiated ACF and PACF plots, the parameters if the ARIMAX modle are that p=0, q=0, and d=1.\nAs both the model fitted manually and the auto.arima parameters are the same, i.e. p = 0, q = 0 and d = 1. We proceed with fitting the model for the parameters.\n\n\nModel Fitting\n\nModel PlotModel\n\n\n\n\nCode\nset.seed(1234)\n\nmodel_output <- capture.output(sarima(res.fit, 0,1,0)) \n\n\n\n\n\n\n\n\n\nCode\ncat(model_output[9:38], model_output[length(model_output)], sep = \"\\n\")\n\n\n$fit\n\nCall:\narima(x = xdata, order = c(p, d, q), seasonal = list(order = c(P, D, Q), period = S), \n    xreg = constant, transform.pars = trans, fixed = fixed, optim.control = list(trace = trc, \n        REPORT = 1, reltol = tol))\n\nCoefficients:\n      constant\n        0.0300\ns.e.    0.0938\n\nsigma^2 estimated as 0.1496:  log likelihood = -7.97,  aic = 19.95\n\n$degrees_of_freedom\n[1] 16\n\n$ttable\n         Estimate     SE t.value p.value\nconstant     0.03 0.0938  0.3203  0.7529\n\n$AIC\n[1] 1.1734\n\n$AICc\n[1] 1.189086\n\n$BIC\n[1] 1.271425\n\n\n\n\n\nThe best model identified is ARIMA(0,1,0). By analyzing the standardized residuals plot, it can be observed that the mean is close to 0. Deviations from these values could indicate poor model fit. However, in this case, the model seems to be well-fitted. The ACF plot of residuals shows very few significant lags, which is a positive sign for the model. The qq-plot also suggests normality in the residuals. Additionally, the p-values for the Ljung-Box test are greater than 0.05, indicating that the residuals are independent, which is favorable for the model’s accuracy.\n\n\nForecast\n::: panel-tabset ### Forecast for Consumer Staples Sector Fund with feature variable\n\n\nCode\n#fiting an ARIMA model to the Inflation variable\ninflation_fit<-auto.arima(normalized_XLP_factor_data_numeric_df$inflation) \nfinflation<-forecast(inflation_fit)\n\n\n# best model fit for forcasting\nxreg <- cbind(Inflation = normalized_XLP_factor_data_ts[, \"inflation\"])\n\nfit <- Arima(normalized_XLP_factor_data_ts[, \"XLP.Adjusted\"],order=c(0,1,0),xreg=xreg)\n\n# Forcast the stock price using feature variables\nfxreg <- cbind(Inflation = finflation$mean)\nfcast <- forecast(fit, xreg=fxreg) \nautoplot(fcast) + xlab(\"Year\") +\n  ylab(\"Normalised XLP.Adjusted\")\n\n\n\n\n\n\n\nARIMA Model for Inflation\n\n\nCode\n#fiting an ARIMA model to the Inflation variable\ninflation_fit<-auto.arima(normalized_XLP_factor_data_numeric_df$inflation) \nfinflation<-forecast(inflation_fit)\nsummary(inflation_fit)\n\n\nSeries: normalized_XLP_factor_data_numeric_df$inflation \nARIMA(0,1,0) \n\nsigma^2 = 0.1378:  log likelihood = -7.28\nAIC=16.55   AICc=16.82   BIC=17.38\n\nTraining set error measures:\n                    ME      RMSE       MAE       MPE     MAPE      MASE\nTraining set 0.1201871 0.3607553 0.2312026 -26.91442 61.15158 0.2676132\n                  ACF1\nTraining set 0.2431721\n\n\n\n\nARIMA Model for Consumer Staples Sector Fund with feature variable\n\n\nCode\n# best model fit for forcasting\nxreg <- cbind(Inflation = normalized_XLP_factor_data_ts[, \"inflation\"])\n\nfit <- Arima(normalized_XLP_factor_data_ts[, \"XLP.Adjusted\"],order=c(0,1,0),xreg=xreg)\nsummary(fit)\n\n\nSeries: normalized_XLP_factor_data_ts[, \"XLP.Adjusted\"] \nRegression with ARIMA(0,1,0) errors \n\nCoefficients:\n        xreg\n      0.5390\ns.e.  0.2455\n\nsigma^2 = 0.15:  log likelihood = -7.48\nAIC=18.96   AICc=19.82   BIC=20.63\n\nTraining set error measures:\n                     ME     RMSE       MAE      MPE    MAPE      MASE\nTraining set 0.05960426 0.365138 0.3186148 7.758641 72.3286 0.4301663\n                    ACF1\nTraining set 0.005736426\n\n\nThe ARIMAX model may have incorporated exogenous variables such as macroeconomic factors, which could indicate the potential impact of external factors on the forecasted values. The findings suggest that inflation rates significantly impact the Consumer Staples Sector Fund stock price, underscoring the importance of including macroeconomic factors in stock market forecasting. This is further supported by the comparison of RMSE, MAE, AIC and BIC values for forecasts with and without the feature variable. It is found that including the feature variable leads to a lower values, indicating that the feature variable makes a significant impact and should be considered in stock market forecasting."
  },
  {
    "objectID": "arimax_sector_energy_macroeconomic.html",
    "href": "arimax_sector_energy_macroeconomic.html",
    "title": "ARIMAX/SARIMAX Page for Energy Sector Fund and macroeconomic factors",
    "section": "",
    "text": "Endogenous variable: Energy Sector Fund Exogenous variables: Macroeconomic indicators such as GDP, inflation rate, unemployment rate and interest rates\nBefore proceeding with model selection, it can be useful to examine the feature variables and select the appropriate ones. This can be done by creating cross-correlation function plots, which can serve as a verification of the correlation heatmap for the variables.\n\nCCF Plot for Energy Sector Fund and Exogenous Variables\n\nGDPInterest RateInflation RateUnemployment Rate\n\n\n\n\nCode\npar(mfrow=c(1,1))\nccf_result <- ccf(normalized_sector_factor_data[, c(\"XLE.Adjusted\")], normalized_sector_factor_data[, c(\"gdp\")], \n    lag.max = 300,\n    main = \"Cros-Correlation Plot for Energy Sector Fund and GDP Growth Rate \",\n    ylab = \"CCF\")\n\n\n\n\n\nCode\ncat(\"The sum of cross correlation function is\", sum(abs(ccf_result$acf)))\n\n\nThe sum of cross correlation function is 5.945692\n\n\n\n\n\n\nCode\npar(mfrow=c(1,1))\nccf_result <- ccf(normalized_sector_factor_data[, c(\"XLE.Adjusted\")], normalized_sector_factor_data[, c(\"interest\")], \n    lag.max = 300,\n    main = \"Cros-Correlation Plot for Energy Sector Fund and Interest Rate\",\n    ylab = \"CCF\")\n\n\n\n\n\nCode\ncat(\"The sum of cross correlation function is\", sum(abs(ccf_result$acf)))\n\n\nThe sum of cross correlation function is 9.147159\n\n\n\n\n\n\nCode\npar(mfrow=c(1,1))\nccf_result <- ccf(normalized_sector_factor_data[, c(\"XLE.Adjusted\")], normalized_sector_factor_data[, c(\"inflation\")], \n    lag.max = 300,\n    main = \"Cros-Correlation Plot for Energy Sector Fund and Inflation Rate\",\n    ylab = \"CCF\")\n\n\n\n\n\nCode\ncat(\"The sum of cross correlation function is\", sum(abs(ccf_result$acf)))\n\n\nThe sum of cross correlation function is 8.754575\n\n\n\n\n\n\nCode\npar(mfrow=c(1,1))\nccf_result <- ccf(normalized_sector_factor_data[, c(\"XLE.Adjusted\")], normalized_sector_factor_data[, c(\"unemployment\")], \n    lag.max = 300,\n    main = \"Cros-Correlation Plot for Energy Sector Fund and Unemployment Rate\",\n    ylab = \"CCF\")\n\n\n\n\n\nCode\ncat(\"The sum of cross correlation function is\", sum(abs(ccf_result$acf)))\n\n\nThe sum of cross correlation function is 7.2758\n\n\n\n\n\nThe cross-correlation feature plots reaffirm the findings from the heatmap analysis, indicating that inflation rate exhibit stronger correlations with the index compared to GDP, interest rate and unemployment rate. The cross-correlation plots for GDP, unemployment and interest rates show weaker and more scattered patterns, with strikes between the blue lines indicating lower correlation coefficients. This suggests that inflation rate are more suitable feature variables for the ARIMAX model when predicting Energy Sector Fund movements.\nFinal Exogenous variables: Macroeconomic indicators: Inflation rate\n\n\nEndogenous and Exogenous Variables Plot\n\n\nCode\nfinal_XLE_factor_data <- sector_factor_data %>%dplyr::select( Date,XLE.Adjusted, inflation)\nnumeric_vars_XLE_factor_data <- c(\"XLE.Adjusted\", \"inflation\")\nnumeric_XLE_factor_data <- final_XLE_factor_data[, numeric_vars_XLE_factor_data]\nnormalized_XLE_factor_data_numeric <- scale(numeric_XLE_factor_data)\nnormalized_XLE_factor_data_numeric_df <- data.frame(normalized_XLE_factor_data_numeric)\nnormalized_XLE_factor_data_ts <- ts(normalized_XLE_factor_data_numeric, start = c(2018, 7), frequency = 4)\n\nautoplot(normalized_XLE_factor_data_ts, facets=TRUE) +\n  xlab(\"Year\") + ylab(\"\") +\n  ggtitle(\"Energy Sector Fund Stock Price, Inflation Rate and Unemployment Rate in USA 2018-2023\")\n\n\n\n\n\n\n\nCheck the stationarity\nBefore proceeding with further analysis, it’s important to check the stationarity of the time series data. If the series is not stationary, it needs to be differentiated before performing a ARIMAX model. The stationarity of a multivariate time series data can be checked using the Phillips-Perron test, which is specifically designed for multivariate data. By performing the Phillips-Perron test on the multivariate time series data, we can determine if the data is stationary or if it requires differencing to make it suitable for ARIMAX modeling.\n\n\nCode\n# Convert your multivariate time series data to a matrix\nfinal_XLE_factor_data_ts_multivariate <- as.matrix(normalized_XLE_factor_data_ts)\n\n# Check for stationarity using Phillips-Perron test\nphillips_perron_test <- ur.pp(final_XLE_factor_data_ts_multivariate)  \nsummary(phillips_perron_test)\n\n\n\n################################## \n# Phillips-Perron Unit Root Test # \n################################## \n\nTest regression with intercept \n\n\nCall:\nlm(formula = y ~ y.l1)\n\nResiduals:\n     Min       1Q   Median       3Q      Max \n-1.88632 -0.19841  0.06096  0.33275  1.36548 \n\nCoefficients:\n            Estimate Std. Error t value Pr(>|t|)    \n(Intercept)  0.02291    0.11006   0.208    0.836    \ny.l1         0.79774    0.11681   6.830 8.58e-08 ***\n---\nSignif. codes:  0 '***' 0.001 '**' 0.01 '*' 0.05 '.' 0.1 ' ' 1\n\nResidual standard error: 0.6502 on 33 degrees of freedom\nMultiple R-squared:  0.5857,    Adjusted R-squared:  0.5731 \nF-statistic: 46.64 on 1 and 33 DF,  p-value: 8.582e-08\n\n\nValue of test-statistic, type: Z-alpha  is: -8.8291 \n\n         aux. Z statistics\nZ-tau-mu            0.1801\n\n\nThe Phillips-Perron unit root test output suggests that the data is stationary. The test regression includes an intercept term and tests whether the coefficient on the lagged value is significantly different from one. The estimated coefficients indicate that the intercept is not significant (p-value > 0.05), while the coefficient on the lagged value is highly significant (p-value < 8.582e-08), suggesting that the data is stationary.\nAdditionally, the “Z-alpha” value is the test statistic, which is compared to critical values of the standard normal distribution to determine significance. In this case, the test statistic is -8.8291, indicating strong evidence against the null hypothesis of a unit root. The “Z-tau-mu” value is an auxiliary test statistic used to correct for autocorrelation in the residuals, and it is close to zero, suggesting that there is no evidence of residual autocorrelation.\n\n\nFitting a ARIMAX model\nFitting an ARIMAX model will be a useful approach to modeling and predicting time series data when external factors are believed to have an impact on the response variable. In this case, we will be using an ARIMAX model to predict the Energy Sector Fund Stock Price, taking into account the effects of inflation on the sector. By including these exogenous variables in our model, we can improve the accuracy of our predictions and gain a better understanding of the underlying dynamics of the time series.\n\nAuto ARIMAResiduals\n\n\n\n\nCode\nxreg <- cbind(Inflation = normalized_XLE_factor_data_ts[, \"inflation\"])\n\nfit <- auto.arima(normalized_XLE_factor_data_ts[, \"XLE.Adjusted\"], xreg = xreg)\nsummary(fit)\n\n\nSeries: normalized_XLE_factor_data_ts[, \"XLE.Adjusted\"] \nRegression with ARIMA(0,1,0) errors \n\nCoefficients:\n        xreg\n      0.9398\ns.e.  0.4018\n\nsigma^2 = 0.4018:  log likelihood = -15.86\nAIC=35.71   AICc=36.57   BIC=37.38\n\nTraining set error measures:\n                      ME      RMSE       MAE       MPE     MAPE      MASE\nTraining set -0.05259885 0.5976308 0.4515601 -349.5647 506.4671 0.3444568\n                   ACF1\nTraining set -0.4988802\n\n\n\n\n\n\nCode\ncheckresiduals(fit)\n\n\n\n\n\n\n    Ljung-Box test\n\ndata:  Residuals from Regression with ARIMA(0,1,0) errors\nQ* = 8.3288, df = 4, p-value = 0.08025\n\nModel df: 0.   Total lags used: 4\n\n\n\n\n\nThe auto.arima function reveals that the best-fit model is (0,1,0). Analysis of the residuals plot indicates that the fluctuates a lot and lies between -1.5 to 1.5. The ACF plot of the residuals shows significant lags, indicating that the model is performing well. The qq-plot also suggests normality in the residuals. The p-value greater than 0.05 actually suggests that there is no evidence of significant autocorrelation in the residuals, which means that the model is performing well in terms of capturing the temporal patterns in the data.\n\n\nFitting the model manually\n\nLinear ModelACF of ResidualsACF of ResidualsDifferentiated Residual\n\n\n\n\nCode\nnormalized_XLE_factor_data_numeric_df$XLE.Adjusted<-ts(normalized_XLE_factor_data_numeric_df$XLE.Adjusted,star=decimal_date(as.Date(\"2018-07-01\",format = \"%Y-%m-%d\")),frequency = 4)\nnormalized_XLE_factor_data_numeric_df$inflation<-ts(normalized_XLE_factor_data_numeric_df$inflation,star=decimal_date(as.Date(\"2018-07-01\",format = \"%Y-%m-%d\")),frequency = 4)\n\n\n############# First fit the linear model##########\nfit.reg <- lm(XLE.Adjusted ~ inflation, data=normalized_XLE_factor_data_numeric_df)\nsummary(fit.reg)\n\n\n\nCall:\nlm(formula = XLE.Adjusted ~ inflation, data = normalized_XLE_factor_data_numeric_df)\n\nResiduals:\n    Min      1Q  Median      3Q     Max \n-1.0438 -0.5173  0.1668  0.4337  1.1635 \n\nCoefficients:\n              Estimate Std. Error t value Pr(>|t|)    \n(Intercept) -6.443e-17  1.597e-01   0.000 1.000000    \ninflation    7.537e-01  1.643e-01   4.588 0.000303 ***\n---\nSignif. codes:  0 '***' 0.001 '**' 0.01 '*' 0.05 '.' 0.1 ' ' 1\n\nResidual standard error: 0.6774 on 16 degrees of freedom\nMultiple R-squared:  0.5681,    Adjusted R-squared:  0.5411 \nF-statistic: 21.05 on 1 and 16 DF,  p-value: 0.0003034\n\n\n\n\n\n\nCode\nres.fit<-ts(residuals(fit.reg),star=decimal_date(as.Date(\"2018-07-01\",format = \"%Y-%m-%d\")),frequency = 4)\n############## Then look at the residuals ############\nacf(res.fit)\n\n\n\n\n\n\n\n\n\nCode\nPacf(res.fit)\n\n\n\n\n\n\n\n\n\nCode\nres.fit %>% diff() %>% ggtsdisplay()\n\n\n\n\n\n\n\n\nThe output displays the results of a linear regression model using two predictor variables, inflation and unemployment, to explain the variation in the response variable, XLE.Adjusted The coefficients of inflation have p-values of less than 0.05, indicating that both variables significantly impact XLE.Adjusted The R-squared value of approximately 57% suggests that the model explains a moderate amount of the variation in the response variable. Examination of the ACF plot of residuals reveals some autocorrelation, and therefore, the series is differentiated to achieve stationarity. By analyzing the differentiated ACF and PACF plots, the parameters if the ARIMAX modle are that p=0, q=0, and d=1.\nAs both the model fitted manually and the auto.arima parameters are the same, i.e. p = 0, q = 0 and d = 1. We proceed with fitting the model for the parameters.\n\n\nModel Fitting\n\nModel PlotModel\n\n\n\n\nCode\nset.seed(1234)\n\nmodel_output <- capture.output(sarima(res.fit, 0,1,0)) \n\n\n\n\n\n\n\n\n\nCode\ncat(model_output[9:38], model_output[length(model_output)], sep = \"\\n\")\n\n\n$fit\n\nCall:\narima(x = xdata, order = c(p, d, q), seasonal = list(order = c(P, D, Q), period = S), \n    xreg = constant, transform.pars = trans, fixed = fixed, optim.control = list(trace = trc, \n        REPORT = 1, reltol = tol))\n\nCoefficients:\n      constant\n       -0.0321\ns.e.    0.1499\n\nsigma^2 estimated as 0.3819:  log likelihood = -15.94,  aic = 35.88\n\n$degrees_of_freedom\n[1] 16\n\n$ttable\n         Estimate     SE t.value p.value\nconstant  -0.0321 0.1499 -0.2139  0.8333\n\n$AIC\n[1] 2.11062\n\n$AICc\n[1] 2.126306\n\n$BIC\n[1] 2.208645\n\n\n\n\n\nThe best model identified is ARIMA(0,1,0). By analyzing the standardized residuals plot, it can be observed that the mean is close to 0. Deviations from these values could indicate poor model fit. However, in this case, the model seems to be well-fitted. The ACF plot of residuals shows very few significant lags, which is a positive sign for the model. The qq-plot also suggests normality in the residuals. Additionally, the p-values for the Ljung-Box test are greater than 0.05, indicating that the residuals are independent, which is favorable for the model’s accuracy.\n\n\nForecast\n::: panel-tabset ### Forecast for Energy Sector Fund with feature variable\n\n\nCode\n#fiting an ARIMA model to the Inflation variable\ninflation_fit<-auto.arima(normalized_XLE_factor_data_numeric_df$inflation) \nfinflation<-forecast(inflation_fit)\n\n\n# best model fit for forcasting\nxreg <- cbind(Inflation = normalized_XLE_factor_data_ts[, \"inflation\"])\n\nfit <- Arima(normalized_XLE_factor_data_ts[, \"XLE.Adjusted\"],order=c(0,1,0),xreg=xreg)\n\n# Forcast the stock price using feature variables\nfxreg <- cbind(Inflation = finflation$mean)\nfcast <- forecast(fit, xreg=fxreg) \nautoplot(fcast) + xlab(\"Year\") +\n  ylab(\"Normalised XLE.Adjusted\")\n\n\n\n\n\n\n\nARIMA Model for Inflation\n\n\nCode\n#fiting an ARIMA model to the Inflation variable\ninflation_fit<-auto.arima(normalized_XLE_factor_data_numeric_df$inflation) \nfinflation<-forecast(inflation_fit)\nsummary(inflation_fit)\n\n\nSeries: normalized_XLE_factor_data_numeric_df$inflation \nARIMA(0,1,0) \n\nsigma^2 = 0.1378:  log likelihood = -7.28\nAIC=16.55   AICc=16.82   BIC=17.38\n\nTraining set error measures:\n                    ME      RMSE       MAE       MPE     MAPE      MASE\nTraining set 0.1201871 0.3607553 0.2312026 -26.91442 61.15158 0.2676132\n                  ACF1\nTraining set 0.2431721\n\n\n\n\nARIMA Model for Energy Sector Fund with feature variable\n\n\nCode\n# best model fit for forecasting\nxreg <- cbind(Inflation = normalized_XLE_factor_data_ts[, \"inflation\"])\n\nfit <- Arima(normalized_XLE_factor_data_ts[, \"XLE.Adjusted\"],order=c(0,1,0),xreg=xreg)\nsummary(fit)\n\n\nSeries: normalized_XLE_factor_data_ts[, \"XLE.Adjusted\"] \nRegression with ARIMA(0,1,0) errors \n\nCoefficients:\n        xreg\n      0.9398\ns.e.  0.4018\n\nsigma^2 = 0.4018:  log likelihood = -15.86\nAIC=35.71   AICc=36.57   BIC=37.38\n\nTraining set error measures:\n                      ME      RMSE       MAE       MPE     MAPE      MASE\nTraining set -0.05259885 0.5976308 0.4515601 -349.5647 506.4671 0.3444568\n                   ACF1\nTraining set -0.4988802\n\n\nThe ARIMAX model may have incorporated exogenous variables such as macroeconomic factors, which could indicate the potential impact of external factors on the forecasted values. The findings suggest that inflation rates significantly impact the Energy Sector Fund stock price, underscoring the importance of including macroeconomic factors in stock market forecasting. This is further supported by the comparison of RMSE, MAE, AIC and BIC values for forecasts with and without the feature variable. It is found that including the feature variable leads to a lower values, indicating that the feature variable makes a significant impact and should be considered in stock market forecasting."
  },
  {
    "objectID": "arimax_sector_financial_macroeconomic.html",
    "href": "arimax_sector_financial_macroeconomic.html",
    "title": "ARIMAX/SARIMAX Page for Financial Sector Fund and macroeconomic factors",
    "section": "",
    "text": "Endogenous variable: Financial Sector Fund Exogenous variables: Macroeconomic indicators such as GDP, inflation rate, unemployment rate and interest rates\nBefore proceeding with model selection, it can be useful to examine the feature variables and select the appropriate ones. This can be done by creating cross-correlation function plots, which can serve as a verification of the correlation heatmap for the variables.\n\nCCF Plot for Financial Sector Fund and Exogenous Variables\n\nGDPInterest RateInflation RateUnemployment Rate\n\n\n\n\nCode\npar(mfrow=c(1,1))\nccf_result <- ccf(normalized_sector_factor_data[, c(\"XLF.Adjusted\")], normalized_sector_factor_data[, c(\"gdp\")], \n    lag.max = 300,\n    main = \"Cros-Correlation Plot for Financial Sector Fund and GDP Growth Rate \",\n    ylab = \"CCF\")\n\n\n\n\n\nCode\ncat(\"The sum of cross correlation function is\", sum(abs(ccf_result$acf)))\n\n\nThe sum of cross correlation function is 5.162983\n\n\n\n\n\n\nCode\npar(mfrow=c(1,1))\nccf_result <- ccf(normalized_sector_factor_data[, c(\"XLF.Adjusted\")], normalized_sector_factor_data[, c(\"interest\")], \n    lag.max = 300,\n    main = \"Cros-Correlation Plot for Financial Sector Fund and Interest Rate\",\n    ylab = \"CCF\")\n\n\n\n\n\nCode\ncat(\"The sum of cross correlation function is\", sum(abs(ccf_result$acf)))\n\n\nThe sum of cross correlation function is 8.914493\n\n\n\n\n\n\nCode\npar(mfrow=c(1,1))\nccf_result <- ccf(normalized_sector_factor_data[, c(\"XLF.Adjusted\")], normalized_sector_factor_data[, c(\"inflation\")], \n    lag.max = 300,\n    main = \"Cros-Correlation Plot for Financial Sector Fund and Inflation Rate\",\n    ylab = \"CCF\")\n\n\n\n\n\nCode\ncat(\"The sum of cross correlation function is\", sum(abs(ccf_result$acf)))\n\n\nThe sum of cross correlation function is 9.547857\n\n\n\n\n\n\nCode\npar(mfrow=c(1,1))\nccf_result <- ccf(normalized_sector_factor_data[, c(\"XLF.Adjusted\")], normalized_sector_factor_data[, c(\"unemployment\")], \n    lag.max = 300,\n    main = \"Cros-Correlation Plot for Financial Sector Fund and Unemployment Rate\",\n    ylab = \"CCF\")\n\n\n\n\n\nCode\ncat(\"The sum of cross correlation function is\", sum(abs(ccf_result$acf)))\n\n\nThe sum of cross correlation function is 6.49013\n\n\n\n\n\nThe cross-correlation feature plots reaffirm the findings from the heatmap analysis, indicating that inflation rate exhibit stronger correlations with the index compared to GDP, interest rate and unemployment rate. The cross-correlation plots for GDP, unemployment and interest rates show weaker and more scattered patterns, with strikes between the blue lines indicating lower correlation coefficients. This suggests that inflation rate are more suitable feature variables for the ARIMAX model when predicting Financial Sector Fund movements.\nFinal Exogenous variables: Macroeconomic indicators: Inflation rate\n\n\nEndogenous and Exogenous Variables Plot\n\n\nCode\nfinal_XLF_factor_data <- sector_factor_data %>%dplyr::select( Date,XLF.Adjusted, inflation)\nnumeric_vars_XLF_factor_data <- c(\"XLF.Adjusted\", \"inflation\")\nnumeric_XLF_factor_data <- final_XLF_factor_data[, numeric_vars_XLF_factor_data]\nnormalized_XLF_factor_data_numeric <- scale(numeric_XLF_factor_data)\nnormalized_XLF_factor_data_numeric_df <- data.frame(normalized_XLF_factor_data_numeric)\nnormalized_XLF_factor_data_ts <- ts(normalized_XLF_factor_data_numeric, start = c(2018, 7), frequency = 4)\n\nautoplot(normalized_XLF_factor_data_ts, facets=TRUE) +\n  xlab(\"Year\") + ylab(\"\") +\n  ggtitle(\"Financial Sector Fund Stock Price, Inflation Rate and Unemployment Rate in USA 2018-2023\")\n\n\n\n\n\n\n\nCheck the stationarity\nBefore proceeding with further analysis, it’s important to check the stationarity of the time series data. If the series is not stationary, it needs to be differentiated before performing a ARIMAX model. The stationarity of a multivariate time series data can be checked using the Phillips-Perron test, which is specifically designed for multivariate data. By performing the Phillips-Perron test on the multivariate time series data, we can determine if the data is stationary or if it requires differencing to make it suitable for ARIMAX modeling.\n\n\nCode\n# Convert your multivariate time series data to a matrix\nfinal_XLF_factor_data_ts_multivariate <- as.matrix(normalized_XLF_factor_data_ts)\n\n# Check for stationarity using Phillips-Perron test\nphillips_perron_test <- ur.pp(final_XLF_factor_data_ts_multivariate)  \nsummary(phillips_perron_test)\n\n\n\n################################## \n# Phillips-Perron Unit Root Test # \n################################## \n\nTest regression with intercept \n\n\nCall:\nlm(formula = y ~ y.l1)\n\nResiduals:\n     Min       1Q   Median       3Q      Max \n-1.79948 -0.17048 -0.03191  0.31535  0.80545 \n\nCoefficients:\n            Estimate Std. Error t value Pr(>|t|)    \n(Intercept)  0.06281    0.09066   0.693    0.493    \ny.l1         0.88078    0.09622   9.154 1.41e-10 ***\n---\nSignif. codes:  0 '***' 0.001 '**' 0.01 '*' 0.05 '.' 0.1 ' ' 1\n\nResidual standard error: 0.5356 on 33 degrees of freedom\nMultiple R-squared:  0.7175,    Adjusted R-squared:  0.7089 \nF-statistic:  83.8 on 1 and 33 DF,  p-value: 1.411e-10\n\n\nValue of test-statistic, type: Z-alpha  is: -6.3819 \n\n         aux. Z statistics\nZ-tau-mu            0.5878\n\n\nThe Phillips-Perron unit root test output suggests that the data is stationary. The test regression includes an intercept term and tests whether the coefficient on the lagged value is significantly different from one. The estimated coefficients indicate that the intercept is not significant (p-value > 0.05), while the coefficient on the lagged value is highly significant (p-value < 1.411e-10), suggesting that the data is stationary.\nAdditionally, the “Z-alpha” value is the test statistic, which is compared to critical values of the standard normal distribution to determine significance. In this case, the test statistic is -6.3819, indicating strong evidence against the null hypothesis of a unit root. The “Z-tau-mu” value is an auxiliary test statistic used to correct for autocorrelation in the residuals, and it is close to zero, suggesting that there is no evidence of residual autocorrelation.\n\n\nFitting a ARIMAX model\nFitting an ARIMAX model will be a useful approach to modeling and predicting time series data when external factors are believed to have an impact on the response variable. In this case, we will be using an ARIMAX model to predict the Financial Sector Fund Stock Price, taking into account the effects of inflation on the sector. By including these exogenous variables in our model, we can improve the accuracy of our predictions and gain a better understanding of the underlying dynamics of the time series.\n\nAuto ARIMAResiduals\n\n\n\n\nCode\nxreg <- cbind(Inflation = normalized_XLF_factor_data_ts[, \"inflation\"])\n\nfit <- auto.arima(normalized_XLF_factor_data_ts[, \"XLF.Adjusted\"], xreg = xreg)\nsummary(fit)\n\n\nSeries: normalized_XLF_factor_data_ts[, \"XLF.Adjusted\"] \nRegression with ARIMA(0,0,0) errors \n\nCoefficients:\n        xreg\n      0.8126\ns.e.  0.1374\n\nsigma^2 = 0.3397:  log likelihood = -15.31\nAIC=34.62   AICc=35.42   BIC=36.4\n\nTraining set error measures:\n                       ME      RMSE       MAE       MPE     MAPE      MASE\nTraining set -3.94746e-16 0.5664279 0.4726719 -13.14624 125.0325 0.4424444\n                 ACF1\nTraining set 0.479335\n\n\n\n\n\n\nCode\ncheckresiduals(fit)\n\n\n\n\n\n\n    Ljung-Box test\n\ndata:  Residuals from Regression with ARIMA(0,0,0) errors\nQ* = 8.8247, df = 4, p-value = 0.06563\n\nModel df: 0.   Total lags used: 4\n\n\n\n\n\nThe auto.arima function reveals that the best-fit model is (0,0,0). Analysis of the residuals plot indicates that the fluctuates a lot and lies between -0.1 to 0.6. The ACF plot of the residuals shows significant lags, indicating that the model is performing well. The qq-plot also suggests normality in the residuals. The p-value greater than 0.05 actually suggests that there is no evidence of significant autocorrelation in the residuals, which means that the model is performing well in terms of capturing the temporal patterns in the data.\n\n\nFitting the model manually\n\nLinear ModelACF of ResidualsACF of ResidualsDifferentiated Residual\n\n\n\n\nCode\nnormalized_XLF_factor_data_numeric_df$XLF.Adjusted<-ts(normalized_XLF_factor_data_numeric_df$XLF.Adjusted,star=decimal_date(as.Date(\"2018-07-01\",format = \"%Y-%m-%d\")),frequency = 4)\nnormalized_XLF_factor_data_numeric_df$inflation<-ts(normalized_XLF_factor_data_numeric_df$inflation,star=decimal_date(as.Date(\"2018-07-01\",format = \"%Y-%m-%d\")),frequency = 4)\n\n\n############# First fit the linear model##########\nfit.reg <- lm(XLF.Adjusted ~ inflation, data=normalized_XLF_factor_data_numeric_df)\nsummary(fit.reg)\n\n\n\nCall:\nlm(formula = XLF.Adjusted ~ inflation, data = normalized_XLF_factor_data_numeric_df)\n\nResiduals:\n     Min       1Q   Median       3Q      Max \n-0.98522 -0.35561 -0.04648  0.49344  0.88641 \n\nCoefficients:\n              Estimate Std. Error t value Pr(>|t|)    \n(Intercept) -4.073e-16  1.416e-01   0.000        1    \ninflation    8.126e-01  1.457e-01   5.577 4.17e-05 ***\n---\nSignif. codes:  0 '***' 0.001 '**' 0.01 '*' 0.05 '.' 0.1 ' ' 1\n\nResidual standard error: 0.6008 on 16 degrees of freedom\nMultiple R-squared:  0.6603,    Adjusted R-squared:  0.6391 \nF-statistic:  31.1 on 1 and 16 DF,  p-value: 4.173e-05\n\n\n\n\n\n\nCode\nres.fit<-ts(residuals(fit.reg),star=decimal_date(as.Date(\"2018-07-01\",format = \"%Y-%m-%d\")),frequency = 4)\n############## Then look at the residuals ############\nacf(res.fit)\n\n\n\n\n\n\n\n\n\nCode\nPacf(res.fit)\n\n\n\n\n\n\n\n\n\nCode\nres.fit %>% diff() %>% ggtsdisplay()\n\n\n\n\n\n\n\n\nThe output displays the results of a linear regression model using two predictor variables, inflation and unemployment, to explain the variation in the response variable, XLF.Adjusted The coefficients of inflation have p-values of less than 0.05, indicating that both variables significantly impact XLF.Adjusted The R-squared value of approximately 64% suggests that the model explains a moderate amount of the variation in the response variable. Examination of the ACF plot of residuals reveals some autocorrelation, and therefore, the series is differentiated to achieve stationarity. By analyzing the differentiated ACF and PACF plots, the parameters if the ARIMAX modle are that p=0, q=0, and d=1.\n\n\nFinding the model parameters.\n\nARIMAX ResultModel Plot 1Model 1Model Plot 2Model 2\n\n\n\n\nCode\nARIMA.c=function(p1,p2,q1,q2,data){\ntemp=c()\nd=1\ni=1\ntemp= data.frame()\nls=matrix(rep(NA,6*30),nrow=30)\n\n\nfor (p in p1:p2)#\n{\n  for(q in q1:q2)#\n  {\n    for(d in 0:1)\n    {\n      \n      if(p+d+q<=6)\n      {\n        \n        model<- Arima(data,order=c(p,d,q))\n        ls[i,]= c(p,d,q,model$aic,model$bic,model$aicc)\n        i=i+1\n        #print(i)\n        \n      }\n      \n    }\n  }\n}\n\n\ntemp= as.data.frame(ls)\nnames(temp)= c(\"p\",\"d\",\"q\",\"AIC\",\"BIC\",\"AICc\")\n\ntemp\n}\n\noutput <- ARIMA.c(0,1,0,1,data=residuals(fit.reg))\n\noutput[which.min(output$AIC),] \n\n\n  p d q      AIC      BIC    AICc\n2 0 1 0 29.22974 30.06295 29.4964\n\n\nCode\noutput[which.min(output$BIC),]\n\n\n  p d q      AIC      BIC    AICc\n2 0 1 0 29.22974 30.06295 29.4964\n\n\nCode\noutput[which.min(output$AICc),]\n\n\n  p d q      AIC      BIC    AICc\n2 0 1 0 29.22974 30.06295 29.4964\n\n\n\n\n\n\nCode\nset.seed(1234)\n\nmodel_output <- capture.output(sarima(res.fit, 0,1,0)) \n\n\n\n\n\n\n\n\n\nCode\ncat(model_output[9:38], model_output[length(model_output)], sep = \"\\n\")\n\n\n$fit\n\nCall:\narima(x = xdata, order = c(p, d, q), seasonal = list(order = c(P, D, Q), period = S), \n    xreg = constant, transform.pars = trans, fixed = fixed, optim.control = list(trace = trc, \n        REPORT = 1, reltol = tol))\n\nCoefficients:\n      constant\n       -0.0389\ns.e.    0.1304\n\nsigma^2 estimated as 0.289:  log likelihood = -13.57,  aic = 31.14\n\n$degrees_of_freedom\n[1] 16\n\n$ttable\n         Estimate     SE t.value p.value\nconstant  -0.0389 0.1304 -0.2986  0.7691\n\n$AIC\n[1] 1.831812\n\n$AICc\n[1] 1.847498\n\n$BIC\n[1] 1.929837\n\n\n\n\n\n\nCode\nset.seed(1234)\n\nmodel_output <- capture.output(sarima(res.fit, 0,0,0)) \n\n\n\n\n\n\n\n\n\nCode\ncat(model_output[9:38], model_output[length(model_output)], sep = \"\\n\")\n\n\n$fit\n\nCall:\narima(x = xdata, order = c(p, d, q), seasonal = list(order = c(P, D, Q), period = S), \n    xreg = xmean, include.mean = FALSE, transform.pars = trans, fixed = fixed, \n    optim.control = list(trace = trc, REPORT = 1, reltol = tol))\n\nCoefficients:\n       xmean\n      0.0000\ns.e.  0.1335\n\nsigma^2 estimated as 0.3208:  log likelihood = -15.31,  aic = 34.62\n\n$degrees_of_freedom\n[1] 17\n\n$ttable\n      Estimate     SE t.value p.value\nxmean        0 0.1335       0       1\n\n$AIC\n[1] 1.923288\n\n$AICc\n[1] 1.937177\n\n$BIC\n[1] 2.022218\n\n\n\n\n\nFollowing the manual fitting process, we identified that the ARIMAX model with the minimum AIC, bic and AICc has the parameters (0,1,0). We fitted both models using both the manual and auto.arima methods and observed that all the models have p-values greater than 0, indicating statistical significance. To determine the best fit model, we performed cross-validation.\n\n\nCross Validation\n\nRMSE PlotRMSE Results\n\n\n\n\nCode\nn=length(res.fit)\nk= 10\n \n \nrmse1 <- matrix(NA, (n-k),4)\nrmse2 <- matrix(NA, (n-k),4)\nrmse3 <- matrix(NA, (n-k),4)\n\nst <- tsp(res.fit)[1]+(k-5)/4 \n\nfor(i in 1:(n-k))\n{\n  xtrain <- window(res.fit, end=st + i/4)\n  xtest <- window(res.fit, start=st + (i+1)/4, end=st + (i+4)/4)\n\n  \n  fit <- Arima(xtrain, order=c(0,1,0),\n                include.drift=TRUE, method=\"ML\")\n  fcast <- forecast(fit, h=4)\n  \n  fit2 <- Arima(xtrain, order=c(0,0,0),\n                include.drift=TRUE, method=\"ML\")\n  fcast2 <- forecast(fit2, h=4)\n\n\n  rmse1[i,1:length(xtest)]   <- sqrt((fcast$mean-xtest)^2)\n  rmse2[i,1:length(xtest)] <- sqrt((fcast2$mean-xtest)^2)\n}\n\nplot(1:4,colMeans(rmse1,na.rm=TRUE), type=\"l\",col=2, xlab=\"horizon\", ylab=\"RMSE\")\nlines(1:4, colMeans(rmse2,na.rm=TRUE), type=\"l\",col=3)\nlegend(\"topleft\",legend=c(\"fit1\",\"fit2\"),col=2:4,lty=1)\n\n\n\n\n\n\n\n\n\nCode\ncat(\"RMSE values for Model 1\\n\", colMeans(rmse1))\n\n\nRMSE values for Model 1\n 0.4874805 0.7412223 1.079872 1.272875\n\n\nCode\ncat(\"RMSE values for Model 2\\n\", colMeans(rmse2))\n\n\nRMSE values for Model 2\n 0.4958342 0.5855811 0.8209764 0.8966811\n\n\n\n\n\nThe RMSE cross-validation plot shows that the RMSE of Model 2 (0,0,0) is lower than that of Model 1 (0,1,0). This suggests that Model 2 (0,0,0) is a better fit for the data when compared other model.\n\n\nForecast\n::: panel-tabset ### Forecast for Financial Sector Fund with feature variable\n\n\nCode\n#fiting an ARIMA model to the Inflation variable\ninflation_fit<-auto.arima(normalized_XLF_factor_data_numeric_df$inflation) \nfinflation<-forecast(inflation_fit)\n\n\n# best model fit for forcasting\nxreg <- cbind(Inflation = normalized_XLF_factor_data_ts[, \"inflation\"])\n\nfit <- Arima(normalized_XLF_factor_data_ts[, \"XLF.Adjusted\"],order=c(0,0,0),xreg=xreg)\n\n# Forcast the stock price using feature variables\nfxreg <- cbind(Inflation = finflation$mean)\nfcast <- forecast(fit, xreg=fxreg) \nautoplot(fcast) + xlab(\"Year\") +\n  ylab(\"Normalised XLF.Adjusted\")\n\n\n\n\n\n\n\nARIMA Model for Inflation\n\n\nCode\n#fiting an ARIMA model to the Inflation variable\ninflation_fit<-auto.arima(normalized_XLF_factor_data_numeric_df$inflation) \nfinflation<-forecast(inflation_fit)\nsummary(inflation_fit)\n\n\nSeries: normalized_XLF_factor_data_numeric_df$inflation \nARIMA(0,1,0) \n\nsigma^2 = 0.1378:  log likelihood = -7.28\nAIC=16.55   AICc=16.82   BIC=17.38\n\nTraining set error measures:\n                    ME      RMSE       MAE       MPE     MAPE      MASE\nTraining set 0.1201871 0.3607553 0.2312026 -26.91442 61.15158 0.2676132\n                  ACF1\nTraining set 0.2431721\n\n\n\n\nARIMA Model for Financial Sector Fund with feature variable\n\n\nCode\n# best model fit for forcasting\nxreg <- cbind(Inflation = normalized_XLF_factor_data_ts[, \"inflation\"])\n\nfit <- Arima(normalized_XLF_factor_data_ts[, \"XLF.Adjusted\"],order=c(0,0,0),xreg=xreg)\nsummary(fit)\n\n\nSeries: normalized_XLF_factor_data_ts[, \"XLF.Adjusted\"] \nRegression with ARIMA(0,0,0) errors \n\nCoefficients:\n      intercept    xreg\n         0.0000  0.8126\ns.e.     0.1335  0.1374\n\nsigma^2 = 0.3609:  log likelihood = -15.31\nAIC=36.62   AICc=38.33   BIC=39.29\n\nTraining set error measures:\n                       ME      RMSE       MAE       MPE     MAPE      MASE\nTraining set 1.572816e-13 0.5664279 0.4726719 -13.14624 125.0325 0.4424444\n                 ACF1\nTraining set 0.479335\n\n\nThe ARIMAX model may have incorporated exogenous variables such as macroeconomic factors, which could indicate the potential impact of external factors on the forecasted values. The findings suggest that inflation rates significantly impact the Financial Sector Fund stock price, underscoring the importance of including macroeconomic factors in stock market forecasting. This is further supported by the comparison of RMSE, MAE, AIC and BIC values for forecasts with and without the feature variable. It is found that including the feature variable leads to a lower values for AIC and BIC values but the RMSE and MAE values arehigher than the arima model, this might be not enough information to make conclude which model is better for the variables."
  },
  {
    "objectID": "arimax_sector_health_care_macroeconomic.html",
    "href": "arimax_sector_health_care_macroeconomic.html",
    "title": "ARIMAX/SARIMAX Page for Health Care Sector Fund and macroeconomic factors",
    "section": "",
    "text": "Endogenous variable: Health Care Sector Fund Exogenous variables: Macroeconomic indicators such as GDP, inflation rate, unemployment rate and interest rates\nBefore proceeding with model selection, it can be useful to examine the feature variables and select the appropriate ones. This can be done by creating cross-correlation function plots, which can serve as a verification of the correlation heatmap for the variables.\n\nCCF Plot for Health Care Sector Fund and Exogenous Variables\n\nGDPInterest RateInflation RateUnemployment Rate\n\n\n\n\nCode\npar(mfrow=c(1,1))\nccf_result <- ccf(normalized_sector_factor_data[, c(\"XLV.Adjusted\")], normalized_sector_factor_data[, c(\"gdp\")], \n    lag.max = 300,\n    main = \"Cros-Correlation Plot for Health Care Sector Fund and GDP Growth Rate \",\n    ylab = \"CCF\")\n\n\n\n\n\nCode\ncat(\"The sum of cross correlation function is\", sum(abs(ccf_result$acf)))\n\n\nThe sum of cross correlation function is 5.024517\n\n\n\n\n\n\nCode\npar(mfrow=c(1,1))\nccf_result <- ccf(normalized_sector_factor_data[, c(\"XLV.Adjusted\")], normalized_sector_factor_data[, c(\"interest\")], \n    lag.max = 300,\n    main = \"Cros-Correlation Plot for Health Care Sector Fund and Interest Rate\",\n    ylab = \"CCF\")\n\n\n\n\n\nCode\ncat(\"The sum of cross correlation function is\", sum(abs(ccf_result$acf)))\n\n\nThe sum of cross correlation function is 9.642841\n\n\n\n\n\n\nCode\npar(mfrow=c(1,1))\nccf_result <- ccf(normalized_sector_factor_data[, c(\"XLV.Adjusted\")], normalized_sector_factor_data[, c(\"inflation\")], \n    lag.max = 300,\n    main = \"Cros-Correlation Plot for Health Care Sector Fund and Inflation Rate\",\n    ylab = \"CCF\")\n\n\n\n\n\nCode\ncat(\"The sum of cross correlation function is\", sum(abs(ccf_result$acf)))\n\n\nThe sum of cross correlation function is 11.25353\n\n\n\n\n\n\nCode\npar(mfrow=c(1,1))\nccf_result <- ccf(normalized_sector_factor_data[, c(\"XLV.Adjusted\")], normalized_sector_factor_data[, c(\"unemployment\")], \n    lag.max = 300,\n    main = \"Cros-Correlation Plot for Health Care Sector Fund and Unemployment Rate\",\n    ylab = \"CCF\")\n\n\n\n\n\nCode\ncat(\"The sum of cross correlation function is\", sum(abs(ccf_result$acf)))\n\n\nThe sum of cross correlation function is 6.769889\n\n\n\n\n\nThe cross-correlation feature plots reaffirm the findings from the heatmap analysis, indicating that inflation rate exhibit stronger correlations with the index compared to GDP, interest rate and unemployment rate. The cross-correlation plots for GDP, unemployment and interest rates show weaker and more scattered patterns, with strikes between the blue lines indicating lower correlation coefficients. This suggests that inflation rate are more suitable feature variables for the ARIMAX model when predicting Health Care Sector Fund movements.\nFinal Exogenous variables: Macroeconomic indicators: Inflation rate\n\n\nEndogenous and Exogenous Variables Plot\n\n\nCode\nfinal_XLV_factor_data <- sector_factor_data %>%dplyr::select( Date,XLV.Adjusted, inflation)\nnumeric_vars_XLV_factor_data <- c(\"XLV.Adjusted\", \"inflation\")\nnumeric_XLV_factor_data <- final_XLV_factor_data[, numeric_vars_XLV_factor_data]\nnormalized_XLV_factor_data_numeric <- scale(numeric_XLV_factor_data)\nnormalized_XLV_factor_data_numeric_df <- data.frame(normalized_XLV_factor_data_numeric)\nnormalized_XLV_factor_data_ts <- ts(normalized_XLV_factor_data_numeric, start = c(2018, 7), frequency = 4)\n\nautoplot(normalized_XLV_factor_data_ts, facets=TRUE) +\n  xlab(\"Year\") + ylab(\"\") +\n  ggtitle(\"Health Care Sector Fund Stock Price, Inflation Rate and Unemployment Rate in USA 2018-2023\")\n\n\n\n\n\n\n\nCheck the stationarity\nBefore proceeding with further analysis, it’s important to check the stationarity of the time series data. If the series is not stationary, it needs to be differentiated before performing a ARIMAX model. The stationarity of a multivariate time series data can be checked using the Phillips-Perron test, which is specifically designed for multivariate data. By performing the Phillips-Perron test on the multivariate time series data, we can determine if the data is stationary or if it requires differencing to make it suitable for ARIMAX modeling.\n\n\nCode\n# Convert your multivariate time series data to a matrix\nfinal_XLV_factor_data_ts_multivariate <- as.matrix(normalized_XLV_factor_data_ts)\n\n# Check for stationarity using Phillips-Perron test\nphillips_perron_test <- ur.pp(final_XLV_factor_data_ts_multivariate)  \nsummary(phillips_perron_test)\n\n\n\n################################## \n# Phillips-Perron Unit Root Test # \n################################## \n\nTest regression with intercept \n\n\nCall:\nlm(formula = y ~ y.l1)\n\nResiduals:\n     Min       1Q   Median       3Q      Max \n-1.35623 -0.21421 -0.06766  0.28914  0.76913 \n\nCoefficients:\n            Estimate Std. Error t value Pr(>|t|)    \n(Intercept)  0.08155    0.07757   1.051    0.301    \ny.l1         0.90369    0.08233  10.977 1.49e-12 ***\n---\nSignif. codes:  0 '***' 0.001 '**' 0.01 '*' 0.05 '.' 0.1 ' ' 1\n\nResidual standard error: 0.4583 on 33 degrees of freedom\nMultiple R-squared:  0.785, Adjusted R-squared:  0.7785 \nF-statistic: 120.5 on 1 and 33 DF,  p-value: 1.492e-12\n\n\nValue of test-statistic, type: Z-alpha  is: -5.2745 \n\n         aux. Z statistics\nZ-tau-mu            0.8787\n\n\nThe Phillips-Perron unit root test output suggests that the data is stationary. The test regression includes an intercept term and tests whether the coefficient on the lagged value is significantly different from one. The estimated coefficients indicate that the intercept is not significant (p-value > 0.05), while the coefficient on the lagged value is highly significant (p-value < 1.49e-12), suggesting that the data is stationary.\nAdditionally, the “Z-alpha” value is the test statistic, which is compared to critical values of the standard normal distribution to determine significance. In this case, the test statistic is -5.2745, indicating strong evidence against the null hypothesis of a unit root. The “Z-tau-mu” value is an auxiliary test statistic used to correct for autocorrelation in the residuals, and it is close to zero, suggesting that there is no evidence of residual autocorrelation.\n\n\nFitting a ARIMAX model\nFitting an ARIMAX model will be a useful approach to modeling and predicting time series data when external factors are believed to have an impact on the response variable. In this case, we will be using an ARIMAX model to predict the Health Care Sector Fund Stock Price, taking into account the effects of inflation on the sector. By including these exogenous variables in our model, we can improve the accuracy of our predictions and gain a better understanding of the underlying dynamics of the time series.\n\nAuto ARIMAResiduals\n\n\n\n\nCode\nxreg <- cbind(Inflation = normalized_XLV_factor_data_ts[, \"inflation\"])\n\nfit <- auto.arima(normalized_XLV_factor_data_ts[, \"XLV.Adjusted\"], xreg = xreg)\nsummary(fit)\n\n\nSeries: normalized_XLV_factor_data_ts[, \"XLV.Adjusted\"] \nRegression with ARIMA(0,1,0) errors \n\nCoefficients:\n        xreg\n      0.5528\ns.e.  0.2538\n\nsigma^2 = 0.1603:  log likelihood = -8.05\nAIC=20.09   AICc=20.95   BIC=21.76\n\nTraining set error measures:\n                     ME      RMSE       MAE       MPE     MAPE      MASE\nTraining set 0.05668425 0.3775153 0.3134693 -28.94416 62.84741 0.4203005\n                   ACF1\nTraining set -0.4581504\n\n\n\n\n\n\nCode\ncheckresiduals(fit)\n\n\n\n\n\n\n    Ljung-Box test\n\ndata:  Residuals from Regression with ARIMA(0,1,0) errors\nQ* = 7.5156, df = 4, p-value = 0.111\n\nModel df: 0.   Total lags used: 4\n\n\n\n\n\nThe auto.arima function reveals that the best-fit model is (0,1,0). Analysis of the residuals plot indicates that the fluctuates a lot and lies between -0.50 to 0.75. The ACF plot of the residuals shows significant lags, indicating that the model is performing well. The qq-plot also suggests normality in the residuals. The p-value greater than 0.05 actually suggests that there is no evidence of significant autocorrelation in the residuals, which means that the model is performing well in terms of capturing the temporal patterns in the data.\n\n\nFitting the model manually\n\nLinear ModelACF of ResidualsACF of ResidualsDifferentiated Residual\n\n\n\n\nCode\nnormalized_XLV_factor_data_numeric_df$XLV.Adjusted<-ts(normalized_XLV_factor_data_numeric_df$XLV.Adjusted,star=decimal_date(as.Date(\"2018-07-01\",format = \"%Y-%m-%d\")),frequency = 4)\nnormalized_XLV_factor_data_numeric_df$inflation<-ts(normalized_XLV_factor_data_numeric_df$inflation,star=decimal_date(as.Date(\"2018-07-01\",format = \"%Y-%m-%d\")),frequency = 4)\n\n\n############# First fit the linear model##########\nfit.reg <- lm(XLV.Adjusted ~ inflation, data=normalized_XLV_factor_data_numeric_df)\nsummary(fit.reg)\n\n\n\nCall:\nlm(formula = XLV.Adjusted ~ inflation, data = normalized_XLV_factor_data_numeric_df)\n\nResiduals:\n     Min       1Q   Median       3Q      Max \n-0.92550 -0.36140  0.00643  0.38970  1.10570 \n\nCoefficients:\n              Estimate Std. Error t value Pr(>|t|)    \n(Intercept) -1.277e-15  1.300e-01   0.000        1    \ninflation    8.448e-01  1.338e-01   6.316 1.03e-05 ***\n---\nSignif. codes:  0 '***' 0.001 '**' 0.01 '*' 0.05 '.' 0.1 ' ' 1\n\nResidual standard error: 0.5515 on 16 degrees of freedom\nMultiple R-squared:  0.7137,    Adjusted R-squared:  0.6958 \nF-statistic: 39.89 on 1 and 16 DF,  p-value: 1.026e-05\n\n\n\n\n\n\nCode\nres.fit<-ts(residuals(fit.reg),star=decimal_date(as.Date(\"2018-01-01\",format = \"%Y-%m-%d\")),frequency = 4)\n############## Then look at the residuals ############\nacf(res.fit)\n\n\n\n\n\n\n\n\n\nCode\nPacf(res.fit)\n\n\n\n\n\n\n\n\n\nCode\nres.fit %>% diff() %>% ggtsdisplay()\n\n\n\n\n\n\n\n\nThe output displays the results of a linear regression model using two predictor variables, inflation and unemployment, to explain the variation in the response variable, XLV.Adjusted The coefficients of inflation have p-values of less than 0.05, indicating that both variables significantly impact XLV.Adjusted The R-squared value of approximately 71% suggests that the model explains a moderate amount of the variation in the response variable. Examination of the ACF plot of residuals reveals some autocorrelation, and therefore, the series is differentiated to achieve stationarity. By analyzing the differentiated ACF and PACF plots, the parameters if the ARIMAX modle are that p=0, q=0, and d=1.\nAs both the model fitted manually and the auto.arima parameters are the same, i.e. p = 0, q = 0 and d = 1. We proceed with fitting the model for the parameters.\n\n\nModel Fitting\n\nModel PlotModel\n\n\n\n\nCode\nset.seed(1234)\n\nmodel_output <- capture.output(sarima(res.fit, 0,1,0)) \n\n\n\n\n\n\n\n\n\nCode\ncat(model_output[9:38], model_output[length(model_output)], sep = \"\\n\")\n\n\n$fit\n\nCall:\narima(x = xdata, order = c(p, d, q), seasonal = list(order = c(P, D, Q), period = S), \n    xreg = constant, transform.pars = trans, fixed = fixed, optim.control = list(trace = trc, \n        REPORT = 1, reltol = tol))\n\nCoefficients:\n      constant\n        0.0229\ns.e.    0.0977\n\nsigma^2 estimated as 0.1621:  log likelihood = -8.66,  aic = 21.32\n\n$degrees_of_freedom\n[1] 16\n\n$ttable\n         Estimate     SE t.value p.value\nconstant   0.0229 0.0977  0.2345  0.8175\n\n$AIC\n[1] 1.253827\n\n$AICc\n[1] 1.269513\n\n$BIC\n[1] 1.351852\n\n\n\n\n\nThe best model identified is ARIMA(0,1,0). By analyzing the standardized residuals plot, it can be observed that the mean is close to 0. Deviations from these values could indicate poor model fit. However, in this case, the model seems to be well-fitted. The ACF plot of residuals shows very few significant lags, which is a positive sign for the model. The qq-plot also suggests normality in the residuals. Additionally, the p-values for the Ljung-Box test are greater than 0.05, indicating that the residuals are independent, which is favorable for the model’s accuracy.\n\n\nForecast\n::: panel-tabset ### Forecast for Health Care Sector Fund with feature variable\n\n\nCode\n#fiting an ARIMA model to the Inflation variable\ninflation_fit<-auto.arima(normalized_XLV_factor_data_numeric_df$inflation) \nfinflation<-forecast(inflation_fit)\n\n\n# best model fit for forcasting\nxreg <- cbind(Inflation = normalized_XLV_factor_data_ts[, \"inflation\"])\n\nfit <- Arima(normalized_XLV_factor_data_ts[, \"XLV.Adjusted\"],order=c(0,1,0),xreg=xreg)\n\n# Forcast the stock price using feature variables\nfxreg <- cbind(Inflation = finflation$mean)\nfcast <- forecast(fit, xreg=fxreg) \nautoplot(fcast) + xlab(\"Year\") +\n  ylab(\"Normalised XLV.Adjusted\")\n\n\n\n\n\n\n\nARIMA Model for Inflation\n\n\nCode\n#fiting an ARIMA model to the Inflation variable\ninflation_fit<-auto.arima(normalized_XLV_factor_data_numeric_df$inflation) \nfinflation<-forecast(inflation_fit)\nsummary(inflation_fit)\n\n\nSeries: normalized_XLV_factor_data_numeric_df$inflation \nARIMA(0,1,0) \n\nsigma^2 = 0.1378:  log likelihood = -7.28\nAIC=16.55   AICc=16.82   BIC=17.38\n\nTraining set error measures:\n                    ME      RMSE       MAE       MPE     MAPE      MASE\nTraining set 0.1201871 0.3607553 0.2312026 -26.91442 61.15158 0.2676132\n                  ACF1\nTraining set 0.2431721\n\n\n\n\nARIMA Model for Health Care Sector Fund with feature variable\n\n\nCode\n# best model fit for forcasting\nxreg <- cbind(Inflation = normalized_XLV_factor_data_ts[, \"inflation\"])\n\nfit <- Arima(normalized_XLV_factor_data_ts[, \"XLV.Adjusted\"],order=c(0,1,0),xreg=xreg)\nsummary(fit)\n\n\nSeries: normalized_XLV_factor_data_ts[, \"XLV.Adjusted\"] \nRegression with ARIMA(0,1,0) errors \n\nCoefficients:\n        xreg\n      0.5528\ns.e.  0.2538\n\nsigma^2 = 0.1603:  log likelihood = -8.05\nAIC=20.09   AICc=20.95   BIC=21.76\n\nTraining set error measures:\n                     ME      RMSE       MAE       MPE     MAPE      MASE\nTraining set 0.05668425 0.3775153 0.3134693 -28.94416 62.84741 0.4203005\n                   ACF1\nTraining set -0.4581504\n\n\nThe ARIMAX model may have incorporated exogenous variables such as macroeconomic factors, which could indicate the potential impact of external factors on the forecasted values. The findings suggest that inflation rates significantly impact the Health Care Sector Fund stock price, underscoring the importance of including macroeconomic factors in stock market forecasting. This is further supported by the comparison of RMSE, MAE, AIC and BIC values for forecasts with and without the feature variable. It is found that including the feature variable leads to a lower values, indicating that the feature variable makes a significant impact and should be considered in stock market forecasting."
  },
  {
    "objectID": "arimax_sector_industrial_macroeconomic.html",
    "href": "arimax_sector_industrial_macroeconomic.html",
    "title": "ARIMAX/SARIMAX Page for Industrial Sector Fund and macroeconomic factors",
    "section": "",
    "text": "Endogenous variable: Industrial Sector Fund Exogenous variables: Macroeconomic indicators such as GDP, inflation rate, unemployment rate and interest rates\nBefore proceeding with model selection, it can be useful to examine the feature variables and select the appropriate ones. This can be done by creating cross-correlation function plots, which can serve as a verification of the correlation heatmap for the variables.\n\nCCF Plot for Industrial Sector Fund and Exogenous Variables\n\nGDPInterest RateInflation RateUnemployment Rate\n\n\n\n\nCode\npar(mfrow=c(1,1))\nccf_result <- ccf(normalized_sector_factor_data[, c(\"XLI.Adjusted\")], normalized_sector_factor_data[, c(\"gdp\")], \n    lag.max = 300,\n    main = \"Cros-Correlation Plot for Industrial Sector Fund and GDP Growth Rate \",\n    ylab = \"CCF\")\n\n\n\n\n\nCode\ncat(\"The sum of cross correlation function is\", sum(abs(ccf_result$acf)))\n\n\nThe sum of cross correlation function is 5.024671\n\n\n\n\n\n\nCode\npar(mfrow=c(1,1))\nccf_result <- ccf(normalized_sector_factor_data[, c(\"XLI.Adjusted\")], normalized_sector_factor_data[, c(\"interest\")], \n    lag.max = 300,\n    main = \"Cros-Correlation Plot for Industrial Sector Fund and Interest Rate\",\n    ylab = \"CCF\")\n\n\n\n\n\nCode\ncat(\"The sum of cross correlation function is\", sum(abs(ccf_result$acf)))\n\n\nThe sum of cross correlation function is 8.965553\n\n\n\n\n\n\nCode\npar(mfrow=c(1,1))\nccf_result <- ccf(normalized_sector_factor_data[, c(\"XLI.Adjusted\")], normalized_sector_factor_data[, c(\"inflation\")], \n    lag.max = 300,\n    main = \"Cros-Correlation Plot for Industrial Sector Fund and Inflation Rate\",\n    ylab = \"CCF\")\n\n\n\n\n\nCode\ncat(\"The sum of cross correlation function is\", sum(abs(ccf_result$acf)))\n\n\nThe sum of cross correlation function is 9.82419\n\n\n\n\n\n\nCode\npar(mfrow=c(1,1))\nccf_result <- ccf(normalized_sector_factor_data[, c(\"XLI.Adjusted\")], normalized_sector_factor_data[, c(\"unemployment\")], \n    lag.max = 300,\n    main = \"Cros-Correlation Plot for Industrial Sector Fund and Unemployment Rate\",\n    ylab = \"CCF\")\n\n\n\n\n\nCode\ncat(\"The sum of cross correlation function is\", sum(abs(ccf_result$acf)))\n\n\nThe sum of cross correlation function is 6.6434\n\n\n\n\n\nThe cross-correlation feature plots reaffirm the findings from the heatmap analysis, indicating that inflation rate exhibit stronger correlations with the index compared to GDP, interest rate and unemployment rate. The cross-correlation plots for GDP, unemployment and interest rates show weaker and more scattered patterns, with strikes between the blue lines indicating lower correlation coefficients. This suggests that inflation rate are more suitable feature variables for the ARIMAX model when predicting Industrial Sector Fund movements.\nFinal Exogenous variables: Macroeconomic indicators: Inflation rate\n\n\nEndogenous and Exogenous Variables Plot\n\n\nCode\nfinal_XLI_factor_data <- sector_factor_data %>%dplyr::select( Date,XLI.Adjusted, inflation)\nnumeric_vars_XLI_factor_data <- c(\"XLI.Adjusted\", \"inflation\")\nnumeric_XLI_factor_data <- final_XLI_factor_data[, numeric_vars_XLI_factor_data]\nnormalized_XLI_factor_data_numeric <- scale(numeric_XLI_factor_data)\nnormalized_XLI_factor_data_numeric_df <- data.frame(normalized_XLI_factor_data_numeric)\nnormalized_XLI_factor_data_ts <- ts(normalized_XLI_factor_data_numeric, start = c(2018, 7), frequency = 4)\n\nautoplot(normalized_XLI_factor_data_ts, facets=TRUE) +\n  xlab(\"Year\") + ylab(\"\") +\n  ggtitle(\"Industrial Sector Fund Stock Price, Inflation Rate and Unemployment Rate in USA 2018-2023\")\n\n\n\n\n\n\n\nCheck the stationarity\nBefore proceeding with further analysis, it’s important to check the stationarity of the time series data. If the series is not stationary, it needs to be differentiated before performing a ARIMAX model. The stationarity of a multivariate time series data can be checked using the Phillips-Perron test, which is specifically designed for multivariate data. By performing the Phillips-Perron test on the multivariate time series data, we can determine if the data is stationary or if it requires differencing to make it suitable for ARIMAX modeling.\n\n\nCode\n# Convert your multivariate time series data to a matrix\nfinal_XLI_factor_data_ts_multivariate <- as.matrix(normalized_XLI_factor_data_ts)\n\n# Check for stationarity using Phillips-Perron test\nphillips_perron_test <- ur.pp(final_XLI_factor_data_ts_multivariate)  \nsummary(phillips_perron_test)\n\n\n\n################################## \n# Phillips-Perron Unit Root Test # \n################################## \n\nTest regression with intercept \n\n\nCall:\nlm(formula = y ~ y.l1)\n\nResiduals:\n     Min       1Q   Median       3Q      Max \n-1.75546 -0.19705 -0.06804  0.37121  0.81303 \n\nCoefficients:\n            Estimate Std. Error t value Pr(>|t|)    \n(Intercept)  0.06906    0.09327   0.740    0.464    \ny.l1         0.86276    0.09899   8.716  4.5e-10 ***\n---\nSignif. codes:  0 '***' 0.001 '**' 0.01 '*' 0.05 '.' 0.1 ' ' 1\n\nResidual standard error: 0.5511 on 33 degrees of freedom\nMultiple R-squared:  0.6971,    Adjusted R-squared:  0.688 \nF-statistic: 75.96 on 1 and 33 DF,  p-value: 4.497e-10\n\n\nValue of test-statistic, type: Z-alpha  is: -6.4883 \n\n         aux. Z statistics\nZ-tau-mu            0.6574\n\n\nThe Phillips-Perron unit root test output suggests that the data is stationary. The test regression includes an intercept term and tests whether the coefficient on the lagged value is significantly different from one. The estimated coefficients indicate that the intercept is not significant (p-value > 0.05), while the coefficient on the lagged value is highly significant (p-value < 4.497e-10), suggesting that the data is stationary.\nAdditionally, the “Z-alpha” value is the test statistic, which is compared to critical values of the standard normal distribution to determine significance. In this case, the test statistic is -6.4883, indicating strong evidence against the null hypothesis of a unit root. The “Z-tau-mu” value is an auxiliary test statistic used to correct for autocorrelation in the residuals, and it is close to zero, suggesting that there is no evidence of residual autocorrelation.\n\n\nFitting a ARIMAX model\nFitting an ARIMAX model will be a useful approach to modeling and predicting time series data when external factors are believed to have an impact on the response variable. In this case, we will be using an ARIMAX model to predict the Industrial Sector Fund Stock Price, taking into account the effects of inflation on the sector. By including these exogenous variables in our model, we can improve the accuracy of our predictions and gain a better understanding of the underlying dynamics of the time series.\n\nAuto ARIMAResiduals\n\n\n\n\nCode\nxreg <- cbind(Inflation = normalized_XLI_factor_data_ts[, \"inflation\"])\n\nfit <- auto.arima(normalized_XLI_factor_data_ts[, \"XLI.Adjusted\"], xreg = xreg)\nsummary(fit)\n\n\nSeries: normalized_XLI_factor_data_ts[, \"XLI.Adjusted\"] \nRegression with ARIMA(1,0,0) errors \n\nCoefficients:\n         ar1    xreg\n      0.5686  0.7112\ns.e.  0.2008  0.2346\n\nsigma^2 = 0.3259:  log likelihood = -14.59\nAIC=35.17   AICc=36.89   BIC=37.84\n\nTraining set error measures:\n                      ME    RMSE       MAE       MPE     MAPE      MASE\nTraining set -0.02217365 0.53825 0.4345546 -23.76264 95.84547 0.4338544\n                    ACF1\nTraining set -0.01895771\n\n\n\n\n\n\nCode\ncheckresiduals(fit)\n\n\n\n\n\n\n    Ljung-Box test\n\ndata:  Residuals from Regression with ARIMA(1,0,0) errors\nQ* = 3.0475, df = 3, p-value = 0.3844\n\nModel df: 1.   Total lags used: 4\n\n\n\n\n\nThe auto.arima function reveals that the best-fit model is (1,0,0). Analysis of the residuals plot indicates that the fluctuates a lot and lies between -0.6 to 0.6. The ACF plot of the residuals shows significant lags, indicating that the model is performing well. The qq-plot also suggests normality in the residuals. The p-value greater than 0.05 actually suggests that there is no evidence of significant autocorrelation in the residuals, which means that the model is performing well in terms of capturing the temporal patterns in the data.\n\n\nFitting the model manually\n\nLinear ModelACF of ResidualsACF of ResidualsDifferentiated Residual\n\n\n\n\nCode\nnormalized_XLI_factor_data_numeric_df$XLI.Adjusted<-ts(normalized_XLI_factor_data_numeric_df$XLI.Adjusted,star=decimal_date(as.Date(\"2018-07-01\",format = \"%Y-%m-%d\")),frequency = 4)\nnormalized_XLI_factor_data_numeric_df$inflation<-ts(normalized_XLI_factor_data_numeric_df$inflation,star=decimal_date(as.Date(\"2018-07-01\",format = \"%Y-%m-%d\")),frequency = 4)\n\n\n############# First fit the linear model##########\nfit.reg <- lm(XLI.Adjusted ~ inflation, data=normalized_XLI_factor_data_numeric_df)\nsummary(fit.reg)\n\n\n\nCall:\nlm(formula = XLI.Adjusted ~ inflation, data = normalized_XLI_factor_data_numeric_df)\n\nResiduals:\n     Min       1Q   Median       3Q      Max \n-1.00573 -0.52568 -0.03409  0.36864  1.16237 \n\nCoefficients:\n             Estimate Std. Error t value Pr(>|t|)    \n(Intercept) 2.971e-16  1.617e-01   0.000 1.000000    \ninflation   7.464e-01  1.664e-01   4.487 0.000374 ***\n---\nSignif. codes:  0 '***' 0.001 '**' 0.01 '*' 0.05 '.' 0.1 ' ' 1\n\nResidual standard error: 0.686 on 16 degrees of freedom\nMultiple R-squared:  0.5571,    Adjusted R-squared:  0.5295 \nF-statistic: 20.13 on 1 and 16 DF,  p-value: 0.0003738\n\n\n\n\n\n\nCode\nres.fit<-ts(residuals(fit.reg),star=decimal_date(as.Date(\"2018-07-01\",format = \"%Y-%m-%d\")),frequency = 4)\n############## Then look at the residuals ############\nacf(res.fit)\n\n\n\n\n\n\n\n\n\nCode\nPacf(res.fit)\n\n\n\n\n\n\n\n\n\nCode\nres.fit %>% diff() %>% ggtsdisplay()\n\n\n\n\n\n\n\n\nThe output displays the results of a linear regression model using two predictor variables, inflation and unemployment, to explain the variation in the response variable, XLI.Adjusted The coefficients of inflation have p-values of less than 0.05, indicating that both variables significantly impact XLI.Adjusted The R-squared value of approximately 56% suggests that the model explains a moderate amount of the variation in the response variable. Examination of the ACF plot of residuals reveals some autocorrelation, and therefore, the series is differentiated to achieve stationarity. By analyzing the differentiated ACF and PACF plots, the parameters if the ARIMAX modle are that p=0, q=0, and d=1.\n\n\nFinding the model parameters.\n\nARIMAX ResultModel Plot 1Model 1Model Plot 2Model 2\n\n\n\n\nCode\nARIMA.c=function(p1,p2,q1,q2,data){\ntemp=c()\nd=1\ni=1\ntemp= data.frame()\nls=matrix(rep(NA,6*30),nrow=30)\n\n\nfor (p in p1:p2)#\n{\n  for(q in q1:q2)#\n  {\n    for(d in 0:1)\n    {\n      \n      if(p+d+q<=6)\n      {\n        \n        model<- Arima(data,order=c(p,d,q))\n        ls[i,]= c(p,d,q,model$aic,model$bic,model$aicc)\n        i=i+1\n        #print(i)\n        \n      }\n      \n    }\n  }\n}\n\n\ntemp= as.data.frame(ls)\nnames(temp)= c(\"p\",\"d\",\"q\",\"AIC\",\"BIC\",\"AICc\")\n\ntemp\n}\n\noutput <- ARIMA.c(0,1,0,1,data=residuals(fit.reg))\n\noutput[which.min(output$AIC),] \n\n\n  p d q      AIC      BIC     AICc\n2 0 1 0 33.00926 33.84248 33.27593\n\n\nCode\noutput[which.min(output$BIC),]\n\n\n  p d q      AIC      BIC     AICc\n2 0 1 0 33.00926 33.84248 33.27593\n\n\nCode\noutput[which.min(output$AICc),]\n\n\n  p d q      AIC      BIC     AICc\n2 0 1 0 33.00926 33.84248 33.27593\n\n\n\n\n\n\nCode\nset.seed(1234)\n\nmodel_output <- capture.output(sarima(res.fit, 0,1,0)) \n\n\n\n\n\n\n\n\n\nCode\ncat(model_output[9:38], model_output[length(model_output)], sep = \"\\n\")\n\n\n$fit\n\nCall:\narima(x = xdata, order = c(p, d, q), seasonal = list(order = c(P, D, Q), period = S), \n    xreg = constant, transform.pars = trans, fixed = fixed, optim.control = list(trace = trc, \n        REPORT = 1, reltol = tol))\n\nCoefficients:\n      constant\n       -0.0207\ns.e.    0.1460\n\nsigma^2 estimated as 0.3624:  log likelihood = -15.49,  aic = 34.99\n\n$degrees_of_freedom\n[1] 16\n\n$ttable\n         Estimate    SE t.value p.value\nconstant  -0.0207 0.146  -0.142  0.8889\n\n$AIC\n[1] 2.058184\n\n$AICc\n[1] 2.07387\n\n$BIC\n[1] 2.156209\n\n\n\n\n\n\nCode\nset.seed(1234)\n\nmodel_output <- capture.output(sarima(res.fit, 1,0,0)) \n\n\n\n\n\n\n\n\n\nCode\ncat(model_output[21:50], model_output[length(model_output)], sep = \"\\n\")\n\n\n$fit\n\nCall:\narima(x = xdata, order = c(p, d, q), seasonal = list(order = c(P, D, Q), period = S), \n    xreg = xmean, include.mean = FALSE, transform.pars = trans, fixed = fixed, \n    optim.control = list(trace = trc, REPORT = 1, reltol = tol))\n\nCoefficients:\n         ar1    xmean\n      0.5780  -0.1029\ns.e.  0.2013   0.2887\n\nsigma^2 estimated as 0.2877:  log likelihood = -14.53,  aic = 35.06\n\n$degrees_of_freedom\n[1] 16\n\n$ttable\n      Estimate     SE t.value p.value\nar1     0.5780 0.2013  2.8707  0.0111\nxmean  -0.1029 0.2887 -0.3565  0.7261\n\n$AIC\n[1] 1.947862\n\n$AICc\n[1] 1.992306\n\n$BIC\n[1] 2.096257\n\n\n\n\n\nFollowing the manual fitting process, we identified that the ARIMAX model with the minimum AIC,BIC and AICc has the parameters (0,1,0). We fitted both models using both the manual and auto.arima methods and observed that all the models have p-values greater than 0, indicating statistical significance. To determine the best fit model, we performed cross-validation.\n\n\nCross Validation\n\nRMSE PlotRMSE Results\n\n\n\n\nCode\nn=length(res.fit)\nk= 10\n \n \nrmse1 <- matrix(NA, (n-k),4)\nrmse2 <- matrix(NA, (n-k),4)\nrmse3 <- matrix(NA, (n-k),4)\n\nst <- tsp(res.fit)[1]+(k-5)/4 \n\nfor(i in 1:(n-k))\n{\n  xtrain <- window(res.fit, end=st + i/4)\n  xtest <- window(res.fit, start=st + (i+1)/4, end=st + (i+4)/4)\n\n  \n  fit <- Arima(xtrain, order=c(0,1,0),\n                include.drift=TRUE, method=\"ML\")\n  fcast <- forecast(fit, h=4)\n  \n  fit2 <- Arima(xtrain, order=c(1,0,0),\n                include.drift=TRUE, method=\"ML\")\n  fcast2 <- forecast(fit2, h=4)\n\n\n  rmse1[i,1:length(xtest)]   <- sqrt((fcast$mean-xtest)^2)\n  rmse2[i,1:length(xtest)] <- sqrt((fcast2$mean-xtest)^2)\n}\n\nplot(1:4,colMeans(rmse1,na.rm=TRUE), type=\"l\",col=2, xlab=\"horizon\", ylab=\"RMSE\")\nlines(1:4, colMeans(rmse2,na.rm=TRUE), type=\"l\",col=3)\nlegend(\"topleft\",legend=c(\"fit1\",\"fit2\"),col=2:4,lty=1)\n\n\n\n\n\n\n\n\n\nCode\ncat(\"RMSE values for Model 1\\n\", colMeans(rmse1))\n\n\nRMSE values for Model 1\n 0.566305 0.8614442 1.146143 1.314074\n\n\nCode\ncat(\"RMSE values for Model 2\\n\", colMeans(rmse2))\n\n\nRMSE values for Model 2\n 0.7080378 0.7809364 0.8256064 1.12462\n\n\n\n\n\nThe RMSE cross-validation plot shows that the RMSE of Model 2 (1,0,0) is lower than that of Model 1 (0,1,0). This suggests that Model 2 (1,0,0) is a better fit for the data when compared other model.\n\n\nForecast\n::: panel-tabset ### Forecast for Industrial Sector Fund with feature variable\n\n\nCode\n#fiting an ARIMA model to the Inflation variable\ninflation_fit<-auto.arima(normalized_XLI_factor_data_numeric_df$inflation) \nfinflation<-forecast(inflation_fit)\n\n\n# best model fit for forcasting\nxreg <- cbind(Inflation = normalized_XLI_factor_data_ts[, \"inflation\"])\n\nfit <- Arima(normalized_XLI_factor_data_ts[, \"XLI.Adjusted\"],order=c(1,0,0),xreg=xreg)\n\n# Forcast the stock price using feature variables\nfxreg <- cbind(Inflation = finflation$mean)\nfcast <- forecast(fit, xreg=fxreg) \nautoplot(fcast) + xlab(\"Year\") +\n  ylab(\"Normalised XLI.Adjusted\")\n\n\n\n\n\n\n\nARIMA Model for Inflation\n\n\nCode\n#fiting an ARIMA model to the Inflation variable\ninflation_fit<-auto.arima(normalized_XLI_factor_data_numeric_df$inflation) \nfinflation<-forecast(inflation_fit)\nsummary(inflation_fit)\n\n\nSeries: normalized_XLI_factor_data_numeric_df$inflation \nARIMA(0,1,0) \n\nsigma^2 = 0.1378:  log likelihood = -7.28\nAIC=16.55   AICc=16.82   BIC=17.38\n\nTraining set error measures:\n                    ME      RMSE       MAE       MPE     MAPE      MASE\nTraining set 0.1201871 0.3607553 0.2312026 -26.91442 61.15158 0.2676132\n                  ACF1\nTraining set 0.2431721\n\n\n\n\nARIMA Model for Industrial Sector Fund with feature variable\n\n\nCode\n# best model fit for forcasting\nxreg <- cbind(Inflation = normalized_XLI_factor_data_ts[, \"inflation\"])\n\nfit <- Arima(normalized_XLI_factor_data_ts[, \"XLI.Adjusted\"],order=c(1,0,0),xreg=xreg)\nsummary(fit)\n\n\nSeries: normalized_XLI_factor_data_ts[, \"XLI.Adjusted\"] \nRegression with ARIMA(1,0,0) errors \n\nCoefficients:\n         ar1  intercept    xreg\n      0.5764    -0.0999  0.7172\ns.e.  0.2002     0.2879  0.2366\n\nsigma^2 = 0.345:  log likelihood = -14.52\nAIC=37.05   AICc=40.12   BIC=40.61\n\nTraining set error measures:\n                     ME      RMSE       MAE       MPE     MAPE      MASE\nTraining set 0.02177766 0.5361627 0.4398962 -27.78407 100.0628 0.4391875\n                    ACF1\nTraining set -0.02341074\n\n\nThe ARIMAX model may have incorporated exogenous variables such as macroeconomic factors, which could indicate the potential impact of external factors on the forecasted values. The findings suggest that inflation rates significantly impact the Industrial Sector Fund stock price, underscoring the importance of including macroeconomic factors in stock market forecasting. This is further supported by the comparison of RMSE, MAE, AIC and BIC values for forecasts with and without the feature variable. It is found that including the feature variable leads to a lower values, indicating that the feature variable makes a significant impact and should be considered in stock market forecasting."
  },
  {
    "objectID": "arimax_sector_market_macroeconomic.html",
    "href": "arimax_sector_market_macroeconomic.html",
    "title": "ARIMAX/SARIMAX Model for Sector market and macroeconomic factors as exogenous variables",
    "section": "",
    "text": "The ARIMAX/SARIMA model is a powerful tool for analyzing the relationships between sector markets, macroeconomic factors, and other exogenous variables. By incorporating both endogenous and exogenous variables, the ARIMAX/SARIMA model can provide a more accurate and comprehensive analysis of the stock market.\nIn the case of sector markets, the ARIMAX/SARIMAX model can be used to identify important relationships between different sectors and their performance. For example, if the technology sector is performing well, we might expect to see a corresponding increase in the performance of other sectors that rely on technology.\nBy including macroeconomic factors as exogenous variables, the ARIMAX/SARIMAX model can also help to identify how changes in the broader economy might impact the performance of different sectors. For example, if interest rates are expected to rise, we might expect to see a decrease in the performance of sectors that are particularly sensitive to interest rates, such as real estate or financial sectors.\nAccording to the findings, the endogenous and exogenous variables in the time series data are not interdependent, then the ARIMAX model can be a good choice for predicting the stock market indices. If there is seasonality in the data, then the SARIMAX model can be used to account for this seasonal variation. If there is no seasonality, then the simpler ARIMA model can be used instead of SARIMAX.\n\n\n\n\n\n\nLet’s examine the relationship between endogenous and exogenous variables before proceeding with the ARIMAX/SARIMAX model.\n\nPlotNormalized Plot\n\n\n\n\nCode\nts_plot(sector_factor_data,\n        title = \"Sector Market and Macroeconomic Variables\",\n        Ytitle = \"Values\",\n        Xtitle = \"Year\")\n\n\n\n\n\n\n\n\n\n\nCode\nnumeric_vars_sector_factor_data <- c(\"XLB.Adjusted\", \"XLC.Adjusted\", \"XLE.Adjusted\",\"XLF.Adjusted\",\"XLI.Adjusted\",\"XLP.Adjusted\",\"XLK.Adjusted\",\"XLRE.Adjusted\",\"XLU.Adjusted\",\"XLV.Adjusted\",\"XLY.Adjusted\",\"gdp\", \"interest\", \"inflation\", \"unemployment\")\nnumeric_sector_factor_data <- sector_factor_data[, numeric_vars_sector_factor_data]\nnormalized_sector_factor_data_numeric <- scale(numeric_sector_factor_data)\nnormalized_sector_factor_data <- ts(normalized_sector_factor_data_numeric, start = c(2010, 1), end = c(2021,10),frequency = 4)\nts_plot(normalized_sector_factor_data,\n        title = \"Normalized Time Series Data for Sector Market and Macroeconomic Variables\",\n        Ytitle = \"Normalized Values\",\n        Xtitle = \"Year\")\n\n\n\n\n\n\n\n\n\nPlotting the raw time series data for these variables can be challenging due to differences in scale and units of measurement. Therefore, normalizing the data can provide a clearer picture of the relationships and patterns in the data.\nIn the Normalized Time Series Data for Sector Market and Macroeconomic Factors plot, the variables have been scaled using a common scaling technique such as z-scores or percentage changes. This process allows for a fair comparison between variables and eliminates the impact of differing scales or units of measurement.\nNormalizing the data is essential when analyzing the relationship between sector market data and macroeconomic factors. It can help remove any bias or distortion introduced by variables with different units or magnitudes and can stabilize the estimation of models. Additionally, normalizing the data can improve the interpretability of model coefficients, making it easier to assess their relative importance. By normalizing the time series data, it is possible to gain insights into the relationships between sector market data and macroeconomic factors and to make informed investment decisions based on these insights.\n\nCross-Correlation for the Variables and Selection of Feature Variables\nCross-correlation is a statistical technique used to measure the relationship between two or more variables in a time series. In the context of ARIMAX modeling, cross-correlation is often used for feature selection. For selecting feature variables in our analysis, we will first examine the correlation through a heatmap among all the variables, and then analyze the autocorrelation function (ACF) plots between the response variable and the exogenous variables.\n\n\nCorrelation Heatmap\n\n\nCode\n# Get upper triangle of the correlation matrix\nget_upper_tri <- function(cormat){\n    cormat[lower.tri(cormat)]<- NA\n    return(cormat)\n}\ncormat <- round(cor(normalized_sector_factor_data_numeric),2)\n\nupper_tri <- get_upper_tri(cormat)\n\nmelted_cormat <- melt(upper_tri, na.rm = TRUE)\n# Create a ggheatmap\nggheatmap <- ggplot(melted_cormat, aes(Var2, Var1, fill = value))+\n geom_tile(color = \"white\")+\n scale_fill_gradient2(low = \"blue\", high = \"red\", mid = \"white\", \n   midpoint = 0, limit = c(-1,1), space = \"Lab\", \n    name=\"Pearson\\nCorrelation\") +\n  theme_minimal()+ # minimal theme\n theme(axis.text.x = element_text(angle = 45, vjust = 1, \n    size = 12, hjust = 1))+\n coord_fixed()\n\nggheatmap + \ngeom_text(aes(Var2, Var1, label = value), color = \"black\", size = 4) +\ntheme(\n  axis.title.x = element_blank(),\n  axis.title.y = element_blank(),\n  panel.grid.major = element_blank(),\n  panel.border = element_blank(),\n  panel.background = element_blank(),\n  axis.ticks = element_blank(),\n  legend.justification = c(1, 0),\n  legend.position = c(0.6, 0.7),\n  legend.direction = \"horizontal\")+\n  guides(fill = guide_colorbar(barwidth = 7, barheight = 1,\n                title.position = \"top\", title.hjust = 0.5))\n\n\n\n\n\nThe heatmap reveal important insights into the relationships between the sector market and various economic indicators. The strong positive correlations between the sector market and inflation, along with the negative correlations with unemployment rate, and gdp growth rate and sector market have positive correlation suggest that these variables may play a significant role in influencing sector market movements. In contrast, the weaker correlations between the sector market interest rates indicate that these variables may have less impact on sector market fluctuations except for the enegry sector. These findings provide valuable guidance for selecting relevant variables in the VAR model to better understand and forecast stock market dynamics.\nClick to view ARIMAX/SARIMAX Page for Consumer Staples Sector Fund and macroeconomic factors as exogenous variables\nClick to view ARIMAX/SARIMAX Page for Utilities Sector Fund and macroeconomic factors as exogenous variables\nClick to view ARIMAX/SARIMAX Page for Health Care Sector Fund and macroeconomic factors as exogenous variables\nClick to view ARIMAX/SARIMAX Page for Industrial Sector Fund and macroeconomic factors as exogenous variables\nClick to view ARIMAX/SARIMAX Page for Financial Sector Fund and macroeconomic factors as exogenous variables\nClick to view ARIMAX/SARIMAX Page for Consumer Discretionary Sector Fund and macroeconomic factors as exogenous variables\nClick to view ARIMAX/SARIMAX Page for Communication Services Sector Fund and macroeconomic factors as exogenous variables\nClick to view ARIMAX/SARIMAX Page for Real Estate Sector Fund and macroeconomic factors as exogenous variables\nClick to view ARIMAX/SARIMAX Page for Materials Sector Fund and macroeconomic factors as exogenous variables\nClick to view ARIMAX/SARIMAX Page for Technology Sector Fund and macroeconomic factors as exogenous variables\nClick to view ARIMAX/SARIMAX Page for Energy Sector Fund and macroeconomic factors as exogenous variables"
  },
  {
    "objectID": "arimax_sector_materials_sector_macroeconomic.html",
    "href": "arimax_sector_materials_sector_macroeconomic.html",
    "title": "ARIMAX/SARIMAX Page for Materials Sector Fund and macroeconomic factors",
    "section": "",
    "text": "Endogenous variable: Materials Sector Fund Exogenous variables: Macroeconomic indicators such as GDP, inflation rate, unemployment rate and interest rates\nBefore proceeding with model selection, it can be useful to examine the feature variables and select the appropriate ones. This can be done by creating cross-correlation function plots, which can serve as a verification of the correlation heatmap for the variables.\n\nCCF Plot for Materials Sector Fund and Exogenous Variables\n\nGDPInterest RateInflation RateUnemployment Rate\n\n\n\n\nCode\npar(mfrow=c(1,1))\nccf_result <- ccf(normalized_sector_factor_data[, c(\"XLB.Adjusted\")], normalized_sector_factor_data[, c(\"gdp\")], \n    lag.max = 300,\n    main = \"Cros-Correlation Plot for Materials Sector Fund and GDP Growth Rate \",\n    ylab = \"CCF\")\n\n\n\n\n\nCode\ncat(\"The sum of cross correlation function is\", sum(abs(ccf_result$acf)))\n\n\nThe sum of cross correlation function is 4.874086\n\n\n\n\n\n\nCode\npar(mfrow=c(1,1))\nccf_result <- ccf(normalized_sector_factor_data[, c(\"XLB.Adjusted\")], normalized_sector_factor_data[, c(\"interest\")], \n    lag.max = 300,\n    main = \"Cros-Correlation Plot for Materials Sector Fund and Interest Rate\",\n    ylab = \"CCF\")\n\n\n\n\n\nCode\ncat(\"The sum of cross correlation function is\", sum(abs(ccf_result$acf)))\n\n\nThe sum of cross correlation function is 9.572994\n\n\n\n\n\n\nCode\npar(mfrow=c(1,1))\nccf_result <- ccf(normalized_sector_factor_data[, c(\"XLB.Adjusted\")], normalized_sector_factor_data[, c(\"inflation\")], \n    lag.max = 300,\n    main = \"Cros-Correlation Plot for Materials Sector Fund and Inflation Rate\",\n    ylab = \"CCF\")\n\n\n\n\n\nCode\ncat(\"The sum of cross correlation function is\", sum(abs(ccf_result$acf)))\n\n\nThe sum of cross correlation function is 10.64938\n\n\n\n\n\n\nCode\npar(mfrow=c(1,1))\nccf_result <- ccf(normalized_sector_factor_data[, c(\"XLB.Adjusted\")], normalized_sector_factor_data[, c(\"unemployment\")], \n    lag.max = 300,\n    main = \"Cros-Correlation Plot for Materials Sector Fund and Unemployment Rate\",\n    ylab = \"CCF\")\n\n\n\n\n\nCode\ncat(\"The sum of cross correlation function is\", sum(abs(ccf_result$acf)))\n\n\nThe sum of cross correlation function is 6.967941\n\n\n\n\n\nThe cross-correlation feature plots reaffirm the findings from the heatmap analysis, indicating that inflation rate exhibit stronger correlations with the index compared to GDP, interest rate and unemployment rate. The cross-correlation plots for GDP, unemployment and interest rates show weaker and more scattered patterns, with strikes between the blue lines indicating lower correlation coefficients. This suggests that inflation rate are more suitable feature variables for the ARIMAX model when predicting Materials Sector Fund movements.\nFinal Exogenous variables: Macroeconomic indicators: Inflation rate\n\n\nEndogenous and Exogenous Variables Plot\n\n\nCode\nfinal_XLB_factor_data <- sector_factor_data %>%dplyr::select( Date,XLB.Adjusted, inflation)\nnumeric_vars_XLB_factor_data <- c(\"XLB.Adjusted\", \"inflation\")\nnumeric_XLB_factor_data <- final_XLB_factor_data[, numeric_vars_XLB_factor_data]\nnormalized_XLB_factor_data_numeric <- scale(numeric_XLB_factor_data)\nnormalized_XLB_factor_data_numeric_df <- data.frame(normalized_XLB_factor_data_numeric)\nnormalized_XLB_factor_data_ts <- ts(normalized_XLB_factor_data_numeric, start = c(2018, 7), frequency = 4)\n\nautoplot(normalized_XLB_factor_data_ts, facets=TRUE) +\n  xlab(\"Year\") + ylab(\"\") +\n  ggtitle(\"Materials Sector Fund Stock Price, Inflation Rate and Unemployment Rate in USA 2018-2023\")\n\n\n\n\n\n\n\nCheck the stationarity\nBefore proceeding with further analysis, it’s important to check the stationarity of the time series data. If the series is not stationary, it needs to be differentiated before performing a ARIMAX model. The stationarity of a multivariate time series data can be checked using the Phillips-Perron test, which is specifically designed for multivariate data. By performing the Phillips-Perron test on the multivariate time series data, we can determine if the data is stationary or if it requires differencing to make it suitable for ARIMAX modeling.\n\n\nCode\n# Convert your multivariate time series data to a matrix\nfinal_XLB_factor_data_ts_multivariate <- as.matrix(normalized_XLB_factor_data_ts)\n\n# Check for stationarity using Phillips-Perron test\nphillips_perron_test <- ur.pp(final_XLB_factor_data_ts_multivariate)  \nsummary(phillips_perron_test)\n\n\n\n################################## \n# Phillips-Perron Unit Root Test # \n################################## \n\nTest regression with intercept \n\n\nCall:\nlm(formula = y ~ y.l1)\n\nResiduals:\n     Min       1Q   Median       3Q      Max \n-1.25487 -0.20793 -0.06239  0.27586  0.77975 \n\nCoefficients:\n            Estimate Std. Error t value Pr(>|t|)    \n(Intercept)  0.06667    0.08207   0.812    0.422    \ny.l1         0.90923    0.08710  10.439 5.46e-12 ***\n---\nSignif. codes:  0 '***' 0.001 '**' 0.01 '*' 0.05 '.' 0.1 ' ' 1\n\nResidual standard error: 0.4849 on 33 degrees of freedom\nMultiple R-squared:  0.7676,    Adjusted R-squared:  0.7605 \nF-statistic:   109 on 1 and 33 DF,  p-value: 5.459e-12\n\n\nValue of test-statistic, type: Z-alpha  is: -5.164 \n\n         aux. Z statistics\nZ-tau-mu            0.6788\n\n\nThe Phillips-Perron unit root test output suggests that the data is stationary. The test regression includes an intercept term and tests whether the coefficient on the lagged value is significantly different from one. The estimated coefficients indicate that the intercept is not significant (p-value > 0.05), while the coefficient on the lagged value is highly significant (p-value < 5.459e-12), suggesting that the data is stationary.\nAdditionally, the “Z-alpha” value is the test statistic, which is compared to critical values of the standard normal distribution to determine significance. In this case, the test statistic is -5.164, indicating strong evidence against the null hypothesis of a unit root. The “Z-tau-mu” value is an auxiliary test statistic used to correct for autocorrelation in the residuals, and it is close to zero, suggesting that there is no evidence of residual autocorrelation.\n\n\nFitting a ARIMAX model\nFitting an ARIMAX model will be a useful approach to modeling and predicting time series data when external factors are believed to have an impact on the response variable. In this case, we will be using an ARIMAX model to predict the Materials Sector Fund Stock Price, taking into account the effects of inflation on the sector. By including these exogenous variables in our model, we can improve the accuracy of our predictions and gain a better understanding of the underlying dynamics of the time series.\n\nAuto ARIMAResiduals\n\n\n\n\nCode\nxreg <- cbind(Inflation = normalized_XLB_factor_data_ts[, \"inflation\"])\n\nfit <- auto.arima(normalized_XLB_factor_data_ts[, \"XLB.Adjusted\"], xreg = xreg)\nsummary(fit)\n\n\nSeries: normalized_XLB_factor_data_ts[, \"XLB.Adjusted\"] \nRegression with ARIMA(1,0,0) errors \n\nCoefficients:\n         ar1    xreg\n      0.6997  0.7067\ns.e.  0.1671  0.2202\n\nsigma^2 = 0.2083:  log likelihood = -10.7\nAIC=27.4   AICc=29.11   BIC=30.07\n\nTraining set error measures:\n                      ME      RMSE       MAE       MPE     MAPE      MASE\nTraining set -0.02304632 0.4303271 0.3380746 -5.294387 55.01567 0.3985993\n                  ACF1\nTraining set 0.0983079\n\n\n\n\n\n\nCode\ncheckresiduals(fit)\n\n\n\n\n\n\n    Ljung-Box test\n\ndata:  Residuals from Regression with ARIMA(1,0,0) errors\nQ* = 1.3297, df = 3, p-value = 0.7221\n\nModel df: 1.   Total lags used: 4\n\n\n\n\n\nThe auto.arima function reveals that the best-fit model is (1,0,0). Analysis of the residuals plot indicates that the fluctuates a lot and lies between -0.6 to 0.6. The ACF plot of the residuals shows significant lags, indicating that the model is performing well. The qq-plot also suggests normality in the residuals. The p-value greater than 0.05 actually suggests that there is no evidence of significant autocorrelation in the residuals, which means that the model is performing well in terms of capturing the temporal patterns in the data.\n\n\nFitting the model manually\n\nLinear ModelACF of ResidualsACF of ResidualsDifferentiated Residual\n\n\n\n\nCode\nnormalized_XLB_factor_data_numeric_df$XLB.Adjusted<-ts(normalized_XLB_factor_data_numeric_df$XLB.Adjusted,star=decimal_date(as.Date(\"2018-07-01\",format = \"%Y-%m-%d\")),frequency = 4)\nnormalized_XLB_factor_data_numeric_df$inflation<-ts(normalized_XLB_factor_data_numeric_df$inflation,star=decimal_date(as.Date(\"2018-07-01\",format = \"%Y-%m-%d\")),frequency = 4)\n\n\n############# First fit the linear model##########\nfit.reg <- lm(XLB.Adjusted ~ inflation, data=normalized_XLB_factor_data_numeric_df)\nsummary(fit.reg)\n\n\n\nCall:\nlm(formula = XLB.Adjusted ~ inflation, data = normalized_XLB_factor_data_numeric_df)\n\nResiduals:\n    Min      1Q  Median      3Q     Max \n-0.9269 -0.3982 -0.1172  0.4342  1.1631 \n\nCoefficients:\n              Estimate Std. Error t value Pr(>|t|)    \n(Intercept) -6.601e-16  1.481e-01   0.000        1    \ninflation    7.927e-01  1.524e-01   5.202 8.74e-05 ***\n---\nSignif. codes:  0 '***' 0.001 '**' 0.01 '*' 0.05 '.' 0.1 ' ' 1\n\nResidual standard error: 0.6284 on 16 degrees of freedom\nMultiple R-squared:  0.6284,    Adjusted R-squared:  0.6052 \nF-statistic: 27.06 on 1 and 16 DF,  p-value: 8.736e-05\n\n\n\n\n\n\nCode\nres.fit<-ts(residuals(fit.reg),star=decimal_date(as.Date(\"2018-07-01\",format = \"%Y-%m-%d\")),frequency = 4)\n############## Then look at the residuals ############\nacf(res.fit)\n\n\n\n\n\n\n\n\n\nCode\nPacf(res.fit)\n\n\n\n\n\n\n\n\n\nCode\nres.fit %>% diff() %>% ggtsdisplay()\n\n\n\n\n\n\n\n\nThe output displays the results of a linear regression model using two predictor variables, inflation and unemployment, to explain the variation in the response variable, XLB.Adjusted The coefficients of inflation have p-values of less than 0.05, indicating that both variables significantly impact XLB.Adjusted The R-squared value of approximately 63% suggests that the model explains a moderate amount of the variation in the response variable. Examination of the ACF plot of residuals reveals some autocorrelation, and therefore, the series is differentiated to achieve stationarity. By analyzing the differentiated ACF and PACF plots, the parameters if the ARIMAX modle are that p=0, q=0, and d=1.\nAs both the model fitted manually and the auto.arima parameters are the same, i.e. p = 0, q = 0 and d = 1. We proceed with fitting the model for the parameters.\n\n\nFitting Model.\n\nModel Plot 1Model 1Model Plot 2Model 2\n\n\n\n\nCode\nset.seed(1234)\n\nmodel_output <- capture.output(sarima(res.fit, 0,1,0)) \n\n\n\n\n\n\n\n\n\nCode\ncat(model_output[9:38], model_output[length(model_output)], sep = \"\\n\")\n\n\n$fit\n\nCall:\narima(x = xdata, order = c(p, d, q), seasonal = list(order = c(P, D, Q), period = S), \n    xreg = constant, transform.pars = trans, fixed = fixed, optim.control = list(trace = trc, \n        REPORT = 1, reltol = tol))\n\nCoefficients:\n      constant\n       -0.0299\ns.e.    0.1126\n\nsigma^2 estimated as 0.2156:  log likelihood = -11.08,  aic = 26.16\n\n$degrees_of_freedom\n[1] 16\n\n$ttable\n         Estimate     SE t.value p.value\nconstant  -0.0299 0.1126 -0.2654  0.7941\n\n$AIC\n[1] 1.538614\n\n$AICc\n[1] 1.5543\n\n$BIC\n[1] 1.636639\n\n\n\n\n\n\nCode\nset.seed(1234)\n\nmodel_output <- capture.output(sarima(res.fit, 1,0,0)) \n\n\n\n\n\n\n\n\n\nCode\ncat(model_output[22:51], model_output[length(model_output)], sep = \"\\n\")\n\n\n$fit\n\nCall:\narima(x = xdata, order = c(p, d, q), seasonal = list(order = c(P, D, Q), period = S), \n    xreg = xmean, include.mean = FALSE, transform.pars = trans, fixed = fixed, \n    optim.control = list(trace = trc, REPORT = 1, reltol = tol))\n\nCoefficients:\n         ar1    xmean\n      0.7071  -0.1423\ns.e.  0.1686   0.3201\n\nsigma^2 estimated as 0.1844:  log likelihood = -10.67,  aic = 27.34\n\n$degrees_of_freedom\n[1] 16\n\n$ttable\n      Estimate     SE t.value p.value\nar1     0.7071 0.1686  4.1951  0.0007\nxmean  -0.1423 0.3201 -0.4445  0.6626\n\n$AIC\n[1] 1.518822\n\n$AICc\n[1] 1.563266\n\n$BIC\n[1] 1.667217\n\n\n\n\n\nWe fitted both models using both the manual and auto.arima methods and observed that all the models have p-values greater than 0, indicating statistical significance. To determine the best fit model, we performed cross-validation.\n\n\nCross Validation\n\nRMSE PlotRMSE Results\n\n\n\n\nCode\nn=length(res.fit)\nk= 10\n \n \nrmse1 <- matrix(NA, (n-k),4)\nrmse2 <- matrix(NA, (n-k),4)\nrmse3 <- matrix(NA, (n-k),4)\n\nst <- tsp(res.fit)[1]+(k-5)/4 \n\nfor(i in 1:(n-k))\n{\n  xtrain <- window(res.fit, end=st + i/4)\n  xtest <- window(res.fit, start=st + (i+1)/4, end=st + (i+4)/4)\n\n  \n  fit <- Arima(xtrain, order=c(0,1,0),\n                include.drift=TRUE, method=\"ML\")\n  fcast <- forecast(fit, h=4)\n  \n  fit2 <- Arima(xtrain, order=c(1,0,0),\n                include.drift=TRUE, method=\"ML\")\n  fcast2 <- forecast(fit2, h=4)\n\n\n  rmse1[i,1:length(xtest)]   <- sqrt((fcast$mean-xtest)^2)\n  rmse2[i,1:length(xtest)] <- sqrt((fcast2$mean-xtest)^2)\n}\n\nplot(1:4,colMeans(rmse1,na.rm=TRUE), type=\"l\",col=2, xlab=\"horizon\", ylab=\"RMSE\")\nlines(1:4, colMeans(rmse2,na.rm=TRUE), type=\"l\",col=3)\nlegend(\"topleft\",legend=c(\"fit1\",\"fit2\"),col=2:4,lty=1)\n\n\n\n\n\n\n\n\n\nCode\ncat(\"RMSE values for Model 1\\n\", colMeans(rmse1))\n\n\nRMSE values for Model 1\n 0.4816911 0.6352956 0.9074147 1.197127\n\n\nCode\ncat(\"RMSE values for Model 2\\n\", colMeans(rmse2))\n\n\nRMSE values for Model 2\n 0.5090903 0.6158319 0.8876596 1.092305\n\n\n\n\n\nThe RMSE cross-validation plot shows that the RMSE of Model 2 (1,2,0) is lower than that of Model 1 (0,1,0). This suggests that Model 2 (1,2,0) is a better fit for the data when compared other model. ##### Forecast\n::: panel-tabset ### Forecast for Materials Sector Fund with feature variable\n\n\nCode\n#fiting an ARIMA model to the Inflation variable\ninflation_fit<-auto.arima(normalized_XLB_factor_data_numeric_df$inflation) \nfinflation<-forecast(inflation_fit)\n\n\n# best model fit for forcasting\nxreg <- cbind(Inflation = normalized_XLB_factor_data_ts[, \"inflation\"])\n\nfit <- Arima(normalized_XLB_factor_data_ts[, \"XLB.Adjusted\"],order=c(1,0,0),xreg=xreg)\n\n# Forcast the stock price using feature variables\nfxreg <- cbind(Inflation = finflation$mean)\nfcast <- forecast(fit, xreg=fxreg) \nautoplot(fcast) + xlab(\"Year\") +\n  ylab(\"Normalised XLB.Adjusted\")\n\n\n\n\n\n\n\nARIMA Model for Inflation\n\n\nCode\n#fiting an ARIMA model to the Inflation variable\ninflation_fit<-auto.arima(normalized_XLB_factor_data_numeric_df$inflation) \nfinflation<-forecast(inflation_fit)\nsummary(inflation_fit)\n\n\nSeries: normalized_XLB_factor_data_numeric_df$inflation \nARIMA(0,1,0) \n\nsigma^2 = 0.1378:  log likelihood = -7.28\nAIC=16.55   AICc=16.82   BIC=17.38\n\nTraining set error measures:\n                    ME      RMSE       MAE       MPE     MAPE      MASE\nTraining set 0.1201871 0.3607553 0.2312026 -26.91442 61.15158 0.2676132\n                  ACF1\nTraining set 0.2431721\n\n\n\n\nARIMA Model for Materials Sector Fund with feature variable\n\n\nCode\n# best model fit for forcasting\nxreg <- cbind(Inflation = normalized_XLB_factor_data_ts[, \"inflation\"])\n\nfit <- Arima(normalized_XLB_factor_data_ts[, \"XLB.Adjusted\"],order=c(1,0,0),xreg=xreg)\nsummary(fit)\n\n\nSeries: normalized_XLB_factor_data_ts[, \"XLB.Adjusted\"] \nRegression with ARIMA(1,0,0) errors \n\nCoefficients:\n         ar1  intercept    xreg\n      0.7045    -0.1307  0.7154\ns.e.  0.1649     0.3156  0.2210\n\nsigma^2 = 0.2198:  log likelihood = -10.61\nAIC=29.22   AICc=32.29   BIC=32.78\n\nTraining set error measures:\n                     ME      RMSE      MAE       MPE     MAPE      MASE\nTraining set 0.01785613 0.4279991 0.342922 -7.378369 56.62805 0.4043146\n                   ACF1\nTraining set 0.09753204\n\n\nThe ARIMAX model may have incorporated exogenous variables such as macroeconomic factors, which could indicate the potential impact of external factors on the forecasted values. The findings suggest that inflation rates significantly impact the Materials Sector Fund stock price, underscoring the importance of including macroeconomic factors in stock market forecasting. This is further supported by the comparison of RMSE, MAE, AIC and BIC values for forecasts with and without the feature variable. It is found that including the feature variable leads to a lower values, indicating that the feature variable makes a significant impact and should be considered in stock market forecasting."
  },
  {
    "objectID": "arimax_sector_real_estate_macroeconomic.html",
    "href": "arimax_sector_real_estate_macroeconomic.html",
    "title": "ARIMAX/SARIMAX Page for Real Estate Sector Fund and macroeconomic factors",
    "section": "",
    "text": "Endogenous variable: Real Estate Sector Fund Exogenous variables: Macroeconomic indicators such as GDP, inflation rate, unemployment rate and interest rates\nBefore proceeding with model selection, it can be useful to examine the feature variables and select the appropriate ones. This can be done by creating cross-correlation function plots, which can serve as a verification of the correlation heatmap for the variables.\n\nCCF Plot for Real Estate Sector Fund and Exogenous Variables\n\nGDPInterest RateInflation RateUnemployment Rate\n\n\n\n\nCode\npar(mfrow=c(1,1))\nccf_result <- ccf(normalized_sector_factor_data[, c(\"XLRE.Adjusted\")], normalized_sector_factor_data[, c(\"gdp\")], \n    lag.max = 300,\n    main = \"Cros-Correlation Plot for Real Estate Sector Fund and GDP Growth Rate \",\n    ylab = \"CCF\")\n\n\n\n\n\nCode\ncat(\"The sum of cross correlation function is\", sum(abs(ccf_result$acf)))\n\n\nThe sum of cross correlation function is 5.423388\n\n\n\n\n\n\nCode\npar(mfrow=c(1,1))\nccf_result <- ccf(normalized_sector_factor_data[, c(\"XLRE.Adjusted\")], normalized_sector_factor_data[, c(\"interest\")], \n    lag.max = 300,\n    main = \"Cros-Correlation Plot for Real Estate Sector Fund and Interest Rate\",\n    ylab = \"CCF\")\n\n\n\n\n\nCode\ncat(\"The sum of cross correlation function is\", sum(abs(ccf_result$acf)))\n\n\nThe sum of cross correlation function is 8.679655\n\n\n\n\n\n\nCode\npar(mfrow=c(1,1))\nccf_result <- ccf(normalized_sector_factor_data[, c(\"XLRE.Adjusted\")], normalized_sector_factor_data[, c(\"inflation\")], \n    lag.max = 300,\n    main = \"Cros-Correlation Plot for Real Estate Sector Fund and Inflation Rate\",\n    ylab = \"CCF\")\n\n\n\n\n\nCode\ncat(\"The sum of cross correlation function is\", sum(abs(ccf_result$acf)))\n\n\nThe sum of cross correlation function is 9.490159\n\n\n\n\n\n\nCode\npar(mfrow=c(1,1))\nccf_result <- ccf(normalized_sector_factor_data[, c(\"XLRE.Adjusted\")], normalized_sector_factor_data[, c(\"unemployment\")], \n    lag.max = 300,\n    main = \"Cros-Correlation Plot for Real Estate Sector Fund and Unemployment Rate\",\n    ylab = \"CCF\")\n\n\n\n\n\nCode\ncat(\"The sum of cross correlation function is\", sum(abs(ccf_result$acf)))\n\n\nThe sum of cross correlation function is 6.085289\n\n\n\n\n\nThe cross-correlation feature plots reaffirm the findings from the heatmap analysis, indicating that inflation rate exhibit stronger correlations with the index compared to GDP, interest rate and unemployment rate. The cross-correlation plots for GDP, unemployment and interest rates show weaker and more scattered patterns, with strikes between the blue lines indicating lower correlation coefficients. This suggests that inflation rate are more suitable feature variables for the ARIMAX model when predicting Real Estate Sector Fund movements.\nFinal Exogenous variables: Macroeconomic indicators: Inflation rate\n\n\nEndogenous and Exogenous Variables Plot\n\n\nCode\nfinal_XLRE_factor_data <- sector_factor_data %>%dplyr::select( Date,XLRE.Adjusted, inflation)\nnumeric_vars_XLRE_factor_data <- c(\"XLRE.Adjusted\", \"inflation\")\nnumeric_XLRE_factor_data <- final_XLRE_factor_data[, numeric_vars_XLRE_factor_data]\nnormalized_XLRE_factor_data_numeric <- scale(numeric_XLRE_factor_data)\nnormalized_XLRE_factor_data_numeric_df <- data.frame(normalized_XLRE_factor_data_numeric)\nnormalized_XLRE_factor_data_ts <- ts(normalized_XLRE_factor_data_numeric, start = c(2018, 7), frequency = 4)\n\nautoplot(normalized_XLRE_factor_data_ts, facets=TRUE) +\n  xlab(\"Year\") + ylab(\"\") +\n  ggtitle(\"Real Estate Sector Fund Stock Price, Inflation Rate and Unemployment Rate in USA 2018-2023\")\n\n\n\n\n\n\n\nCheck the stationarity\nBefore proceeding with further analysis, it’s important to check the stationarity of the time series data. If the series is not stationary, it needs to be differentiated before performing a ARIMAX model. The stationarity of a multivariate time series data can be checked using the Phillips-Perron test, which is specifically designed for multivariate data. By performing the Phillips-Perron test on the multivariate time series data, we can determine if the data is stationary or if it requires differencing to make it suitable for ARIMAX modeling.\n\n\nCode\n# Convert your multivariate time series data to a matrix\nfinal_XLRE_factor_data_ts_multivariate <- as.matrix(normalized_XLRE_factor_data_ts)\n\n# Check for stationarity using Phillips-Perron test\nphillips_perron_test <- ur.pp(final_XLRE_factor_data_ts_multivariate)  \nsummary(phillips_perron_test)\n\n\n\n################################## \n# Phillips-Perron Unit Root Test # \n################################## \n\nTest regression with intercept \n\n\nCall:\nlm(formula = y ~ y.l1)\n\nResiduals:\n     Min       1Q   Median       3Q      Max \n-1.26257 -0.18719 -0.08398  0.26427  1.00292 \n\nCoefficients:\n            Estimate Std. Error t value Pr(>|t|)    \n(Intercept)  0.07513    0.08539   0.880    0.385    \ny.l1         0.88596    0.09063   9.776 2.85e-11 ***\n---\nSignif. codes:  0 '***' 0.001 '**' 0.01 '*' 0.05 '.' 0.1 ' ' 1\n\nResidual standard error: 0.5045 on 33 degrees of freedom\nMultiple R-squared:  0.7433,    Adjusted R-squared:  0.7355 \nF-statistic: 95.57 on 1 and 33 DF,  p-value: 2.847e-11\n\n\nValue of test-statistic, type: Z-alpha  is: -6.2131 \n\n         aux. Z statistics\nZ-tau-mu            0.7396\n\n\nThe Phillips-Perron unit root test output suggests that the data is stationary. The test regression includes an intercept term and tests whether the coefficient on the lagged value is significantly different from one. The estimated coefficients indicate that the intercept is not significant (p-value > 0.05), while the coefficient on the lagged value is highly significant (p-value < 2.847e-11), suggesting that the data is stationary.\nAdditionally, the “Z-alpha” value is the test statistic, which is compared to critical values of the standard normal distribution to determine significance. In this case, the test statistic is -6.2131, indicating strong evidence against the null hypothesis of a unit root. The “Z-tau-mu” value is an auxiliary test statistic used to correct for autocorrelation in the residuals, and it is close to zero, suggesting that there is no evidence of residual autocorrelation.\n\n\nFitting a ARIMAX model\nFitting an ARIMAX model will be a useful approach to modeling and predicting time series data when external factors are believed to have an impact on the response variable. In this case, we will be using an ARIMAX model to predict the Real Estate Sector Fund Stock Price, taking into account the effects of inflation on the sector. By including these exogenous variables in our model, we can improve the accuracy of our predictions and gain a better understanding of the underlying dynamics of the time series.\n\nAuto ARIMAResiduals\n\n\n\n\nCode\nxreg <- cbind(Inflation = normalized_XLRE_factor_data_ts[, \"inflation\"])\n\nfit <- auto.arima(normalized_XLRE_factor_data_ts[, \"XLRE.Adjusted\"], xreg = xreg)\nsummary(fit)\n\n\nSeries: normalized_XLRE_factor_data_ts[, \"XLRE.Adjusted\"] \nRegression with ARIMA(1,0,0) errors \n\nCoefficients:\n         ar1    xreg\n      0.6998  0.8171\ns.e.  0.2199  0.2459\n\nsigma^2 = 0.2316:  log likelihood = -11.65\nAIC=29.3   AICc=31.02   BIC=31.97\n\nTraining set error measures:\n                      ME     RMSE       MAE       MPE     MAPE      MASE\nTraining set -0.03870243 0.453705 0.3575106 -90.06175 114.9634 0.3657111\n                  ACF1\nTraining set 0.1202663\n\n\n\n\n\n\nCode\ncheckresiduals(fit)\n\n\n\n\n\n\n    Ljung-Box test\n\ndata:  Residuals from Regression with ARIMA(1,0,0) errors\nQ* = 1.4709, df = 3, p-value = 0.689\n\nModel df: 1.   Total lags used: 4\n\n\n\n\n\nThe auto.arima function reveals that the best-fit model is (1,0,0). Analysis of the residuals plot indicates that the fluctuates a lot and lies between -1 to 0.5. The ACF plot of the residuals shows significant lags, indicating that the model is performing well. The qq-plot also suggests normality in the residuals. The p-value greater than 0.05 actually suggests that there is no evidence of significant autocorrelation in the residuals, which means that the model is performing well in terms of capturing the temporal patterns in the data.\n\n\nFitting the model manually\n\nLinear ModelACF of ResidualsACF of ResidualsDifferentiated Residual\n\n\n\n\nCode\nnormalized_XLRE_factor_data_numeric_df$XLRE.Adjusted<-ts(normalized_XLRE_factor_data_numeric_df$XLRE.Adjusted,star=decimal_date(as.Date(\"2018-07-01\",format = \"%Y-%m-%d\")),frequency = 4)\nnormalized_XLRE_factor_data_numeric_df$inflation<-ts(normalized_XLRE_factor_data_numeric_df$inflation,star=decimal_date(as.Date(\"2018-07-01\",format = \"%Y-%m-%d\")),frequency = 4)\n\n\n############# First fit the linear model##########\nfit.reg <- lm(XLRE.Adjusted ~ inflation, data=normalized_XLRE_factor_data_numeric_df)\nsummary(fit.reg)\n\n\n\nCall:\nlm(formula = XLRE.Adjusted ~ inflation, data = normalized_XLRE_factor_data_numeric_df)\n\nResiduals:\n    Min      1Q  Median      3Q     Max \n-1.2846 -0.4851  0.2955  0.4518  0.7921 \n\nCoefficients:\n             Estimate Std. Error t value Pr(>|t|)    \n(Intercept) 3.085e-16  1.429e-01   0.000        1    \ninflation   8.086e-01  1.471e-01   5.498 4.86e-05 ***\n---\nSignif. codes:  0 '***' 0.001 '**' 0.01 '*' 0.05 '.' 0.1 ' ' 1\n\nResidual standard error: 0.6064 on 16 degrees of freedom\nMultiple R-squared:  0.6539,    Adjusted R-squared:  0.6323 \nF-statistic: 30.23 on 1 and 16 DF,  p-value: 4.864e-05\n\n\n\n\n\n\nCode\nres.fit<-ts(residuals(fit.reg),star=decimal_date(as.Date(\"2018-07-01\",format = \"%Y-%m-%d\")),frequency = 4)\n############## Then look at the residuals ############\nacf(res.fit)\n\n\n\n\n\n\n\n\n\nCode\nPacf(res.fit)\n\n\n\n\n\n\n\n\n\nCode\nres.fit %>% diff() %>% ggtsdisplay()\n\n\n\n\n\n\n\n\nThe output displays the results of a linear regression model using two predictor variables, inflation and unemployment, to explain the variation in the response variable, XLRE.Adjusted The coefficients of inflation have p-values of less than 0.05, indicating that both variables significantly impact XLRE.Adjusted The R-squared value of approximately 65% suggests that the model explains a moderate amount of the variation in the response variable. Examination of the ACF plot of residuals reveals some autocorrelation, and therefore, the series is differentiated to achieve stationarity. By analyzing the differentiated ACF and PACF plots, the parameters if the ARIMAX modle are that p=0, q=0, and d=1.\n\n\nFitting Model.\n\nModel Plot 1Model 1Model Plot 2Model 2\n\n\n\n\nCode\nset.seed(1234)\n\nmodel_output <- capture.output(sarima(res.fit, 0,1,0)) \n\n\n\n\n\n\n\n\n\nCode\ncat(model_output[9:38], model_output[length(model_output)], sep = \"\\n\")\n\n\n$fit\n\nCall:\narima(x = xdata, order = c(p, d, q), seasonal = list(order = c(P, D, Q), period = S), \n    xreg = constant, transform.pars = trans, fixed = fixed, optim.control = list(trace = trc, \n        REPORT = 1, reltol = tol))\n\nCoefficients:\n      constant\n       -0.0316\ns.e.    0.1155\n\nsigma^2 estimated as 0.2269:  log likelihood = -11.51,  aic = 27.03\n\n$degrees_of_freedom\n[1] 16\n\n$ttable\n         Estimate     SE t.value p.value\nconstant  -0.0316 0.1155 -0.2735   0.788\n\n$AIC\n[1] 1.589926\n\n$AICc\n[1] 1.605612\n\n$BIC\n[1] 1.687951\n\n\n\n\n\n\nCode\nset.seed(1234)\n\nmodel_output <- capture.output(sarima(res.fit, 1,0,0)) \n\n\n\n\n\n\n\n\n\nCode\ncat(model_output[25:54], model_output[length(model_output)], sep = \"\\n\")\n\n\n$fit\n\nCall:\narima(x = xdata, order = c(p, d, q), seasonal = list(order = c(P, D, Q), period = S), \n    xreg = xmean, include.mean = FALSE, transform.pars = trans, fixed = fixed, \n    optim.control = list(trace = trc, REPORT = 1, reltol = tol))\n\nCoefficients:\n         ar1    xmean\n      0.7152  -0.2216\ns.e.  0.1956   0.3674\n\nsigma^2 estimated as 0.2004:  log likelihood = -11.43,  aic = 28.86\n\n$degrees_of_freedom\n[1] 16\n\n$ttable\n      Estimate     SE t.value p.value\nar1     0.7152 0.1956  3.6568  0.0021\nxmean  -0.2216 0.3674 -0.6032  0.5548\n\n$AIC\n[1] 1.603332\n\n$AICc\n[1] 1.647777\n\n$BIC\n[1] 1.751728\n\n\n\n\n\nWe fitted both models using both the manual and auto.arima methods and observed that all the models have p-values greater than 0, indicating statistical significance. To determine the best fit model, we performed cross-validation.\n\n\nCross Validation\n\nRMSE PlotRMSE Results\n\n\n\n\nCode\nn=length(res.fit)\nk= 10\n \n \nrmse1 <- matrix(NA, (n-k),4)\nrmse2 <- matrix(NA, (n-k),4)\nrmse3 <- matrix(NA, (n-k),4)\n\nst <- tsp(res.fit)[1]+(k-5)/4 \n\nfor(i in 1:(n-k))\n{\n  xtrain <- window(res.fit, end=st + i/4)\n  xtest <- window(res.fit, start=st + (i+1)/4, end=st + (i+4)/4)\n\n  \n  fit <- Arima(xtrain, order=c(0,1,0),\n                include.drift=TRUE, method=\"ML\")\n  fcast <- forecast(fit, h=4)\n  \n  fit2 <- Arima(xtrain, order=c(1,0,0),\n                include.drift=TRUE, method=\"ML\")\n  fcast2 <- forecast(fit2, h=4)\n\n\n  rmse1[i,1:length(xtest)]   <- sqrt((fcast$mean-xtest)^2)\n  rmse2[i,1:length(xtest)] <- sqrt((fcast2$mean-xtest)^2)\n}\n\nplot(1:4,colMeans(rmse1,na.rm=TRUE), type=\"l\",col=2, xlab=\"horizon\", ylab=\"RMSE\")\nlines(1:4, colMeans(rmse2,na.rm=TRUE), type=\"l\",col=3)\nlegend(\"topleft\",legend=c(\"fit1\",\"fit2\"),col=2:4,lty=1)\n\n\n\n\n\n\n\n\n\nCode\ncat(\"RMSE values for Model 1\\n\", colMeans(rmse1))\n\n\nRMSE values for Model 1\n 0.3030661 0.2579258 0.4671162 0.7678888\n\n\nCode\ncat(\"RMSE values for Model 2\\n\", colMeans(rmse2))\n\n\nRMSE values for Model 2\n 0.2511232 0.2178128 0.4407134 0.7708769\n\n\n\n\n\nThe RMSE cross-validation plot shows that the RMSE of Model 2 (1,0,0) is lower than that of Model 1 (0,1,0). This suggests that Model 2 (1,0,0) is a better fit for the data when compared other model. ##### Forecast\n::: panel-tabset ### Forecast for Real Estate Sector Fund with feature variable\n\n\nCode\n#fiting an ARIMA model to the Inflation variable\ninflation_fit<-auto.arima(normalized_XLRE_factor_data_numeric_df$inflation) \nfinflation<-forecast(inflation_fit)\n\n\n# best model fit for forcasting\nxreg <- cbind(Inflation = normalized_XLRE_factor_data_ts[, \"inflation\"])\n\nfit <- Arima(normalized_XLRE_factor_data_ts[, \"XLRE.Adjusted\"],order=c(1,0,0),xreg=xreg)\n\n# Forcast the stock price using feature variables\nfxreg <- cbind(Inflation = finflation$mean)\nfcast <- forecast(fit, xreg=fxreg) \nautoplot(fcast) + xlab(\"Year\") +\n  ylab(\"Normalised XLRE.Adjusted\")\n\n\n\n\n\n\n\nARIMA Model for Inflation\n\n\nCode\n#fiting an ARIMA model to the Inflation variable\ninflation_fit<-auto.arima(normalized_XLRE_factor_data_numeric_df$inflation) \nfinflation<-forecast(inflation_fit)\nsummary(inflation_fit)\n\n\nSeries: normalized_XLRE_factor_data_numeric_df$inflation \nARIMA(0,1,0) \n\nsigma^2 = 0.1378:  log likelihood = -7.28\nAIC=16.55   AICc=16.82   BIC=17.38\n\nTraining set error measures:\n                    ME      RMSE       MAE       MPE     MAPE      MASE\nTraining set 0.1201871 0.3607553 0.2312026 -26.91442 61.15158 0.2676132\n                  ACF1\nTraining set 0.2431721\n\n\n\n\nARIMA Model for Real Estate Sector Fund with feature variable\n\n\nCode\n# best model fit for forcasting\nxreg <- cbind(Inflation = normalized_XLRE_factor_data_ts[, \"inflation\"])\n\nfit <- Arima(normalized_XLRE_factor_data_ts[, \"XLRE.Adjusted\"],order=c(1,0,0),xreg=xreg)\nsummary(fit)\n\n\nSeries: normalized_XLRE_factor_data_ts[, \"XLRE.Adjusted\"] \nRegression with ARIMA(1,0,0) errors \n\nCoefficients:\n         ar1  intercept    xreg\n      0.7267    -0.2368  0.8445\ns.e.  0.2147     0.4018  0.2617\n\nsigma^2 = 0.2397:  log likelihood = -11.42\nAIC=30.84   AICc=33.92   BIC=34.4\n\nTraining set error measures:\n                     ME      RMSE      MAE       MPE     MAPE      MASE\nTraining set 0.02855094 0.4469379 0.363796 -98.75629 121.8159 0.3721406\n                  ACF1\nTraining set 0.1157745\n\n\nThe ARIMAX model may have incorporated exogenous variables such as macroeconomic factors, which could indicate the potential impact of external factors on the forecasted values. The findings suggest that inflation rates significantly impact the Real Estate Sector Fund stock price, underscoring the importance of including macroeconomic factors in stock market forecasting. This is further supported by the comparison of RMSE, MAE, AIC and BIC values for forecasts with and without the feature variable. It is found that including the feature variable leads to a lower values but the MAE value is slightly higher with factor varaibles, indicating that the feature variable makes a significant impact and should be considered in stock market forecasting."
  },
  {
    "objectID": "arimax_sector_technology_macroeconomic.html",
    "href": "arimax_sector_technology_macroeconomic.html",
    "title": "ARIMAX/SARIMAX Page for Technology Sector Fund and macroeconomic factors",
    "section": "",
    "text": "Endogenous variable: Technology Sector Fund Exogenous variables: Macroeconomic indicators such as GDP, inflation rate, unemployment rate and interest rates\nBefore proceeding with model selection, it can be useful to examine the feature variables and select the appropriate ones. This can be done by creating cross-correlation function plots, which can serve as a verification of the correlation heatmap for the variables.\n\nCCF Plot for Technology Sector Fund and Exogenous Variables\n\nGDPInterest RateInflation RateUnemployment Rate\n\n\n\n\nCode\npar(mfrow=c(1,1))\nccf_result <- ccf(normalized_sector_factor_data[, c(\"XLK.Adjusted\")], normalized_sector_factor_data[, c(\"gdp\")], \n    lag.max = 300,\n    main = \"Cros-Correlation Plot for Technology Sector Fund and GDP Growth Rate \",\n    ylab = \"CCF\")\n\n\n\n\n\nCode\ncat(\"The sum of cross correlation function is\", sum(abs(ccf_result$acf)))\n\n\nThe sum of cross correlation function is 5.131569\n\n\n\n\n\n\nCode\npar(mfrow=c(1,1))\nccf_result <- ccf(normalized_sector_factor_data[, c(\"XLK.Adjusted\")], normalized_sector_factor_data[, c(\"interest\")], \n    lag.max = 300,\n    main = \"Cros-Correlation Plot for Technology Sector Fund and Interest Rate\",\n    ylab = \"CCF\")\n\n\n\n\n\nCode\ncat(\"The sum of cross correlation function is\", sum(abs(ccf_result$acf)))\n\n\nThe sum of cross correlation function is 9.940572\n\n\n\n\n\n\nCode\npar(mfrow=c(1,1))\nccf_result <- ccf(normalized_sector_factor_data[, c(\"XLK.Adjusted\")], normalized_sector_factor_data[, c(\"inflation\")], \n    lag.max = 300,\n    main = \"Cros-Correlation Plot for Technology Sector Fund and Inflation Rate\",\n    ylab = \"CCF\")\n\n\n\n\n\nCode\ncat(\"The sum of cross correlation function is\", sum(abs(ccf_result$acf)))\n\n\nThe sum of cross correlation function is 11.25703\n\n\n\n\n\n\nCode\npar(mfrow=c(1,1))\nccf_result <- ccf(normalized_sector_factor_data[, c(\"XLK.Adjusted\")], normalized_sector_factor_data[, c(\"unemployment\")], \n    lag.max = 300,\n    main = \"Cros-Correlation Plot for Technology Sector Fund and Unemployment Rate\",\n    ylab = \"CCF\")\n\n\n\n\n\nCode\ncat(\"The sum of cross correlation function is\", sum(abs(ccf_result$acf)))\n\n\nThe sum of cross correlation function is 6.990579\n\n\n\n\n\nThe cross-correlation feature plots reaffirm the findings from the heatmap analysis, indicating that inflation rate exhibit stronger correlations with the index compared to GDP, interest rate and unemployment rate. The cross-correlation plots for GDP, unemployment and interest rates show weaker and more scattered patterns, with strikes between the blue lines indicating lower correlation coefficients. This suggests that inflation rate are more suitable feature variables for the ARIMAX model when predicting Technology Sector Fund movements.\nFinal Exogenous variables: Macroeconomic indicators: Inflation rate\n\n\nEndogenous and Exogenous Variables Plot\n\n\nCode\nfinal_XLK_factor_data <- sector_factor_data %>%dplyr::select( Date,XLK.Adjusted, inflation)\nnumeric_vars_XLK_factor_data <- c(\"XLK.Adjusted\", \"inflation\")\nnumeric_XLK_factor_data <- final_XLK_factor_data[, numeric_vars_XLK_factor_data]\nnormalized_XLK_factor_data_numeric <- scale(numeric_XLK_factor_data)\nnormalized_XLK_factor_data_numeric_df <- data.frame(normalized_XLK_factor_data_numeric)\nnormalized_XLK_factor_data_ts <- ts(normalized_XLK_factor_data_numeric, start = c(2018, 7), frequency = 4)\n\nautoplot(normalized_XLK_factor_data_ts, facets=TRUE) +\n  xlab(\"Year\") + ylab(\"\") +\n  ggtitle(\"Technology Sector Fund Stock Price, Inflation Rate and Unemployment Rate in USA 2018-2023\")\n\n\n\n\n\n\n\nCheck the stationarity\nBefore proceeding with further analysis, it’s important to check the stationarity of the time series data. If the series is not stationary, it needs to be differentiated before performing a ARIMAX model. The stationarity of a multivariate time series data can be checked using the Phillips-Perron test, which is specifically designed for multivariate data. By performing the Phillips-Perron test on the multivariate time series data, we can determine if the data is stationary or if it requires differencing to make it suitable for ARIMAX modeling.\n\n\nCode\n# Convert your multivariate time series data to a matrix\nfinal_XLK_factor_data_ts_multivariate <- as.matrix(normalized_XLK_factor_data_ts)\n\n# Check for stationarity using Phillips-Perron test\nphillips_perron_test <- ur.pp(final_XLK_factor_data_ts_multivariate)  \nsummary(phillips_perron_test)\n\n\n\n################################## \n# Phillips-Perron Unit Root Test # \n################################## \n\nTest regression with intercept \n\n\nCall:\nlm(formula = y ~ y.l1)\n\nResiduals:\n     Min       1Q   Median       3Q      Max \n-0.90362 -0.17250 -0.03719  0.22961  0.75107 \n\nCoefficients:\n            Estimate Std. Error t value Pr(>|t|)    \n(Intercept)  0.07863    0.07069   1.112    0.274    \ny.l1         0.93102    0.07502  12.410  5.6e-14 ***\n---\nSignif. codes:  0 '***' 0.001 '**' 0.01 '*' 0.05 '.' 0.1 ' ' 1\n\nResidual standard error: 0.4176 on 33 degrees of freedom\nMultiple R-squared:  0.8235,    Adjusted R-squared:  0.8182 \nF-statistic:   154 on 1 and 33 DF,  p-value: 5.604e-14\n\n\nValue of test-statistic, type: Z-alpha  is: -4.5045 \n\n         aux. Z statistics\nZ-tau-mu            0.8813\n\n\nThe Phillips-Perron unit root test output suggests that the data is stationary. The test regression includes an intercept term and tests whether the coefficient on the lagged value is significantly different from one. The estimated coefficients indicate that the intercept is not significant (p-value > 0.05), while the coefficient on the lagged value is highly significant (p-value < 5.604e-14), suggesting that the data is stationary.\nAdditionally, the “Z-alpha” value is the test statistic, which is compared to critical values of the standard normal distribution to determine significance. In this case, the test statistic is -4.5045, indicating strong evidence against the null hypothesis of a unit root. The “Z-tau-mu” value is an auxiliary test statistic used to correct for autocorrelation in the residuals, and it is close to zero, suggesting that there is no evidence of residual autocorrelation.\n\n\nFitting a ARIMAX model\nFitting an ARIMAX model will be a useful approach to modeling and predicting time series data when external factors are believed to have an impact on the response variable. In this case, we will be using an ARIMAX model to predict the Technology Sector Fund Stock Price, taking into account the effects of inflation on the sector. By including these exogenous variables in our model, we can improve the accuracy of our predictions and gain a better understanding of the underlying dynamics of the time series.\n\nAuto ARIMAResiduals\n\n\n\n\nCode\nxreg <- cbind(Inflation = normalized_XLK_factor_data_ts[, \"inflation\"])\n\nfit <- auto.arima(normalized_XLK_factor_data_ts[, \"XLK.Adjusted\"], xreg = xreg)\nsummary(fit)\n\n\nSeries: normalized_XLK_factor_data_ts[, \"XLK.Adjusted\"] \nRegression with ARIMA(0,2,1) errors \n\nCoefficients:\n          ma1    xreg\n      -0.6612  0.6823\ns.e.   0.1973  0.2670\n\nsigma^2 = 0.1721:  log likelihood = -7.85\nAIC=21.69   AICc=23.69   BIC=24.01\n\nTraining set error measures:\n                     ME      RMSE       MAE       MPE     MAPE      MASE\nTraining set -0.0773098 0.3658797 0.2700827 -35.34724 52.98199 0.3336819\n                   ACF1\nTraining set -0.1131779\n\n\n\n\n\n\nCode\ncheckresiduals(fit)\n\n\n\n\n\n\n    Ljung-Box test\n\ndata:  Residuals from Regression with ARIMA(0,2,1) errors\nQ* = 0.93391, df = 3, p-value = 0.8172\n\nModel df: 1.   Total lags used: 4\n\n\n\n\n\nThe auto.arima function reveals that the best-fit model is (0,2,1). Analysis of the residuals plot indicates that the fluctuates a lot and lies between -0.6 to 0.6. The ACF plot of the residuals shows significant lags, indicating that the model is performing well. The qq-plot also suggests normality in the residuals. The p-value greater than 0.05 actually suggests that there is no evidence of significant autocorrelation in the residuals, which means that the model is performing well in terms of capturing the temporal patterns in the data.\n\n\nFitting the model manually\n\nLinear ModelACF of ResidualsACF of ResidualsDifferentiated Residual\n\n\n\n\nCode\nnormalized_XLK_factor_data_numeric_df$XLK.Adjusted<-ts(normalized_XLK_factor_data_numeric_df$XLK.Adjusted,star=decimal_date(as.Date(\"2018-07-01\",format = \"%Y-%m-%d\")),frequency = 4)\nnormalized_XLK_factor_data_numeric_df$inflation<-ts(normalized_XLK_factor_data_numeric_df$inflation,star=decimal_date(as.Date(\"2018-07-01\",format = \"%Y-%m-%d\")),frequency = 4)\n\n\n############# First fit the linear model##########\nfit.reg <- lm(XLK.Adjusted ~ inflation, data=normalized_XLK_factor_data_numeric_df)\nsummary(fit.reg)\n\n\n\nCall:\nlm(formula = XLK.Adjusted ~ inflation, data = normalized_XLK_factor_data_numeric_df)\n\nResiduals:\n    Min      1Q  Median      3Q     Max \n-0.9576 -0.5535 -0.1653  0.6354  1.2228 \n\nCoefficients:\n             Estimate Std. Error t value Pr(>|t|)    \n(Intercept) 4.627e-16  1.694e-01   0.000 1.000000    \ninflation   7.168e-01  1.743e-01   4.112 0.000816 ***\n---\nSignif. codes:  0 '***' 0.001 '**' 0.01 '*' 0.05 '.' 0.1 ' ' 1\n\nResidual standard error: 0.7187 on 16 degrees of freedom\nMultiple R-squared:  0.5138,    Adjusted R-squared:  0.4834 \nF-statistic: 16.91 on 1 and 16 DF,  p-value: 0.0008158\n\n\n\n\n\n\nCode\nres.fit<-ts(residuals(fit.reg),star=decimal_date(as.Date(\"2018-07-01\",format = \"%Y-%m-%d\")),frequency = 4)\n############## Then look at the residuals ############\nacf(res.fit)\n\n\n\n\n\n\n\n\n\nCode\nPacf(res.fit)\n\n\n\n\n\n\n\n\n\nCode\nres.fit %>% diff() %>% ggtsdisplay()\n\n\n\n\n\n\n\n\nThe output displays the results of a linear regression model using two predictor variables, inflation and unemployment, to explain the variation in the response variable, XLK.Adjusted The coefficients of inflation have p-values of less than 0.05, indicating that both variables significantly impact XLK.Adjusted The R-squared value of approximately 51% suggests that the model explains a moderate amount of the variation in the response variable. Examination of the ACF plot of residuals reveals some autocorrelation, and therefore, the series is differentiated to achieve stationarity. By analyzing the differentiated ACF and PACF plots, the parameters if the ARIMAX modle are that p=0, q=0, and d=1.\n\n\nFitting Model.\n\nModel Plot 1Model 1Model Plot 2Model 2\n\n\n\n\nCode\nset.seed(1234)\n\nmodel_output <- capture.output(sarima(res.fit, 0,1,0)) \n\n\n\n\n\n\n\n\n\nCode\ncat(model_output[9:38], model_output[length(model_output)], sep = \"\\n\")\n\n\n$fit\n\nCall:\narima(x = xdata, order = c(p, d, q), seasonal = list(order = c(P, D, Q), period = S), \n    xreg = constant, transform.pars = trans, fixed = fixed, optim.control = list(trace = trc, \n        REPORT = 1, reltol = tol))\n\nCoefficients:\n      constant\n        0.0005\ns.e.    0.0921\n\nsigma^2 estimated as 0.1441:  log likelihood = -7.66,  aic = 19.31\n\n$degrees_of_freedom\n[1] 16\n\n$ttable\n         Estimate     SE t.value p.value\nconstant    5e-04 0.0921  0.0049  0.9962\n\n$AIC\n[1] 1.13613\n\n$AICc\n[1] 1.151816\n\n$BIC\n[1] 1.234155\n\n\n\n\n\n\nCode\nset.seed(1234)\n\nmodel_output <- capture.output(sarima(res.fit, 0,2,1)) \n\n\n\n\n\n\n\n\n\nCode\ncat(model_output[21:49], model_output[length(model_output)], sep = \"\\n\")\n\n\n$fit\n\nCall:\narima(x = xdata, order = c(p, d, q), seasonal = list(order = c(P, D, Q), period = S), \n    include.mean = !no.constant, transform.pars = trans, fixed = fixed, optim.control = list(trace = trc, \n        REPORT = 1, reltol = tol))\n\nCoefficients:\n          ma1\n      -0.6563\ns.e.   0.1932\n\nsigma^2 estimated as 0.1509:  log likelihood = -7.85,  aic = 19.71\n\n$degrees_of_freedom\n[1] 15\n\n$ttable\n    Estimate     SE t.value p.value\nma1  -0.6563 0.1932 -3.3973   0.004\n\n$AIC\n[1] 1.231717\n\n$AICc\n[1] 1.249574\n\n$BIC\n[1] 1.32829\n\n\n\n\n\nWe fitted both models using both the manual and auto.arima methods and observed that all the models have p-values greater than 0, indicating statistical significance. To determine the best fit model, we performed cross-validation.\n\n\nCross Validation\n\nRMSE PlotRMSE Results\n\n\n\n\nCode\nn=length(res.fit)\nk= 10\n \n \nrmse1 <- matrix(NA, (n-k),4)\nrmse2 <- matrix(NA, (n-k),4)\nrmse3 <- matrix(NA, (n-k),4)\n\nst <- tsp(res.fit)[1]+(k-5)/4 \n\nfor(i in 1:(n-k))\n{\n  xtrain <- window(res.fit, end=st + i/4)\n  xtest <- window(res.fit, start=st + (i+1)/4, end=st + (i+4)/4)\n\n  \n  fit <- Arima(xtrain, order=c(0,1,0),\n                include.drift=TRUE, method=\"ML\")\n  fcast <- forecast(fit, h=4)\n  \n  fit2 <- Arima(xtrain, order=c(0,2,1),\n                include.drift=TRUE, method=\"ML\")\n  fcast2 <- forecast(fit2, h=4)\n\n\n  rmse1[i,1:length(xtest)]   <- sqrt((fcast$mean-xtest)^2)\n  rmse2[i,1:length(xtest)] <- sqrt((fcast2$mean-xtest)^2)\n}\n\nplot(1:4,colMeans(rmse1,na.rm=TRUE), type=\"l\",col=2, xlab=\"horizon\", ylab=\"RMSE\")\nlines(1:4, colMeans(rmse2,na.rm=TRUE), type=\"l\",col=3)\nlegend(\"topleft\",legend=c(\"fit1\",\"fit2\"),col=2:4,lty=1)\n\n\n\n\n\n\n\n\n\nCode\ncat(\"RMSE values for Model 1\\n\", colMeans(rmse1))\n\n\nRMSE values for Model 1\n 0.2847241 0.5452483 0.8481385 1.136754\n\n\nCode\ncat(\"RMSE values for Model 2\\n\", colMeans(rmse2))\n\n\nRMSE values for Model 2\n 0.2893202 0.5651061 0.8779249 1.176469\n\n\n\n\n\nThe RMSE cross-validation plot shows that the RMSE of Model 1 (0,1,0) is lower than that of Model 1 (0,2,1). This suggests that Model 1 (0,1,0) is a better fit for the data when compared other model. ##### Forecast\n::: panel-tabset ### Forecast for Technology Sector Fund with feature variable\n\n\nCode\n#fiting an ARIMA model to the Inflation variable\ninflation_fit<-auto.arima(normalized_XLK_factor_data_numeric_df$inflation) \nfinflation<-forecast(inflation_fit)\n\n\n# best model fit for forcasting\nxreg <- cbind(Inflation = normalized_XLK_factor_data_ts[, \"inflation\"])\n\nfit <- Arima(normalized_XLK_factor_data_ts[, \"XLK.Adjusted\"],order=c(0,1,0),xreg=xreg)\n\n# Forcast the stock price using feature variables\nfxreg <- cbind(Inflation = finflation$mean)\nfcast <- forecast(fit, xreg=fxreg) \nautoplot(fcast) + xlab(\"Year\") +\n  ylab(\"Normalised XLK.Adjusted\")\n\n\n\n\n\n\n\nARIMA Model for Inflation\n\n\nCode\n#fiting an ARIMA model to the Inflation variable\ninflation_fit<-auto.arima(normalized_XLK_factor_data_numeric_df$inflation) \nfinflation<-forecast(inflation_fit)\nsummary(inflation_fit)\n\n\nSeries: normalized_XLK_factor_data_numeric_df$inflation \nARIMA(0,1,0) \n\nsigma^2 = 0.1378:  log likelihood = -7.28\nAIC=16.55   AICc=16.82   BIC=17.38\n\nTraining set error measures:\n                    ME      RMSE       MAE       MPE     MAPE      MASE\nTraining set 0.1201871 0.3607553 0.2312026 -26.91442 61.15158 0.2676132\n                  ACF1\nTraining set 0.2431721\n\n\n\n\nARIMA Model for Technology Sector Fund with feature variable\n\n\nCode\n# best model fit for forcasting\nxreg <- cbind(Inflation = normalized_XLK_factor_data_ts[, \"inflation\"])\n\nfit <- Arima(normalized_XLK_factor_data_ts[, \"XLK.Adjusted\"],order=c(0,1,0),xreg=xreg)\nsummary(fit)\n\n\nSeries: normalized_XLK_factor_data_ts[, \"XLK.Adjusted\"] \nRegression with ARIMA(0,1,0) errors \n\nCoefficients:\n        xreg\n      0.5427\ns.e.  0.2444\n\nsigma^2 = 0.1487:  log likelihood = -7.41\nAIC=18.81   AICc=19.67   BIC=20.48\n\nTraining set error measures:\n                     ME      RMSE       MAE       MPE     MAPE      MASE\nTraining set 0.02130958 0.3635602 0.2907369 -33.27718 67.01639 0.3591998\n                  ACF1\nTraining set 0.1503141\n\n\nThe ARIMAX model may have incorporated exogenous variables such as macroeconomic factors, which could indicate the potential impact of external factors on the forecasted values. The findings suggest that inflation rates significantly impact the Technology Sector Fund stock price, underscoring the importance of including macroeconomic factors in stock market forecasting. This is further supported by the comparison of RMSE, MAE, AIC and BIC values for forecasts with and without the feature variable. It is found that including the feature variable leads to a lower values, indicating that the feature variable makes a significant impact and should be considered in stock market forecasting."
  },
  {
    "objectID": "arimax_sector_utilites_macroeconomic.html",
    "href": "arimax_sector_utilites_macroeconomic.html",
    "title": "ARIMAX/SARIMAX Page for Utilities Sector Fund and macroeconomic factors",
    "section": "",
    "text": "Endogenous variable: Utilities Sector Fund Exogenous variables: Macroeconomic indicators such as GDP, inflation rate, unemployment rate and interest rates\nBefore proceeding with model selection, it can be useful to examine the feature variables and select the appropriate ones. This can be done by creating cross-correlation function plots, which can serve as a verification of the correlation heatmap for the variables.\n\nCCF Plot for Utilities Sector Fund and Exogenous Variables\n\nGDPInterest RateInflation RateUnemployment Rate\n\n\n\n\nCode\npar(mfrow=c(1,1))\nccf_result <- ccf(normalized_sector_factor_data[, c(\"XLU.Adjusted\")], normalized_sector_factor_data[, c(\"gdp\")], \n    lag.max = 300,\n    main = \"Cros-Correlation Plot for Utilities Sector Fund and GDP Growth Rate \",\n    ylab = \"CCF\")\n\n\n\n\n\nCode\ncat(\"The sum of cross correlation function is\", sum(abs(ccf_result$acf)))\n\n\nThe sum of cross correlation function is 5.255006\n\n\n\n\n\n\nCode\npar(mfrow=c(1,1))\nccf_result <- ccf(normalized_sector_factor_data[, c(\"XLU.Adjusted\")], normalized_sector_factor_data[, c(\"interest\")], \n    lag.max = 300,\n    main = \"Cros-Correlation Plot for Utilities Sector Fund and Interest Rate\",\n    ylab = \"CCF\")\n\n\n\n\n\nCode\ncat(\"The sum of cross correlation function is\", sum(abs(ccf_result$acf)))\n\n\nThe sum of cross correlation function is 8.616293\n\n\n\n\n\n\nCode\npar(mfrow=c(1,1))\nccf_result <- ccf(normalized_sector_factor_data[, c(\"XLU.Adjusted\")], normalized_sector_factor_data[, c(\"inflation\")], \n    lag.max = 300,\n    main = \"Cros-Correlation Plot for Utilities Sector Fund and Inflation Rate\",\n    ylab = \"CCF\")\n\n\n\n\n\nCode\ncat(\"The sum of cross correlation function is\", sum(abs(ccf_result$acf)))\n\n\nThe sum of cross correlation function is 10.01093\n\n\n\n\n\n\nCode\npar(mfrow=c(1,1))\nccf_result <- ccf(normalized_sector_factor_data[, c(\"XLU.Adjusted\")], normalized_sector_factor_data[, c(\"unemployment\")], \n    lag.max = 300,\n    main = \"Cros-Correlation Plot for Utilities Sector Fund and Unemployment Rate\",\n    ylab = \"CCF\")\n\n\n\n\n\nCode\ncat(\"The sum of cross correlation function is\", sum(abs(ccf_result$acf)))\n\n\nThe sum of cross correlation function is 5.849381\n\n\n\n\n\nThe cross-correlation feature plots reaffirm the findings from the heatmap analysis, indicating that inflation rate exhibit stronger correlations with the index compared to GDP, interest rate and unemployment rate. The cross-correlation plots for GDP, unemployment and interest rates show weaker and more scattered patterns, with strikes between the blue lines indicating lower correlation coefficients. This suggests that inflation rate are more suitable feature variables for the ARIMAX model when predicting Utilities Sector Fund movements.\nFinal Exogenous variables: Macroeconomic indicators: Inflation rate\n\n\nEndogenous and Exogenous Variables Plot\n\n\nCode\nfinal_XLU_factor_data <- sector_factor_data %>%dplyr::select( Date,XLU.Adjusted, inflation)\nnumeric_vars_XLU_factor_data <- c(\"XLU.Adjusted\", \"inflation\")\nnumeric_XLU_factor_data <- final_XLU_factor_data[, numeric_vars_XLU_factor_data]\nnormalized_XLU_factor_data_numeric <- scale(numeric_XLU_factor_data)\nnormalized_XLU_factor_data_numeric_df <- data.frame(normalized_XLU_factor_data_numeric)\nnormalized_XLU_factor_data_ts <- ts(normalized_XLU_factor_data_numeric, start = c(2018, 7), frequency = 4)\n\nautoplot(normalized_XLU_factor_data_ts, facets=TRUE) +\n  xlab(\"Year\") + ylab(\"\") +\n  ggtitle(\"Utilities Sector Fund Stock Price, Inflation Rate and Unemployment Rate in USA 2018-2023\")\n\n\n\n\n\n\n\nCheck the stationarity\nBefore proceeding with further analysis, it’s important to check the stationarity of the time series data. If the series is not stationary, it needs to be differentiated before performing a ARIMAX model. The stationarity of a multivariate time series data can be checked using the Phillips-Perron test, which is specifically designed for multivariate data. By performing the Phillips-Perron test on the multivariate time series data, we can determine if the data is stationary or if it requires differencing to make it suitable for ARIMAX modeling.\n\n\nCode\n# Convert your multivariate time series data to a matrix\nfinal_XLU_factor_data_ts_multivariate <- as.matrix(normalized_XLU_factor_data_ts)\n\n# Check for stationarity using Phillips-Perron test\nphillips_perron_test <- ur.pp(final_XLU_factor_data_ts_multivariate)  \nsummary(phillips_perron_test)\n\n\n\n################################## \n# Phillips-Perron Unit Root Test # \n################################## \n\nTest regression with intercept \n\n\nCall:\nlm(formula = y ~ y.l1)\n\nResiduals:\n     Min       1Q   Median       3Q      Max \n-1.47539 -0.19741 -0.04694  0.29952  0.80845 \n\nCoefficients:\n            Estimate Std. Error t value Pr(>|t|)    \n(Intercept)  0.08310    0.08383   0.991    0.329    \ny.l1         0.87760    0.08897   9.864 2.28e-11 ***\n---\nSignif. codes:  0 '***' 0.001 '**' 0.01 '*' 0.05 '.' 0.1 ' ' 1\n\nResidual standard error: 0.4953 on 33 degrees of freedom\nMultiple R-squared:  0.7467,    Adjusted R-squared:  0.7391 \nF-statistic: 97.29 on 1 and 33 DF,  p-value: 2.28e-11\n\n\nValue of test-statistic, type: Z-alpha  is: -6.1735 \n\n         aux. Z statistics\nZ-tau-mu            0.8524\n\n\nThe Phillips-Perron unit root test output suggests that the data is stationary. The test regression includes an intercept term and tests whether the coefficient on the lagged value is significantly different from one. The estimated coefficients indicate that the intercept is not significant (p-value > 0.05), while the coefficient on the lagged value is highly significant (p-value < 2.28e-11), suggesting that the data is stationary.\nAdditionally, the “Z-alpha” value is the test statistic, which is compared to critical values of the standard normal distribution to determine significance. In this case, the test statistic is -6.1735, indicating strong evidence against the null hypothesis of a unit root. The “Z-tau-mu” value is an auxiliary test statistic used to correct for autocorrelation in the residuals, and it is close to zero, suggesting that there is no evidence of residual autocorrelation.\n\n\nFitting a ARIMAX model\nFitting an ARIMAX model will be a useful approach to modeling and predicting time series data when external factors are believed to have an impact on the response variable. In this case, we will be using an ARIMAX model to predict the Utilities Sector Fund Stock Price, taking into account the effects of inflation on the sector. By including these exogenous variables in our model, we can improve the accuracy of our predictions and gain a better understanding of the underlying dynamics of the time series.\n\nAuto ARIMAResiduals\n\n\n\n\nCode\nxreg <- cbind(Inflation = normalized_XLU_factor_data_ts[, \"inflation\"])\n\nfit <- auto.arima(normalized_XLU_factor_data_ts[, \"XLU.Adjusted\"], xreg = xreg)\nsummary(fit)\n\n\nSeries: normalized_XLU_factor_data_ts[, \"XLU.Adjusted\"] \nRegression with ARIMA(0,1,0) errors \n\nCoefficients:\n        xreg\n      0.7131\ns.e.  0.2737\n\nsigma^2 = 0.1865:  log likelihood = -9.33\nAIC=22.66   AICc=23.52   BIC=24.33\n\nTraining set error measures:\n                     ME      RMSE       MAE       MPE     MAPE      MASE\nTraining set 0.05185477 0.4071088 0.3328825 -716.5274 805.8123 0.3886691\n                   ACF1\nTraining set 0.04186762\n\n\n\n\n\n\nCode\ncheckresiduals(fit)\n\n\n\n\n\n\n    Ljung-Box test\n\ndata:  Residuals from Regression with ARIMA(0,1,0) errors\nQ* = 4.4695, df = 4, p-value = 0.3462\n\nModel df: 0.   Total lags used: 4\n\n\n\n\n\nThe auto.arima function reveals that the best-fit model is (0,1,0). Analysis of the residuals plot indicates that the fluctuates a lot and lies between -0.8 to 0.6. The ACF plot of the residuals shows significant lags, indicating that the model is performing well. The qq-plot also suggests normality in the residuals. The p-value greater than 0.05 actually suggests that there is no evidence of significant autocorrelation in the residuals, which means that the model is performing well in terms of capturing the temporal patterns in the data.\n\n\nFitting the model manually\n\nLinear ModelACF of ResidualsACF of ResidualsDifferentiated Residual\n\n\n\n\nCode\nnormalized_XLU_factor_data_numeric_df$XLU.Adjusted<-ts(normalized_XLU_factor_data_numeric_df$XLU.Adjusted,star=decimal_date(as.Date(\"2018-07-01\",format = \"%Y-%m-%d\")),frequency = 4)\nnormalized_XLU_factor_data_numeric_df$inflation<-ts(normalized_XLU_factor_data_numeric_df$inflation,star=decimal_date(as.Date(\"2018-07-01\",format = \"%Y-%m-%d\")),frequency = 4)\n\n\n############# First fit the linear model##########\nfit.reg <- lm(XLU.Adjusted ~ inflation, data=normalized_XLU_factor_data_numeric_df)\nsummary(fit.reg)\n\n\n\nCall:\nlm(formula = XLU.Adjusted ~ inflation, data = normalized_XLU_factor_data_numeric_df)\n\nResiduals:\n     Min       1Q   Median       3Q      Max \n-1.02519 -0.25326 -0.01658  0.46674  0.84797 \n\nCoefficients:\n              Estimate Std. Error t value Pr(>|t|)    \n(Intercept) -4.079e-16  1.306e-01   0.000        1    \ninflation    8.433e-01  1.344e-01   6.275 1.11e-05 ***\n---\nSignif. codes:  0 '***' 0.001 '**' 0.01 '*' 0.05 '.' 0.1 ' ' 1\n\nResidual standard error: 0.554 on 16 degrees of freedom\nMultiple R-squared:  0.7111,    Adjusted R-squared:  0.693 \nF-statistic: 39.38 on 1 and 16 DF,  p-value: 1.106e-05\n\n\n\n\n\n\nCode\nres.fit<-ts(residuals(fit.reg),star=decimal_date(as.Date(\"2018-01-01\",format = \"%Y-%m-%d\")),frequency = 4)\n############## Then look at the residuals ############\nacf(res.fit)\n\n\n\n\n\n\n\n\n\nCode\nPacf(res.fit)\n\n\n\n\n\n\n\n\n\nCode\nres.fit %>% diff() %>% ggtsdisplay()\n\n\n\n\n\n\n\n\nThe output displays the results of a linear regression model using two predictor variables, inflation and unemployment, to explain the variation in the response variable, XLU.Adjusted The coefficients of inflation have p-values of less than 0.05, indicating that both variables significantly impact XLU.Adjusted The R-squared value of approximately 71% suggests that the model explains a moderate amount of the variation in the response variable. Examination of the ACF plot of residuals reveals some autocorrelation, and therefore, the series is differentiated to achieve stationarity. By analyzing the differentiated ACF and PACF plots, the parameters if the ARIMAX modle are that p=5, q=0, and d=1.\n\n\nFinding the model parameters.\n\nARIMAX ResultModel PlotModel\n\n\n\n\nCode\nARIMA.c=function(p1,p2,q1,data){\ntemp=c()\nd=1\ni=1\ntemp= data.frame()\nls=matrix(rep(NA,6*30),nrow=30)\n\n\nfor (p in p1:p2)#\n{\n  for(q in q1)#\n  {\n    for(d in 0:1)\n    {\n      \n      if(p+d+q<=6)\n      {\n        \n        model<- Arima(data,order=c(p,d,q))\n        ls[i,]= c(p,d,q,model$aic,model$bic,model$aicc)\n        i=i+1\n        #print(i)\n        \n      }\n      \n    }\n  }\n}\n\n\ntemp= as.data.frame(ls)\nnames(temp)= c(\"p\",\"d\",\"q\",\"AIC\",\"BIC\",\"AICc\")\n\ntemp\n}\n\noutput <- ARIMA.c(0,5,0,data=residuals(fit.reg))\n\noutput[which.min(output$AIC),] \n\n\n  p d q      AIC      BIC     AICc\n2 0 1 0 20.88538 21.71859 21.15204\n\n\nCode\noutput[which.min(output$BIC),]\n\n\n  p d q      AIC      BIC     AICc\n2 0 1 0 20.88538 21.71859 21.15204\n\n\nCode\noutput[which.min(output$AICc),]\n\n\n  p d q      AIC      BIC     AICc\n2 0 1 0 20.88538 21.71859 21.15204\n\n\n\n\n\n\nCode\nset.seed(1234)\n\nmodel_output <- capture.output(sarima(res.fit, 0,1,0)) \n\n\n\n\n\n\n\n\n\nCode\ncat(model_output[9:38], model_output[length(model_output)], sep = \"\\n\")\n\n\n$fit\n\nCall:\narima(x = xdata, order = c(p, d, q), seasonal = list(order = c(P, D, Q), period = S), \n    xreg = constant, transform.pars = trans, fixed = fixed, optim.control = list(trace = trc, \n        REPORT = 1, reltol = tol))\n\nCoefficients:\n      constant\n        0.0384\ns.e.    0.1019\n\nsigma^2 estimated as 0.1763:  log likelihood = -9.37,  aic = 22.74\n\n$degrees_of_freedom\n[1] 16\n\n$ttable\n         Estimate     SE t.value p.value\nconstant   0.0384 0.1019   0.377  0.7111\n\n$AIC\n[1] 1.337872\n\n$AICc\n[1] 1.353558\n\n$BIC\n[1] 1.435897\n\n\n\n\n\nFollowing the manual fitting process, we identified that the ARIMAX model with the minimum AIC,BIC and AICc has the parameters (0,1,0), which is the same as the auto.arima parameters.\n\n\nForecast\n::: panel-tabset ### Forecast for Utilities Sector Fund with feature variable\n\n\nCode\n#fiting an ARIMA model to the Inflation variable\ninflation_fit<-auto.arima(normalized_XLU_factor_data_numeric_df$inflation) \nfinflation<-forecast(inflation_fit)\n\n\n# best model fit for forcasting\nxreg <- cbind(Inflation = normalized_XLU_factor_data_ts[, \"inflation\"])\n\nfit <- Arima(normalized_XLU_factor_data_ts[, \"XLU.Adjusted\"],order=c(0,1,0),xreg=xreg)\n\n# Forcast the stock price using feature variables\nfxreg <- cbind(Inflation = finflation$mean)\nfcast <- forecast(fit, xreg=fxreg) \nautoplot(fcast) + xlab(\"Year\") +\n  ylab(\"Normalised XLU.Adjusted\")\n\n\n\n\n\n\n\nARIMA Model for Inflation\n\n\nCode\n#fiting an ARIMA model to the Inflation variable\ninflation_fit<-auto.arima(normalized_XLU_factor_data_numeric_df$inflation) \nfinflation<-forecast(inflation_fit)\nsummary(inflation_fit)\n\n\nSeries: normalized_XLU_factor_data_numeric_df$inflation \nARIMA(0,1,0) \n\nsigma^2 = 0.1378:  log likelihood = -7.28\nAIC=16.55   AICc=16.82   BIC=17.38\n\nTraining set error measures:\n                    ME      RMSE       MAE       MPE     MAPE      MASE\nTraining set 0.1201871 0.3607553 0.2312026 -26.91442 61.15158 0.2676132\n                  ACF1\nTraining set 0.2431721\n\n\n\n\nARIMA Model for Utilities Sector Fund with feature variable\n\n\nCode\n# best model fit for forcasting\nxreg <- cbind(Inflation = normalized_XLU_factor_data_ts[, \"inflation\"])\n\nfit <- Arima(normalized_XLU_factor_data_ts[, \"XLU.Adjusted\"],order=c(0,1,0),xreg=xreg)\nsummary(fit)\n\n\nSeries: normalized_XLU_factor_data_ts[, \"XLU.Adjusted\"] \nRegression with ARIMA(0,1,0) errors \n\nCoefficients:\n        xreg\n      0.7131\ns.e.  0.2737\n\nsigma^2 = 0.1865:  log likelihood = -9.33\nAIC=22.66   AICc=23.52   BIC=24.33\n\nTraining set error measures:\n                     ME      RMSE       MAE       MPE     MAPE      MASE\nTraining set 0.05185477 0.4071088 0.3328825 -716.5274 805.8123 0.3886691\n                   ACF1\nTraining set 0.04186762\n\n\nThe ARIMAX model may have incorporated exogenous variables such as macroeconomic factors, which could indicate the potential impact of external factors on the forecasted values. The findings suggest that inflation rates significantly impact the Utilities Sector Fund stock price, underscoring the importance of including macroeconomic factors in stock market forecasting. This is further supported by the comparison of RMSE, MAE, AIC and BIC values for forecasts with and without the feature variable. It is found that including the feature variable leads to a lower values but MAE is slightly high i.e with factor it is 0.33 and without it is 0.28, indicating that the feature variable makes a significant impact and should be considered in stock market forecasting.\n```"
  },
  {
    "objectID": "arimax_sp500_macroeconomic.html",
    "href": "arimax_sp500_macroeconomic.html",
    "title": "ARIMAX/SARIMAX Model for S&P 500 index and macroeconomic factors",
    "section": "",
    "text": "Endogenous variable: S&P 500 Index Exogenous variables: Macroeconomic indicators such as GDP, inflation rate, unemployment rate and interest rates\nBefore proceeding with model selection, it can be useful to examine the feature variables and select the appropriate ones. This can be done by creating cross-correlation function plots, which can serve as a verification of the correlation heatmap for the variables.\n\nCCF Plot for S&P 500 Index and Exogenous Variables\n\nGDPInterest RateInflation RateUnemployment Rate\n\n\n\n\nCode\npar(mfrow=c(1,1))\nccf_result <- ccf(normalized_index_factor_data[, c(\"GSPC.Adjusted\")], normalized_index_factor_data[, c(\"gdp\")], \n    lag.max = 300,\n    main = \"Cros-Correlation Plot for S&P 500 Index and GDP Growth Rate \",\n    ylab = \"CCF\")\n\n\n\n\n\nCode\ncat(\"The sum of cross correlation function is\", sum(abs(ccf_result$acf)))\n\n\nThe sum of cross correlation function is 4.829354\n\n\n\n\n\n\nCode\npar(mfrow=c(1,1))\nccf_result <- ccf(normalized_index_factor_data[, c(\"GSPC.Adjusted\")], normalized_index_factor_data[, c(\"interest\")], \n    lag.max = 300,\n    main = \"Cros-Correlation Plot for S&P 500 Index and Interest Rate\",\n    ylab = \"CCF\")\n\n\n\n\n\nCode\ncat(\"The sum of cross correlation function is\", sum(abs(ccf_result$acf)))\n\n\nThe sum of cross correlation function is 10.66717\n\n\n\n\n\n\nCode\npar(mfrow=c(1,1))\nccf_result <- ccf(normalized_index_factor_data[, c(\"GSPC.Adjusted\")], normalized_index_factor_data[, c(\"inflation\")], \n    lag.max = 300,\n    main = \"Cros-Correlation Plot for S&P 500 Index and Inflation Rate\",\n    ylab = \"CCF\")\n\n\n\n\n\nCode\ncat(\"The sum of cross correlation function is\", sum(abs(ccf_result$acf)))\n\n\nThe sum of cross correlation function is 19.21394\n\n\n\n\n\n\nCode\npar(mfrow=c(1,1))\nccf_result <- ccf(normalized_index_factor_data[, c(\"GSPC.Adjusted\")], normalized_index_factor_data[, c(\"unemployment\")], \n    lag.max = 300,\n    main = \"Cros-Correlation Plot for S&P 500 Index and Unemployment Rate\",\n    ylab = \"CCF\")\n\n\n\n\n\nCode\ncat(\"The sum of cross correlation function is\", sum(abs(ccf_result$acf)))\n\n\nThe sum of cross correlation function is 22.33667\n\n\n\n\n\nThe cross-correlation feature plots reaffirm the findings from the heatmap analysis, indicating that inflation and unemployment rate exhibit stronger correlations with the index compared to GDP and interest rates. The cross-correlation plots for GDP and interest rates show weaker and more scattered patterns, with strikes between the blue lines indicating lower correlation coefficients. This suggests that inflation and unemployment rate are more suitable feature variables for the ARIMAX model when predicting S&P 500 movements.\nFinal Exogenous variables: Macroeconomic indicators: Inflation rate and unemployment rate.\n\n\nEndogenous and Exogenous Variables Plot\n\n\nCode\nfinal_sp500_factor_data <- index_factor_data %>%dplyr::select( Date,GSPC.Adjusted, inflation,unemployment)\nnumeric_vars_sp500_factor_data <- c(\"GSPC.Adjusted\", \"inflation\", \"unemployment\")\nnumeric_sp500_factor_data <- final_sp500_factor_data[, numeric_vars_sp500_factor_data]\nnormalized_sp500_factor_data_numeric <- scale(numeric_sp500_factor_data)\nnormalized_sp500_factor_data_numeric_df <- data.frame(normalized_sp500_factor_data_numeric)\nnormalized_sp500_factor_data_ts <- ts(normalized_sp500_factor_data_numeric, start = c(2010, 1), frequency = 4)\n\nautoplot(normalized_sp500_factor_data_ts, facets=TRUE) +\n  xlab(\"Year\") + ylab(\"\") +\n  ggtitle(\"NASDAQ  Index Stock Price, Inflation Rate and Unemployment Rate in USA 2010-2023\")\n\n\n\n\n\n\n\nCheck the stationarity\nBefore proceeding with further analysis, it’s important to check the stationarity of the time series data. If the series is not stationary, it needs to be differentiated before performing a ARIMAX model. The stationarity of a multivariate time series data can be checked using the Phillips-Perron test, which is specifically designed for multivariate data. By performing the Phillips-Perron test on the multivariate time series data, we can determine if the data is stationary or if it requires differencing to make it suitable for ARIMAX modeling.\n\n\nCode\n# Convert your multivariate time series data to a matrix\nfinal_sp500_factor_data_ts_multivariate <- as.matrix(normalized_sp500_factor_data_ts)\n\n# Check for stationarity using Phillips-Perron test\nphillips_perron_test <- ur.pp(final_sp500_factor_data_ts_multivariate)  \nsummary(phillips_perron_test)\n\n\n\n################################## \n# Phillips-Perron Unit Root Test # \n################################## \n\nTest regression with intercept \n\n\nCall:\nlm(formula = y ~ y.l1)\n\nResiduals:\n    Min      1Q  Median      3Q     Max \n-1.6643 -0.1555 -0.0444  0.0823  4.4931 \n\nCoefficients:\n            Estimate Std. Error t value Pr(>|t|)    \n(Intercept) 0.002211   0.040927   0.054    0.957    \ny.l1        0.857650   0.041342  20.745   <2e-16 ***\n---\nSignif. codes:  0 '***' 0.001 '**' 0.01 '*' 0.05 '.' 0.1 ' ' 1\n\nResidual standard error: 0.5095 on 153 degrees of freedom\nMultiple R-squared:  0.7377,    Adjusted R-squared:  0.736 \nF-statistic: 430.4 on 1 and 153 DF,  p-value: < 2.2e-16\n\n\nValue of test-statistic, type: Z-alpha  is: -23.4208 \n\n         aux. Z statistics\nZ-tau-mu             0.054\n\n\nThe Phillips-Perron unit root test output suggests that the data is stationary. The test regression includes an intercept term and tests whether the coefficient on the lagged value is significantly different from one. The estimated coefficients indicate that the intercept is not significant (p-value > 0.05), while the coefficient on the lagged value is highly significant (p-value < 2.2e-16), suggesting that the data is stationary.\nAdditionally, the “Z-alpha” value is the test statistic, which is compared to critical values of the standard normal distribution to determine significance. In this case, the test statistic is -23.4208, indicating strong evidence against the null hypothesis of a unit root. The “Z-tau-mu” value is an auxiliary test statistic used to correct for autocorrelation in the residuals, and it is close to zero, suggesting that there is no evidence of residual autocorrelation.\n\n\nFitting a ARIMAX model\nFitting an ARIMAX model will be a useful approach to modeling and predicting time series data when external factors are believed to have an impact on the response variable. In this case, we will be using an ARIMAX model to predict the S&P 500 Index, taking into account the effects of inflation and unemployment on the Index. By including these exogenous variables in our model, we can improve the accuracy of our predictions and gain a better understanding of the underlying dynamics of the time series.\n\nAuto ARIMAResiduals\n\n\n\n\nCode\nxreg <- cbind(Inflation = normalized_sp500_factor_data_ts[, \"inflation\"],\n              Unemployment = normalized_sp500_factor_data_ts[, \"unemployment\"])\n\nfit <- auto.arima(normalized_sp500_factor_data_ts[, \"GSPC.Adjusted\"], xreg = xreg)\nsummary(fit)\n\n\nSeries: normalized_sp500_factor_data_ts[, \"GSPC.Adjusted\"] \nRegression with ARIMA(0,1,0) errors \n\nCoefficients:\n      Inflation  Unemployment\n         0.1629       -0.1812\ns.e.     0.0824        0.0374\n\nsigma^2 = 0.03594:  log likelihood = 13.47\nAIC=-20.94   AICc=-20.42   BIC=-15.14\n\nTraining set error measures:\n                    ME      RMSE       MAE       MPE    MAPE      MASE\nTraining set 0.0292091 0.1840189 0.1345003 -186.6889 215.484 0.3969253\n                   ACF1\nTraining set 0.05410462\n\n\n\n\n\n\nCode\ncheckresiduals(fit)\n\n\n\n\n\n\n    Ljung-Box test\n\ndata:  Residuals from Regression with ARIMA(0,1,0) errors\nQ* = 5.1399, df = 8, p-value = 0.7425\n\nModel df: 0.   Total lags used: 8\n\n\n\n\n\nThe auto.arima function reveals that the best-fit model is (0,1,0). Analysis of the residuals plot indicates that the fluctuates a lot and lies between -0.5 to 0.5.The ACF plot of the residuals shows significant lags, indicating that the model is performing well. The qq-plot also suggests normality in the residuals. The p-value greater than 0.05 actually suggests that there is no evidence of significant autocorrelation in the residuals, which means that the model is performing well in terms of capturing the temporal patterns in the data.\n\n\nFitting the model manually\n\nLinear ModelACF of ResidualsACF of ResidualsDifferentiated Residual\n\n\n\n\nCode\nnormalized_sp500_factor_data_numeric_df$GSPC.Adjusted<-ts(normalized_sp500_factor_data_numeric_df$GSPC.Adjusted,star=decimal_date(as.Date(\"2010-01-01\",format = \"%Y-%m-%d\")),frequency = 4)\nnormalized_sp500_factor_data_numeric_df$inflation<-ts(normalized_sp500_factor_data_numeric_df$inflation,star=decimal_date(as.Date(\"2010-01-01\",format = \"%Y-%m-%d\")),frequency = 4)\nnormalized_sp500_factor_data_numeric_df$unemployment<-ts(normalized_sp500_factor_data_numeric_df$unemployment,star=decimal_date(as.Date(\"2010-01-01\",format = \"%Y-%m-%d\")),frequency = 4)\n\n############# First fit the linear model##########\nfit.reg <- lm(GSPC.Adjusted ~ inflation+unemployment, data=normalized_sp500_factor_data_numeric_df)\nsummary(fit.reg)\n\n\n\nCall:\nlm(formula = GSPC.Adjusted ~ inflation + unemployment, data = normalized_sp500_factor_data_numeric_df)\n\nResiduals:\n    Min      1Q  Median      3Q     Max \n-0.9069 -0.3819 -0.1741  0.2899  1.7751 \n\nCoefficients:\n               Estimate Std. Error t value Pr(>|t|)    \n(Intercept)   1.679e-16  8.960e-02   0.000   1.0000    \ninflation     6.104e-01  1.032e-01   5.914 3.17e-07 ***\nunemployment -2.653e-01  1.032e-01  -2.571   0.0132 *  \n---\nSignif. codes:  0 '***' 0.001 '**' 0.01 '*' 0.05 '.' 0.1 ' ' 1\n\nResidual standard error: 0.6461 on 49 degrees of freedom\nMultiple R-squared:  0.5989,    Adjusted R-squared:  0.5825 \nF-statistic: 36.58 on 2 and 49 DF,  p-value: 1.905e-10\n\n\n\n\n\n\nCode\nres.fit<-ts(residuals(fit.reg),star=decimal_date(as.Date(\"2010-01-01\",format = \"%Y-%m-%d\")),frequency = 4)\n############## Then look at the residuals ############\nacf(res.fit)\n\n\n\n\n\n\n\n\n\nCode\nPacf(res.fit)\n\n\n\n\n\n\n\n\n\nCode\nres.fit %>% diff() %>% ggtsdisplay()\n\n\n\n\n\n\n\n\nThe output displays the results of a linear regression model using two predictor variables, inflation and unemployment, to explain the variation in the response variable, IXIC.Adjusted. The coefficients of inflation and unemployment have p-values of less than 0.05, indicating that both variables significantly impact IXIC.Adjusted. The R-squared value of approximately 59% suggests that the model explains a moderate amount of the variation in the response variable. Examination of the ACF plot of residuals reveals some autocorrelation, and therefore, the series is differentiated to achieve stationarity. By analyzing the differentiated ACF and PACF plots, the parameters if the ARIMAX model are that p=0, q=3, and d=1.\n\n\nFinding the model parameters.\n\nARIMAX ResultModel 1 PlotModel 1Model 2 PlotModel 2Model 3 PlotModel 3\n\n\n\n\nCode\nARIMA.c=function(p1,q1,q2,data){\ntemp=c()\nd=1\ni=1\ntemp= data.frame()\nls=matrix(rep(NA,6*30),nrow=30)\n\n\nfor (p in p1)#\n{\n  for(q in q1:q2)#\n  {\n    for(d in 0:1)\n    {\n      \n      if(p+d+q<=6)\n      {\n        \n        model<- Arima(data,order=c(p,d,q))\n        ls[i,]= c(p,d,q,model$aic,model$bic,model$aicc)\n        i=i+1\n        #print(i)\n        \n      }\n      \n    }\n  }\n}\n\n\ntemp= as.data.frame(ls)\nnames(temp)= c(\"p\",\"d\",\"q\",\"AIC\",\"BIC\",\"AICc\")\n\ntemp\n}\n\noutput <- ARIMA.c(0,1,3,data=residuals(fit.reg))\n\noutput[which.min(output$AIC),] \n\n\n  p d q     AIC      BIC     AICc\n6 0 1 3 6.14935 13.87665 7.018916\n\n\nCode\noutput[which.min(output$BIC),]\n\n\n  p d q      AIC      BIC     AICc\n2 0 1 1 9.834173 13.69782 10.08417\n\n\nCode\noutput[which.min(output$AICc),]\n\n\n  p d q     AIC      BIC     AICc\n6 0 1 3 6.14935 13.87665 7.018916\n\n\n\n\n\n\nCode\nset.seed(1234)\n\nmodel_output_1 <- capture.output(sarima(res.fit, 0,1,3)) \n\n\n\n\n\n\n\n\n\nCode\ncat(model_output_1[23:54], model_output_1[length(model_output_1)], sep = \"\\n\")\n\n\n$fit\n\nCall:\narima(x = xdata, order = c(p, d, q), seasonal = list(order = c(P, D, Q), period = S), \n    xreg = constant, transform.pars = trans, fixed = fixed, optim.control = list(trace = trc, \n        REPORT = 1, reltol = tol))\n\nCoefficients:\n         ma1     ma2     ma3  constant\n      0.1540  0.2014  0.3736   -0.0118\ns.e.  0.1427  0.1720  0.1263    0.0562\n\nsigma^2 estimated as 0.05585:  log likelihood = 0.95,  aic = 8.11\n\n$degrees_of_freedom\n[1] 47\n\n$ttable\n         Estimate     SE t.value p.value\nma1        0.1540 0.1427  1.0791  0.2860\nma2        0.2014 0.1720  1.1709  0.2476\nma3        0.3736 0.1263  2.9583  0.0048\nconstant  -0.0118 0.0562 -0.2105  0.8342\n\n$AIC\n[1] 0.1589218\n\n$AICc\n[1] 0.1759721\n\n$BIC\n[1] 0.3483165\n\n\n\n\n\n\nCode\nmodel_output_2 <- capture.output(sarima(res.fit,0,1,1)) \n\n\n\n\n\n\n\n\n\nCode\ncat(model_output_2[18:48], model_output_2[length(model_output_2)], sep = \"\\n\")\n\n\n$fit\n\nCall:\narima(x = xdata, order = c(p, d, q), seasonal = list(order = c(P, D, Q), period = S), \n    xreg = constant, transform.pars = trans, fixed = fixed, optim.control = list(trace = trc, \n        REPORT = 1, reltol = tol))\n\nCoefficients:\n         ma1  constant\n      0.1506   -0.0057\ns.e.  0.1364    0.0412\n\nsigma^2 estimated as 0.06559:  log likelihood = -2.91,  aic = 11.82\n\n$degrees_of_freedom\n[1] 49\n\n$ttable\n         Estimate     SE t.value p.value\nma1        0.1506 0.1364  1.1046  0.2747\nconstant  -0.0057 0.0412 -0.1373  0.8914\n\n$AIC\n[1] 0.2316735\n\n$AICc\n[1] 0.2365755\n\n$BIC\n[1] 0.3453103\n\n\n\n\n\n\nCode\nmodel_output_3 <- capture.output(sarima(res.fit,0,1,0)) \n\n\n\n\n\n\n\n\n\nCode\ncat(model_output_3[9:38], model_output_3[length(model_output_3)], sep = \"\\n\")\n\n\n$fit\n\nCall:\narima(x = xdata, order = c(p, d, q), seasonal = list(order = c(P, D, Q), period = S), \n    xreg = constant, transform.pars = trans, fixed = fixed, optim.control = list(trace = trc, \n        REPORT = 1, reltol = tol))\n\nCoefficients:\n      constant\n       -0.0062\ns.e.    0.0363\n\nsigma^2 estimated as 0.06729:  log likelihood = -3.55,  aic = 11.1\n\n$degrees_of_freedom\n[1] 50\n\n$ttable\n         Estimate     SE t.value p.value\nconstant  -0.0062 0.0363 -0.1719  0.8642\n\n$AIC\n[1] 0.2175886\n\n$AICc\n[1] 0.2191893\n\n$BIC\n[1] 0.2933465\n\n\n\n\n\nFollowing the manual fitting process, we identified that the ARIMAX model with the minimum AIC and AICc has the parameters (0,1,3), while the model with the minimum BIC has the parameters (0,1,1). We fitted both models using both the manual and auto.arima methods and observed that all the models have p-values greater than 0, indicating statistical significance. To determine the best fit model, we performed cross-validation.\n\n\nCross Validation\n\nRMSE PlotRMSE Results\n\n\n\n\nCode\nn=length(res.fit)\nk= 51\n \n \nrmse1 <- matrix(NA, (n-k),4)\nrmse2 <- matrix(NA, (n-k),4)\nrmse3 <- matrix(NA, (n-k),4)\n\nst <- tsp(res.fit)[1]+(k-5)/4 \n\nfor(i in 1:(n-k))\n{\n  xtrain <- window(res.fit, end=st + i/4)\n  xtest <- window(res.fit, start=st + (i+1)/4, end=st + (i+4)/4)\n\n  \n  fit <- Arima(xtrain, order=c(0,1,3),\n                include.drift=TRUE, method=\"ML\")\n  fcast <- forecast(fit, h=4)\n  \n  fit2 <- Arima(xtrain, order=c(0,1,1),\n                include.drift=TRUE, method=\"ML\")\n  fcast2 <- forecast(fit2, h=4)\n\n  fit3 <- Arima(xtrain, order=c(0,1,0),\n                include.drift=TRUE, method=\"ML\")\n  fcast3 <- forecast(fit3, h=4)\n\n  rmse1[i,1:length(xtest)]   <- sqrt((fcast$mean-xtest)^2)\n  rmse2[i,1:length(xtest)] <- sqrt((fcast2$mean-xtest)^2)\n  rmse3[i,1:length(xtest)] <- sqrt((fcast3$mean-xtest)^2)\n}\n\nplot(1:4,colMeans(rmse1,na.rm=TRUE), type=\"l\",col=2, xlab=\"horizon\", ylab=\"RMSE\")\nlines(1:4, colMeans(rmse2,na.rm=TRUE), type=\"l\",col=3)\nlines(1:4, colMeans(rmse3,na.rm=TRUE), type=\"l\",col=4)\nlegend(\"topleft\",legend=c(\"fit1\",\"fit2\",\"fit3\"),col=2:4,lty=1)\n\n\n\n\n\n\n\n\n\nCode\ncat(\"RMSE values for Model 1\\n\", colMeans(rmse1))\n\n\nRMSE values for Model 1\n 0.09801778 0.3890418 0.9544379 1.291413\n\n\nCode\ncat(\"RMSE values for Model 2\\n\", colMeans(rmse2))\n\n\nRMSE values for Model 2\n 0.3002188 0.718625 1.342683 1.684389\n\n\nCode\ncat(\"RMSE values for Model 3\\n\", colMeans(rmse3))\n\n\nRMSE values for Model 3\n 0.2982561 0.7166822 1.34076 1.682486\n\n\n\n\n\nThe RMSE cross-validation plot shows that the RMSE of Model 1 (0,1,3) is lower than that of Model 2 (0,1,1) and Model 3 (0,1,0). This suggests that Model 3 (0,1,3) is a better fit for the data when compared other models.\n\n\nForecast\n\nForecast for S&P 500 Index with feature variablesARIMA Model for InflationARIMA Model for EmploymentARIMA Model for S&P 500 Index with feature variables\n\n\n\n\nCode\n#fiting an ARIMA model to the Inflation variable\ninflation_fit<-auto.arima(normalized_sp500_factor_data_numeric_df$inflation) \nfinflation<-forecast(inflation_fit)\n\n#fitting an ARIMA model to the Unemployment variable\nunemployment_fit<-auto.arima(normalized_sp500_factor_data_numeric_df$unemployment) \nfunemployment<-forecast(unemployment_fit)\n\n# best model fit for forcasting\nxreg <- cbind(Inflation = normalized_sp500_factor_data_ts[, \"inflation\"],\n              Unemployment = normalized_sp500_factor_data_ts[, \"unemployment\"])\n\nfit <- Arima(normalized_sp500_factor_data_ts[, \"GSPC.Adjusted\"],order=c(0,1,3),xreg=xreg)\n\n# Forcast the stock price using feature variables\nfxreg <- cbind(Inflation = finflation$mean,\n              Unemployment = funemployment$mean)\nfcast <- forecast(fit, xreg=fxreg) \nautoplot(fcast) + xlab(\"Year\") +\n  ylab(\"Normalised GSPC.Adjusted\")\n\n\n\n\n\n\n\n\n\nCode\n#fiting an ARIMA model to the Inflation variable\ninflation_fit<-auto.arima(normalized_sp500_factor_data_numeric_df$inflation) \nfinflation<-forecast(inflation_fit)\nsummary(inflation_fit)\n\n\nSeries: normalized_sp500_factor_data_numeric_df$inflation \nARIMA(0,1,3)(0,0,1)[4] with drift \n\nCoefficients:\n         ma1     ma2     ma3     sma1   drift\n      0.2153  0.5354  0.4353  -0.3707  0.0567\ns.e.  0.1719  0.1915  0.1455   0.1942  0.0499\n\nsigma^2 = 0.0708:  log likelihood = -3.48\nAIC=18.96   AICc=20.87   BIC=30.55\n\nTraining set error measures:\n                      ME     RMSE       MAE       MPE    MAPE      MASE\nTraining set 0.002515126 0.250258 0.1802161 -17.11236 114.163 0.3180481\n                    ACF1\nTraining set -0.06141191\n\n\n\n\n\n\nCode\n#fitting an ARIMA model to the Unemployment variable\nunemployment_fit<-auto.arima(normalized_sp500_factor_data_numeric_df$unemployment) \nfunemployment<-forecast(unemployment_fit)\nsummary(unemployment_fit)\n\n\nSeries: normalized_sp500_factor_data_numeric_df$unemployment \nARIMA(0,1,2) \n\nCoefficients:\n          ma1      ma2\n      -0.3027  -0.2410\ns.e.   0.1376   0.1472\n\nsigma^2 = 0.4982:  log likelihood = -53.72\nAIC=113.45   AICc=113.96   BIC=119.24\n\nTraining set error measures:\n                     ME      RMSE      MAE      MPE     MAPE      MASE\nTraining set -0.1001611 0.6851533 0.281119 140.1191 219.8632 0.4565275\n                    ACF1\nTraining set -0.02079058\n\n\n\n\n\n\nCode\n# best model fit for forcasting\nxreg <- cbind(Inflation = normalized_sp500_factor_data_ts[, \"inflation\"],\n              Unemployment = normalized_sp500_factor_data_ts[, \"unemployment\"])\n\nfit <- Arima(normalized_sp500_factor_data_ts[, \"GSPC.Adjusted\"],order=c(0,1,3),xreg=xreg)\nsummary(fit)\n\n\nSeries: normalized_sp500_factor_data_ts[, \"GSPC.Adjusted\"] \nRegression with ARIMA(0,1,3) errors \n\nCoefficients:\n         ma1     ma2     ma3  Inflation  Unemployment\n      0.1048  0.0781  0.4723     0.1549       -0.1928\ns.e.  0.1705  0.1723  0.1810     0.0866        0.0326\n\nsigma^2 = 0.0333:  log likelihood = 16.64\nAIC=-21.27   AICc=-19.36   BIC=-9.68\n\nTraining set error measures:\n                     ME     RMSE       MAE       MPE     MAPE      MASE\nTraining set 0.01014764 0.171641 0.1307816 -111.0777 138.5378 0.3859508\n                    ACF1\nTraining set -0.01615945\n\n\n\n\n\nThe ARIMAX model may have incorporated exogenous variables such as macroeconomic factors, which could indicate the potential impact of external factors on the forecasted values. The findings suggest that inflation and unemployment rates significantly impact the S&P 500 Index stock price, underscoring the importance of including macroeconomic factors in stock market forecasting.This is further supported by the comparison of RMSE, MAE, AIC and BIC values for forecasts with and without the feature variable. It is found that including the feature variable leads to a lower values, indicating that the feature variable makes a significant impact and should be considered in stock market forecasting."
  },
  {
    "objectID": "arimax_stock_index_macroeconomic.html",
    "href": "arimax_stock_index_macroeconomic.html",
    "title": "ARIMAX/SARIMAX Model for US stock indices and macroeconomic factors as exogenous variables",
    "section": "",
    "text": "In this case, we are interested in predicting the performance of three stock indices, such as the S&P 500, NASDAQ, and Dow Jones Industrial Average. We will use macroeconomic factors as exogenous variables to improve the accuracy of our predictions. Some examples of macroeconomic factors we might consider include GDP growth, inflation rates, and interest rates.\nIncluding these exogenous variables can help us to better understand how the stock market might be impacted by changes in the broader economy. For example, if GDP growth is predicted to increase, we might expect to see a corresponding increase in the stock market indices as well.\nAccording to the findings, the endogenous and exogenous variables in the time series data are not interdependent, then the ARIMAX model can be a good choice for predicting the stock market indices. If there is seasonality in the data, then the SARIMAX model can be used to account for this seasonal variation. If there is no seasonality, then the simpler ARIMA model can be used instead of SARIMAX.\n\n\n\n\n\n\nLet’s examine the relationship between endogenous and exogenous variables before proceeding with the ARIMAX/SARIMAX model.\n\nPlotNormalized Plot\n\n\n\n\nCode\nts_plot(index_factor_data,\n        title = \"Stock Prices and Macroeconomic Variables\",\n        Ytitle = \"Values\",\n        Xtitle = \"Year\")\n\n\n\n\n\n\n\n\n\n\nCode\nnumeric_vars_index_factor_data <- c(\"DJI.Adjusted\", \"IXIC.Adjusted\", \"GSPC.Adjusted\", \"gdp\", \"interest\", \"inflation\", \"unemployment\")\nnumeric_index_factor_data <- index_factor_data[, numeric_vars_index_factor_data]\nnormalized_index_factor_data_numeric <- scale(numeric_index_factor_data)\nnormalized_index_factor_data <- ts(normalized_index_factor_data_numeric, start = c(2010, 1), frequency = 4)\nts_plot(normalized_index_factor_data,\n        title = \"Normalized Time Series Data for Stock Prices and Macroeconomic Variables\",\n        Ytitle = \"Normalized Values\",\n        Xtitle = \"Year\")\n\n\n\n\n\n\n\n\n\nThe Stock Prices and Macroeconomic Variables plot, displays the time series data of various stock prices and macroeconomic variables from 2010 to 2022. Since the variables in the time series data have different scales or units, it can make the plot difficult to interpret, as the differences between variables may be obscured by the varying magnitudes. Normalizing the data by scaling it to a common scale, such as z-scores or percentage changes, can help to eliminate this issue and provide a clearer view of the relationships and patterns in the data.\nThe Normalized Time Series Data for Stock Prices and Macroeconomic Variables plot, shows the same variables as the first plot, but the data has been normalized. Normalization is the process of scaling data to a common range, usually between 0 and 1, to eliminate the impact of different scales or units of measurement. In this case, the data has been scaled using the scale() function in R, which standardizes the variables to have a mean of 0 and a standard deviation of 1.\nNormalizing the time series data is beneficial for several reasons. First, it helps to remove any bias or distortion that may be introduced by variables with different units or magnitudes, allowing for a fair comparison between variables. Second, normalizing the data can help to stabilize the VAR model estimation, as variables with large values or extreme fluctuations may disproportionately influence the results. Lastly, normalizing the data can also improve the interpretability of the model coefficients, as the coefficients will be in the same scale and can be directly compared to assess their relative importance.\n\nCross-Correlation for the Variables and Selection of Feature Variables\nCross-correlation is a statistical technique used to measure the relationship between two or more variables in a time series. In the context of ARIMAX modeling, cross-correlation is often used for feature selection. For selecting feature variables in our analysis, we will first examine the correlation through a heatmap among all the variables, and then analyze the autocorrelation function (ACF) plots between the response variable and the exogenous variables.\n\n\nCorrelation Heatmap\n\n\nCode\n# Get upper triangle of the correlation matrix\nget_upper_tri <- function(cormat){\n    cormat[lower.tri(cormat)]<- NA\n    return(cormat)\n}\ncormat <- round(cor(normalized_index_factor_data_numeric),2)\n\nupper_tri <- get_upper_tri(cormat)\n\nmelted_cormat <- melt(upper_tri, na.rm = TRUE)\n# Create a ggheatmap\nggheatmap <- ggplot(melted_cormat, aes(Var2, Var1, fill = value))+\n geom_tile(color = \"white\")+\n scale_fill_gradient2(low = \"blue\", high = \"red\", mid = \"white\", \n   midpoint = 0, limit = c(-1,1), space = \"Lab\", \n    name=\"Pearson\\nCorrelation\") +\n  theme_minimal()+ # minimal theme\n theme(axis.text.x = element_text(angle = 45, vjust = 1, \n    size = 12, hjust = 1))+\n coord_fixed()\n\nggheatmap + \ngeom_text(aes(Var2, Var1, label = value), color = \"black\", size = 4) +\ntheme(\n  axis.title.x = element_blank(),\n  axis.title.y = element_blank(),\n  panel.grid.major = element_blank(),\n  panel.border = element_blank(),\n  panel.background = element_blank(),\n  axis.ticks = element_blank(),\n  legend.justification = c(1, 0),\n  legend.position = c(0.6, 0.7),\n  legend.direction = \"horizontal\")+\n  guides(fill = guide_colorbar(barwidth = 7, barheight = 1,\n                title.position = \"top\", title.hjust = 0.5))\n\n\n\n\n\nThe heatmap reveal important insights into the relationships between the stock market indices and various economic indicators. The strong positive correlations between the stock market indices and inflation, along with the negative correlations with unemployment rate, suggest that these variables may play a significant role in influencing stock market movements. In contrast, the weaker correlations between the stock market indices and GDP and interest rates indicate that these variables may have less impact on stock market fluctuations. These findings provide valuable guidance for selecting relevant variables in the VAR model to better understand and forecast stock market dynamics.\nClick to view ARIMAX/SARIMAX Model for Dow Jones index and macroeconomic factors as exogenous variables\nClick to view ARIMAX/SARIMAX Model for NASDAQ Composite index and macroeconomic factors as exogenous variables\nClick to view ARIMAX/SARIMAX Model for S&P 500 index and macroeconomic factors as exogenous variables"
  },
  {
    "objectID": "arma.html",
    "href": "arma.html",
    "title": "",
    "section": "",
    "text": "ARMA/ARIMA/SARIMA Model\nWhen working with time series data, it is common to start with autoregressive (AR), moving average (MA), or autoregressive moving average (ARMA) models. Additionally, autoregressive integrated moving average (ARIMA) and seasonal autoregressive integrated moving average (SARIMA) models are widely used for better understanding the data and forecasting future values based on historical data. However, before using these models, it is crucial to check for stationarity in the time series data, which means that the mean and variance should not change over time. Non-stationarity is often caused by trends, where the values slowly increase or decrease over time.\nTo test for stationarity, an autocorrelation function (ACF) plot can be used to check for correlations between the time series and its lagged version. If there is a significant correlation, then the data is likely non-stationary. In such cases, taking the first or second differences of the data can help remove any trends or seasonality and make the data stationary.\nIn the EDA tab of our project, we have executed an ACF plot to test for stationarity and made the necessary data transformations to make the data stationary. The ACF and partial autocorrelation (PACF) plots for the stationary data can help identify the appropriate parameters for our ARIMA or SARIMA models.\n\n\nARIMA Model for US Stock Indicies\nThe ARIMA model is a popular choice for modeling and forecasting US stock indices despite the fact that they do not exhibit stationarity in their raw form. This is because the ARIMA model can handle non-stationary time series data by first differencing the series to make it stationary.\nThe first step in using the ARIMA model for US stock indices is to identify the appropriate order of differencing required to make the series stationary. This can be done using statistical tests, such as the Augmented Dickey-Fuller (ADF) test, which test for the presence of unit roots in the series. Once the series has been differenced, the ARIMA model can be fitted to the data to identify the appropriate autoregressive (AR), integrated (I), and moving average (MA) parameters. The model can then be used to make forecasts of future values based on the identified patterns and trends in the data.\nOverall, the ARIMA model is a powerful tool for modeling and forecasting US stock indices, despite their non-stationarity. It can account for the complex patterns and trends present in the data and provide valuable insights into the behavior of the stock market.\nClick to view the ARIMA Page for US Stock Indices\n\n\nARIMA Model for Sector Market\nFrom previous exploratory data analysis (EDA), it was found that sector markets, such as the technology or energy sectors, exhibit unique patterns and trends over time. These patterns may be driven by specific economic and industry-related factors that can impact stock prices.\nTo model and forecast sector markets, the ARIMA model is commonly used. The appropriate order of differencing can be identified using statistical tests, such as the ADF test. Once identified, the ARIMA model can be fitted to the data to identify the appropriate AR, I, and MA parameters.\nOverall, the ARIMA model is a powerful tool for modeling and forecasting sector markets, allowing analysts to identify unique patterns and trends and make more accurate predictions about future values. However, as with any forecasting model, it is important to carefully consider the underlying assumptions and limitations of the model and to incorporate relevant exogenous variables where appropriate.\nClick to view the ARIMA Page for Sector Market\n\n\nARIMA/SARIMA Model for Macroeconomic factors\nFrom previous exploratory data analysis (EDA), it was found that macroeconomic factors such as GDP, inflation, and unemployment rates exhibit complex patterns and trends over time. These patterns may be driven by long-term economic cycles, seasonal effects, and the impact of specific economic policies.\nTo model and forecast these factors, the ARIMA/SARIMA model is often used due to its ability to capture and model these complex patterns. The appropriate order of differencing and seasonality can be identified using statistical tests, such as the ADF and seasonal ADF tests. Once identified, the ARIMA/SARIMA model can be fitted to the data to identify the appropriate AR, I, and MA parameters.\nOverall, the ARIMA/SARIMA model is a powerful tool for modeling and forecasting macroeconomic factors, allowing analysts to identify and model the complex trends and patterns that are often present in economic data. However, it is important to carefully consider the underlying assumptions and limitations of the model and to incorporate relevant exogenous variables where appropriate.\nClick to view the ARIMA Page for Macroeconomic Factors"
  },
  {
    "objectID": "asv.html",
    "href": "asv.html",
    "title": "",
    "section": "",
    "text": "ARIMAX Model\nARIMAX (Autoregressive Integrated Moving Average with Exogenous Variables) and SARIMAX (Seasonal Autoregressive Integrated Moving Average with Exogenous Variables) are extensions of the ARIMA and SARIMA models, respectively, that allow for the inclusion of exogenous variables in the modeling process. Exogenous variables are independent variables that may affect the dependent variable (i.e., the time series of interest) but are not affected by it.\nVAR (Vector Autoregression) is another model commonly used for time series analysis, which allows for the inclusion of multiple time series as both dependent and independent variables in the modeling process.\nThese models are useful when there are external factors that may influence the time series of interest, and we want to account for these factors in our forecasting. For example in the project, when predicting stock prices, we may want to include economic indicators such as interest rates, inflation rates, unemployment rate, and GDP growth as exogenous variables, as these factors can impact the stock price of indices and sector market.\nChoosing the appropriate model depends on the characteristics of the data and the research question at hand. - If there is no clear seasonal pattern in the data and only a few exogenous variables, ARIMAX may be appropriate. - If there is a clear seasonal pattern in the data and/or multiple exogenous variables, SARIMAX may be more appropriate. - If there are multiple time series that are potentially interacting with each other, VAR may be more appropriate.\nHowever, the selection of the appropriate model should be based on a thorough analysis of the data and model diagnostics, such as checking for stationarity, autocorrelation, and heteroscedasticity. It is also important to consider the interpretability and ease of implementation of the chosen model. By using ARIMAX or VAR models to analyze the relationship between macroeconomic factors and the stock market, we can better understand how changes in the broader economy may impact the stock prices of specific indices or sectors. These models take into account both endogenous and exogenous variables, which allows us to examine how different factors interact with one another and make more accurate predictions of future stock prices.\n\n\n\nARIMAX/SARIMAX Model for US stock indices and macroeconomic factors as exogenous variables\nARIMAX and SARIMAX models can be useful in analyzing the relationship between US stock indices and macroeconomic factors as exogenous variables. In ARIMAX models, exogenous variables are included to help explain the behavior of the endogenous variable, which is the stock price of a specific US stock index or sector. SARIMAX models, on the other hand, are designed to capture the complex patterns and seasonality in the data.\nChoosing between ARIMAX and SARIMAX models depends on the nature of the data and the research question. If the focus is on the impact of macroeconomic factors on a specific US stock index or sector, then ARIMAX may be more appropriate. However, if the data has clear seasonal patterns, such as monthly or quarterly data, then SARIMAX may be more effective in capturing these patterns and providing more accurate forecasts. Incorporating exogenous variables in ARIMAX or SARIMAX models can help identify patterns and relationships between the US stock market and the broader economy.\nClick to view ARIMAX/SARIMAX Model Page for US stock indices and macroeconomic factors as exogenous variables\n\n\nARIMAX/SARIMAX Model for Sector market and macroeconomic factors as exogenous variables\nARIMAX and SARIMA models can also be useful in analyzing the relationship between sector market data and macroeconomic factors as exogenous variables. In this case, the endogenous variable is the stock price of a particular sector, and the exogenous variables may include factors such as interest rates, inflation, and GDP growth.\nARIMAX models can help to identify the relationship between the sector market and macroeconomic factors while controlling for the impact of other endogenous variables. SARIMA models, on the other hand, are effective in capturing the complex seasonal patterns in the data.\nWhen deciding between ARIMAX and SARIMA models, it is important to consider the nature of the data and the research question. If the data has clear seasonal patterns, such as quarterly or monthly data, then SARIMA may be more effective in capturing these patterns and providing accurate forecasts. On the other hand, if the focus is on understanding the impact of macroeconomic factors on a specific sector market, then ARIMAX may be more appropriate.\nOverall, incorporating exogenous variables in ARIMAX or SARIMA models can provide valuable insights into the relationship between sector market data and macroeconomic factors.\nClick to view ARIMAX/SARIMAX Model Page for Sector market and macroeconomic factors as exogenous variables"
  },
  {
    "objectID": "conclusion.html",
    "href": "conclusion.html",
    "title": "",
    "section": "",
    "text": "Conclusion\nThe stock market and sector market are vital components of the global economy, and various macroeconomic factors influence their performance. As investors seek opportunities to grow their wealth, they look to the stock market to invest in publicly traded companies or the sector market to focus on specific industries or segments of the economy. However, the performance of these markets is separate from the broader economic environment. Changes in macroeconomic factors such as inflation, interest rates, and economic growth can significantly impact the stock and sector markets.\nOur project aimed to investigate how macroeconomic factors affect the performance of the US stock market indices and sector markets. By analyzing the data and exploring the relationships between these factors and the stock market, we aimed to gain a deeper understanding of the complex interactions between macroeconomic indicators and financial markets. Ultimately, this project aimed to provide insights into how investors, businesses, and policymakers can navigate the dynamic landscape of the stock market and sector market in response to macroeconomic changes.\n\nThe COVID-19 pandemic has profoundly impacted the stock market indices and sector market, causing significant fluctuations in stock prices and investor sentiment. In the early stages of the pandemic, stock markets worldwide experienced a sharp decline as investors feared the economic impact of the virus. As the pandemic continued to spread, some sectors of the economy, such as travel and hospitality, were hit particularly hard, leading to a further decline in the sector market. However, other sectors, such as technology and healthcare, saw significant growth as companies adapted to the new normal of remote work and increased demand for medical equipment and services.\n\nDespite the initial shock, the stock market has shown resilience and since partially recovered, thanks in part to government stimulus measures and vaccine development. However, the impact of the pandemic on the stock market and sector market is far from over, as uncertainties and risks still loom large, including new variants of the virus and inflation concerns. As a result, investors and businesses must remain vigilant and adapt to the evolving market conditions to navigate the post-COVID landscape successfully.\n\nThe analysis conducted in our project confirms that macroeconomic factors play a significant role in the performance of the stock market indices and sector markets. Our results show that including macroeconomic variables in the models led to better predictive accuracy than models that only considered historical prices. Among the macroeconomic factors, inflation and unemployment rates significantly impacted the stock and sector prices.\nAs a future direction, our project could be expanded to include a more in-depth analysis of the top companies in each sector market and how macroeconomic factors have impacted them. By quantifying the performance of these companies and analyzing their responses to changes in macroeconomic indicators, we can better understand the specific drivers of sector market performance.\nIn addition to traditional time series models, we also explored the potential of deep learning models in predicting stock and sector prices. Our results indicate that deep learning models can offer superior predictive accuracy compared to traditional time series models, but they often require more computational resources and data. While the benefits of deep learning models should be considered, it is essential to consider the advantages of time series models, such as their simplicity, interpretability, and computational efficiency. Time series models can provide valuable insights into the underlying drivers of stock and sector prices, and they can be a practical choice for applications with limited resources. Moreover, time series models can generate long-term forecasts, making them useful for planning and decision-making.\nDespite the robust findings of our analysis, there were some limitations to our research. One notable limitation is that our analysis did not find a significant relationship between GDP growth rate and interest rate, and stock price, contrary to prior research in this area. While our research is consistent with some recent studies, this finding highlights the need for further research to investigate the complex relationship between macroeconomic factors and stock market performance.\nOne possible explanation for the lack of a significant relationship between GDP growth rate, interest rate, and stock price is that our analysis needed to account for all the potential factors that may impact the stock market. For example, other factors such as political events, company-specific news, and investor sentiment changes may also significantly impact the stock market and sector market.\n\nMoreover, our analysis only focused on the US macroeconomic factors, which may limit the generalizability of our findings. The performance of the US stock market and sector market is also influenced by factors outside the United States, such as international trade policies and geopolitical events. Thus, future research could expand on our analysis by examining the impact of global macroeconomic factors on the US stock market and sector market.\nDespite these limitations, our research provides valuable insights into the relationship between macroeconomic factors and the US stock market and sector market. Our findings can assist investors, businesses, and policymakers make informed decisions based on macroeconomic conditions and provide a framework for further research."
  },
  {
    "objectID": "dv_macroeconomic_factor.html",
    "href": "dv_macroeconomic_factor.html",
    "title": "",
    "section": "",
    "text": "Code#import the data\ngdp <- read.csv(\"DATA/RAW DATA/gdp-growth.csv\")\n\n#change date format\ngdp$DATE <- as.Date(gdp$DATE , \"%m/%d/%Y\")\n\n#plot gdp growth rate \nfig <- plot_ly(gdp, x = ~DATE, y = ~value, type = 'scatter', mode = 'lines',line = list(color = 'rgb(179, 210, 165)'))\nfig <- fig %>% layout(title = \"U.S GPD Growth Rate: 2010 - 2022\",xaxis = list(title = \"Time\"),yaxis = list(title =\"GDP Growth Rate\"))\nfig\n\n\n\n\n\nThe period between 2010 and 2022 has been characterized by moderate fluctuations in economic growth. After a period of slow growth following the 2008 financial crisis, the US economy experienced a notable uptick in GDP growth from 2012 to 2015, with rates reaching a peak of 2.9% in 2015. However, growth rates began to decline again from 2016 to 2019, falling to a low of 2.2% in 2019. The COVID-19 pandemic caused a sharp contraction in the economy in 2020, with GDP growth falling by 3.5%, the largest annual decline since the 1940s. However, there was a partial recovery in 2021, with growth rates projected to reach 6.3% by the end of the year, reflecting a combination of fiscal stimulus measures and the easing of pandemic-related restrictions. Overall, the trend in GDP growth rate in the United States from 2010 to 2022 has been characterized by moderate fluctuations, with notable shifts in response to both domestic and global economic conditions.\n\n\nCode#import the data\ninterest_data <- read.csv(\"DATA/RAW DATA/interest-rate.csv\")\n\n#change date format\ninterest_data$Date <- as.Date(interest_data$Date , \"%m/%d/%Y\")\n\n#plot interest rate \nfig <- plot_ly(interest_data, x = ~Date, y = ~value, type = 'scatter', mode = 'lines',line = list(color = 'rgb(59, 14, 37)'))\nfig <- fig %>% layout(title = \"U.S Interest Rate: January 2010 - March 2023\",xaxis = list(title = \"Time\"),yaxis = list(title =\"Interest Rate\"))\nfig\n\n\n\n\n\nThe graph of the real interest rate in the US from 2010 to March 2023 shows a general trend of volatility and fluctuation. The real interest rate is a measure of the cost of borrowing for the US government and is adjusted for inflation.\nFrom 2010 to mid-2012, the real interest rate remained relatively low and stable, with only minor fluctuations.This increase in the real interest rate was likely due to concerns about inflation and the impact of the US government’s monetary policy measures. From mid-2013 to mid-2016, the real interest rate remained relatively low, with only minor fluctuations. However, from mid-2016 to mid-2018, there was another sharp increase in the real interest rate. This increase was driven by a combination of factors, including the improving US economy, rising inflation expectations, and the Federal Reserve’s decision to raise interest rates.\nFrom mid-2018 to mid-2019, the real interest rate declined sharply, and then remained relatively stable at lower levels until early 2021. This decline was largely due to concerns about a slowing global economy, trade tensions, and the impact of the COVID-19 pandemic. Since early 2021, the real interest rate has been increasing again, and it remains at a relatively high level as of March 2023. This increase may be due to concerns about inflation, the impact of government stimulus measures, and the possibility of an economic recovery.\nOverall, the trend of the real interest rate in the US from 2010 to March 2023 has been characterized by periods of volatility and fluctuation, driven by a range of economic and policy factors.\n\n\nCode#import the data\ninflation_rate <- read.csv(\"DATA/RAW DATA/inflation-rate.csv\")\n\n\n#cleaning the data\n#remove unwanted columns\ninflation_rate_clean <- subset(inflation_rate, select = -c(1,HALF1,HALF2))\n\n#convert the data to time series data\ninflation_data_ts <- ts(as.vector(t(as.matrix(inflation_rate_clean))), start=c(2010,1), end=c(2023,2), frequency=12)\n\n#plot inflation rate \nfig <- autoplot(inflation_data_ts, ylab = \"Inflation Rate\", color=\"#8B8695\")+ggtitle(\"U.S Inflation Rate: January 2010 - February 2023\")+theme_bw()\nggplotly(fig)\n\n\n\n\n\nThe U.S inflation rate from January 2010 to February 2023 has been a topic of concern for many individuals and businesses. From 2010 to 2012, the inflation rate remained relatively low. However, from 2013 to 2023, the inflation rate fluctuated significantly. The COVID-19 pandemic had a significant impact on the inflation rate, causing it to rise rapidly in 2021 due to supply chain disruptions and other factors. The Federal Reserve has implemented various policies to try and manage the inflation rate, including adjusting interest rates and reducing bond purchases. The U.S inflation rate remains a closely watched indicator of economic health, and its fluctuations can have significant impacts on individuals, businesses, and the broader economy.\n\n\nCode#import the data\nunemployment_rate <- read.csv(\"DATA/RAW DATA/unemployment-rate.csv\")\n\n#change date format\nunemployment_rate$Date <- as.Date(unemployment_rate$Date , \"%m/%d/%Y\")\n\n#plot unemployment rate \n#plot interest rate \nfig <- plot_ly(unemployment_rate, x = ~Date, y = ~Value, type = 'scatter', mode = 'lines',line = list(color = 'rgb(235, 231, 115)'))\nfig <- fig %>% layout(title = \"U.S Unemployment Rate: January 2010 - March 2023\",xaxis = list(title = \"Time\"),yaxis = list(title =\"Unemployment Rate\"))\nfig\n\n\n\n\n\nThe U.S unemployment rate has experienced significant fluctuations between January 2010 and February 2023. Following the Great Recession of 2008, the unemployment rate peaked in October 2009, but gradually decreased to by January 2010. Throughout the years, the rate has continued to fluctuate, before increasing in April 2020 due to the COVID-19 pandemic. However, as the pandemic-related restrictions were lifted and the economy started to recover, the unemployment rate began to decline. Despite the progress made in recent years, the unemployment rate remains a significant economic indicator and a source of concern for policymakers, as persistent unemployment can have long-term effects on the economy and the well-being of individuals and families."
  },
  {
    "objectID": "dv_sector_market.html",
    "href": "dv_sector_market.html",
    "title": "",
    "section": "",
    "text": "On the stock market, a sector is a group of stocks that are all in the same industry and are very similar to each other. The Global Industrial Classification Standard, which is the most common way to group things, says that there are 11 different stock market sectors (GICS).\n\n\n\n\nName\nSymbol\n\n\n\nConsumer Staples Sector Fund\nXLP\n\n\nUtilities Sector Fund\nXLU\n\n\nHealth Care Sector Fund\nXLV\n\n\nIndustrial Sector Fund\nXLI\n\n\nFinancial Sector Fund\nXLF\n\n\nConsumer Discretionary Sector Fund\nXLY\n\n\nCommunication Services Sector Fund\nXLC\n\n\nReal Estate Sector Fund\nXLRE\n\n\nMaterials Sector Fund\nXLB\n\n\nTechnology Sector Fund\nXLK\n\n\nEnergy Sector Fund\nXLE\n\n\n\n\n\n\n\nView the visualization\n\nThere are 11 parts to the sector market, and each has companies with good stock prices. When we look at the graph, we can see that the Consumer Discretionary sector has higher stock prices than other sectors. This is because there is a chance for high returns, especially when the economy is doing well and consumers are spending a lot. Because they are near the middle of the risk spectrum, stocks in the Financial Sector are worth the least. They can be prone to recessions and are sensitive to changes in interest rates, to name just two major risks. But like most other kinds of businesses, the risk of bank stocks can vary a lot from one company to the next. We can also guess that most sector prices have been going up since 2021, which could be because of post covid."
  },
  {
    "objectID": "dv_stock_indices.html",
    "href": "dv_stock_indices.html",
    "title": "",
    "section": "",
    "text": "Stock market indexes all over the world are good measures of both the world economy and the economies of individual countries. In the United States, the S&P 500, the Dow Jones Industrial Average, and the Nasdaq Composite get the most attention from investors and the media. There are more than just these three indexes that make up the U.S. stock market. About 5,000 more are there.\nWith so many indexes, the U.S. market has many ways to classify things and methods that can be used for many different things. Most of the time, the news tells us several times a day how the top three indexes are going, using important news stories to show how they are going up or down. Investment managers use indexes to measure how well an investment is doing.\nIndexes are used by all types of investors as proxies for performance and guides for how to put their money to work. Indexes are also the basis for passive index investing, which is usually done through exchange-traded funds that track indexes. Overall, knowing how market indexes are made and how they are used can make many different types of investing easier to understand.\n\nCodeoptions(\"getSymbols.warning4.0\"=FALSE)\noptions(\"getSymbols.yahoo.warning\"=FALSE)\n\n#America's top stock market index\ntickers = c(\"^GSPC\",\"^DJI\",\"^IXIC\")\n\nfor (i in tickers){\n  getSymbols(i,\n             from = \"2000-01-01\",\n             to = \"2023-01-01\")}\n\nx <- list(\n  title = \"date\"\n)\ny <- list(\n  title = \"value\"\n)\n\n#create dataframe\ndv_america_stock_index_data = cbind(GSPC,DJI,IXIC)\ndv_america_stock_index_data = as.data.frame(dv_america_stock_index_data)\n#export it to csv file\nwrite_csv(dv_america_stock_index_data, \"DATA/RAW DATA/dv_america_stock_index_data.csv\")\n\nstock <- data.frame(GSPC$GSPC.Adjusted,\n                    DJI$DJI.Adjusted,\n                    IXIC$IXIC.Adjusted)\n\n\n\nstock <- data.frame(stock,rownames(stock))\ncolnames(stock) <- append(tickers,'Dates')\n\nstock$date<-as.Date(stock$Dates,\"%Y-%m-%d\")\ncolnames(stock)=c(\"GSPC\",\"DJI\",\"IXIC\",\"Dates\",\"date\")\n\n\n#remove columns\nstock <- stock[,-c(4)]\n\ng1<- ggplot(stock, aes(x=date)) +\n  geom_line(aes(y=GSPC, colour=\"GSPC\"))+\n  geom_line(aes(y=DJI, colour=\"DJI\"))+\n  geom_line(aes(y=IXIC, colour=\"IXIC\"))+\n  scale_color_brewer(palette=\"Greens\")+\n  theme_bw()+\n   labs(\n    title = \"America's Top 3 Stock Market Index History\",\n    subtitle = \"From Jan 2000-Jan 2023\",\n    x = \"Date\",\n    y = \"Adjusted Closing Prices\")+\n    guides(colour=guide_legend(title=\"Indices\")) \n\nplot = ggplotly(g1)%>%\n  layout(title = list(text = paste0(\"America's Top 3 Stock Market Index History\",\n                                    '<br>',\n                                    '<sup>',\n                                    'From Jan 2000-Jan 2023',\n                                    '</sup>')))\nggplotly(plot)%>%layout(hovermode = \"x\")\n\n\n\n\n\nEach of the indices’ stock prices tend to have a upward trend. This is because the companies in each of the indices are growing. Compared to S&P 500 and NASDAQ, the Dow Jones index has the largest share. This could be because of the companies in each index and how many companies are in each. The effect of covud on all businesses causes stock prices to drop at the beginning of 2020.\n\nThe Dow Jones Industrial Average (DJIA) is one of the oldest, best-known, and most-used indexes in the world. It has the shares of 30 of the biggest and most powerful companies in the US.The DJIA is an index based on prices. At first, it was made by adding up the price per share of each company’s stock in the index and dividing by the number of companies. The index is no longer this easy to figure out, though. Over time, stock splits, spin-offs, and other things have changed the divisor, which is a number that Dow Jones uses to figure out the level of the DJIA. This has made the divisor a very small number.\nAbout a quarter of the value of the whole U.S. stock market is represented by the DJIA, but a percent change in the Dow is not a sure sign that the whole market has dropped by the same percent. When the Dow goes up or down, it shows how investors feel about the earnings and risks of the big companies in the index. Because the way people feel about large-cap stocks is often different from how they feel about small-cap stocks, international stocks, or technology stocks, the Dow shouldn’t be used to show how people feel about other types of stocks in the market.\nIn general, the Dow is known for having a list of the best blue-chip companies on the U.S. market that pay regular dividends. So, it doesn’t have to be a reflection of the whole market, but it can be a reflection of the market for blue-chip, dividend-value stocks.\n\n\n\n\n\nCompany\nSymbol\nWeight\n\n\n\nUnitedHealth Group Incorporated\nUNH\n9.531581\n\n\nGoldman Sachs Group, Inc.\nGS\n7.240363\n\n\nHome Depot Inc.\nHD\n6.282804\n\n\nMcDonald’s Corporation\nMCD\n5.199097\n\n\nMicrosoft Corporation\nMSFT\n5.127123\n\n\nCaterpillar Inc.\nCAT\n4.821433\n\n\nAmgen Inc.\nAMGN\n4.580870\n\n\nVisa Inc. Class A\nV\n4.416778\n\n\nBoeing Company\nBA\n4.150398\n\n\nHoneywell International Inc.\nHON\n3.899078\n\n\n\n\n\n\nCodeoptions(\"getSymbols.warning4.0\"=FALSE)\noptions(\"getSymbols.yahoo.warning\"=FALSE)\n\ntickers = c(\"UNH\",\"GS\",\"HD\", \"MCD\",\"MSFT\",\"CAT\",\"AMGN\",\"V\",\"BA\",\"HON\")\nfor (i in tickers){\n  getSymbols(i,\n             from = \"2012-01-01\",\n             to = \"2023-01-01\")}\n\nx <- list(\n  title = \"date\"\n)\ny <- list(\n  title = \"value\"\n)\n\nstock <- data.frame(UNH$UNH.Adjusted,\n                    GS$GS.Adjusted,\n                    HD$HD.Adjusted,\n                    MCD$MCD.Adjusted,\n                    MSFT$MSFT.Adjusted,\n                    CAT$CAT.Adjusted,\n                    AMGN$AMGN.Adjusted,\n                    V$V.Adjusted,\n                    HON$HON.Adjusted,\n                    BA$BA.Adjusted)\n\nstock <- data.frame(stock,rownames(stock))\ncolnames(stock) <- append(tickers,'Dates')\n\nstock$date<-as.Date(stock$Dates,\"%Y-%m-%d\")\n\n\ng1<- ggplot(stock, aes(x=date)) +\n  geom_line(aes(y=UNH, colour=\"UNH\"))+\n  geom_line(aes(y=GS, colour=\"GS\"))+\n  geom_line(aes(y=HD, colour=\"HD\"))+\n  geom_line(aes(y=MCD, colour=\"MCD\"))+\n  geom_line(aes(y=MSFT, colour=\"MSFT\"))+\n  geom_line(aes(y=CAT, colour=\"CAT\"))+\n  geom_line(aes(y=AMGN, colour=\"AMGN\"))+\n  geom_line(aes(y=V, colour=\"V\"))+\n  geom_line(aes(y=HON, colour=\"HON\"))+\n  geom_line(aes(y=BA, colour=\"BA\"))+\n  scale_color_brewer(palette=\"OrRd\")+\n  theme_bw()+\n   labs(\n    title = \"Stock Prices for the Top 10 Dow Jones Companies\",\n    subtitle = \"From Jan 2012-Jan 2023\",\n    x = \"Date\",\n    y = \"Adjusted Closing Prices\")+\n    guides(colour=guide_legend(title=\"Companies\")) \n\nplot = ggplotly(g1)%>%\n  layout(title = list(text = paste0(\"Stock Prices for the Top 10 Dow Jones Companies\",\n                                    '<br>',\n                                    '<sup>',\n                                    'From Jan 2012-Jan 2023',\n                                    '</sup>')))\nggplotly(plot)%>%layout(hovermode = \"x\")\n\n\n\n\n\nHere, the top 10 companies in the Dow Jones Index are shown as a time series. The companies are sorted by how much they make up the index. As it is clear that all the companies tend to go up, we can see that UNH has the biggest share of stock prices compared to the other companies. There has been a drop in the price of Home Depot Inc.’s stock, which was one of the best-performing stocks from 2018 to 2020. However, the drop may have been caused more by macro conditions and negative sentiment than by problems with the company itself or investors’ worries about a slowdown in the home improvement market. Because of the effect of covid, the price of UNH stock is going up. People started investing in health insurance, but the price has gone up and down because of this. Goldman Sachs Group, Inc. has a good stock price, but the price has gone up and down because of the pandemic in the fourth quarter. This was caused by weakness in investment banking and asset management, as well as a large loss in the unit that includes its consumer banking business.\n\nMost investors know that the Nasdaq is where tech stocks trade. The Nasdaq Composite Index is a list of all the stocks that are traded on the Nasdaq stock exchange. It is based on how much each stock is worth on the market. Some of the companies in this index are not from the U.S. People know that this index has a lot of tech companies in it. It has things from the tech market like software, biotech, semiconductors, and more.\nThere are a lot of technology stocks in this index, but there are also stocks from other industries. Investors can also buy securities from a wide range of industries, such as financials, industrials, insurance, transportation, and others.\nThere are both big and small companies in the Nasdaq Composite. However, unlike the Dow and the S&P 500, it also has a lot of small, risky companies. So, its movement is usually a good sign of how well the technology industry is doing and how investors feel about riskier stocks.\n\n\n\n\n\nCompany\nSymbol\nWeight\n\n\n\nApple Inc\nAAPL\n12.230\n\n\nMicrosoft Corp\nMSFT\n12.101\n\n\nAmazon.com Inc\nAMZN\n6.226\n\n\nNVIDIA Corp\nNVDA\n4.366\n\n\nTesla Inc\nTSLA\n3.967\n\n\nAlphabet Inc\nGOOG\n3.625\n\n\nAlphabet Inc\nGOOGL\n3.616\n\n\nMeta Platforms Inc\nMETA\n3.128\n\n\nBroadcom Inc\nAVGO\n1.962\n\n\nPepsiCo Inc\nPEP\n1.951\n\n\n\n\n\n\nCodeoptions(\"getSymbols.warning4.0\"=FALSE)\noptions(\"getSymbols.yahoo.warning\"=FALSE)\n\ntickers = c(\"AAPL\",\"MSFT\",\"AMZN\", \"NVDA\",\"TSLA\",\"GOOG\",\"PEP\",\"GOOGL\",\"META\",\"AVGO\")\nfor (i in tickers){\n  getSymbols(i,\n             from = \"2015-01-01\",\n             to = \"2023-01-01\")}\n\nx <- list(\n  title = \"date\"\n)\ny <- list(\n  title = \"value\"\n)\n\nstock <- data.frame(AAPL$AAPL.Adjusted,\n                    MSFT$MSFT.Adjusted,\n                    AMZN$AMZN.Adjusted,\n                    NVDA$NVDA.Adjusted,\n                    TSLA$TSLA.Adjusted,\n                    GOOG$GOOG.Adjusted,\n                    GOOGL$GOOGL.Adjusted,\n                    META$META.Adjusted,\n                    PEP$PEP.Adjusted,\n                    AVGO$AVGO.Adjusted)\n\nstock <- data.frame(stock,rownames(stock))\ncolnames(stock) <- append(tickers,'Dates')\n\nstock$date<-as.Date(stock$Dates,\"%Y-%m-%d\")\n\n\ng2<- ggplot(stock, aes(x=date)) +\n  geom_line(aes(y=AAPL, colour=\"AAPL\"))+\n  geom_line(aes(y=MSFT, colour=\"MSFT\"))+\n  geom_line(aes(y=AMZN, colour=\"AMZN\"))+\n  geom_line(aes(y=NVDA, colour=\"NVDA\"))+\n  geom_line(aes(y=TSLA, colour=\"TSLA\"))+\n  geom_line(aes(y=GOOG, colour=\"GOOG\"))+\n  geom_line(aes(y=GOOGL, colour=\"GOOGL\"))+\n  geom_line(aes(y=META, colour=\"META\"))+\n  geom_line(aes(y=PEP, colour=\"PEP\"))+\n  geom_line(aes(y=AVGO, colour=\"AVGO\"))+\n  scale_color_brewer(palette=\"PuRd\")+\n  theme_bw()+\n   labs(\n    title = \"Stock Prices for the Top 10 NASDAQ Companies\",\n    subtitle = \"From Jan 2015-Jan 2023\",\n    x = \"Date\",\n    y = \"Adjusted Closing Prices\")+\n    guides(colour=guide_legend(title=\"Companies\")) \n\nplot = ggplotly(g2)%>%\n  layout(title = list(text = paste0(\"Stock Prices for the Top 10 NASDAQ Companies\",\n                                    '<br>',\n                                    '<sup>',\n                                    'From Jan 2015-Jan 2023',\n                                    '</sup>')))\nggplotly(plot)%>%layout(hovermode = \"x\")\n\n\n\n\n\nThe weights of the top 10 companies in the NASDAQ Index are used to filter the time series shown here. Broadcom Inc.’s stock price is high compared to others, but it goes up and down a lot. The price drop is mostly due to the fact that the company’s earnings growth will slow in fiscal 2019, while the price rise is due to its designs for data centers and networking. When you look at other companies, you can see that they all follow the same pattern. Most of them see a drop in their stock prices at the beginning of 2020, which is because of the covid. Google’s stock price has gone down recently, which is because its AI chatbot, Bard, gave a wrong answer.\n\nThe Standard & Poor’s 500 Index, or S&P 500, is a list of the 500 best companies in the United States. Stocks are chosen for the index based on their market capitalization, but the constituent committee also looks at their liquidity, public float, sector classification, financial stability, and trading history.\nThe S&P 500 Index contains about 80% of the total value of the U.S. stock market. The S&P 500 Index is a good way to get a general idea of how the whole U.S. market is doing. Most indexes are based on what something is worth on the market. The S&P 500 Index is the market-weighted index (also referred to as capitalization-weighted).\nSo, the weight of each stock in the index is the same as its total market capitalization. In other words, the value of the index falls by 10% if the market value of all 500 companies in the S&P 500 falls by 10%.\n\n\n\n\n\nCompany\nSymbol\nWeight\n\n\n\nApple Inc.\nAAPL\n6.711304\n\n\nMicrosoft Corporation\nMSFT\n5.705910\n\n\nAmazon.com Inc.\nAMZN\n2.543539\n\n\nAlphabet Inc. Class A\nGOOGL\n1.665711\n\n\nBerkshire Hathaway Inc. Class B\nBRK.B\n1.621257\n\n\nNVIDIA Corporation\nNVDA\n1.601427\n\n\nTesla Inc\nTSLA\n1.583414\n\n\nAlphabet Inc. Class C\nGOOG\n1.480755\n\n\nExxon Mobil Corporation\nXOM\n1.391616\n\n\nUnitedHealth Group Incorporated\nUNH\n1.329559\n\n\n\n\n\n\nCodeoptions(\"getSymbols.warning4.0\"=FALSE)\noptions(\"getSymbols.yahoo.warning\"=FALSE)\n\ntickers = c(\"AAPL\",\"MSFT\",\"AMZN\", \"NVDA\",\"TSLA\",\"GOOGL\",\"GOOG\",\"BRK\",\"UNH\",\"XOM\")\nfor (i in tickers){\n  getSymbols(i,\n             from = \"2015-01-01\",\n             to = \"2023-01-01\")}\n\nx <- list(\n  title = \"date\"\n)\ny <- list(\n  title = \"value\"\n)\n\nstock <- data.frame(AAPL$AAPL.Adjusted,\n                    MSFT$MSFT.Adjusted,\n                    AMZN$AMZN.Adjusted,\n                    NVDA$NVDA.Adjusted,\n                    TSLA$TSLA.Adjusted,\n                    GOOG$GOOG.Adjusted,\n                    GOOGL$GOOGL.Adjusted,\n                    BRK$BRK.Adjusted,\n                    UNH$UNH.Adjusted,\n                    XOM$XOM.Adjusted)\n\nstock <- data.frame(stock,rownames(stock))\ncolnames(stock) <- append(tickers,'Dates')\n\nstock$date<-as.Date(stock$Dates,\"%Y-%m-%d\")\n\n\ng3<- ggplot(stock, aes(x=date)) +\n  geom_line(aes(y=AAPL, colour=\"AAPL\"))+\n  geom_line(aes(y=MSFT, colour=\"MSFT\"))+\n  geom_line(aes(y=AMZN, colour=\"AMZN\"))+\n  geom_line(aes(y=NVDA, colour=\"NVDA\"))+\n  geom_line(aes(y=TSLA, colour=\"TSLA\"))+\n  geom_line(aes(y=GOOG, colour=\"GOOG\"))+\n  geom_line(aes(y=GOOGL, colour=\"GOOGL\"))+\n  geom_line(aes(y=BRK, colour=\"BRK\"))+\n  geom_line(aes(y=UNH, colour=\"UNH\"))+\n  geom_line(aes(y=XOM, colour=\"XOM\"))+\n  scale_color_brewer(palette=\"GnBu\")+\n  theme_bw()+\n   labs(\n    title = \"Stock Prices for the Top 10 S&P 500 Companies\",\n    subtitle = \"From Jan 2015-Jan 2023\",\n    x = \"Date\",\n    y = \"Adjusted Closing Prices\")+\n    guides(colour=guide_legend(title=\"Companies\")) \n\nplot = ggplotly(g3)%>%\n  layout(title = list(text = paste0(\"Stock Prices for the Top 10 S&P 500 Companies\",\n                                    '<br>',\n                                    '<sup>',\n                                    'From Jan 2015-Jan 2023',\n                                    '</sup>')))\nggplotly(plot)%>%layout(hovermode = \"x\")\n\n\n\n\n\nThe time series shown here is filtered by the weights of the top 10 companies in the S&P 500 Index. How much each company makes up the index is used to sort the companies. Since it’s clear that all the companies’ stock prices tend to go up, we can see that UNH has the most stock prices compared to the other companies. Most of their stock prices will go down at the start of 2020 because of the covid. Recently, Google’s stock price went down because its artificial intelligence chatbot, Bard, gave the wrong answer. The price of TESLA stock has been going down since early 2022. This is because investors worry that CEO Elon Musk is too busy with his plan to take over Twitter."
  },
  {
    "objectID": "dv.html",
    "href": "dv.html",
    "title": "",
    "section": "",
    "text": "Data visualization plays a crucial role in understanding and analyzing complex financial data, such as stock market indices. In the United States, the stock market is represented by various indices, including the widely known S&P 500, Dow Jones Industrial Average (DJIA), and Nasdaq Composite. These indices are used as indicators of the overall health and performance of the U.S. economy and are closely monitored by investors, financial analysts, and the media.\nThis data visualization focuses on three prominent U.S. stock market indices(DJI, S&P 500 and Nasdaq Composite) and provides insights into the top companies within these indices based on their weightage. The DJI, consisting of 30 large and established companies, is known for its historical significance and representation of blue-chip stocks. S&P 500 is known for its diverse representation of the U.S. stock market, encompassing 500 of the largest publicly traded companies across various industries. As a market-capitalization-weighted index, the S&P 500 reflects changes in the stock prices of these companies, making it a widely recognized benchmark for measuring the performance of the U.S. stock market. On the other hand, the Nasdaq Composite is known for its focus on technology companies, but also includes stocks from other industries.\n\nBy visualizing the stock prices of the top companies within these indices over time, we can gain valuable insights into their trends, performance, and the impact of external factors such as the COVID-19 pandemic. This analysis can be useful for investors, financial professionals, and anyone interested in understanding the dynamics of the U.S. stock market and the performance of top companies within these indices.\nClick to view the data visualizations\n\n\nThe stock market is comprised of various sectors, which are groups of companies operating in the same industry and sharing similar characteristics. The Global Industry Classification Standard (GICS) is a widely used framework that classifies stocks into 11 different sectors, including Consumer Discretionary, Consumer Staples, Energy, Financials, Health Care, Industrials, Information Technology, Materials, Real Estate, Telecommunication Services, and Utilities.\n\nData visualization of the sector market stocks provides a valuable tool for investors, financial analysts, and decision-makers to gain insights into the performance of different sectors within the stock market. By visually representing stock prices, trends, and performance of sectors over time, data visualization can help identify investment opportunities, assess risks, and make informed decisions.\nClick to view the data visualizations\n\n\n\nMacroeconomic factors play a crucial role in shaping the overall health and performance of an economy. These factors, including gross domestic product (GDP) growth rate, interest rate, inflation rate, and others, have a significant impact on businesses, consumers, and policymakers. Monitoring and understanding these macroeconomic factors is essential for making informed decisions, formulating economic policies, and assessing the overall health of an economy.\n\nVisualizing macroeconomic factors can provide valuable insights and help stakeholders better understand the trends, patterns, and relationships between these factors. Through visualizations, complex data can be presented in a clear and concise manner, allowing for easier interpretation and analysis. Visualizations can highlight historical trends, identify potential correlations, and provide a visual context for understanding the impact of policy changes or external events on macroeconomic factors.\nIn this analysis, we will use visualizations to explore and analyze key macroeconomic factors, including GDP growth rate, interest rate, and inflation rate in the United States from 2010 to present. Through these visualizations, we aim to provide a comprehensive overview of the trends and dynamics of these macroeconomic factors over time, highlighting important events or shifts that may have influenced their trajectories. By visualizing these macroeconomic factors, we can gain a deeper understanding of their impact on the U.S. economy and make informed assessments about their future implications.\nClick to view the data visualizations"
  },
  {
    "objectID": "eda_index_dow_jones.html",
    "href": "eda_index_dow_jones.html",
    "title": "EDA for Dow Jones Index",
    "section": "",
    "text": "Exploratory Data Analysis (EDA) for the Dow Jones Index involves analyzing time series data to identify the underlying patterns and characteristics of the index. The Dow Jones is a stock market index that measures the performance of 30 large companies listed on the US stock exchanges. EDA for the Dow Jones typically involves analyzing the daily closing prices of the index and examining key aspects such as autocorrelation, seasonality, trend, and stationarity. This information can be used to identify potential patterns and trends in the data, inform our modeling approach, and potentially improve our investment strategies.\n\nTime Series Plot\n\n\nCode\n# get data\noptions(\"getSymbols.warning4.0\"=FALSE)\noptions(\"getSymbols.yahoo.warning\"=FALSE)\n\n\ndata = getSymbols(\"^DJI\",src='yahoo', from = '2010-01-01',to = \"2023-03-01\")\n\ndf <- data.frame(Date=index(DJI),coredata(DJI))\n\n# create Bollinger Bands\nbbands <- BBands(DJI[,c(\"DJI.High\",\"DJI.Low\",\"DJI.Close\")])\n\n# join and subset data\ndf <- subset(cbind(df, data.frame(bbands[,1:3])), Date >= \"2010-01-01\")\n\n#export the data \ndji_data <- df\nwrite.csv(dji_data, \"DATA/CLEANED DATA/dji_raw_data.csv\", row.names=FALSE)\n\n# colors column for increasing and decreasing\nfor (i in 1:length(df[,1])) {\n  if (df$DJI.Close[i] >= df$DJI.Open[i]) {\n      df$direction[i] = 'Increasing'\n  } else {\n      df$direction[i] = 'Decreasing'\n  }\n}\n\ni <- list(line = list(color = '#6F9860'))\nd <- list(line = list(color = '#7F7F7F'))\n\n# plot candlestick chart\n\nfig <- df %>% plot_ly(x = ~Date, type=\"candlestick\",\n          open = ~DJI.Open, close = ~DJI.Close,\n          high = ~DJI.High, low = ~DJI.Low, name = \"DJI\",\n          increasing = i, decreasing = d) \nfig <- fig %>% add_lines(x = ~Date, y = ~up , name = \"B Bands\",\n            line = list(color = '#ccc', width = 0.5),\n            legendgroup = \"Bollinger Bands\",\n            hoverinfo = \"none\", inherit = F) \nfig <- fig %>% add_lines(x = ~Date, y = ~dn, name = \"B Bands\",\n            line = list(color = '#ccc', width = 0.5),\n            legendgroup = \"Bollinger Bands\", inherit = F,\n            showlegend = FALSE, hoverinfo = \"none\") \nfig <- fig %>% add_lines(x = ~Date, y = ~mavg, name = \"Mv Avg\",\n            line = list(color = '#E377C2', width = 0.5),\n            hoverinfo = \"none\", inherit = F) \nfig <- fig %>% layout(yaxis = list(title = \"Price\"))\n\n# plot volume bar chart\nfig2 <- df \nfig2 <- fig2 %>% plot_ly(x=~Date, y=~DJI.Volume, type='bar', name = \"DJI Volume\",\n          color = ~direction, colors = c('#6F9860','#7F7F7F')) \nfig2 <- fig2 %>% layout(yaxis = list(title = \"Volume\"))\n\n# create rangeselector buttons\nrs <- list(visible = TRUE, x = 0.5, y = -0.055,\n           xanchor = 'center', yref = 'paper',\n           font = list(size = 9),\n           buttons = list(\n             list(count=1,\n                  label='RESET',\n                  step='all'),\n             list(count=3,\n                  label='3 YR',\n                  step='year',\n                  stepmode='backward'),\n             list(count=1,\n                  label='1 YR',\n                  step='year',\n                  stepmode='backward'),\n             list(count=1,\n                  label='1 MO',\n                  step='month',\n                  stepmode='backward')\n           ))\n\n# subplot with shared x axis\nfig <- subplot(fig, fig2, heights = c(0.7,0.2), nrows=2,\n             shareX = TRUE, titleY = TRUE)\nfig <- fig %>% layout(title = paste(\"DOW Jones Index Stock Price: January 2010 - March 2023\"),\n         xaxis = list(rangeselector = rs),\n         legend = list(orientation = 'h', x = 0.5, y = 1,\n                       xanchor = 'center', yref = 'paper',\n                       font = list(size = 10),\n                       bgcolor = 'transparent'))\n\nfig\n\n\n\n\n\n\nThe Dow Jones Industrial Average (DJIA) stock prices from 2010 to March 2023 have displayed a volatile trend, reflecting the changes in the US stock market during this period. The DJIA witnessed a steady growth in the early 2010s, achieving new all-time highs by 2013. However, it experienced a sharp correction in the second half of 2015 and early 2016, followed by a quick recovery. The DJIA hit new highs in 2018 and 2019, only to be impacted by the COVID-19 pandemic in 2020, resulting in a significant decline followed by a quick recovery aided by government stimulus measures. The DJIA reached a new all-time high in May 2021.\nInflation has played a crucial role in shaping the DJIA stock prices during this period. The US experienced moderate inflation rates during the early 2010s, and inflation remained subdued until the COVID-19 pandemic hit in 2020, causing significant disruptions to supply chains, resulting in higher prices for goods and services. Inflation rates surged in 2021, leading to concerns about its impact on the economy and the stock market. However, the Federal Reserve has indicated that the current inflationary pressures are transitory.\nFor stock prices, a multiplicative decomposition is typically preferred because the percentage changes in stock prices tend to be more important than the absolute changes. For example, a $1 increase in a $10 stock price is more significant than a $1 increase in a $100 stock price. Therefore, the relative changes in stock prices are more relevant than the absolute changes. Additionally, stock prices tend to exhibit non-constant variance, meaning that the variance of the series changes over time. A multiplicative decomposition can handle this non-constant variance more effectively than an additive decomposition.\n\n\nDecomposed Time Series\n\nDecomposition PlotAdjusted Decomposition Plot\n\n\n\n\nCode\n#time series data\nmyts<-ts(df$DJI.Adjusted,frequency=252,start=c(2010,1,1)) \n#original plot for time series data\norginial_plot <- autoplot(myts,xlab =\"Year\", ylab = \"Adjusted Closing Price\", main = \"DOW Jones Index Stock price: Jan 2010 - March 2023\")\n#decompose the data\ndecompose = decompose(myts, \"multiplicative\")\n#decomposition plot\nautoplot(decompose)\n\n\n\n\n\n\n\n\n\nCode\n#adjusted plot\ntrendadj <- myts/decompose$trend\ndecompose_adjtrend_plot <- autoplot(trendadj,ylab='trend') +ggtitle('Adjusted trend component in the multiplicative time series model')\nseasonaladj <- myts/decompose$seasonal\ndecompose_adjseasonal_plot <- autoplot(seasonaladj,ylab='seasonal') +ggtitle('Adjusted seasonal component in the multiplicative time series model')\ngrid.arrange(orginial_plot, decompose_adjtrend_plot,decompose_adjseasonal_plot, nrow=3)\n\n\n\n\n\n\n\n\nThe adjusted seasonal component tend to have upward trend and there is more variability in the model when compared to the original plot where the variation during the years but the adjusted trend then to have more fluctuation showing no trend when compared to the original plot.\n\n\nLag Plots\n\nDaily Time LagsMonthly Time Lags\n\n\n\n\nCode\n#Lag plots \ngglagplot(myts, do.lines=FALSE, lags=1)+xlab(\"Lag 1\")+ylab(\"Yi\")+ggtitle(\"Lag Plot for DOW Jones Index Stock Jan 2010 - March 2023\")\n\n\n\n\n\n\n\n\n\nCode\n#montly data\nmean_data <- df %>% \n  mutate(month = month(Date), year = year(Date)) %>% \n  group_by(year, month) %>% \n  summarize(mean_value = mean(DJI.Adjusted))\nmonth<-ts(mean_data$mean_value,star=decimal_date(as.Date(\"2010-01-01\",format = \"%Y-%m-%d\")),frequency = 12)\n#Lag plot\nts_lags(month)\n\n\n\n\n\n\n\n\n\nLag Plot for DOW Jones Index Stock Jan 2010 - March 2023, there should be a strong link between the series and the related lag as there are positive correlation and inclined to 45 degree.This is the lag plot signature of a process with strong positive autocorrelation. Such processes are highly non-random, there is strong association between an observation and a succeeding observation. Additionally, seasonality can be examined by plotting observations for a larger number of time periods i.e. the lags. Using the mean function, the time series data is aggregated to monthly data for better understanding of the series and for the clearer plots. Observing the last graph closely reveals that more dots are on to the diagonal line at 45 degrees.the second graph indicates the monthly of the variable on the vertical axis. The lines connect points in chronological order. This suggest that there is strong association between an observation and a succeeding observation.\n\n\nSeasonality\n\nSeasonal HeatmapSeasonal Line plot\n\n\n\n\nCode\n# Create seasonal plot\nts_heatmap(month, color = \"PuRd\", title = 'Seasonality Heatmap of DOW Jones Index Stock Jan 2010 - March 2023')\n\n\n\n\n\n\n\n\n\n\nCode\n# Create a line graph for each year with months on the x-axis\nggseasonplot(month, datecol = \"date\", valuecol = \"value\")+ggtitle(\"Seasonal Yearly Plot for DOW Jones Index Stock Jan 2010 - March 2023\")\n\n\n\n\n\n\n\n\nThe Seasonality Heatmap for the DOW Jones Index Stock Jan 2010 - March 2023 does not reveal any clear seasonality in the data. The heatmap shows the mean value of the time series for each month and year combination, with the darker colors indicating higher values. The lack of clear patterns or darker colors in specific months or years suggests that there is no consistent seasonal pattern in the data. However, the yearly line graph shows a slight upward trend in the stock price from 2010 to 2023, but does not show any clear seasonality. Each year’s data is represented by a line, and the months are plotted on the x-axis. Overall, the lack of clear seasonality in both the heatmap and yearly line graph suggests that other factors beyond seasonality are driving the stock price fluctuations.\n\n\nMoving Average\n\n4 Month MA1 Year MA3 Year MA5 Year MA\n\n\n\n\nCode\n#SMA Smoothing \nma <- autoplot(month, series=\"Data\") +\n  autolayer(ma(month,5), series=\"4 Month MA\") +\n  xlab(\"Year\") + ylab(\"GWh\") +\n  ggtitle(\"DOW Jones Index Stock Jan 2010 - March 2023(4 Month Moving Average)\") +\n  scale_colour_manual(values=c(\"Data\"=\"grey50\",\"4 Month MA\"=\"red\"),\n                      breaks=c(\"Data\",\"4 Month MA\"))\nma\n\n\n\n\n\n\n\n\n\nCode\n#SMA Smoothing \nma <- autoplot(month, series=\"Data\") +\n  autolayer(ma(month,13), series=\"1 Year MA\") +\n  xlab(\"Year\") + ylab(\"GWh\") +\n  ggtitle(\"DOW Jones Index Stock Jan 2010 - March 2023(1 Year Moving Average)\") +\n  scale_colour_manual(values=c(\"Data\"=\"grey50\",\"1 Year MA\"=\"red\"),\n                      breaks=c(\"Data\",\"1 Year MA\"))\nma\n\n\n\n\n\n\n\n\n\nCode\n#SMA Smoothing \nma <- autoplot(month, series=\"Data\") +\n  autolayer(ma(month,37), series=\"3 Year MA\") +\n  xlab(\"Year\") + ylab(\"GWh\") +\n  ggtitle(\"DOW Jones Index Stock Jan 2010 - March 2023(3 Year Moving Average)\") +\n  scale_colour_manual(values=c(\"Data\"=\"grey50\",\"3 Year MA\"=\"red\"),\n                      breaks=c(\"Data\",\"3 Year MA\"))\nma\n\n\n\n\n\n\n\n\n\nCode\n#SMA Smoothing \nma <- autoplot(month, series=\"Data\") +\n  autolayer(ma(month,61), series=\"5 Year MA\") +\n  xlab(\"Year\") + ylab(\"GWh\") +\n  ggtitle(\"DOW Jones Index Stock Jan 2010 - March 2023(5 Year Moving Average)\") +\n  scale_colour_manual(values=c(\"Data\"=\"grey50\",\"5 Year MA\"=\"red\"),\n                      breaks=c(\"Data\",\"5 Year MA\"))\nma\n\n\n\n\n\n\n\n\nThe four plots show the DOW Jones Index stock prices from January 2010 to March 2023, along with the moving averages for 4 months, 1 year, 3 years and 5 years. As the window of the moving average increases, the smoother the trend line becomes, reducing the impact of noise and fluctuations in the original time series.\nThe 4-month moving average plot shows frequent fluctuations in the stock price, with the trend line following the general direction of the time series. The 1-year moving average plot shows a smoother trend, following the overall upward trend of the stock price.\nThe 3-year moving average plot shows a similar trend to the 1-year plot but is even smoother, with fewer fluctuations. Finally, the 5-year moving average plot shows the smoothest trend, with an almost constant upward slope.As the moving average window increases, the smoother trend allows for a clearer identification of the general trend of the DOW Jones Index stock prices over time. From the moving average obtained above we can see that there is upward tend in the stock price of DOW Jones Index.\n\n\nAutocorrelation Time Series\n\nACFPACFADF Test\n\n\n\n\nCode\n#ACF for  data\nggAcf(month)+ggtitle(\"ACF Plot for DOW Jones Index Stock Jan 2010 - March 2023\")\n\n\n\n\n\n\n\n\n\nCode\n#PACF for data\nggPacf(month)+ggtitle(\"PACF Plot for DOW Jones Index Stock Jan 2010 - March 2023\")\n\n\n\n\n\n\n\n\n\nCode\n#check the stationarity\ntseries::adf.test(month)\n\n\n\n    Augmented Dickey-Fuller Test\n\ndata:  month\nDickey-Fuller = -2.8227, Lag order = 5, p-value = 0.2333\nalternative hypothesis: stationary\n\n\n\n\n\nIn the plot of autocorrelation function, which is the acf graph for monthly data, there are clear autocorrelation in lag. The above lag plots and autocorrelation plot indicates seasonality in the series, which means the series is not stationary.. It was also verified using Augmented Dickey-Fuller Test which tells us that as the p value is greater than 0.05, the series is not stationary.\n\n\nDetrend and Differenced Time Series\n\nLinear Fitting ModelACF Plot\n\n\n\n\nCode\nfit = lm(myts~time(myts), na.action=NULL) \nsummary(fit) \n\n\n\nCall:\nlm(formula = myts ~ time(myts), na.action = NULL)\n\nResiduals:\n    Min      1Q  Median      3Q     Max \n-9403.6  -926.2     6.2   952.0  5318.6 \n\nCoefficients:\n              Estimate Std. Error t value Pr(>|t|)    \n(Intercept) -3.907e+06  1.716e+04  -227.6   <2e-16 ***\ntime(myts)   1.948e+03  8.511e+00   228.8   <2e-16 ***\n---\nSignif. codes:  0 '***' 0.001 '**' 0.01 '*' 0.05 '.' 0.1 ' ' 1\n\nResidual standard error: 1858 on 3309 degrees of freedom\nMultiple R-squared:  0.9406,    Adjusted R-squared:  0.9405 \nF-statistic: 5.236e+04 on 1 and 3309 DF,  p-value: < 2.2e-16\n\n\n\n\n\n\nCode\n# plot ACFs\nplot1 <- ggAcf(myts, 48, main=\"Original Data: DOW Jones Index Stock Stock Price\")\nplot2 <- ggAcf(resid(fit), 48, main=\"Detrended data\")\nplot3 <- ggAcf(diff(myts), 48, main=\"First differenced data\")\ngrid.arrange(plot1, plot2, plot3, nrow=3)\n\n\n\n\n\n\n\n\nThe estimated slope coefficient β1, 2.821e+03. With a standard error of 1.233e+01, yielding a significant estimated increase of stock price is very less yearly. Equation of the fit for stationary process: \\[\\hat{y}_{t} = x_{t}+(5.662e+06)-(2.821e+03)t\\]\nFrom the above graph we can say that there is high correlation in the original plot, but in the detrended plot the correlation is reduced but there is still high correlation in the detrended data.But when the first order difference is applied the high correlation is removed but there is no seasonal correlation.\nAs depicted in the above figure, the series is now stationary and ready for future study."
  },
  {
    "objectID": "eda_index_sp500.html",
    "href": "eda_index_sp500.html",
    "title": "EDA for S&P 500 Index",
    "section": "",
    "text": "Exploratory Data Analysis (EDA) for the S&P 500 Index involves analyzing time series data to identify the underlying patterns and characteristics of the index. The S&P 500 is a stock market index that measures the performance of 500 large companies listed on the US stock exchanges. EDA for the S&P 500 typically involves analyzing the daily closing prices of the index and examining key aspects such as autocorrelation, seasonality, trend, and stationarity. This information can be used to identify potential patterns and trends in the data, inform our modeling approach, and potentially improve our investment strategies.\n\nTime Series Plot\n\n\nCode\n# get data\noptions(\"getSymbols.warning4.0\"=FALSE)\noptions(\"getSymbols.yahoo.warning\"=FALSE)\n\n\ndata = getSymbols(\"^GSPC\",src='yahoo',from = '2010-01-01',to = \"2023-03-01\")\n\ndf <- data.frame(Date=index(GSPC),coredata(GSPC))\n\n# create Bollinger Bands\nbbands <- BBands(GSPC[,c(\"GSPC.High\",\"GSPC.Low\",\"GSPC.Close\")])\n\n# join and subset data\ndf <- subset(cbind(df, data.frame(bbands[,1:3])), Date >= \"2010-01-01\")\n\n#export data\nsp_raw_data <- df\nwrite.csv(sp_raw_data, \"DATA/CLEANED DATA/sp500_raw_data.csv\", row.names=FALSE)\n\n# colors column for increasing and decreasing\nfor (i in 1:length(df[,1])) {\n  if (df$GSPC.Close[i] >= df$GSPC.Open[i]) {\n      df$direction[i] = 'Increasing'\n  } else {\n      df$direction[i] = 'Decreasing'\n  }\n}\n\ni <- list(line = list(color = '#EBD168'))\nd <- list(line = list(color = '#7F7F7F'))\n\n# plot candlestick chart\n\nfig <- df %>% plot_ly(x = ~Date, type=\"candlestick\",\n          open = ~GSPC.Open, close = ~GSPC.Close,\n          high = ~GSPC.High, low = ~GSPC.Low, name = \"GSPC\",\n          increasing = i, decreasing = d) \nfig <- fig %>% add_lines(x = ~Date, y = ~up , name = \"B Bands\",\n            line = list(color = '#ccc', width = 0.5),\n            legendgroup = \"Bollinger Bands\",\n            hoverinfo = \"none\", inherit = F) \nfig <- fig %>% add_lines(x = ~Date, y = ~dn, name = \"B Bands\",\n            line = list(color = '#ccc', width = 0.5),\n            legendgroup = \"Bollinger Bands\", inherit = F,\n            showlegend = FALSE, hoverinfo = \"none\") \nfig <- fig %>% add_lines(x = ~Date, y = ~mavg, name = \"Mv Avg\",\n            line = list(color = '#E377C2', width = 0.5),\n            hoverinfo = \"none\", inherit = F) \nfig <- fig %>% layout(yaxis = list(title = \"Price\"))\n\n# plot volume bar chart\nfig2 <- df \nfig2 <- fig2 %>% plot_ly(x=~Date, y=~GSPC.Volume, type='bar', name = \"GSPC Volume\",\n          color = ~direction, colors = c('#EBD168','#7F7F7F')) \nfig2 <- fig2 %>% layout(yaxis = list(title = \"Volume\"))\n\n# create rangeselector buttons\nrs <- list(visible = TRUE, x = 0.5, y = -0.055,\n           xanchor = 'center', yref = 'paper',\n           font = list(size = 9),\n           buttons = list(\n             list(count=1,\n                  label='RESET',\n                  step='all'),\n             list(count=1,\n                  label='1 YR',\n                  step='year',\n                  stepmode='backward'),\n             list(count=3,\n                  label='3 MO',\n                  step='month',\n                  stepmode='backward'),\n             list(count=1,\n                  label='1 MO',\n                  step='month',\n                  stepmode='backward')\n           ))\n\n# subplot with shared x axis\nfig <- subplot(fig, fig2, heights = c(0.7,0.2), nrows=2,\n             shareX = TRUE, titleY = TRUE)\nfig <- fig %>% layout(title = paste(\"S&P 500 Index Stock Price: January 2010 - March 2023\"),\n         xaxis = list(rangeselector = rs),\n         legend = list(orientation = 'h', x = 0.5, y = 1,\n                       xanchor = 'center', yref = 'paper',\n                       font = list(size = 10),\n                       bgcolor = 'transparent'))\n\nfig\n\n\n\n\n\n\nOver the period from January 2010 to March 2023, the S&P 500 index exhibited both upward and downward trends in its stock price. The index started off on a positive note, rising steadily from early 2010 to early 2011. However, it then experienced a significant decline in value, dropping by over 15% by October 2011. From there, the index began a slow but steady climb, reaching new all-time highs by mid-2015.\nThe S&P 500 continued to rise throughout 2016 and into 2017, with a few minor dips along the way. However, it then experienced a sharp drop in value in early 2018, losing over 10% of its value in just a few days. The index then regained some of its losses but continued to fluctuate throughout the remainder of 2018.\nIn 2019, the S&P 500 once again resumed its upward trend, with a few minor dips along the way. However, the COVID-19 pandemic in 2020 caused a significant drop in the index’s value, with the index losing nearly 34% of its value in just a few weeks. However, the index quickly rebounded, aided by government stimulus measures and low-interest rates, and continued its upward trend throughout 2020 and 2021.\nOverall, the S&P 500 index’s stock price exhibited significant fluctuations over the period from January 2010 to March 2023. However, despite the fluctuations, the index exhibited an overall upward trend, reaching new all-time highs multiple times over the period.\nFor stock prices, a multiplicative decomposition is typically preferred because the percentage changes in stock prices tend to be more important than the absolute changes. Additionally, stock prices tend to exhibit non-constant variance, meaning that the variance of the series changes over time. A multiplicative decomposition can handle this non-constant variance more effectively than an additive decomposition.\n\n\nDecomposed Time Series\n\nDecomposition PlotAdjusted Decomposition Plot\n\n\n\n\nCode\n#convert data to ts data\nmyts<-ts(df$GSPC.Adjusted,frequency=252,start=c(2010,1,1)) \norginial_plot <- autoplot(myts,xlab =\"Year\", ylab = \"Adjusted Closing Price\", main = \"S&P 500 Index Stock price: Jan 2010 - March 2023\")\ndecompose = decompose(myts, \"multiplicative\")\nautoplot(decompose)\n\n\n\n\n\n\n\n\n\nCode\n#adjusted decomposition\ntrendadj <- myts/decompose$trend\ndecompose_adjtrend_plot <- autoplot(trendadj,ylab='trend') +ggtitle('Adjusted trend component in the multiplicative time series model')\nseasonaladj <- myts/decompose$seasonal\ndecompose_adjseasonal_plot <- autoplot(seasonaladj,ylab='seasonal') +ggtitle('Adjusted seasonal component in the multiplicative time series model')\ngrid.arrange(orginial_plot, decompose_adjtrend_plot,decompose_adjseasonal_plot, nrow=3)\n\n\n\n\n\n\n\n\nWhen compared to the original figure, the corrected seasonal component has an upward trend and greater variability in the model, but the adjusted trend component has a stable trend through time.\n\n\nLag Plots\n\nDaily Time LagsMonthly Time Lags\n\n\n\n\nCode\n#Lag plots \ngglagplot(myts, do.lines=FALSE, lags=1)+xlab(\"Lag 1\")+ylab(\"Yi\")+ggtitle(\"Lag Plot for S&P 500 Index Stock Jan 2010 - March 2023\")\n\n\n\n\n\n\n\n\n\nCode\n#montly data\nmean_data <- df %>% \n  mutate(month = month(Date), year = year(Date)) %>% \n  group_by(year, month) %>% \n  summarize(mean_value = mean(GSPC.Adjusted))\n\n#ts for month data\nmonth<-ts(mean_data$mean_value,star=decimal_date(as.Date(\"2010-01-01\",format = \"%Y-%m-%d\")),frequency = 12)\n\n#Lag plot for month\nts_lags(month)\n\n\n\n\n\n\n\n\n\nS&P 500 Index Stock Lag Plot As there is a positive correlation and an inclination angle of 45 degrees, there should be a significant link between the series and the relevant lag from January 2010 to March 2023. This is the lag plot hallmark of a process that has a high degree of positive autocorrelation. Such processes are highly non-random, there is a substantial relationship between one observation and the next. Seasonality can also be investigated by plotting observations for a wider number of time periods, i.e. the lags. The time series data is aggregated to monthly data using the mean function for a better comprehension of the series and crisper plots. Further inspection of the last graph reveals that more dots are on the diagonal line at 45 degrees. The second graph shows the monthly variation of the variable on the vertical axis. In chronological order, the lines connect the points. This suggest that there is strong association between an observation and a succeeding observation.\n\n\nSeasonality\n\nSeasonal HeatmapSeasonal Line plot\n\n\n\n\nCode\n# Create seasonal plot\nts_heatmap(month, color = \"YlOrBr\", title = 'Seasonality Heatmap of S&P 500 Index Stock Jan 2010 - March 2023')\n\n\n\n\n\n\n\n\n\n\nCode\n# Create a line graph for each year with months on the x-axis\nggseasonplot(month, datecol = \"date\", valuecol = \"value\")+ggtitle(\"Seasonal Yearly Plot for S&P 500 Index Stock Jan 2010 - March 2023\")\n\n\n\n\n\n\n\n\nThe Seasonality Heatmap for the S&P 500 Index stock from January 2010 to March 2023 shows some evidence of seasonality in the data, although the patterns are not consistent across all years. The heatmap displays the mean value of the time series for each month and year combination, with darker colors indicating higher values. The heatmap reveals that the S&P 500 Index tends to exhibit higher values during the months of December and January in many of the years studied. However, this pattern is not consistently observed across all years. Similarly, the yearly line graph also shows some evidence of seasonality, with a general trend of higher values during the latter part of the year. However, this trend is not present in all years, and the magnitude of the seasonal effect varies between years. Overall, the presence of some evidence of seasonality in the S&P 500 Index suggests that seasonality may be one of the factors contributing to the fluctuations in the stock price. However, other factors beyond seasonality, such as economic and political events, also play a significant role in determining the stock price.\n\n\nMoving Average\n\n4 Month MA1 Year MA3 Year MA3 Year MA\n\n\n\n\nCode\n#SMA Smoothing - 4 month\nma <- autoplot(month, series=\"Data\") +\n  autolayer(ma(month,5), series=\"4 month MA\") +\n  xlab(\"Year\") + ylab(\"GWh\") +\n  ggtitle(\"S&P 500 Index Stock Jan 2010 - March 2023 (4 Month Moving Average\") +\n  scale_colour_manual(values=c(\"Data\"=\"grey50\",\"4 month MA\"=\"red\"),\n                      breaks=c(\"Data\",\"4 month MA\"))\nma\n\n\n\n\n\n\n\n\n\nCode\n#SMA Smoothing - 1 Year\nma <- autoplot(month, series=\"Data\") +\n  autolayer(ma(month,13), series=\"1 Year MA\") +\n  xlab(\"Year\") + ylab(\"GWh\") +\n  ggtitle(\"S&P 500 Index Stock Jan 2010 - March 2023 (1 Year Moving Average\") +\n  scale_colour_manual(values=c(\"Data\"=\"grey50\",\"1 Year MA\"=\"red\"),\n                      breaks=c(\"Data\",\"1 Year MA\"))\nma\n\n\n\n\n\n\n\n\n\nCode\n#SMA Smoothing - 3 Year\nma <- autoplot(month, series=\"Data\") +\n  autolayer(ma(month,37), series=\"3 Year MA\") +\n  xlab(\"Year\") + ylab(\"GWh\") +\n  ggtitle(\"S&P 500 Index Stock Jan 2010 - March 2023 (3 Year Moving Average)\") +\n  scale_colour_manual(values=c(\"Data\"=\"grey50\",\"3 Year MA\"=\"red\"),\n                      breaks=c(\"Data\",\"3 Year MA\"))\nma\n\n\n\n\n\n\n\n\n\nCode\n#SMA Smoothing - 5 Year\nma <- autoplot(month, series=\"Data\") +\n  autolayer(ma(month,61), series=\"5 Year MA\") +\n  xlab(\"Year\") + ylab(\"GWh\") +\n  ggtitle(\"S&P 500 Index Stock Jan 2010 - March 2023 (5 Year Moving Average)\") +\n  scale_colour_manual(values=c(\"Data\"=\"grey50\",\"5 Year MA\"=\"red\"),\n                      breaks=c(\"Data\",\"5 Year MA\"))\nma\n\n\n\n\n\n\n\n\nThe four plots above show the S&P 500 Index stock prices for the period between Jan 2010 and March 2023, smoothed using 4-month, 1-year, 3-year and 5-year moving averages.\nLooking at the plots, we can see that the moving average values are increasing over time. The 4-month MA plot shows a lot of fluctuations, which is expected because it captures the short-term variations in the stock prices. On the other hand, the 1-year and 3-year and 5-year MA plots smooth out the fluctuations and show the overall trend of the stock prices.\nWe can observe that the 5-year MA plot provides a smoother trend as it takes into account a longer time period compared to the other plots. The 1-year MA plot also provides a relatively smooth trend, but it captures shorter-term variations compared to the 5-year MA plot. The 4-month MA plot is even more sensitive to shorter-term variations in the stock prices. From the moving average obtained above we can see that there is upward tend in the stock price of S&P 500 Index.\n\n\nAutocorrelation Time Series\n\nACFPACFADF Test\n\n\n\n\nCode\n#ACF plots for month data\nggAcf(month)+ggtitle(\"ACF Plot for S&P 500 Index Stock Jan 2010 - March 2023\")\n\n\n\n\n\n\n\n\n\nCode\n#PACF plots for month data\nggPacf(month)+ggtitle(\"PACF Plot for S&P 500 Index Stock Jan 2010 - March 2023\")\n\n\n\n\n\n\n\n\n\nCode\n# ADF Test\ntseries::adf.test(month)\n\n\n\n    Augmented Dickey-Fuller Test\n\ndata:  month\nDickey-Fuller = -2.5431, Lag order = 5, p-value = 0.3499\nalternative hypothesis: stationary\n\n\n\n\n\nThere is clear autocorrelation in lag in the plot of autocorrelation function, which is the acf graph for monthly data. The lag plots and autocorrelation plots shown above suggest seasonality in the series, indicating that it is not stationary. It was also validated using the Augmented Dickey-Fuller Test, which indicates that the series is not stationary as the p value is more than 0.05.\n\n\nDetrend and Differenced Time Series\n\nLinear Fitting ModelACF Plot\n\n\n\n\nCode\nfit = lm(myts~time(myts), na.action=NULL) \nsummary(fit) \n\n\n\nCall:\nlm(formula = myts ~ time(myts), na.action = NULL)\n\nResiduals:\n     Min       1Q   Median       3Q      Max \n-1100.29  -200.78   -24.77    78.55  1008.30 \n\nCoefficients:\n              Estimate Std. Error t value Pr(>|t|)    \n(Intercept) -5.064e+05  2.628e+03  -192.7   <2e-16 ***\ntime(myts)   2.523e+02  1.303e+00   193.6   <2e-16 ***\n---\nSignif. codes:  0 '***' 0.001 '**' 0.01 '*' 0.05 '.' 0.1 ' ' 1\n\nResidual standard error: 284.5 on 3309 degrees of freedom\nMultiple R-squared:  0.9189,    Adjusted R-squared:  0.9188 \nF-statistic: 3.748e+04 on 1 and 3309 DF,  p-value: < 2.2e-16\n\n\n\n\n\n\nCode\n# plot ACFs\nplot1 <- ggAcf(myts, 48, main=\"Original Data: S&P 500 Index Stock Price\")\nplot2 <- ggAcf(resid(fit), 48, main=\"detrended data\") \nplot3 <- ggAcf(diff(myts), 48, main=\"first differenced data\")\ngrid.arrange(plot1, plot2, plot3,nrow=3)\n\n\n\n\n\n\n\n\nThe estimated slope coefficient β1, 3.655e+02. With a standard error of 1.888e+00, yielding a significant estimated increase of stock price is very less yearly. Equation of the fit for stationary process: \\[\\hat{y}_{t} = x_{t}+(7.338e+05)-(3.655e+02)t\\]\nFrom the above graph we can say that there is high correlation in the original plot, but in the detrended plot the correlation is reduced but there is still high correlation in the detrended data.But when the first order difference is applied the high correlation is removed but there is seasonal correlation.\nAs depicted in the above figure, the series is now stationary and ready for future study."
  },
  {
    "objectID": "eda_index_nadsaq.html",
    "href": "eda_index_nadsaq.html",
    "title": "EDA for NADSAQ Composite Index",
    "section": "",
    "text": "Time Series Plot\n\n\nCode\n# get data\noptions(\"getSymbols.warning4.0\"=FALSE)\noptions(\"getSymbols.yahoo.warning\"=FALSE)\n\n\ndata = getSymbols(\"^IXIC\",src='yahoo',from = '2010-01-01',to = \"2023-03-01\")\n\ndf <- data.frame(Date=index(IXIC),coredata(IXIC))\n\n# create Bollinger Bands\nbbands <- BBands(IXIC[,c(\"IXIC.High\",\"IXIC.Low\",\"IXIC.Close\")])\n\n# join and subset data\ndf <- subset(cbind(df, data.frame(bbands[,1:3])), Date >= \"2010-01-01\")\n\n#export data\nnasdaq_raw_data <- df\nwrite.csv(nasdaq_raw_data, \"DATA/CLEANED DATA/nasdaq_raw_data.csv\", row.names=FALSE)\n\n# colors column for increasing and decreasing\nfor (i in 1:length(df[,1])) {\n  if (df$IXIC.Close[i] >= df$IXIC.Open[i]) {\n      df$direction[i] = 'Increasing'\n  } else {\n      df$direction[i] = 'Decreasing'\n  }\n}\n\ni <- list(line = list(color = '#CCCCFF'))\nd <- list(line = list(color = '#7F7F7F'))\n\n# plot candlestick chart\n\nfig <- df %>% plot_ly(x = ~Date, type=\"candlestick\",\n          open = ~IXIC.Open, close = ~IXIC.Close,\n          high = ~IXIC.High, low = ~IXIC.Low, name = \"IXIC\",\n          increasing = i, decreasing = d) \nfig <- fig %>% add_lines(x = ~Date, y = ~up , name = \"B Bands\",\n            line = list(color = '#ccc', width = 0.5),\n            legendgroup = \"Bollinger Bands\",\n            hoverinfo = \"none\", inherit = F) \nfig <- fig %>% add_lines(x = ~Date, y = ~dn, name = \"B Bands\",\n            line = list(color = '#ccc', width = 0.5),\n            legendgroup = \"Bollinger Bands\", inherit = F,\n            showlegend = FALSE, hoverinfo = \"none\") \nfig <- fig %>% add_lines(x = ~Date, y = ~mavg, name = \"Mv Avg\",\n            line = list(color = '#E377C2', width = 0.5),\n            hoverinfo = \"none\", inherit = F) \nfig <- fig %>% layout(yaxis = list(title = \"Price\"))\n\n# plot volume bar chart\nfig2 <- df \nfig2 <- fig2 %>% plot_ly(x=~Date, y=~IXIC.Volume, type='bar', name = \"IXIC Volume\",\n          color = ~direction, colors = c('#CCCCFF','#7F7F7F')) \nfig2 <- fig2 %>% layout(yaxis = list(title = \"Volume\"))\n\n# create rangeselector buttons\nrs <- list(visible = TRUE, x = 0.5, y = -0.055,\n           xanchor = 'center', yref = 'paper',\n           font = list(size = 9),\n           buttons = list(\n             list(count=1,\n                  label='RESET',\n                  step='all'),\n             list(count=1,\n                  label='1 YR',\n                  step='year',\n                  stepmode='backward'),\n             list(count=3,\n                  label='3 MO',\n                  step='month',\n                  stepmode='backward'),\n             list(count=1,\n                  label='1 MO',\n                  step='month',\n                  stepmode='backward')\n           ))\n\n# subplot with shared x axis\nfig <- subplot(fig, fig2, heights = c(0.7,0.2), nrows=2,\n             shareX = TRUE, titleY = TRUE)\nfig <- fig %>% layout(title = paste(\"NASDAQ Composite Index Stock Price: January 2010 - March 2023\" ),\n         xaxis = list(rangeselector = rs),\n         legend = list(orientation = 'h', x = 0.5, y = 1,\n                       xanchor = 'center', yref = 'paper',\n                       font = list(size = 10),\n                       bgcolor = 'transparent'))\n\nfig\n\n\n\n\n\n\nOver the past decade, the NASDAQ Composite Index has shown a strong upward trend, with periods of volatility and corrections along the way. One of the key drivers of this growth has been the rapid expansion of the technology industry, which has fueled investor optimism and driven up the prices of technology stocks. As a result, the NASDAQ Composite Index has become closely associated with the technology sector, and investors often view it as a barometer of the industry’s health.\nHowever, inflation has also played a role in the movement of the NASDAQ Composite Index stock price. Inflation erodes the value of money, making it more expensive to purchase goods and services. This can lead to higher interest rates, which can negatively impact the stock market. Inflation concerns have been a major factor in market volatility, and recent increases in inflation have caused some investors to become cautious. The Federal Reserve has responded to these concerns by raising interest rates and scaling back its bond-buying program.\nDespite these challenges, the NASDAQ Composite Index has continued to rise, reflecting the underlying strength of the technology industry and the broader economy. As technology continues to transform the way we live and work, the NASDAQ Composite Index is likely to remain an important indicator of trends in the sector and a key benchmark for investors.\nFor stock prices, a multiplicative decomposition is typically preferred because the percentage changes in stock prices tend to be more important than the absolute changes. Additionally, stock prices tend to exhibit non-constant variance, meaning that the variance of the series changes over time. A multiplicative decomposition can handle this non-constant variance more effectively than an additive decomposition.\n\n\nDecomposed Time Series\n\nDecomposition PlotAdjusted Decomposition Plot\n\n\n\n\nCode\n#convert to ts data\nmyts<-ts(df$IXIC.Adjusted,frequency=252,start=c(2010,1,1)) \norginial_plot <- autoplot(myts,xlab =\"Year\", ylab = \"Adjusted Closing Price\", main = \"NASDAQ Composite Index Stock price: Jan 2010 - March 2023\")\ndecompose = decompose(myts, \"multiplicative\")\nautoplot(decompose)\n\n\n\n\n\n\n\n\n\nCode\n#adjusted decomposition\ntrendadj <- myts/decompose$trend\ndecompose_adjtrend_plot <- autoplot(trendadj,ylab='trend') +ggtitle('Adjusted trend component in the multiplicative time series model')\nseasonaladj <- myts/decompose$seasonal\ndecompose_adjseasonal_plot <- autoplot(seasonaladj,ylab='seasonal') +ggtitle('Adjusted seasonal component in the multiplicative time series model')\ngrid.arrange(orginial_plot, decompose_adjtrend_plot,decompose_adjseasonal_plot, nrow=3)\n\n\n\n\n\n\n\n\nWhen compared to the original plot, the adjusted seasonal component tends to have an upward trend, and the model is more variable than the original plot, where the plot changes over time but the trend stays the same.\n\n\nLag Plots\n\nDaily Time LagsMonthly Time Lags\n\n\n\n\nCode\n#Lag plots \ngglagplot(myts, do.lines=FALSE, lags=1)+xlab(\"Lag 1\")+ylab(\"Yi\")+ggtitle(\"Lag Plot for NASDAQ Composite Index Stock Jan 2010 - March 2023\")\n\n\n\n\n\n\n\n\n\nCode\n#montly data\nmean_data <- df %>% \n  mutate(month = month(Date), year = year(Date)) %>% \n  group_by(year, month) %>% \n  summarize(mean_value = mean(IXIC.Adjusted))\n#ts of montly data\nmonth<-ts(mean_data$mean_value,star=decimal_date(as.Date(\"2010-01-01\",format = \"%Y-%m-%d\")),frequency = 12)\n\n#Lag plot for month\nts_lags(month)\n\n\n\n\n\n\n\n\n\nNASDAQ Composite Index Stock’s Lag Plot From January 2010 to March 2023, there should be a strong connection between the series and the related lag, since there is a positive correlation and a 45-degree slope. This is how a process that has strong positive autocorrelation shows up in a lag plot. Such processes are not very random, because there is a strong link between one observation and the next. Another way to look at seasonality is to plot observations for a larger number of time periods, called “lags.” Using the mean function, the time series data is turned into monthly data so that the series can be better understood and the plots can be more clear. If you look closely at the last graph, you can see that there are more dots on the diagonal line at 45 degrees. On the vertical axis of the second graph, the month of the variable is shown. The lines link the points in order of time. This suggest that there is strong association between an observation and a succeeding observation.\n\n\nSeasonality\n\nSeasonal HeatmapSeasonal Line plot\n\n\n\n\nCode\n# Create seasonal plot\nts_heatmap(month, color = \"YlGn\", title = 'Seasonality Heatmap of NASDAQ Composite Index Stock Jan 2010 - March 2023')\n\n\n\n\n\n\n\n\n\n\nCode\n# Create a line graph for each year with months on the x-axis\nggseasonplot(month, datecol = \"date\", valuecol = \"value\")+ggtitle(\"Seasonal Yearly Plot for NASDAQ Composite Index Stock Jan 2010 - March 2023\")\n\n\n\n\n\n\n\n\nThe Seasonality Heatmap for the NASDAQ Composite Index Stock from January 2010 to March 2023 does not reveal any significant seasonality in the data. The heatmap displays the mean value of the time series for each month and year combination, with darker colors indicating higher values. The absence of any consistent patterns or darker colors in specific months or years indicates that there is no clear seasonal trend in the data. Similarly, the yearly line graph also does not show any discernible seasonality. Each year’s data is represented by a line, and the months are plotted on the x-axis. However, the graph does display a strong upward trend in the stock price from 2010 to 2023. Overall, the lack of clear seasonality in both the heatmap and yearly line graph suggests that other factors beyond seasonality are the primary drivers of the fluctuations in the NASDAQ Composite Index.\n\n\nMoving Average\n\n4 Month MA1 Year MA3 Year MA5 Year MA\n\n\n\n\nCode\n#SMA Smoothing \nma <- autoplot(month, series=\"Data\") +\n  autolayer(ma(month,5), series=\"4 Month MA\") +\n  xlab(\"Year\") + ylab(\"GWh\") +\n  ggtitle(\"NASDAQ Composite Index Stock Jan 2010 - March 2023(4 Month Moving Average)\") +\n  scale_colour_manual(values=c(\"Data\"=\"grey50\",\"4 Month MA\"=\"red\"),\n                      breaks=c(\"Data\",\"4 Month MA\"))\nma\n\n\n\n\n\n\n\n\n\nCode\n#SMA Smoothing \nma <- autoplot(month, series=\"Data\") +\n  autolayer(ma(month,13), series=\"1 Year MA\") +\n  xlab(\"Year\") + ylab(\"GWh\") +\n  ggtitle(\"NASDAQ Composite Index Stock Jan 2010 - March 2023 (1 Year Moving Average)\") +\n  scale_colour_manual(values=c(\"Data\"=\"grey50\",\"1 Year MA\"=\"red\"),\n                      breaks=c(\"Data\",\"1 Year MA\"))\nma\n\n\n\n\n\n\n\n\n\nCode\n#SMA Smoothing \nma <- autoplot(month, series=\"Data\") +\n  autolayer(ma(month,37), series=\"3 Year MA\") +\n  xlab(\"Year\") + ylab(\"GWh\") +\n  ggtitle(\"NASDAQ Composite Index Stock Jan 2010 - March 2023(3 Year Moving Average)\") +\n  scale_colour_manual(values=c(\"Data\"=\"grey50\",\"3 Year MA\"=\"red\"),\n                      breaks=c(\"Data\",\"3 Year MA\"))\nma\n\n\n\n\n\n\n\n\n\nCode\n#SMA Smoothing \nma <- autoplot(month, series=\"Data\") +\n  autolayer(ma(month,61), series=\"5 Year MA\") +\n  xlab(\"Year\") + ylab(\"GWh\") +\n  ggtitle(\"NASDAQ Composite Index Stock Jan 2010 - March 2023(5 Year Moving Average)\") +\n  scale_colour_manual(values=c(\"Data\"=\"grey50\",\"5 Year MA\"=\"red\"),\n                      breaks=c(\"Data\",\"5 Year MA\"))\nma\n\n\n\n\n\n\n\n\nThe four plots above show the NASDAQ Composite Index stock price from January 2010 to March 2023 with different timeframes of moving averages (MA) overlaid. The moving average is a common smoothing technique used to reduce the noise in the time series data and highlight underlying trends. As the length of the MA window increases, the smoother the plot becomes and the trend is more visible. The first plot shows a 4-month moving average, the second plot shows a 1-year moving average, the third plot shows a 3-year moving average and the fourth plot shows a 5-year moving average\nComparing the plots, we can see that as the length of the moving average window increases, the plot becomes smoother, and the trend becomes clearer. In the 4-month moving average plot, the stock price fluctuates significantly, making it challenging to identify the trend. However, in the 5-year moving average plot, we can observe a clear upward trend in the stock price, and it is easier to identify the long-term trend. From the moving average obtained above we can see that there is upward tend in the stock price of NASDAQ Composite Index.\n\n\nAutocorrelation Time Series\n\nACFPACFADF Test\n\n\n\n\nCode\n#ACF plots for month data\nggAcf(month, 120)+ggtitle(\"ACF Plot for NASDAQ Composite Index Stock Jan 2010 - March 2023\")\n\n\n\n\n\n\n\n\n\nCode\n#PACF plots for month data\nggPacf(month, 120)+ggtitle(\"PACF Plot for NASDAQ Composite Index Stock Jan 2010 - March 2023\")\n\n\n\n\n\n\n\n\n\nCode\n# ADF Test\ntseries::adf.test(month)\n\n\n\n    Augmented Dickey-Fuller Test\n\ndata:  month\nDickey-Fuller = -2.514, Lag order = 5, p-value = 0.362\nalternative hypothesis: stationary\n\n\n\n\n\nThere is clear autocorrelation in lag in the plot of autocorrelation function, which is the acf graph for monthly data. The above lag plots and autocorrelation plots show that the series has seasonality, which means that the series doesn’t stay the same over time. It was also checked with the Augmented Dickey-Fuller Test. This test tells us that the series is not stationary because the p value is greater than 0.05.\n\n\nDetrend and Differenced Time Series\n\nLinear Fitting ModelACF Plot\n\n\n\n\nCode\nfit = lm(myts~time(myts), na.action=NULL) \nsummary(fit) \n\n\n\nCall:\nlm(formula = myts ~ time(myts), na.action = NULL)\n\nResiduals:\n    Min      1Q  Median      3Q     Max \n-2998.7 -1052.2  -276.2   735.6  4672.4 \n\nCoefficients:\n              Estimate Std. Error t value Pr(>|t|)    \n(Intercept) -1.835e+06  1.280e+04  -143.3   <2e-16 ***\ntime(myts)   9.132e+02  6.350e+00   143.8   <2e-16 ***\n---\nSignif. codes:  0 '***' 0.001 '**' 0.01 '*' 0.05 '.' 0.1 ' ' 1\n\nResidual standard error: 1386 on 3309 degrees of freedom\nMultiple R-squared:  0.8621,    Adjusted R-squared:  0.8621 \nF-statistic: 2.069e+04 on 1 and 3309 DF,  p-value: < 2.2e-16\n\n\n\n\n\n\nCode\n# plot ACFs\nplot1 <- ggAcf(myts, 48, main=\"Original Data: NASDAQ Composite Index Stock Price\")\nplot2 <- ggAcf(resid(fit), 48, main=\"Detrended data\") \nplot3 <- ggAcf(diff(myts), 48, main=\"First differenced data\")\ngrid.arrange(plot1, plot2, plot3,nrow=3)\n\n\n\n\n\n\n\n\nThe estimated slope coefficient β1, 1.323e+03. With a standard error of 9.197e+00, yielding a significant estimated increase of stock price is very less yearly. Equation of the fit for stationary process: \\[\\hat{y}_{t} = x_{t}+(2.658e+06)-(1.323e+03)t\\] From the above graph, we can see that the original plot has a high correlation, while the detrended plot has a lower correlation but still a high correlation. But when the first order difference is used, the high correlation goes away, but there is still a correlation between the time of year and the data.\nAs depicted in the above figure, the series is now stationary and ready for future study."
  },
  {
    "objectID": "eda_macroeconomic_factors.html",
    "href": "eda_macroeconomic_factors.html",
    "title": "EDA for Macroeconomic Factors",
    "section": "",
    "text": "Exploratory Data Analysis (EDA) is a crucial step in understanding the behavior of macroeconomic factors such as GDP growth rate, inflation, interest rates, and unemployment rates. These factors play a critical role in shaping the overall performance of an economy. EDA involves various statistical techniques and tools that help in analyzing and visualizing data to identify patterns, trends, and anomalies.\nOne of the key aspects of EDA is time series analysis, which is used to analyze data collected over time. Time series data such as GDP growth rate, inflation, interest rates, and unemployment rates are characterized by autocorrelation, which means that the value of a variable at a particular time is dependent on its past values. Autocorrelation can be quantified using statistical measures such as the autocorrelation function (ACF) and partial autocorrelation function (PACF). These measures are used to identify the strength and direction of the relationship between variables.\nAnother important concept in time series analysis is stationarity, which refers to the stability of the statistical properties of a time series over time. Stationarity is a critical assumption in many time series models, and it can be tested using techniques such as the Augmented Dickey-Fuller (ADF) test. Seasonality is another aspect of time series data that can be identified using EDA. Seasonality refers to the presence of regular and predictable patterns in the data that recur over a fixed time period.\nMoving averages are another statistical technique commonly used in EDA for time series data. Moving averages smooth out fluctuations in the data and help identify trends and patterns. Autocorrelation is also a key concept in EDA for time series data. Autocorrelation refers to the degree of correlation between a variable and its past values.\nFinally, detrend check is a technique used to identify trends in time series data. Detrending involves removing the trend component from the data to isolate the cyclical and irregular components. EDA for macroeconomic factors such as GDP growth rate, inflation, interest rates, and unemployment rates involves a careful consideration of all these concepts and techniques to identify patterns and trends that can provide valuable insights into the behavior of these critical economic indicators.\nClick to view EDA Page for GDP Growth Rate\nClick to view EDA Page for Interest Rate\nClick to view EDA Page for Inflation Rate\nClick to view EDA Page for Unemployment Rate"
  },
  {
    "objectID": "eda_macroeconomic_gdp.html",
    "href": "eda_macroeconomic_gdp.html",
    "title": "EDA for GDP Growth Rate",
    "section": "",
    "text": "GDP growth rate is one of the most important indicators of a country’s economic performance, and analyzing its behavior is critical to understanding the underlying factors driving economic growth. Exploratory Data Analysis (EDA) is a powerful tool for gaining insights into GDP growth rate data and identifying patterns and trends that can inform economic policy decisions. In this page, we will explore various EDA techniques that can be applied to GDP growth rate data, such as time series analysis, autocorrelation analysis, seasonality analysis, moving averages, and detrending. By the end of this page, you will have a better understanding of how to apply EDA techniques to GDP growth rate data and draw valuable insights from it.\n\nTime Series Plot\n\n\nCode\n#import the data\ngdp <- read.csv(\"DATA/RAW DATA/gdp-growth.csv\")\n\n#change date format\ngdp$Date <- as.Date(gdp$DATE , \"%m/%d/%Y\")\n\n#drop DATE column\ngdp <- subset(gdp, select = -c(1))\n\n#export the cleaned data\ngdp_clean <- gdp\nwrite.csv(gdp_clean, \"DATA/CLEANED DATA/gdp_clean_data.csv\", row.names=FALSE)\n\n#plot gdp growth rate \nfig <- plot_ly(gdp, x = ~Date, y = ~value, type = 'scatter', mode = 'lines',line = list(color = 'rgb(220,20,60)'))\nfig <- fig %>% layout(title = \"U.S GPD Growth Rate: 2010 - 2022\",xaxis = list(title = \"Time\"),yaxis = list(title =\"GDP Growth Rate\"))\nfig\n\n\n\n\n\n\nThe trend in GDP growth rate in the United States from 2010 to 2022 has been characterized by moderate fluctuations, reflecting a range of economic conditions and policy responses. Between 2010 and 2022, the United States experienced a range of GDP growth rates, reflecting various economic conditions and policy responses. In the early years of this period, the economy was still recovering from the 2008 financial crisis, which had led to a prolonged period of slow growth. In 2012, GDP growth began to pick up, reaching 2.8% that year, followed by 1.8% in 2013 and 2.5% in 2014. The peak in this period came in 2018, when the GDP growth rate reached 2.9%. However, the momentum of growth slowed in the years that followed. In 2016, GDP growth declined to 1.6%, followed by 2.2% in 2018, and 2.9% in 2018. By 2019, growth had slowed again to 2.2%. This period of slower growth was attributed to a range of factors, including the tightening of monetary policy by the Federal Reserve, global economic headwinds, and ongoing concerns about political instability and trade tensions. The COVID-19 pandemic in 2020 led to a sharp contraction in economic activity, with GDP growth declining by 3.5%, the largest annual decline since the 1940s. The pandemic resulted in widespread shutdowns of businesses, schools, and public spaces, as well as disruptions to global supply chains and trade. However, the US government and the Federal Reserve responded with a range of fiscal and monetary policies, including direct payments to households, increased unemployment benefits, and massive injections of liquidity into financial markets. These measures helped to mitigate the impact of the pandemic on the economy. In 2021, the US economy began to recover, with GDP growth projected to reach 6.3% by the end of the year. This rebound was due to a combination of factors, including the easing of pandemic-related restrictions, increased vaccination rates, and the continuation of government stimulus measures.\nThe GDP growth rate in the United States from 2010 to 2021, there appears to be some seasonality in the data. he seasonality appears to be relatively consistent over time, with spikes in GDP growth rate occurring in the second quarter of each year, followed by a dip in the third quarter. This pattern is likely due to various factors, such as changes in consumer spending and production schedules. Multiplicative decomposition model may be more appropriate, as it accounts for changes in both the level and the variability of the data.\n\n\nDecomposed Time Series\n\nDecomposition PlotAdjusted Decomposition Plot\n\n\n\n\nCode\n#convert to ts data\nmyts<-ts(gdp$value,frequency=4,start=c(2010/1/1))\n#decomposition\norginial_plot <- autoplot(myts,xlab =\"Year\", ylab = \"GDP Growth Rate\", main = \"U.S GDP Growth Rate: 2010 - 2021\")\ndecompose = decompose(myts,\"multiplicative\")\nautoplot(decompose)\n\n\n\n\n\n\n\n\n\nCode\n#adjusted decomposition\ntrendadj <- myts/decompose$trend\ndecompose_adjtrend_plot <- autoplot(trendadj,ylab='seasonal') +ggtitle('Adjusted trend component in the multiplicative time series model')\nseasonaladj <- myts/decompose$seasonal\ndecompose_adjseasonal_plot <- autoplot(seasonaladj,ylab='seasonal') +ggtitle('Adjusted seasonal component in the multiplicative time series model')\ngrid.arrange(orginial_plot, decompose_adjtrend_plot,decompose_adjseasonal_plot, nrow=3)\n\n\n\n\n\n\n\n\nWhen compared to the original plot, the adjusted seasonal component tends to have more fluctuation, and the model is more variable than the original plot, where the plot changes over time but the trend stays the same.\n\n\nLag Plots\n\nDaily Time LagsMonthly Time Lags\n\n\n\n\nCode\n#Lag plots \ngglagplot(myts, do.lines=FALSE, lags=1)+xlab(\"Lag 1\")+ylab(\"Yi\")+ggtitle(\"Lag Plot for U.S GDP Growth Rate: 2010 - 2021\")\n\n\n\n\n\n\n\n\n\nCode\n#montly data\nmean_data <- gdp %>% \n  mutate(month = month(Date), year = year(Date)) %>% \n  group_by(year, month) %>% \n  summarize(mean_value = mean(value))\n\nmonth<-ts(mean_data$mean_value,star=decimal_date(as.Date(\"2010-01-01\",format = \"%Y-%m-%d\")),frequency = 4)\n\n#Lag plot for month\nts_lags(month,lags = c(1, 4, 7, 10) )\n\n\n\n\n\n\n\n\n\nThe lag plot shows that there is a cluster in the middle, and the monthly lag plot shows that there is no autocorrelation.\n\n\nSeasonality\n\nSeasonal HeatmapSeasonal Line plot\n\n\n\n\nCode\n# Create seasonal plot\nts_heatmap(month, color = \"Purples\",title = 'Seasonality U.S GDP Growth Rate: 2010 - 2021')\n\n\n\n\n\n\n\n\n\n\nCode\n# Create a line graph for each year with months on the x-axis\nggseasonplot(month, datecol = \"date\", valuecol = \"value\")+ggtitle(\"Seasonal Yearly Plot forU.S GDP Growth Rate: 2010 - 2021\")\n\n\n\n\n\n\n\n\nThe seasonality plots shows that series follow seasonality, as the heat map shows no much difference with each year and the line graph lie on similar values for most of the year.\n\n\nMoving Average\n\n4 Month MA1 Year MA3 Year MA\n\n\n\n\nCode\n#SMA Smoothing \nma <- autoplot(month, series=\"gdp\") +\n  autolayer(ma(month,5), series=\"4 Month MA\") +\n  xlab(\"Year\") + ylab(\"GWh\") +\n  ggtitle(\"GDP Growth Rate 2010 - 2022 (4 Month Moving Average)\") +\n  scale_colour_manual(values=c(\"Data\"=\"grey50\",\"4 Month MA\"=\"red\"),\n                      breaks=c(\"Data\",\"4 Month MA\"))\nma\n\n\n\n\n\n\n\n\n\nCode\n#SMA Smoothing \nma <- autoplot(month, series=\"gdp\") +\n  autolayer(ma(month,13), series=\"1 Year MA\") +\n  xlab(\"Year\") + ylab(\"GWh\") +\n  ggtitle(\"GDP Growth Rate 2010 - 2022 (1 Year Moving Average)\") +\n  scale_colour_manual(values=c(\"Data\"=\"grey50\",\"1 Year MA\"=\"red\"),\n                      breaks=c(\"Data\",\"1 Year MA\"))\nma\n\n\n\n\n\n\n\n\n\nCode\n#SMA Smoothing \nma <- autoplot(month, series=\"gdp\") +\n  autolayer(ma(month,37), series=\"3 Year MA\") +\n  xlab(\"Year\") + ylab(\"GWh\") +\n  ggtitle(\"GDP Growth Rate 2010 - 2022 (3 Year Moving Average)\") +\n  scale_colour_manual(values=c(\"Data\"=\"grey50\",\"3 Year MA\"=\"red\"),\n                      breaks=c(\"Data\",\"3 Year MA\"))\nma\n\n\n\n\n\n\n\n\nThe three plots show the same data series of GDP growth rate from 2010 to 2022, but each plot has a different moving average smoothing applied to it. The first plot shows a 4-month moving average, the second plot shows a 1-year moving average, and the third plot shows a 3-year moving average.\nLooking at the three plots, we can see that the 4-month moving average plot has a lot of fluctuations, and it follows the ups and downs of the original data series more closely. The 1-year moving average plot has less fluctuations compared to the 4-month moving average plot, and it provides a smoother trend of the data series. The 3-year moving average plot has even less fluctuations and a much smoother trend than the previous two plots.\nThe choice of moving average window size depends on the analyst’s preference and the objective of the analysis. Shorter window sizes like the 4-month moving average can provide more detailed insights into the data series, but they may also be more susceptible to noise and fluctuations. Longer window sizes like the 3-year moving average can provide a more stable and robust trend but may smooth out important details in the data series. As the moving average increases, GDP Growth Rtae tend to have no trend, it seem to be stable.\n\n\nAutocorrelation Time Series\n\nACFPACFADF Test\n\n\n\n\nCode\n#ACF plots for month data\nggAcf(myts)+ggtitle(\"ACF Plot for GDP Growth Rate: 2010 - 2022\")\n\n\n\n\n\n\n\n\n\nCode\n#PACF plots for month data\nggPacf(myts)+ggtitle(\"PACF Plot for GDP Growth Rate: 2010 - 2022\")\n\n\n\n\n\n\n\n\n\nCode\n# ADF Test\ntseries::adf.test(myts)\n\n\n\n    Augmented Dickey-Fuller Test\n\ndata:  myts\nDickey-Fuller = -5.768, Lag order = 3, p-value = 0.01\nalternative hypothesis: stationary\n\n\n\n\n\nThe above autocorrelation plots show that the series doesn’t change with the seasons, which indicates that there series is stationary. This is verified was checked using the Augmented Dickey-Fuller Test and the result of the test says that series is stationary because the p value is less than 0.05."
  },
  {
    "objectID": "eda_macroeconomic_inflation.html",
    "href": "eda_macroeconomic_inflation.html",
    "title": "EDA for Inflation Rate",
    "section": "",
    "text": "Inflation is a crucial macroeconomic indicator that measures the rate at which the prices of goods and services in an economy are rising over time. Exploratory Data Analysis (EDA) is a powerful technique for analyzing inflation rate data and gaining insights into the factors driving inflation. In this page, we will explore various EDA techniques that can be applied to inflation rate data, such as time series analysis, autocorrelation analysis, seasonality analysis, moving averages, and detrending. By the end of this page, you will have a better understanding of how to apply EDA techniques to inflation rate data and draw valuable insights from it.\n\nTime Series Plot\n\n\nCode\n#import the data\ninflation_rate <- read.csv(\"DATA/RAW DATA/inflation-rate.csv\")\n\n#cleaning the data\n#remove unwanted columns\ninflation_rate_clean <- subset(inflation_rate, select = -c(1,HALF1,HALF2))\n\n#convert the data to time series data\ninflation_data_ts <- ts(as.vector(t(as.matrix(inflation_rate_clean))), start=c(2010,1), end=c(2023,2), frequency=12)\n\n#export the data\nwrite.csv(inflation_rate_clean, \"DATA/CLEANED DATA/inflation_rate_clean_data.csv\", row.names=FALSE)\n\n\n#plot inflation rate \nfig <- autoplot(inflation_data_ts, ylab = \"Inflation Rate\", color=\"#DB7093\")+ggtitle(\"U.S Inflation Rate: January 2010 - February 2023\")+theme_bw()\nggplotly(fig)\n\n\n\n\n\n\nThe inflation rate in the United States has varied from year to year since 2010. From 2010 to 2018, the inflation rate generally remained below 2% per year, with some slight fluctuations. In 2016, it started to rise gradually and continued to increase until it reached a peak of 6.6% in June 2022. Since then, it has slightly decreased and as of February 2023. The COVID-19 pandemic has played a significant role in driving up inflation in the United States, as supply chain disruptions and increased demand have led to higher prices for goods and services. The Federal Reserve has taken steps to address inflation, including raising interest rates and reducing asset purchases, in order to keep it under control.\nBased on the plot of the inflation rate in the USA from 2010 to Feb 2023, it appears that there is a clear upward trend, and some level of seasonality as well. Therefore, it would be appropriate to use a multiplicative decomposition method for this time series data. A multiplicative model will allow us to separate the overall trend from the seasonal variations in a way that is appropriate for this type of data.\n\n\nDecomposed Time Series\n\nDecomposition PlotAdjusted Decomposition Plot\n\n\n\n\nCode\n#decomposition\norginial_plot <- autoplot(inflation_data_ts,xlab =\"Year\", ylab = \"Interest Rate\", main = \"U.S Inflation Rate: January 2010 - Feb 2023\")\ndecompose = decompose(inflation_data_ts,\"multiplicative\")\nautoplot(decompose)\n\n\n\n\n\n\n\n\n\nCode\ntrendadj <- inflation_data_ts/decompose$trend\ndecompose_adjtrend_plot <- autoplot(trendadj,ylab='seasonal') +ggtitle('Adjusted trend component in the additive time series model')\nseasonaladj <- inflation_data_ts/decompose$seasonal\ndecompose_adjseasonal_plot <- autoplot(seasonaladj,ylab='seasonal') +ggtitle('Adjusted seasonal component in the additive time series model')\ngrid.arrange(orginial_plot, decompose_adjtrend_plot,decompose_adjseasonal_plot, nrow=3)\n\n\n\n\n\n\n\n\nWhen compared to the original figure, the corrected seasonal component shows tend to show upward trend in the model, but the adjusted trend component has a stable trend through time with some fluctuation.\n\n\nLag Plots\n\nDaily Time LagsMonthly Time Lags\n\n\n\n\nCode\n#Lag plots \ngglagplot(inflation_data_ts, do.lines=FALSE, lags=1)+xlab(\"Lag 1\")+ylab(\"Yi\")+ggtitle(\"Lag Plot for U.S Inflation Rate: January 2010 - Feb 2023\")\n\n\n\n\n\n\n\n\n\nCode\n#Lag plot for monthly data\nts_lags(inflation_data_ts)\n\n\n\n\n\n\n\n\n\nThere should be a strong relationship between the series and the pertinent lag from January 2010 to Feb 2023 because there is a positive correlation and an inclination angle of 45 degrees in the lag plot. This is the characteristic lag plot of a process with positive autocorrelation. One observation and the next have a significant link, making such processes remarkably random. Investigating seasonality also involves graphing observations over a larger range of time intervals, or the lags. To make the time series data easier to understand and create graphs with more clarity, the time series data is combined with monthly data using the mean function. The second graph displays the variable’s monthly variation along the vertical axis. The lines link the points in the order of time. There is a correlation and it is significantly positive, supporting the seasonality of the data.\n\n\nSeasonality\n\nSeasonal HeatmapSeasonal Line plot\n\n\n\n\nCode\n# Create seasonal plot\nts_heatmap(inflation_data_ts,color = \"Reds\", title = 'Seasonality U.S Inflation Rate: 2010 - 2021')\n\n\n\n\n\n\n\n\n\n\nCode\n# Create a line graph for each year with months on the x-axis\nggseasonplot(inflation_data_ts, datecol = \"date\", valuecol = \"value\")+ggtitle(\"Seasonal Yearly Plot forU.S Inflation Rate: 2010 - 2021\")\n\n\n\n\n\n\n\n\nThe Seasonality Heatmap for the Inflation Rate data from JAN 2010 - March 2023 does indicate some significant seasonality for few years in the data, but there seem no to be seasonality in the line graph. To confirm the seasonality we can check on the acf plot for the series.\n\n\nMoving Average\n\n4 Month MA1 Year MA3 Year MA5 Year MA\n\n\n\n\nCode\n#SMA Smoothing \nma <- autoplot(inflation_data_ts, series=\"inflation_rate_clean\") +\n  autolayer(ma(inflation_data_ts,5), series=\"4 Month MA\") +\n  xlab(\"Year\") + ylab(\"GWh\") +\n  ggtitle(\"Inflation Rate January 2010 - Feb 2023 (4 Month Moving Average)\") +\n  scale_colour_manual(values=c(\"Data\"=\"grey50\",\"4 Month MA\"=\"red\"),\n                      breaks=c(\"Data\",\"4 Month MA\"))\nma\n\n\n\n\n\n\n\n\n\nCode\n#SMA Smoothing \nma <- autoplot(inflation_data_ts, series=\"inflation_rate_clean\") +\n  autolayer(ma(inflation_data_ts,13), series=\"1 Year MA\") +\n  xlab(\"Year\") + ylab(\"GWh\") +\n  ggtitle(\"Inflation Rate January 2010 - Feb 20233 (1 Year Moving Average)\") +\n  scale_colour_manual(values=c(\"Data\"=\"grey50\",\"1 Year MA\"=\"red\"),\n                      breaks=c(\"Data\",\"1 Year MA\"))\nma\n\n\n\n\n\n\n\n\n\nCode\n#SMA Smoothing \nma <- autoplot(inflation_data_ts, series=\"inflation_rate_clean\") +\n  autolayer(ma(inflation_data_ts,37), series=\"3 Year MA\") +\n  xlab(\"Year\") + ylab(\"GWh\") +\n  ggtitle(\"Inflation Rate January 2010 - Feb 2023 (3 Year Moving Average)\") +\n  scale_colour_manual(values=c(\"Data\"=\"grey50\",\"3 Year MA\"=\"red\"),\n                      breaks=c(\"Data\",\"3 Year MA\"))\nma\n\n\n\n\n\n\n\n\n\nCode\n#SMA Smoothing \nma <- autoplot(inflation_data_ts, series=\"inflation_rate_clean\") +\n  autolayer(ma(inflation_data_ts,61), series=\"5 Year MA\") +\n  xlab(\"Year\") + ylab(\"GWh\") +\n  ggtitle(\"Inflation Rate January 2010 - Feb 2023 (5 Year Moving Average)\") +\n  scale_colour_manual(values=c(\"Data\"=\"grey50\",\"5 Year MA\"=\"red\"),\n                      breaks=c(\"Data\",\"5 Year MA\"))\nma\n\n\n\n\n\n\n\n\nThe Inflation Rate is displayed for the time period between January 2010 and Feb 2023 in the four plots above, which have been smoothed using 4-month, 1-year, 3-year, and 5-year moving averages. We can observe from the graphs that the moving average values have been rising over time. Given that it captures the short-term differences in stock prices, the 4-month MA plot exhibits a great deal of volatility, which is to be expected. The 1-year, 3-year, and 5-year MA plots, on the other hand, tame the oscillations and reveal the general direction of stock prices. We can see that the 5-year MA plot, which considers a longer time period than the other plots, shows a trend that is smoother. The 3-year MA plot similarly shows a fairly smooth trend, but unlike the 5-year MA plot, it also shows shorter-term variability. Even more susceptible to short-term changes in stock prices is the 1-year MA plot. As the moving average increases we can notive that the trend for Inflation Rate isn’t stable is towards upward.\n\n\nAutocorrelation Time Series\n\nACFPACFADF Test\n\n\n\n\nCode\n#ACF plots for monthly data\nggAcf(inflation_data_ts)+ggtitle(\"ACF Plot for Inflation Rate: January 2010 - Feb 2023\")\n\n\n\n\n\n\n\n\n\nCode\n#PACF plots for monthly data\nggPacf(inflation_data_ts)+ggtitle(\"PACF Plot for Inflation Rate: January 2010 - Feb 2023\")\n\n\n\n\n\n\n\n\n\nCode\n# ADF Test\ntseries::adf.test(inflation_data_ts)\n\n\n\n    Augmented Dickey-Fuller Test\n\ndata:  inflation_data_ts\nDickey-Fuller = -1.7207, Lag order = 5, p-value = 0.6928\nalternative hypothesis: stationary\n\n\n\n\n\nThe autocorrelation function plot, which is the acf graph for monthly data, clearly shows autocorrelation in lag and seasonality. The series appears to be seasonal, according to the lag plots and autocorrelation plots displayed above, proving that it is not stable. The Augmented Dickey-Fuller Test, which reveals that the series is not stationary if the p value is more than 0.05, was also used to validate it. The p value obtained from ADF test is greater than 0.05, which indicates taht the series is not stationary.\n\n\nDetrend and Differenced Time Series\n\nLinear Fitting ModelACF Plot\n\n\n\n\nCode\nfit = lm(inflation_data_ts~time(inflation_data_ts), na.action=NULL) \nsummary(fit) \n\n\n\nCall:\nlm(formula = inflation_data_ts ~ time(inflation_data_ts), na.action = NULL)\n\nResiduals:\n    Min      1Q  Median      3Q     Max \n-2.1136 -0.6549 -0.1224  0.4632  2.8301 \n\nCoefficients:\n                          Estimate Std. Error t value Pr(>|t|)    \n(Intercept)             -474.68145   43.32414  -10.96   <2e-16 ***\ntime(inflation_data_ts)    0.23655    0.02148   11.01   <2e-16 ***\n---\nSignif. codes:  0 '***' 0.001 '**' 0.01 '*' 0.05 '.' 0.1 ' ' 1\n\nResidual standard error: 1.026 on 156 degrees of freedom\nMultiple R-squared:  0.4373,    Adjusted R-squared:  0.4337 \nF-statistic: 121.2 on 1 and 156 DF,  p-value: < 2.2e-16\n\n\n\n\n\n\nCode\n# plot ACFs\nplot1 <- ggAcf(inflation_data_ts, 48, main=\"Original Data: Inflation Rate\")\nplot2 <- ggAcf(resid(fit), 48, main=\"Detrended data\") \nplot3 <- ggAcf(diff(inflation_data_ts), 48, main=\"First differenced data\")\ngrid.arrange(plot1, plot2, plot3,nrow=3)\n\n\n\n\n\n\n\n\nThe estimated slope coefficient β1, 0.23655 With a standard error of 0.02148, yielding a significant estimated increase of stock price is very less yearly. Equation of the fit for stationary process: \\[\\hat{y}_{t} = x_{t}+(474.68145)-(0.23655)t\\]\nFrom the above graph we can say that there is correlation in the original plot, but in the detrended plot the correlation is reduced but there is still high correlation in the detrended data.But when the first order difference is applied the high correlation is removed but there is seasonal correlation.\nAs depicted in the above figure, the series is now stationary and ready for future study."
  },
  {
    "objectID": "eda_macroeconomic_interest.html",
    "href": "eda_macroeconomic_interest.html",
    "title": "EDA for Interest Rate",
    "section": "",
    "text": "Interest rate is a critical macroeconomic variable that plays a key role in shaping economic growth and financial stability. Exploratory Data Analysis (EDA) is a powerful technique for analyzing interest rate data and gaining insights into the factors driving interest rate movements. In this page, we will explore various EDA techniques that can be applied to interest rate data, such as time series analysis, autocorrelation analysis, seasonality analysis, moving averages, and detrending. By the end of this page, you will have a better understanding of how to apply EDA techniques to interest rate data and draw valuable insights from it.\n\nTime Series Plot\n\n\nCode\n#import the data\ninterest_data <- read.csv(\"DATA/RAW DATA/interest-rate.csv\")\n\n#change date format\ninterest_data$Date <- as.Date(interest_data$Date , \"%m/%d/%Y\")\n\n#export the cleaned data\ninterest_clean_data <- interest_data\nwrite.csv(interest_clean_data, \"DATA/CLEANED DATA/interest_rate_clean_data.csv\", row.names=FALSE)\n\n#plot interest rate \nfig <- plot_ly(interest_data, x = ~Date, y = ~value, type = 'scatter', mode = 'lines',line = list(color='rgb(176,224,230)'))\nfig <- fig %>% layout(title = \"U.S Interest Rate: January 2010 - March 2023\",xaxis = list(title = \"Time\"),yaxis = list(title =\"Interest Rate\"))\nfig\n\n\n\n\n\n\nThe interest rate in the United States has exhibited fluctuations from 2010 to 2022. In the years following the 2008 global financial crisis, the interest rate was very low. The interest rate in the US was very low and stable from 2010 to 2018. During this period, the Federal Reserve implemented several monetary policy measures, such as quantitative easing and forward guidance, in order to stimulate the economy and support economic recovery after the global financial crisis.\nStarting in 2018, the Federal Reserve began a gradual process of raising interest rates as the US economy continued to improve. This process of increasing interest rates was driven by a combination of factors such as low unemployment rates, a steady increase in GDP, and the need to prevent inflation from rising too quickly.\nHowever, as global economic conditions became more uncertain, the Federal Reserve began to pause its process of increasing interest rates. The US-China trade war and concerns about the potential impact of Brexit led to a more cautious approach from the Federal Reserve. In 2019, the Federal Reserve lowered interest rates three times in response to these external factors.\nIn 2020, the COVID-19 pandemic caused a major shock to the global economy, leading the Federal Reserve to take unprecedented measures to support the US economy. The Federal Reserve lowered interest rates to near-zero levels, implemented quantitative easing, and established several lending facilities to support businesses and households.\nThere is no clear evidence of a relationship between the variability of the series and its level, which suggests that an additive model might be more appropriate. Additionally, an additive model can be useful when the trend is relatively stable and the amplitude of seasonal fluctuations remains constant over time.\n\n\nDecomposed Time Series\n\nDecomposition PlotAdjusted Decomposition Plot\n\n\n\n\nCode\n#convert to ts data\nmyts<-ts(interest_data$value,frequency=12,start=c(2010/1/1))\n#decomposition\norginial_plot <- autoplot(myts,xlab =\"Year\", ylab = \"Interest Rate\", main = \"U.S Interest Rate: January 2010 - March 2023\")\ndecompose = decompose(myts,\"additive\")\nautoplot(decompose)\n\n\n\n\n\n\n\n\n\nCode\ntrendadj <- myts/decompose$trend\ndecompose_adjtrend_plot <- autoplot(trendadj,ylab='seasonal') +ggtitle('Adjusted trend component in the additive time series model')\nseasonaladj <- myts/decompose$seasonal\ndecompose_adjseasonal_plot <- autoplot(seasonaladj,ylab='seasonal') +ggtitle('Adjusted seasonal component in the additive time series model')\ngrid.arrange(orginial_plot, decompose_adjtrend_plot,decompose_adjseasonal_plot, nrow=3)\n\n\n\n\n\n\n\n\nWhen compared to the original figure, the corrected seasonal component shows seasonality in the model, but the adjusted trend component has a stable trend through time.\n\n\nLag Plots\n\nDaily Time LagsMonthly Time Lags\n\n\n\n\nCode\n#Lag plots \ngglagplot(myts, do.lines=FALSE, lags=1)+xlab(\"Lag 1\")+ylab(\"Yi\")+ggtitle(\"Lag Plot for U.S Interest Rate: January 2010 - March 2023\")\n\n\n\n\n\n\n\n\n\nCode\n#monthly data\nmean_data <- interest_data %>% \n  mutate(month = month(Date), year = year(Date)) %>% \n  group_by(year, month) %>% \n  summarize(mean_value = mean(value))\n\nmonth<-ts(mean_data$mean_value,star=decimal_date(as.Date(\"2010-01-01\",format = \"%Y-%m-%d\")),frequency = 12)\n\n#Lag plot for monthly data\nts_lags(month)\n\n\n\n\n\n\n\n\n\nThere should be a strong relationship between the series and the pertinent lag from January 2010 to March 2023 because there is a positive correlation and an inclination angle of 45 degrees in the lag plot. This is the characteristic lag plot of a process with strong positive autocorrelation. One observation and the next have a significant link, making such processes remarkably non-random. Investigating seasonality also involves graphing observations over a larger range of time intervals, or the lags. To make the time series data easier to understand and create graphs with more clarity, the time series data is combined with monthly data using the mean function. A closer look at the previous graph indicates that there are more dots on the diagonal line at 45 degrees. The second graph displays the variable’s monthly variation along the vertical axis. The lines link the points in the order of time. There is a correlation and it is significantly positive, supporting the strong seasonality of the data.\n\n\nSeasonality\n\nSeasonal HeatmapSeasonal Line plot\n\n\n\n\nCode\n# Create seasonal plot\nts_heatmap(month,color = \"Greens\", title = 'Seasonality U.S Interest Rate: 2010 - 2021')\n\n\n\n\n\n\n\n\n\n\nCode\n# Create a line graph for each year with months on the x-axis\nggseasonplot(month, datecol = \"date\", valuecol = \"value\")+ggtitle(\"Seasonal Yearly Plot forU.S Interest Rate: 2010 - 2021\")\n\n\n\n\n\n\n\n\nThe seasonality plots shows that series follow seasonality, as the heat map shows no much difference with each year and the line graph shows some kind of seasonality as th is rise in the rate during september this can be confirmed using the acf plot.\n\n\nMoving Average\n\n4 Month MA1 Year MA3 Year MA5 Year MA\n\n\n\n\nCode\n#SMA Smoothing \nma <- autoplot(month, series=\"interest_data\") +\n  autolayer(ma(month,5), series=\"4 Month MA\") +\n  xlab(\"Year\") + ylab(\"GWh\") +\n  ggtitle(\"Interest Rate January 2010 - March 2023 (4 Month Moving Average)\") +\n  scale_colour_manual(values=c(\"Data\"=\"grey50\",\"4 Month MA\"=\"red\"),\n                      breaks=c(\"Data\",\"4 Month MA\"))\nma\n\n\n\n\n\n\n\n\n\nCode\n#SMA Smoothing \nma <- autoplot(month, series=\"interest_data\") +\n  autolayer(ma(month,13), series=\"1 Year MA\") +\n  xlab(\"Year\") + ylab(\"GWh\") +\n  ggtitle(\"Interest Rate January 2010 - March 2023 (1 Year Moving Average)\") +\n  scale_colour_manual(values=c(\"Data\"=\"grey50\",\"1 Year MA\"=\"red\"),\n                      breaks=c(\"Data\",\"1 Year MA\"))\nma\n\n\n\n\n\n\n\n\n\nCode\n#SMA Smoothing \nma <- autoplot(month, series=\"interest_data\") +\n  autolayer(ma(month,37), series=\"3 Year MA\") +\n  xlab(\"Year\") + ylab(\"GWh\") +\n  ggtitle(\"Interest Rate January 2010 - March 2023 (3 Year Moving Average)\") +\n  scale_colour_manual(values=c(\"Data\"=\"grey50\",\"3 Year MA\"=\"red\"),\n                      breaks=c(\"Data\",\"3 Year MA\"))\nma\n\n\n\n\n\n\n\n\n\nCode\n#SMA Smoothing \nma <- autoplot(month, series=\"interest_data\") +\n  autolayer(ma(month,61), series=\"5 Year MA\") +\n  xlab(\"Year\") + ylab(\"GWh\") +\n  ggtitle(\"Interest Rate January 2010 - March 2023 (5 Year Moving Average)\") +\n  scale_colour_manual(values=c(\"Data\"=\"grey50\",\"5 Year MA\"=\"red\"),\n                      breaks=c(\"Data\",\"5 Year MA\"))\nma\n\n\n\n\n\n\n\n\nThe Interest Rate is displayed for the time period between January 2010 and March 2023 in the four plots above, which have been smoothed using 4-month, 1-year, 3-year, and 5-year moving averages. We can observe from the graphs that the moving average values have been rising over time. Given that it captures the short-term differences in stock prices, the 4-month MA plot exhibits a great deal of volatility, which is to be expected. The 1-year, 3-year, and 5-year MA plots, on the other hand, tame the oscillations and reveal the general direction of stock prices. We can see that the 5-year MA plot, which considers a longer time period than the other plots, shows a trend that is smoother. The 3-year MA plot similarly shows a fairly smooth trend, but unlike the 5-year MA plot, it also shows shorter-term variability. Even more susceptible to short-term changes in stock prices is the 1-year MA plot. The trend for Interest Rate isn’t stable, there is fluctuation in the trend but it seems that there is upward trend from 2020.\n\n\nAutocorrelation Time Series\n\nACFPACFADF Test\n\n\n\n\nCode\n#ACF plots for monthly data\nggAcf(myts)+ggtitle(\"ACF Plot for Interest Rate: January 2010 - March 2023\")\n\n\n\n\n\n\n\n\n\nCode\n#PACF plots for monthly data\nggPacf(myts)+ggtitle(\"PACF Plot for Interest Rate: January 2010 - March 2023\")\n\n\n\n\n\n\n\n\n\nCode\n# ADF Test\ntseries::adf.test(myts)\n\n\n\n    Augmented Dickey-Fuller Test\n\ndata:  myts\nDickey-Fuller = -1.3832, Lag order = 5, p-value = 0.8336\nalternative hypothesis: stationary\n\n\n\n\n\nThe autocorrelation function plot, which is the acf graph for monthly data, clearly shows autocorrelation in lag and seasonality. The series appears to be seasonal, according to the lag plots and autocorrelation plots displayed above, proving that it is not stable. The Augmented Dickey-Fuller Test, which reveals that the series is not stationary if the p value is more than 0.05, was also used to validate it.\n\n\nDetrend and Differenced Time Series\n\nLinear Fitting ModelACF Plot\n\n\n\n\nCode\nfit = lm(myts~time(myts), na.action=NULL) \nsummary(fit) \n\n\n\nCall:\nlm(formula = myts ~ time(myts), na.action = NULL)\n\nResiduals:\n     Min       1Q   Median       3Q      Max \n-0.97697 -0.33564  0.00804  0.23469  1.46755 \n\nCoefficients:\n              Estimate Std. Error t value Pr(>|t|)\n(Intercept) -18.300672  20.233604  -0.904    0.367\ntime(myts)    0.009339   0.010034   0.931    0.353\n\nResidual standard error: 0.4839 on 157 degrees of freedom\nMultiple R-squared:  0.005488,  Adjusted R-squared:  -0.0008465 \nF-statistic: 0.8664 on 1 and 157 DF,  p-value: 0.3534\n\n\n\n\n\n\nCode\nplot1 <- ggAcf(myts, 48, main=\"Original Data: Interest Rate\")\nplot2 <- ggAcf(resid(fit), 48, main=\"Detrended data\") \nplot3 <- ggAcf(diff(myts), 48, main=\"First differenced data\")\ngrid.arrange(plot1, plot2, plot3,nrow=3)\n\n\n\n\n\n\n\n\nThe estimated slope coefficient β1, 0.00933. With a standard error of 0.010034, yielding a significant estimated increase of stock price is very less yearly. Equation of the fit for stationary process: \\[\\hat{y}_{t} = x_{t}+(18.300672)-(0.00933.)t\\]\nFrom the above graph we can say that there is high correlation in the original plot, but in the detrended plot the correlation is reduced but there is still high correlation in the detrended data.But when the first order difference is applied the high correlation is removed but there is seasonal correlation.\nAs depicted in the above figure, the series is now stationary and ready for future study."
  },
  {
    "objectID": "eda_macroeconomic_unemployment.html",
    "href": "eda_macroeconomic_unemployment.html",
    "title": "EDA for Unemployment Rate",
    "section": "",
    "text": "Unemployment is a critical macroeconomic indicator that measures the percentage of the labor force that is actively seeking employment but unable to find work. Exploratory Data Analysis (EDA) is a powerful technique for analyzing unemployment rate data and gaining insights into the factors driving unemployment. In this page, we will explore various EDA techniques that can be applied to unemployment rate data, such as time series analysis, autocorrelation analysis, seasonality analysis, moving averages, and detrending. By the end of this page, you will have a better understanding of how to apply EDA techniques to unemployment rate data and draw valuable insights from it.\n\nTime Series Plot\n\n\nCode\n#import the data\nunemployment_rate <- read.csv(\"DATA/RAW DATA/unemployment-rate.csv\")\n\n#change date format\nunemployment_rate$Date <- as.Date(unemployment_rate$Date , \"%m/%d/%Y\")\n\n# export the data\nwrite.csv(unemployment_rate, \"DATA/CLEANED DATA/unemployment_rate_clean_data.csv\", row.names=FALSE)\n\n#plot unemployment rate \nfig <- plot_ly(unemployment_rate, x = ~Date, y = ~Value, type = 'scatter', mode = 'lines',line = list(color = 'rgb(255,215,0)'))\nfig <- fig %>% layout(title = \"U.S Unemployment Rate: January 2010 - March 2023\",xaxis = list(title = \"Time\"),yaxis = list(title =\"Unemployment Rate\"))\nfig\n\n\n\n\n\n\nThe unemployment rate in the United States has seen significant fluctuations since 2010, with various economic factors contributing to changes in the rate over the years. The data from the Federal Reserve Economic Data (FRED) series shows that the unemployment rate peaked at 9.9% in 2010, following the 2008 financial crisis. However, it has steadily declined over the years and currently stands at 3.9% as of February 2023.\nThe first half of the 2010s saw a slow but steady decline in the unemployment rate, dropping from the 9.9% peak in 2010 to 5.3% by 2018. The latter half of the decade saw even further improvements, with the rate hitting a low of 3.5% in September 2019. However, the onset of the COVID-19 pandemic in early 2020 led to a sharp increase in unemployment, with the rate skyrocketing to 14.8% in April of that year.\nSince then, the unemployment rate has been slowly but steadily improving as the economy recovers from the pandemic-induced recession. By the end of 2021, the rate had fallen to 4.2% and has continued to decline into 2022 and 2023. However, it is worth noting that some industries and sectors are still struggling to recover from the pandemic, and some individuals have not yet returned to the labor force, which could impact the overall unemployment rate.\nOverall, the unemployment rate in the United States has undergone significant fluctuations over the past decade, with various economic and social factors contributing to the changes.\nFrom the graph, it appears that the magnitude of the seasonal fluctuations in the unemployment rate has remained relatively constant over time, while the overall trend has shown both increasing and decreasing phases. Therefore, an additive decomposition method could be appropriate for analyzing the time series data.\n\n\nDecomposed Time Series\n\nDecomposition PlotAdjusted Decomposition Plot\n\n\n\n\nCode\n#convert the data to ts data\nmyts<-ts(unemployment_rate$Value,frequency=12,start=c(2010/1/1))\n#decomposition\norginial_plot <- autoplot(myts,xlab =\"Year\", ylab = \"Interest Rate\", main = \"U.S Unemployment Rate: January 2010 - March 2023\")\ndecompose = decompose(myts,\"additive\")\nautoplot(decompose)\n\n\n\n\n\n\n\n\n\nCode\ntrendadj <- myts/decompose$trend\ndecompose_adjtrend_plot <- autoplot(trendadj,ylab='seasonal') +ggtitle('Adjusted trend component in the additive time series model')\nseasonaladj <- myts/decompose$seasonal\ndecompose_adjseasonal_plot <- autoplot(seasonaladj,ylab='seasonal') +ggtitle('Adjusted seasonal component in the additive time series model')\ngrid.arrange(orginial_plot, decompose_adjtrend_plot,decompose_adjseasonal_plot, nrow=3)\n\n\n\n\n\n\n\n\nWhen compared to the original figure, the corrected seasonal component shows some seasonality in the model, but the adjusted trend component has a stable trend through time with a high increase due to pandemic, but there the trend dropped after few months.\n\n\nLag Plots\n\nDaily Time LagsMonthly Time Lags\n\n\n\n\nCode\n#Lag plots \ngglagplot(myts, do.lines=FALSE, lags=1)+xlab(\"Lag 1\")+ylab(\"Yi\")+ggtitle(\"Lag Plot for U.S Unemployment Rate: January 2010 - March 2023\")\n\n\n\n\n\n\n\n\n\nCode\n#monthly data\nmean_data <- unemployment_rate %>% \n  mutate(month = month(Date), year = year(Date)) %>% \n  group_by(year, month) %>% \n  summarize(mean_value = mean(Value))\nmonth<-ts(mean_data$mean_value,star=decimal_date(as.Date(\"2010-01-01\",format = \"%Y-%m-%d\")),frequency = 12)\n#Lag plot for monthly data\nts_lags(month)\n\n\n\n\n\n\n\n\n\nThere should be a strong relationship between the series and the pertinent lag from January 2010 to March 2023 because there is a positive correlation and an inclination angle of 45 degrees in the lag plot. This is the characteristic lag plot of a process with strong positive autocorrelation. One observation and the next have a significant link, making such processes remarkably non-random. Investigating seasonality also involves graphing observations over a larger range of time intervals, or the lags. To make the time series data easier to understand and create graphs with more clarity, the time series data is combined with monthly data using the mean function. A closer look at the previous graph indicates that there are more dots on the diagonal line at 45 degrees. The second graph displays the variable’s monthly variation along the vertical axis. The lines link the points in the order of time. There is a correlation and it is significantly positive, supporting the strong seasonality of the data.\n\n\nSeasonality\n\nSeasonal HeatmapSeasonal Line plot\n\n\n\n\nCode\n# Create seasonal plot\nts_heatmap(myts, color = \"Oranges\", title = \"Seasonality plot for Unemployment Rate USA: 2010 - 2021\")\n\n\n\n\n\n\n\n\n\n\nCode\n# Create a line graph for each year with months on the x-axis\nggseasonplot(month, datecol = \"date\", valuecol = \"value\")+ggtitle(\"Seasonal Yearly Plot forU.S Interest Rate: 2010 - 2021\")\n\n\n\n\n\n\n\n\nThe Seasonality Heatmap for the Unemployment Rate data from JAN 2010 - March 2023 does not indicate any significant seasonality in the data. The heatmap displays the mean value of the time series for each month and year combination, with darker colors indicating higher values. The absence of any discernible patterns or darker colors in specific months or years suggests that there is no consistent seasonal trend in the data. However, the yearly line graph shows some variations in the interest rates over the years, with some years showing higher rates than others. Each year’s data is represented by a line, and the months are plotted on the x-axis. Overall, the lack of clear seasonality in both the heatmap and yearly line graph suggests that other factors beyond seasonality, such as economic conditions, government policies, and global events, are likely driving the fluctuations in unemployment rates. #### Moving Average\n\n4 Month MA1 Year MA3 Year MA5 Year MA\n\n\n\n\nCode\n#SMA Smoothing \nma <- autoplot(month, series=\"unemployment_rate\") +\n  autolayer(ma(month,5), series=\"4 Month MA\") +\n  xlab(\"Year\") + ylab(\"GWh\") +\n  ggtitle(\"Unemployment Rate January 2010 - March 2023 (4 Month Moving Average)\") +\n  scale_colour_manual(values=c(\"Data\"=\"grey50\",\"4 Month MA\"=\"red\"),\n                      breaks=c(\"Data\",\"4 Month MA\"))\nma\n\n\n\n\n\n\n\n\n\nCode\n#SMA Smoothing \nma <- autoplot(month, series=\"unemployment_rate\") +\n  autolayer(ma(month,13), series=\"1 Year MA\") +\n  xlab(\"Year\") + ylab(\"GWh\") +\n  ggtitle(\"Unemployment Rate January 2010 - March 2023 (1 Year Moving Average)\") +\n  scale_colour_manual(values=c(\"Data\"=\"grey50\",\"1 Year MA\"=\"red\"),\n                      breaks=c(\"Data\",\"1 Year MA\"))\nma\n\n\n\n\n\n\n\n\n\nCode\n#SMA Smoothing \nma <- autoplot(month, series=\"unemployment_rate\") +\n  autolayer(ma(month,37), series=\"3 Year MA\") +\n  xlab(\"Year\") + ylab(\"GWh\") +\n  ggtitle(\"Unemployment Rate January 2010 - March 2023 (3 Year Moving Average)\") +\n  scale_colour_manual(values=c(\"Data\"=\"grey50\",\"3 Year MA\"=\"red\"),\n                      breaks=c(\"Data\",\"3 Year MA\"))\nma\n\n\n\n\n\n\n\n\n\nCode\n#SMA Smoothing \nma <- autoplot(month, series=\"unemployment_rate\") +\n  autolayer(ma(month,61), series=\"5 Year MA\") +\n  xlab(\"Year\") + ylab(\"GWh\") +\n  ggtitle(\"Unemployment Rate January 2010 - March 2023 (5 Year Moving Average)\") +\n  scale_colour_manual(values=c(\"Data\"=\"grey50\",\"5 Year MA\"=\"red\"),\n                      breaks=c(\"Data\",\"5 Year MA\"))\nma\n\n\n\n\n\n\n\n\nThe Unemployment Rate is displayed for the time period between January 2010 and March 2023 in the four plots above, which have been smoothed using 4-month, 1-year, 3-year, and 5-year moving averages. We can observe from the graphs that the moving average values have been rising over time. Given that it captures the short-term differences in stock prices, the 4-month MA plot exhibits a great deal of volatility, which is to be expected. The 1-year, 3-year, and 5-year MA plots, on the other hand, tame the oscillations and reveal the general direction of stock prices. We can see that the 5-year MA plot, which considers a longer time period than the other plots, shows a trend that is smoother. The 3-year MA plot similarly shows a fairly smooth trend, but unlike the 5-year MA plot, it also shows shorter-term variability. Even more susceptible to short-term changes in stock prices is the 1-year MA plot. The trend for Unemployment Rate was downward from 2010 to 2019, but there is increase in the moving average due to the increase in unemployment rate in US during pandemic.\n\n\nAutocorrelation Time Series\n\nACFPACFADF Test\n\n\n\n\nCode\n#ACF plots \nggAcf(myts)+ggtitle(\"ACF Plot for Unemployment Rate: January 2010 - March 2023\")\n\n\n\n\n\n\n\n\n\nCode\n#PACF plots for monthly data\nggPacf(myts)+ggtitle(\"PACF Plot for Unemployment Rate: January 2010 - March 2023\")\n\n\n\n\n\n\n\n\n\nCode\n# ADF Test\ntseries::adf.test(myts)\n\n\n\n    Augmented Dickey-Fuller Test\n\ndata:  myts\nDickey-Fuller = -2.7517, Lag order = 5, p-value = 0.2629\nalternative hypothesis: stationary\n\n\n\n\n\nThe autocorrelation function plot, which is the acf graph for monthly data, clearly shows autocorrelation in lag. The series appears to be seasonal, according to the lag plots and autocorrelation plots displayed above, proving that it is not stable. The Augmented Dickey-Fuller Test, which reveals that the series is not stationary if the p value is more than 0.05, was also used to validate it. The p value obtained from ADF test is greater than 0.05, which indicates taht the series is not stationary.\n\n\nDetrend and Differenced Time Series\n\nLinear Fitting ModelACF Plot\n\n\n\n\nCode\nfit = lm(myts~time(myts), na.action=NULL) \nsummary(fit) \n\n\n\nCall:\nlm(formula = myts ~ time(myts), na.action = NULL)\n\nResiduals:\n    Min      1Q  Median      3Q     Max \n-1.5761 -1.2396 -0.2468  0.7124 10.0644 \n\nCoefficients:\n             Estimate Std. Error t value Pr(>|t|)    \n(Intercept) 785.15846   72.32427   10.86   <2e-16 ***\ntime(myts)   -0.38635    0.03587  -10.77   <2e-16 ***\n---\nSignif. codes:  0 '***' 0.001 '**' 0.01 '*' 0.05 '.' 0.1 ' ' 1\n\nResidual standard error: 1.713 on 156 degrees of freedom\nMultiple R-squared:  0.4266,    Adjusted R-squared:  0.4229 \nF-statistic:   116 on 1 and 156 DF,  p-value: < 2.2e-16\n\n\n\n\n\n\nCode\n# plot ACFs\nplot1 <- ggAcf(myts, 48, main=\"Original Data: Unemployment Rate\")\nplot2 <- ggAcf(resid(fit), 48, main=\"Detrended data\") \nplot3 <- ggAcf(diff(myts), 48, main=\"First differenced data\")\ngrid.arrange(plot1, plot2, plot3,nrow=3)\n\n\n\n\n\n\n\n\nThe estimated slope coefficient β1, -0.38635 With a standard error of 0.03587, yielding a significant estimated increase of stock price is very less yearly. Equation of the fit for stationary process: \\[\\hat{y}_{t} = x_{t}+(785.15846)-(0.38635)t\\]\nFrom the above graph we can say that there is high correlation in the original plot, but in the detrended plot the correlation is reduced but there is still correlation in the detrended data.But when the first order difference is applied the high correlation is removed but there is seasonal correlation.\nAs depicted in the above figure, the series is now stationary and ready for future study."
  },
  {
    "objectID": "eda_sector_communication_services.html",
    "href": "eda_sector_communication_services.html",
    "title": "EDA for Communication Services Sector Fund",
    "section": "",
    "text": "The Communication Services Sector Fund (XLC) is an exchange-traded fund (ETF) that seeks to track the performance of companies in the communication services sector of the S&P 500 index. The fund’s holdings include companies such as Alphabet Inc., Facebook Inc., and Verizon Communications Inc. The communication services sector is a relatively new sector, created in 2018, and encompasses companies that provide communication services such as telecommunication, media, and entertainment. The XLC ETF provides investors with exposure to the growth potential of these companies, which are often at the forefront of technological innovation and changing consumer habits. The fund is relatively diversified, with holdings across different sub-sectors within communication services. As with any ETF, the XLC fund provides investors with the ability to invest in a portfolio of stocks with a single trade, making it a convenient option for those seeking exposure to the communication services sector.\n\nTime Series Plot\n\n\nCode\n# get data\noptions(\"getSymbols.warning4.0\"=FALSE)\noptions(\"getSymbols.yahoo.warning\"=FALSE)\n\n\ndata = getSymbols(\"XLC\",src='yahoo', from = '2018-06-19',to = \"2023-03-01\")\n\ndf <- data.frame(Date=index(XLC),coredata(XLC))\n\n# create Bollinger Bands\nbbands <- BBands(XLC[,c(\"XLC.High\",\"XLC.Low\",\"XLC.Close\")])\n\n# join and subset data\ndf <- subset(cbind(df, data.frame(bbands[,1:3])), Date >= \"2018-06-19\")\n\n#export the data \nxlc_data <- df\nwrite.csv(xlc_data, \"DATA/CLEANED DATA/xlc_raw_data.csv\", row.names=FALSE)\n\n# colors column for increasing and decreasing\nfor (i in 1:length(df[,1])) {\n  if (df$XLC.Close[i] >= df$XLC.Open[i]) {\n      df$direction[i] = 'Increasing'\n  } else {\n      df$direction[i] = 'Decreasing'\n  }\n}\n\ni <- list(line = list(color = '#6F9860'))\nd <- list(line = list(color = '#7F7F7F'))\n\n# plot candlestick chart\n\nfig <- df %>% plot_ly(x = ~Date, type=\"candlestick\",\n          open = ~XLC.Open, close = ~XLC.Close,\n          high = ~XLC.High, low = ~XLC.Low, name = \"XLC\",\n          increasing = i, decreasing = d) \nfig <- fig %>% add_lines(x = ~Date, y = ~up , name = \"B Bands\",\n            line = list(color = '#ccc', width = 0.5),\n            legendgroup = \"Bollinger Bands\",\n            hoverinfo = \"none\", inherit = F) \nfig <- fig %>% add_lines(x = ~Date, y = ~dn, name = \"B Bands\",\n            line = list(color = '#ccc', width = 0.5),\n            legendgroup = \"Bollinger Bands\", inherit = F,\n            showlegend = FALSE, hoverinfo = \"none\") \nfig <- fig %>% add_lines(x = ~Date, y = ~mavg, name = \"Mv Avg\",\n            line = list(color = '#E377C2', width = 0.5),\n            hoverinfo = \"none\", inherit = F) \nfig <- fig %>% layout(yaxis = list(title = \"Price\"))\n\n# plot volume bar chart\nfig2 <- df \nfig2 <- fig2 %>% plot_ly(x=~Date, y=~XLC.Volume, type='bar', name = \"XLC Volume\",\n          color = ~direction, colors = c('#6F9860','#7F7F7F')) \nfig2 <- fig2 %>% layout(yaxis = list(title = \"Volume\"))\n\n# create rangeselector buttons\nrs <- list(visible = TRUE, x = 0.5, y = -0.055,\n           xanchor = 'center', yref = 'paper',\n           font = list(size = 9),\n           buttons = list(\n             list(count=1,\n                  label='RESET',\n                  step='all'),\n             list(count=3,\n                  label='3 YR',\n                  step='year',\n                  stepmode='backward'),\n             list(count=1,\n                  label='1 YR',\n                  step='year',\n                  stepmode='backward'),\n             list(count=1,\n                  label='1 MO',\n                  step='month',\n                  stepmode='backward')\n           ))\n\n# subplot with shared x axis\nfig <- subplot(fig, fig2, heights = c(0.7,0.2), nrows=2,\n             shareX = TRUE, titleY = TRUE)\nfig <- fig %>% layout(title = paste(\"Communication Services Sector Fund Stock Price: June 2018 - March 2023\"),\n         xaxis = list(rangeselector = rs),\n         legend = list(orientation = 'h', x = 0.5, y = 1,\n                       xanchor = 'center', yref = 'paper',\n                       font = list(size = 10),\n                       bgcolor = 'transparent'))\n\nfig\n\n\n\n\n\n\nSince its inception in June 2018, the Communication Services Sector Fund (XLC) has exhibited both upward and downward trends in its stock price. The fund started off on a positive note, rising steadily in 2018 and reaching new highs by the end of the year. However, like most other stocks and funds, it experienced a significant decline in value in early 2020 due to the COVID-19 pandemic, losing over 33% of its value in just a few weeks.\nThe XLC fund then regained some of its losses, aided by government stimulus measures and low-interest rates, and continued its upward trend throughout 2020 and into 2021. However, the fund experienced some volatility during the latter part of 2021 and early 2022, with a few significant dips in its stock price. This volatility can be attributed to various factors such as rising interest rates, concerns about inflation, and geopolitical tensions.\nOverall, the Communication Services Sector Fund’s stock price has exhibited significant fluctuations since its inception in 2018, with multiple peaks and troughs. However, despite the fluctuations, the fund has exhibited an overall upward trend, reaching new all-time highs multiple times over the period. The fund’s trend is reflective of the growth potential of the communication services sector, which continues to evolve rapidly as new technologies and changing consumer habits drive innovation and demand for new services.\nFor stock prices, a multiplicative decomposition is typically preferred because the percentage changes in stock prices tend to be more important than the absolute changes. Additionally, stock prices tend to exhibit non-constant variance, meaning that the variance of the series changes over time. A multiplicative decomposition can handle this non-constant variance more effectively than an additive decomposition.\n\n\nDecomposed Time Series\n\nDecomposition PlotAdjusted Decomposition Plot\n\n\n\n\nCode\n#time series data\nmyts<-ts(df$XLC.Adjusted,frequency=252,start=c(2018,6,19), end = c(2023,3,1)) \n#original plot for time series data\norginial_plot <- autoplot(myts,xlab =\"Year\", ylab = \"Adjusted Closing Price\", main = \"Communication Services Sector Fund Stock price: June 2018 - March 2023\")\n#decompose the data\ndecompose = decompose(myts, \"multiplicative\")\n#decomposition plot\nautoplot(decompose)\n\n\n\n\n\n\n\n\n\nCode\n#adjusted plot\ntrendadj <- myts/decompose$trend\ndecompose_adjtrend_plot <- autoplot(trendadj,ylab='trend') +ggtitle('Adjusted trend component in the multiplicative time series model')\nseasonaladj <- myts/decompose$seasonal\ndecompose_adjseasonal_plot <- autoplot(seasonaladj,ylab='seasonal') +ggtitle('Adjusted seasonal component in the multiplicative time series model')\ngrid.arrange(orginial_plot, decompose_adjtrend_plot,decompose_adjseasonal_plot, nrow=3)\n\n\n\n\n\n\n\n\nThe adjusted seasonal component tend to have upward trend and there is more variability in the model when compared to the original plot where the variation during the years but the adjusted trend then to have more fluctuation showing no trend when compared to the original plot.\n\n\nLag Plots\n\nDaily Time LagsMonthly Time Lags\n\n\n\n\nCode\n#Lag plots \ngglagplot(myts, do.lines=FALSE, lags=1)+xlab(\"Lag 1\")+ylab(\"Yi\")+ggtitle(\"Lag Plot for Communication Services Sector Fund Stock June 2018 - March 2023\")\n\n\n\n\n\n\n\n\n\nCode\n#montly data\nmean_data <- df %>% \n  mutate(month = month(Date), year = year(Date)) %>% \n  group_by(year, month) %>% \n  summarize(mean_value = mean(XLC.Adjusted))\nmonth<-ts(mean_data$mean_value,start = c(2018, 6),frequency = 12)\n#Lag plot\nts_lags(month)\n\n\n\n\n\n\n\n\n\nThe first lag plot shows the daily time lags of the Communication Services Sector Fund stock price from June 2018 to March 2023. The plot indicates that there is a strong positive correlation between the current value and the previous day’s value, as seen by the points clustering along the diagonal line. This suggests that the stock price has a positive autocorrelation at a lag of one day.\nThe second lag plot shows the monthly time lags of the mean value of the Communication Services Sector Fund stock price from June 2018 to March 2023. The plot indicates that there is a positive correlation between the current value and the value from the previous month. This suggests that the mean value of the stock price has a positive autocorrelation at a lag of one month.\nOverall, the lag plots indicate that there is a positive autocorrelation present in the Communication Services Sector Fund stock price data, with the strongest correlation observed in the daily time series.\n\n\nSeasonality\n\nSeasonal HeatmapSeasonal Line plot\n\n\n\n\nCode\n# Create seasonal plot\nts_heatmap(month, color = \"PuRd\", title = 'Seasonality Heatmap of Communication Services Sector Fund Stock Jan 2018 - March 2023')\n\n\n\n\n\n\n\n\n\n\nCode\n# Create a line graph for each year with months on the x-axis\nggseasonplot(month, datecol = \"date\", valuecol = \"value\")+ggtitle(\"Seasonal Yearly Plot for Communication Services Sector Fund Stock Jan 2018 - March 2023\")\n\n\n\n\n\n\n\n\nThe Seasonality Heatmap for the Communication Services Sector Fund Stock June 2018 - March 2023 does not reveal any clear seasonality in the data. The heatmap shows the mean value of the time series for each month and year combination, with the darker colors indicating higher values. The lack of clear patterns or darker colors in specific months or years suggests that there is no consistent seasonal pattern in the data. However, the yearly line graph shows a slight upward trend in the stock price from 2018 to 2023, but does not show any clear seasonality. Each year’s data is represented by a line, and the months are plotted on the x-axis. Overall, the lack of clear seasonality in both the heatmap and yearly line graph suggests that other factors beyond seasonality are driving the stock price fluctuations.\n\n\nMoving Average\n\n4 Month MA1 Year MA3 Year MA\n\n\n\n\nCode\n#SMA Smoothing \nma <- autoplot(month, series=\"Data\") +\n  autolayer(ma(month,5), series=\"4 Month MA\") +\n  xlab(\"Year\") + ylab(\"GWh\") +\n  ggtitle(\"Communication Services Sector Fund Stock June 2018 - March 2023(4 Month Moving Average)\") +\n  scale_colour_manual(values=c(\"Data\"=\"grey50\",\"4 Month MA\"=\"red\"),\n                      breaks=c(\"Data\",\"4 Month MA\"))\nma\n\n\n\n\n\n\n\n\n\nCode\n#SMA Smoothing \nma <- autoplot(month, series=\"Data\") +\n  autolayer(ma(month,13), series=\"1 Year MA\") +\n  xlab(\"Year\") + ylab(\"GWh\") +\n  ggtitle(\"Communication Services Sector Fund Stock June 2018 - March 2023(1 Year Moving Average)\") +\n  scale_colour_manual(values=c(\"Data\"=\"grey50\",\"1 Year MA\"=\"red\"),\n                      breaks=c(\"Data\",\"1 Year MA\"))\nma\n\n\n\n\n\n\n\n\n\nCode\n#SMA Smoothing \nma <- autoplot(month, series=\"Data\") +\n  autolayer(ma(month,37), series=\"3 Year MA\") +\n  xlab(\"Year\") + ylab(\"GWh\") +\n  ggtitle(\"Communication Services Sector Fund Stock June 2018 - March 2023(3 Year Moving Average)\") +\n  scale_colour_manual(values=c(\"Data\"=\"grey50\",\"3 Year MA\"=\"red\"),\n                      breaks=c(\"Data\",\"3 Year MA\"))\nma\n\n\n\n\n\n\n\n\nThe three plots show the Communication Services Sector Fund stock prices from June 2018 to March 2023, along with the moving averages for 4 months, 1 year and 3 years. As the window of the moving average increases, the smoother the trend line becomes, reducing the impact of noise and fluctuations in the original time series.\nThe 4-month moving average plot shows frequent fluctuations in the stock price, with the trend line following the general direction of the time series. The 1-year moving average plot shows a smoother trend, following the overall upward trend of the stock price.\nThe 1-year moving average plot shows a similar trend to the 4-month plot but is even smoother, with fewer fluctuations. Finally, the 3-year moving average plot shows the smoothest trend, with an almost constant upward slope.As the moving average window increases, the smoother trend allows for a clearer identification of the general trend of the Communication Services Sector Fund stock prices over time. From the moving average obtained above we can see that there is upward tend in the stock price of Communication Services Sector Fund.\n\n\nAutocorrelation Time Series\n\nACFPACFADF Test\n\n\n\n\nCode\n#ACF for  data\nggAcf(month)+ggtitle(\"ACF Plot for Communication Services Sector Fund Stock June 2018 - March 2023\")\n\n\n\n\n\n\n\n\n\nCode\n#PACF for data\nggPacf(month)+ggtitle(\"PACF Plot for Communication Services Sector Fund Stock June 2018 - March 2023\")\n\n\n\n\n\n\n\n\n\nCode\n#check the stationarity\ntseries::adf.test(month)\n\n\n\n    Augmented Dickey-Fuller Test\n\ndata:  month\nDickey-Fuller = -1.6946, Lag order = 3, p-value = 0.6979\nalternative hypothesis: stationary\n\n\n\n\n\nIn the plot of autocorrelation function, which is the acf graph for monthly data, there are clear autocorrelation in lag. The above lag plots and autocorrelation plot indicates seasonality in the series, which means the series is not stationary.. It was also verified using Augmented Dickey-Fuller Test which tells us that as the p value is greater than 0.05, the series is not stationary.\n\n\nDetrend and Differenced Time Series\n\nLinear Fitting ModelACF Plot\n\n\n\n\nCode\nfit = lm(myts~time(myts), na.action=NULL) \nsummary(fit) \n\n\n\nCall:\nlm(formula = myts ~ time(myts), na.action = NULL)\n\nResiduals:\n    Min      1Q  Median      3Q     Max \n-19.055  -6.857  -2.895   7.079  25.301 \n\nCoefficients:\n              Estimate Std. Error t value Pr(>|t|)    \n(Intercept) -6478.0495   435.9097  -14.86   <2e-16 ***\ntime(myts)      3.2341     0.2157   14.99   <2e-16 ***\n---\nSignif. codes:  0 '***' 0.001 '**' 0.01 '*' 0.05 '.' 0.1 ' ' 1\n\nResidual standard error: 11.03 on 1256 degrees of freedom\nMultiple R-squared:  0.1518,    Adjusted R-squared:  0.1511 \nF-statistic: 224.7 on 1 and 1256 DF,  p-value: < 2.2e-16\n\n\n\n\n\n\nCode\n# plot ACFs\nplot1 <- ggAcf(myts, 48, main=\"Original Data: Communication Services Sector Fund Stock Stock Price\")\nplot2 <- ggAcf(resid(fit), 48, main=\"Detrended data\")\nplot3 <- ggAcf(diff(myts), 48, main=\"First differenced data\")\ngrid.arrange(plot1, plot2, plot3, nrow=3)\n\n\n\n\n\n\n\n\nThe estimated slope coefficient β1, 0.2562. With a standard error of 0.1795, yielding a significant estimated increase of stock price is very less yearly. Equation of the fit for stationary process: \\[\\hat{y}_{t} = x_{t}+(462.6703)-(0.2562)t\\]\nFrom the above graph we can say that there is no change in detrended plot and the original data acf plot, it typically means that the data is stationary. But when the first order difference is applied the high correlation is removed but there is no seasonal correlation.\nAs depicted in the above figure, the series is now stationary and ready for future study."
  },
  {
    "objectID": "eda_sector_consumer_discretionary.html",
    "href": "eda_sector_consumer_discretionary.html",
    "title": "EDA for Consumer Discretionary Sector Fund",
    "section": "",
    "text": "The Consumer Discretionary Sector Fund (XLY) is an exchange-traded fund (ETF) that aims to track the performance of companies within the consumer discretionary sector. This sector includes businesses that offer non-essential goods and services such as apparel, leisure, media, and retail. The XLY fund invests in companies such as Amazon, Walt Disney, Nike, and Home Depot, among others. The consumer discretionary sector is known for being highly sensitive to economic conditions and consumer sentiment, and as such, it tends to be more volatile than other sectors. This makes the XLY fund a popular choice among investors looking to take on higher levels of risk in pursuit of potentially higher returns.\n\nTime Series Plot\n\n\nCode\n# get data\noptions(\"getSymbols.warning4.0\"=FALSE)\noptions(\"getSymbols.yahoo.warning\"=FALSE)\n\n\ndata = getSymbols(\"XLY\",src='yahoo', from = '2010-01-01',to = \"2023-03-01\")\n\ndf <- data.frame(Date=index(XLY),coredata(XLY))\n\n# create Bollinger Bands\nbbands <- BBands(XLY[,c(\"XLY.High\",\"XLY.Low\",\"XLY.Close\")])\n\n# join and subset data\ndf <- subset(cbind(df, data.frame(bbands[,1:3])), Date >= \"2010-01-01\")\n\n#export the data \nXLY_data <- df\nwrite.csv(XLY_data, \"DATA/CLEANED DATA/XLY_raw_data.csv\", row.names=FALSE)\n\n# colors column for increasing and decreasing\nfor (i in 1:length(df[,1])) {\n  if (df$XLY.Close[i] >= df$XLY.Open[i]) {\n      df$direction[i] = 'Increasing'\n  } else {\n      df$direction[i] = 'Decreasing'\n  }\n}\n\ni <- list(line = list(color = '#8B3A3A'))\nd <- list(line = list(color = '#7F7F7F'))\n\n# plot candlestick chart\n\nfig <- df %>% plot_ly(x = ~Date, type=\"candlestick\",\n          open = ~XLY.Open, close = ~XLY.Close,\n          high = ~XLY.High, low = ~XLY.Low, name = \"XLY\",\n          increasing = i, decreasing = d) \nfig <- fig %>% add_lines(x = ~Date, y = ~up , name = \"B Bands\",\n            line = list(color = '#ccc', width = 0.5),\n            legendgroup = \"Bollinger Bands\",\n            hoverinfo = \"none\", inherit = F) \nfig <- fig %>% add_lines(x = ~Date, y = ~dn, name = \"B Bands\",\n            line = list(color = '#ccc', width = 0.5),\n            legendgroup = \"Bollinger Bands\", inherit = F,\n            showlegend = FALSE, hoverinfo = \"none\") \nfig <- fig %>% add_lines(x = ~Date, y = ~mavg, name = \"Mv Avg\",\n            line = list(color = '#E377C2', width = 0.5),\n            hoverinfo = \"none\", inherit = F) \nfig <- fig %>% layout(yaxis = list(title = \"Price\"))\n\n# plot volume bar chart\nfig2 <- df \nfig2 <- fig2 %>% plot_ly(x=~Date, y=~XLY.Volume, type='bar', name = \"XLY Volume\",\n          color = ~direction, colors = c('#8B3A3A','#7F7F7F')) \nfig2 <- fig2 %>% layout(yaxis = list(title = \"Volume\"))\n\n# create rangeselector buttons\nrs <- list(visible = TRUE, x = 0.5, y = -0.055,\n           xanchor = 'center', yref = 'paper',\n           font = list(size = 9),\n           buttons = list(\n             list(count=1,\n                  label='RESET',\n                  step='all'),\n             list(count=3,\n                  label='3 YR',\n                  step='year',\n                  stepmode='backward'),\n             list(count=1,\n                  label='1 YR',\n                  step='year',\n                  stepmode='backward'),\n             list(count=1,\n                  label='1 MO',\n                  step='month',\n                  stepmode='backward')\n           ))\n\n# subplot with shared x axis\nfig <- subplot(fig, fig2, heights = c(0.7,0.2), nrows=2,\n             shareX = TRUE, titleY = TRUE)\nfig <- fig %>% layout(title = paste(\"Consumer Discretionary Sector Fund Stock Price: JAN 2010 - March 2023\"),\n         xaxis = list(rangeselector = rs),\n         legend = list(orientation = 'h', x = 0.5, y = 1,\n                       xanchor = 'center', yref = 'paper',\n                       font = list(size = 10),\n                       bgcolor = 'transparent'))\n\nfig\n\n\n\n\n\n\nThe Consumer Discretionary Sector Fund (XLY) has shown an overall upward trend from 2010 to March 2023, with some fluctuations along the way. One significant fluctuation occurred during the COVID-19 pandemic in early 2020, where the XLY experienced a significant decline as people spent less on discretionary items due to economic uncertainty and lockdowns. However, it has since recovered and reached new highs.\nOne reason for the overall upward trend could be attributed to the increasing consumer spending on discretionary items over the years, boosted by a growing economy and increasing disposable income. Additionally, the rise of e-commerce and online shopping has also contributed to the sector’s growth, with many consumers opting for the convenience and accessibility of shopping online. However, competition within the sector and changing consumer trends can also affect the fund’s fluctuations.\nFor stock prices, a multiplicative decomposition is typically preferred because the percentage changes in stock prices tend to be more important than the absolute changes. Additionally, stock prices tend to exhibit non-constant variance, meaning that the variance of the series changes over time. A multiplicative decomposition can handle this non-constant variance more effectively than an additive decomposition.\n\n\nDecomposed Time Series\n\nDecomposition PlotAdjusted Decomposition Plot\n\n\n\n\nCode\n#time series data\nmyts<-ts(df$XLY.Adjusted,frequency=252,start=c(2010,01,01), end = c(2023,3,1)) \n#original plot for time series data\norginial_plot <- autoplot(myts,xlab =\"Year\", ylab = \"Adjusted Closing Price\", main = \"Consumer Discretionary Sector Fund Stock price: JAN 2010 - March 2023\")\n#decompose the data\ndecompose = decompose(myts, \"multiplicative\")\n#decomposition plot\nautoplot(decompose)\n\n\n\n\n\n\n\n\n\nCode\n#adjusted plot\ntrendadj <- myts/decompose$trend\ndecompose_adjtrend_plot <- autoplot(trendadj,ylab='trend') +ggtitle('Adjusted trend component in the multiplicative time series model')\nseasonaladj <- myts/decompose$seasonal\ndecompose_adjseasonal_plot <- autoplot(seasonaladj,ylab='seasonal') +ggtitle('Adjusted seasonal component in the multiplicative time series model')\ngrid.arrange(orginial_plot, decompose_adjtrend_plot,decompose_adjseasonal_plot, nrow=3)\n\n\n\n\n\n\n\n\nThe adjusted seasonal component tend to have upward trend till 2019 and drops during the covid period and there is more variability in the model when compared to the original plot where the variation during the years but the adjusted trend then to have more fluctuation showing no trend when compared to the original plot.\n\n\nLag Plots\n\nDaily Time LagsMonthly Time Lags\n\n\n\n\nCode\n#Lag plots \ngglagplot(myts, do.lines=FALSE, lags=1)+xlab(\"Lag 1\")+ylab(\"Yi\")+ggtitle(\"Lag Plot for Consumer Discretionary Sector Fund Stock JAN 2010 - March 2023\")\n\n\n\n\n\n\n\n\n\nCode\n#montly data\nmean_data <- df %>% \n  mutate(month = month(Date), year = year(Date)) %>% \n  group_by(year, month) %>% \n  summarize(mean_value = mean(XLY.Adjusted))\nmonth<-ts(mean_data$mean_value,start = c(2010, 1),frequency = 12)\n#Lag plot\nts_lags(month)\n\n\n\n\n\n\n\n\n\nThe first lag plot shows the daily time lags of the Consumer Discretionary Sector Fund stock price from JAN 2010 to March 2023. The plot indicates that there is a strong positive correlation between the current value and the previous day’s value, as seen by the points clustering along the diagonal line. This suggests that the stock price has a positive autocorrelation at a lag of one day.\nThe second lag plot shows the monthly time lags of the mean value of the Consumer Discretionary Sector Fund stock price from JAN 2010 to March 2023. The plot indicates that there is a positive correlation between the current value and the value from the previous month. This suggests that the mean value of the stock price has a positive autocorrelation at a lag of one month.\nOverall, the lag plots indicate that there is a positive autocorrelation present in the Consumer Discretionary Sector Fund stock price data, with the strongest correlation observed in the daily time series.\n\n\nSeasonality\n\nSeasonal HeatmapSeasonal Line plot\n\n\n\n\nCode\n# Create seasonal plot\nts_heatmap(month, color = \"PuBu\", title = 'Seasonality Heatmap of Consumer Discretionary Sector Fund Stock Jan 2010 - March 2023')\n\n\n\n\n\n\n\n\n\n\nCode\n# Create a line graph for each year with months on the x-axis\nggseasonplot(month, datecol = \"date\", valuecol = \"value\")+ggtitle(\"Seasonal Yearly Plot for Consumer Discretionary Sector Fund Stock Jan 2010 - March 2023\")\n\n\n\n\n\n\n\n\nThe Seasonality Heatmap for the Consumer Discretionary Sector Fund Stock JAN 2010 - March 2023 does not reveal any clear seasonality in the data. The heatmap shows the mean value of the time series for each month and year combination, with the darker colors indicating higher values. The lack of clear patterns or darker colors in specific months or years suggests that there is no consistent seasonal pattern in the data. However, the yearly line graph shows a slight upward trend in the stock price from 2010 to 2023, but does not show any clear seasonality. Each year’s data is represented by a line, and the months are plotted on the x-axis. Overall, the lack of clear seasonality in both the heatmap and yearly line graph suggests that other factors beyond seasonality are driving the stock price fluctuations.\n\n\nMoving Average\n\n4 Month MA1 Year MA3 Year MA5 Year MA\n\n\n\n\nCode\n#SMA Smoothing \nma <- autoplot(month, series=\"Data\") +\n  autolayer(ma(month,5), series=\"4 Month MA\") +\n  xlab(\"Year\") + ylab(\"GWh\") +\n  ggtitle(\"Consumer Discretionary Sector Fund Stock JAN 2010 - March 2023(4 Month Moving Average)\") +\n  scale_colour_manual(values=c(\"Data\"=\"grey50\",\"4 Month MA\"=\"red\"),\n                      breaks=c(\"Data\",\"4 Month MA\"))\nma\n\n\n\n\n\n\n\n\n\nCode\n#SMA Smoothing \nma <- autoplot(month, series=\"Data\") +\n  autolayer(ma(month,13), series=\"1 Year MA\") +\n  xlab(\"Year\") + ylab(\"GWh\") +\n  ggtitle(\"Consumer Discretionary Sector Fund Stock JAN 2010 - March 2023(1 Year Moving Average)\") +\n  scale_colour_manual(values=c(\"Data\"=\"grey50\",\"1 Year MA\"=\"red\"),\n                      breaks=c(\"Data\",\"1 Year MA\"))\nma\n\n\n\n\n\n\n\n\n\nCode\n#SMA Smoothing \nma <- autoplot(month, series=\"Data\") +\n  autolayer(ma(month,37), series=\"3 Year MA\") +\n  xlab(\"Year\") + ylab(\"GWh\") +\n  ggtitle(\"Consumer Discretionary Sector Fund Stock JAN 2010 - March 2023(3 Year Moving Average)\") +\n  scale_colour_manual(values=c(\"Data\"=\"grey50\",\"3 Year MA\"=\"red\"),\n                      breaks=c(\"Data\",\"3 Year MA\"))\nma\n\n\n\n\n\n\n\n\n\nCode\n#SMA Smoothing \nma <- autoplot(month, series=\"Data\") +\n  autolayer(ma(month,61), series=\"5 Year MA\") +\n  xlab(\"Year\") + ylab(\"GWh\") +\n  ggtitle(\"Consumer Discretionary Sector Fund Stock JAN 2010 - March 2023(5 Year Moving Average)\") +\n  scale_colour_manual(values=c(\"Data\"=\"grey50\",\"5 Year MA\"=\"red\"),\n                      breaks=c(\"Data\",\"5 Year MA\"))\nma\n\n\n\n\n\n\n\n\nThe four plots show the Consumer Discretionary Sector Fund stock prices from JAN 2010 to March 2023, along with the moving averages for 4 months, 1 year 3 years and 4 years. As the window of the moving average increases, the smoother the trend line becomes, reducing the impact of noise and fluctuations in the original time series.\nThe 4-month moving average plot shows frequent fluctuations in the stock price, with the trend line following the general direction of the time series. The 1-year moving average plot shows a smoother trend, following the overall upward trend of the stock price.\nThe 1-year moving average plot shows a similar trend to the 4-month plot but is even smoother, with fewer fluctuations. Finally, the 5-year moving average plot shows the smoothest trend, with an almost constant upward slope.As the moving average window increases, the smoother trend allows for a clearer identification of the general trend of the Consumer Discretionary Sector Fund stock prices over time. From the moving average obtained above we can see that there is upward tend in the stock price of Consumer Discretionary Sector Fund.\n\n\nAutocorrelation Time Series\n\nACFPACFADF Test\n\n\n\n\nCode\n#ACF for  data\nggAcf(month)+ggtitle(\"ACF Plot for Consumer Discretionary Sector Fund Stock JAN 2010 - March 2023\")\n\n\n\n\n\n\n\n\n\nCode\n#PACF for data\nggPacf(month)+ggtitle(\"PACF Plot for Consumer Discretionary Sector Fund Stock JAN 2010 - March 2023\")\n\n\n\n\n\n\n\n\n\nCode\n#check the stationarity\ntseries::adf.test(month)\n\n\n\n    Augmented Dickey-Fuller Test\n\ndata:  month\nDickey-Fuller = -2.716, Lag order = 5, p-value = 0.2778\nalternative hypothesis: stationary\n\n\n\n\n\nIn the plot of autocorrelation function, which is the acf graph for monthly data, there are clear autocorrelation in lag. The above lag plots and autocorrelation plot indicates seasonality in the series, which means the series is not stationary. This can be verified by the Augmented Dickey-Fuller Test which tells us that as the p value is greater than 0.05.\n\n\nDetrend and Differenced Time Series\n\nLinear Fitting ModelACF Plot\n\n\n\n\nCode\nfit = lm(myts~time(myts), na.action=NULL) \nsummary(fit) \n\n\n\nCall:\nlm(formula = myts ~ time(myts), na.action = NULL)\n\nResiduals:\n    Min      1Q  Median      3Q     Max \n-44.408  -9.726  -2.247   6.546  59.145 \n\nCoefficients:\n              Estimate Std. Error t value Pr(>|t|)    \n(Intercept) -2.359e+04  1.343e+02  -175.7   <2e-16 ***\ntime(myts)   1.174e+01  6.661e-02   176.3   <2e-16 ***\n---\nSignif. codes:  0 '***' 0.001 '**' 0.01 '*' 0.05 '.' 0.1 ' ' 1\n\nResidual standard error: 14.33 on 3277 degrees of freedom\nMultiple R-squared:  0.9046,    Adjusted R-squared:  0.9046 \nF-statistic: 3.108e+04 on 1 and 3277 DF,  p-value: < 2.2e-16\n\n\n\n\n\n\nCode\n# plot ACFs\nplot1 <- ggAcf(myts, 48, main=\"Original Data: Consumer Discretionary Sector Fund Stock Stock Price\")\nplot2 <- ggAcf(resid(fit), 48, main=\"Detrended data\")\nplot3 <- ggAcf(diff(myts), 48, main=\"First differenced data\")\ngrid.arrange(plot1, plot2, plot3, nrow=3)\n\n\n\n\n\n\n\n\nThe estimated slope coefficient β1, 2.0846 With a standard error of 0.1679, yielding a significant estimated increase of stock price is very less yearly. Equation of the fit for stationary process: \\[\\hat{y}_{t} = x_{t}+(4129.4701)-(2.0846)t\\]\nFrom the above graph we can say that there is no change in detrended plot and the original data acf plot, it typically means that the data is stationary. But when the first order difference is applied the high correlation is removed but there is no seasonal correlation.\nAs depicted in the above figure, the series is now stationary and ready for future study."
  },
  {
    "objectID": "eda_sector_consumer_staples.html",
    "href": "eda_sector_consumer_staples.html",
    "title": "EDA for Consumer Staples Sector Fund",
    "section": "",
    "text": "The Consumer Staples Sector Fund (XLP) is an exchange-traded fund (ETF) that provides investors with exposure to the consumer staples sector of the US economy. The fund holds a diverse range of companies, including those involved in food and beverage production, personal and household products, and tobacco. XLP is a popular choice for investors looking for stability and consistent dividends, as the companies within the sector tend to have steady earnings and demand regardless of economic conditions. Overall, XLP can be a valuable addition to a well-diversified portfolio, providing exposure to a resilient sector of the US economy.\n\nTime Series Plot\n\n\nCode\n# get data\noptions(\"getSymbols.warning4.0\"=FALSE)\noptions(\"getSymbols.yahoo.warning\"=FALSE)\n\n\ndata = getSymbols(\"XLP\",src='yahoo', from = '2010-01-01',to = \"2023-03-01\")\n\ndf <- data.frame(Date=index(XLP),coredata(XLP))\n\n# create Bollinger Bands\nbbands <- BBands(XLP[,c(\"XLP.High\",\"XLP.Low\",\"XLP.Close\")])\n\n# join and subset data\ndf <- subset(cbind(df, data.frame(bbands[,1:3])), Date >= \"2010-01-01\")\n\n#export the data \nXLP_data <- df\nwrite.csv(XLP_data, \"DATA/CLEANED DATA/XLP_raw_data.csv\", row.names=FALSE)\n\n# colors column for increasing and decreasing\nfor (i in 1:length(df[,1])) {\n  if (df$XLP.Close[i] >= df$XLP.Open[i]) {\n      df$direction[i] = 'Increasing'\n  } else {\n      df$direction[i] = 'Decreasing'\n  }\n}\n\ni <- list(line = list(color = '#2297E6'))\nd <- list(line = list(color = '#7F7F7F'))\n\n# plot candlestick chart\n\nfig <- df %>% plot_ly(x = ~Date, type=\"candlestick\",\n          open = ~XLP.Open, close = ~XLP.Close,\n          high = ~XLP.High, low = ~XLP.Low, name = \"XLP\",\n          increasing = i, decreasing = d) \nfig <- fig %>% add_lines(x = ~Date, y = ~up , name = \"B Bands\",\n            line = list(color = '#ccc', width = 0.5),\n            legendgroup = \"Bollinger Bands\",\n            hoverinfo = \"none\", inherit = F) \nfig <- fig %>% add_lines(x = ~Date, y = ~dn, name = \"B Bands\",\n            line = list(color = '#ccc', width = 0.5),\n            legendgroup = \"Bollinger Bands\", inherit = F,\n            showlegend = FALSE, hoverinfo = \"none\") \nfig <- fig %>% add_lines(x = ~Date, y = ~mavg, name = \"Mv Avg\",\n            line = list(color = '#E377C2', width = 0.5),\n            hoverinfo = \"none\", inherit = F) \nfig <- fig %>% layout(yaxis = list(title = \"Price\"))\n\n# plot volume bar chart\nfig2 <- df \nfig2 <- fig2 %>% plot_ly(x=~Date, y=~XLP.Volume, type='bar', name = \"XLP Volume\",\n          color = ~direction, colors = c('#2297E6','#7F7F7F')) \nfig2 <- fig2 %>% layout(yaxis = list(title = \"Volume\"))\n\n# create rangeselector buttons\nrs <- list(visible = TRUE, x = 0.5, y = -0.055,\n           xanchor = 'center', yref = 'paper',\n           font = list(size = 9),\n           buttons = list(\n             list(count=1,\n                  label='RESET',\n                  step='all'),\n             list(count=3,\n                  label='3 YR',\n                  step='year',\n                  stepmode='backward'),\n             list(count=1,\n                  label='1 YR',\n                  step='year',\n                  stepmode='backward'),\n             list(count=1,\n                  label='1 MO',\n                  step='month',\n                  stepmode='backward')\n           ))\n\n# subplot with shared x axis\nfig <- subplot(fig, fig2, heights = c(0.7,0.2), nrows=2,\n             shareX = TRUE, titleY = TRUE)\nfig <- fig %>% layout(title = paste(\"Consumer Staples Sector Fund Stock Price: JAN 2010 - March 2023\"),\n         xaxis = list(rangeselector = rs),\n         legend = list(orientation = 'h', x = 0.5, y = 1,\n                       xanchor = 'center', yref = 'paper',\n                       font = list(size = 10),\n                       bgcolor = 'transparent'))\n\nfig\n\n\n\n\n\n\nSince its inception in 1998, the Consumer Staples Sector Fund (XLP) has experienced various trends and fluctuations over the years. In the period from 2010 to March 2023, XLP has seen several ups and downs due to various factors, including changes in the global economy, geopolitical events, and consumer behavior.\nIn the early part of the decade, XLP experienced a relatively stable period with consistent growth, as the economy began to recover from the financial crisis of 2008. However, in 2015 and 2016, XLP experienced a downturn, as the global economy slowed down, and there was increased volatility in the markets.\nThe fund saw a significant uptick in 2017, as investors sought safety in defensive stocks amid political and economic uncertainty. The fund’s performance remained relatively strong in 2018 and 2019, as consumer staples continued to outperform other sectors.\nHowever, the COVID-19 pandemic in 2020 caused significant disruptions to the global economy, leading to a sharp decline in XLP in the first quarter. Still, the fund bounced back quickly, as consumer staples became a preferred investment for investors seeking safety during the pandemic.\nIn 2021, XLP’s performance remained relatively stable, with minor fluctuations due to inflation concerns and rising interest rates. Overall, XLP has proven to be a reliable investment option for investors looking for stability and consistent returns over the years.\nFor stock prices, a multiplicative decomposition is typically preferred because the percentage changes in stock prices tend to be more important than the absolute changes. Additionally, stock prices tend to exhibit non-constant variance, meaning that the variance of the series changes over time. A multiplicative decomposition can handle this non-constant variance more effectively than an additive decomposition.\n\n\nDecomposed Time Series\n\nDecomposition PlotAdjusted Decomposition Plot\n\n\n\n\nCode\n#time series data\nmyts<-ts(df$XLP.Adjusted,frequency=252,start=c(2010,01,01), end = c(2023,3,1)) \n#original plot for time series data\norginial_plot <- autoplot(myts,xlab =\"Year\", ylab = \"Adjusted Closing Price\", main = \"Consumer Staples Sector Fund Stock price: JAN 2010 - March 2023\")\n#decompose the data\ndecompose = decompose(myts, \"multiplicative\")\n#decomposition plot\nautoplot(decompose)\n\n\n\n\n\n\n\n\n\nCode\n#adjusted plot\ntrendadj <- myts/decompose$trend\ndecompose_adjtrend_plot <- autoplot(trendadj,ylab='trend') +ggtitle('Adjusted trend component in the multiplicative time series model')\nseasonaladj <- myts/decompose$seasonal\ndecompose_adjseasonal_plot <- autoplot(seasonaladj,ylab='seasonal') +ggtitle('Adjusted seasonal component in the multiplicative time series model')\ngrid.arrange(orginial_plot, decompose_adjtrend_plot,decompose_adjseasonal_plot, nrow=3)\n\n\n\n\n\n\n\n\nThe adjusted seasonal component tend to have upward trend and there is more variability in the model when compared to the original plot where the variation during the years but the adjusted trend then to have more fluctuation showing no trend when compared to the original plot.\n\n\nLag Plots\n\nDaily Time LagsMonthly Time Lags\n\n\n\n\nCode\n#Lag plots \ngglagplot(myts, do.lines=FALSE, lags=1)+xlab(\"Lag 1\")+ylab(\"Yi\")+ggtitle(\"Lag Plot for Consumer Staples Sector Fund Stock JAN 2010 - March 2023\")\n\n\n\n\n\n\n\n\n\nCode\n#montly data\nmean_data <- df %>% \n  mutate(month = month(Date), year = year(Date)) %>% \n  group_by(year, month) %>% \n  summarize(mean_value = mean(XLP.Adjusted))\nmonth<-ts(mean_data$mean_value,start = c(2010, 1),frequency = 12)\n#Lag plot\nts_lags(month)\n\n\n\n\n\n\n\n\n\nThe first lag plot shows the daily time lags of the Consumer Staples Sector Fund stock price from JAN 2010 to March 2023. The plot indicates that there is a strong positive correlation between the current value and the previous day’s value, as seen by the points clustering along the diagonal line. This suggests that the stock price has a positive autocorrelation at a lag of one day.\nThe second lag plot shows the monthly time lags of the mean value of the Consumer Staples Sector Fund stock price from JAN 2010 to March 2023. The plot indicates that there is a positive correlation between the current value and the value from the previous month. This suggests that the mean value of the stock price has a positive autocorrelation at a lag of one month.\nOverall, the lag plots indicate that there is a positive autocorrelation present in the Consumer Staples Sector Fund stock price data, with the strongest correlation observed in the daily time series.\n\n\nSeasonality\n\nSeasonal HeatmapSeasonal Line plot\n\n\n\n\nCode\n# Create seasonal plot\nts_heatmap(month, color = \"BuGn\", title = 'Seasonality Heatmap of Consumer Staples Sector Fund Stock Jan 2010 - March 2023')\n\n\n\n\n\n\n\n\n\n\nCode\n# Create a line graph for each year with months on the x-axis\nggseasonplot(month, datecol = \"date\", valuecol = \"value\")+ggtitle(\"Seasonal Yearly Plot for Consumer Staples Sector Fund Stock Jan 2010 - March 2023\")\n\n\n\n\n\n\n\n\nThe Seasonality Heatmap for the Consumer Staples Sector Fund Stock JAN 2010 - March 2023 does not reveal any clear seasonality in the data. The heatmap shows the mean value of the time series for each month and year combination, with the darker colors indicating higher values. The lack of clear patterns or darker colors in specific months or years suggests that there is no consistent seasonal pattern in the data. However, the yearly line graph shows a slight upward trend in the stock price from 2010 to 2023, but does not show any clear seasonality. Each year’s data is represented by a line, and the months are plotted on the x-axis. Overall, the lack of clear seasonality in both the heatmap and yearly line graph suggests that other factors beyond seasonality are driving the stock price fluctuations.\n\n\nMoving Average\n\n4 Month MA1 Year MA3 Year MA5 Year MA\n\n\n\n\nCode\n#SMA Smoothing \nma <- autoplot(month, series=\"Data\") +\n  autolayer(ma(month,5), series=\"4 Month MA\") +\n  xlab(\"Year\") + ylab(\"GWh\") +\n  ggtitle(\"Consumer Staples Sector Fund Stock JAN 2010 - March 2023(4 Month Moving Average)\") +\n  scale_colour_manual(values=c(\"Data\"=\"grey50\",\"4 Month MA\"=\"red\"),\n                      breaks=c(\"Data\",\"4 Month MA\"))\nma\n\n\n\n\n\n\n\n\n\nCode\n#SMA Smoothing \nma <- autoplot(month, series=\"Data\") +\n  autolayer(ma(month,13), series=\"1 Year MA\") +\n  xlab(\"Year\") + ylab(\"GWh\") +\n  ggtitle(\"Consumer Staples Sector Fund Stock JAN 2010 - March 2023(1 Year Moving Average)\") +\n  scale_colour_manual(values=c(\"Data\"=\"grey50\",\"1 Year MA\"=\"red\"),\n                      breaks=c(\"Data\",\"1 Year MA\"))\nma\n\n\n\n\n\n\n\n\n\nCode\n#SMA Smoothing \nma <- autoplot(month, series=\"Data\") +\n  autolayer(ma(month,37), series=\"3 Year MA\") +\n  xlab(\"Year\") + ylab(\"GWh\") +\n  ggtitle(\"Consumer Staples Sector Fund Stock JAN 2010 - March 2023(3 Year Moving Average)\") +\n  scale_colour_manual(values=c(\"Data\"=\"grey50\",\"3 Year MA\"=\"red\"),\n                      breaks=c(\"Data\",\"3 Year MA\"))\nma\n\n\n\n\n\n\n\n\n\nCode\n#SMA Smoothing \nma <- autoplot(month, series=\"Data\") +\n  autolayer(ma(month,61), series=\"5 Year MA\") +\n  xlab(\"Year\") + ylab(\"GWh\") +\n  ggtitle(\"Consumer Staples Sector Fund Stock JAN 2010 - March 2023(5 Year Moving Average)\") +\n  scale_colour_manual(values=c(\"Data\"=\"grey50\",\"5 Year MA\"=\"red\"),\n                      breaks=c(\"Data\",\"5 Year MA\"))\nma\n\n\n\n\n\n\n\n\nThe four plots show the Consumer Staples Sector Fund stock prices from JAN 2010 to March 2023, along with the moving averages for 4 months, 1 year 3 years and 4 years. As the window of the moving average increases, the smoother the trend line becomes, reducing the impact of noise and fluctuations in the original time series.\nThe 4-month moving average plot shows frequent fluctuations in the stock price, with the trend line following the general direction of the time series. The 1-year moving average plot shows a smoother trend, following the overall upward trend of the stock price.\nThe 1-year moving average plot shows a similar trend to the 4-month plot but is even smoother, with fewer fluctuations. Finally, the 5-year moving average plot shows the smoothest trend, with an almost constant upward slope.As the moving average window increases, the smoother trend allows for a clearer identification of the general trend of the Consumer Staples Sector Fund stock prices over time. From the moving average obtained above we can see that there is upward tend in the stock price of Consumer Staples Sector Fund.\n\n\nAutocorrelation Time Series\n\nACFPACFADF Test\n\n\n\n\nCode\n#ACF for  data\nggAcf(month)+ggtitle(\"ACF Plot for Consumer Staples Sector Fund Stock JAN 2010 - March 2023\")\n\n\n\n\n\n\n\n\n\nCode\n#PACF for data\nggPacf(month)+ggtitle(\"PACF Plot for Consumer Staples Sector Fund Stock JAN 2010 - March 2023\")\n\n\n\n\n\n\n\n\n\nCode\n#check the stationarity\ntseries::adf.test(month)\n\n\n\n    Augmented Dickey-Fuller Test\n\ndata:  month\nDickey-Fuller = -2.0836, Lag order = 5, p-value = 0.5415\nalternative hypothesis: stationary\n\n\n\n\n\nIn the plot of autocorrelation function, which is the acf graph for monthly data, there are clear autocorrelation in lag. The above lag plots and autocorrelation plot indicates seasonality in the series, which means the series is not stationary. It was also verified using Augmented Dickey-Fuller Test which tells us that as the p value is greater than 0.05, the series is not stationary.\n\n\nDetrend and Differenced Time Series\n\nLinear Fitting ModelACF Plot\n\n\n\n\nCode\nfit = lm(myts~time(myts), na.action=NULL) \nsummary(fit) \n\n\n\nCall:\nlm(formula = myts ~ time(myts), na.action = NULL)\n\nResiduals:\n     Min       1Q   Median       3Q      Max \n-13.5256  -1.0564   0.2343   1.3352  10.7104 \n\nCoefficients:\n              Estimate Std. Error t value Pr(>|t|)    \n(Intercept) -8.255e+03  2.630e+01  -313.9   <2e-16 ***\ntime(myts)   4.115e+00  1.304e-02   315.5   <2e-16 ***\n---\nSignif. codes:  0 '***' 0.001 '**' 0.01 '*' 0.05 '.' 0.1 ' ' 1\n\nResidual standard error: 2.805 on 3277 degrees of freedom\nMultiple R-squared:  0.9681,    Adjusted R-squared:  0.9681 \nF-statistic: 9.953e+04 on 1 and 3277 DF,  p-value: < 2.2e-16\n\n\n\n\n\n\nCode\n# plot ACFs\nplot1 <- ggAcf(myts, 48, main=\"Original Data: Consumer Staples Sector Fund Stock Stock Price\")\nplot2 <- ggAcf(resid(fit), 48, main=\"Detrended data\")\nplot3 <- ggAcf(diff(myts), 48, main=\"First differenced data\")\ngrid.arrange(plot1, plot2, plot3, nrow=3)\n\n\n\n\n\n\n\n\nThe estimated slope coefficient β1, 0.7645 With a standard error of 0.0597, yielding a significant estimated increase of stock price is very less yearly. Equation of the fit for stationary process: \\[\\hat{y}_{t} = x_{t}+(1502.6506)-(0.7645)t\\]\nFrom the above graph we can say that there is no change in detrended plot and the original data acf plot, it typically means that the data is stationary. But when the first order difference is applied the high correlation is removed but there is no seasonal correlation.\nAs depicted in the above figure, the series is now stationary and ready for future study."
  },
  {
    "objectID": "eda_sector_energy.html",
    "href": "eda_sector_energy.html",
    "title": "EDA for Energy Sector Fund",
    "section": "",
    "text": "The Energy Sector Fund (XLE) tracks the performance of companies involved in the exploration, production, and distribution of energy. From 2010 to March 2023, the fund has shown a lot of volatility due to the fluctuating prices of oil and natural gas. During this period, the fund experienced a significant decline in 2014-2015 as oil prices dropped sharply, but it rebounded in 2016-2017 as prices stabilized. However, the fund experienced another sharp decline in 2020 due to the COVID-19 pandemic and a subsequent drop in demand for energy. The trend for this sector is closely tied to geopolitical events, global economic growth, and changes in regulations. As the world transitions towards renewable energy sources, it remains to be seen how the energy sector will perform in the future.\n\nTime Series Plot\n\n\nCode\n# get data\noptions(\"getSymbols.warning4.0\"=FALSE)\noptions(\"getSymbols.yahoo.warning\"=FALSE)\n\n\ndata = getSymbols(\"XLE\",src='yahoo', from = '2010-01-01',to = \"2023-03-01\")\n\ndf <- data.frame(Date=index(XLE),coredata(XLE))\n\n# create Bollinger Bands\nbbands <- BBands(XLE[,c(\"XLE.High\",\"XLE.Low\",\"XLE.Close\")])\n\n# join and subset data\ndf <- subset(cbind(df, data.frame(bbands[,1:3])), Date >= \"2010-01-01\")\n\n#export the data \nXLE_data <- df\nwrite.csv(XLE_data, \"DATA/CLEANED DATA/XLE_raw_data.csv\", row.names=FALSE)\n\n# colors column for increasing and decreasing\nfor (i in 1:length(df[,1])) {\n  if (df$XLE.Close[i] >= df$XLE.Open[i]) {\n      df$direction[i] = 'Increasing'\n  } else {\n      df$direction[i] = 'Decreasing'\n  }\n}\n\ni <- list(line = list(color = '#FFE4E1'))\nd <- list(line = list(color = '#7F7F7F'))\n\n# plot candlestick chart\n\nfig <- df %>% plot_ly(x = ~Date, type=\"candlestick\",\n          open = ~XLE.Open, close = ~XLE.Close,\n          high = ~XLE.High, low = ~XLE.Low, name = \"XLE\",\n          increasing = i, decreasing = d) \nfig <- fig %>% add_lines(x = ~Date, y = ~up , name = \"B Bands\",\n            line = list(color = '#ccc', width = 0.5),\n            legendgroup = \"Bollinger Bands\",\n            hoverinfo = \"none\", inherit = F) \nfig <- fig %>% add_lines(x = ~Date, y = ~dn, name = \"B Bands\",\n            line = list(color = '#ccc', width = 0.5),\n            legendgroup = \"Bollinger Bands\", inherit = F,\n            showlegend = FALSE, hoverinfo = \"none\") \nfig <- fig %>% add_lines(x = ~Date, y = ~mavg, name = \"Mv Avg\",\n            line = list(color = '#E377C2', width = 0.5),\n            hoverinfo = \"none\", inherit = F) \nfig <- fig %>% layout(yaxis = list(title = \"Price\"))\n\n# plot volume bar chart\nfig2 <- df \nfig2 <- fig2 %>% plot_ly(x=~Date, y=~XLE.Volume, type='bar', name = \"XLE Volume\",\n          color = ~direction, colors = c('#FFE4E1','#7F7F7F')) \nfig2 <- fig2 %>% layout(yaxis = list(title = \"Volume\"))\n\n# create rangeselector buttons\nrs <- list(visible = TRUE, x = 0.5, y = -0.055,\n           xanchor = 'center', yref = 'paper',\n           font = list(size = 9),\n           buttons = list(\n             list(count=1,\n                  label='RESET',\n                  step='all'),\n             list(count=3,\n                  label='3 YR',\n                  step='year',\n                  stepmode='backward'),\n             list(count=1,\n                  label='1 YR',\n                  step='year',\n                  stepmode='backward'),\n             list(count=1,\n                  label='1 MO',\n                  step='month',\n                  stepmode='backward')\n           ))\n\n# subplot with shared x axis\nfig <- subplot(fig, fig2, heights = c(0.7,0.2), nrows=2,\n             shareX = TRUE, titleY = TRUE)\nfig <- fig %>% layout(title = paste(\"Energy Sector Fund Stock Price: JAN 2010 - March 2023\"),\n         xaxis = list(rangeselector = rs),\n         legend = list(orientation = 'h', x = 0.5, y = 1,\n                       xanchor = 'center', yref = 'paper',\n                       font = list(size = 10),\n                       bgcolor = 'transparent'))\n\nfig\n\n\n\n\n\n\nThe Energy Sector Fund (XLE) is an exchange-traded fund that tracks the performance of the energy sector in the S&P 500 index. The fund includes companies involved in exploration, production, refining, and distribution of energy. From 2010 to March 2023, the XLE experienced significant fluctuations due to several factors.\nIn 2010-2014, the XLE saw steady growth due to rising demand for energy, increased production from shale, and higher oil prices. However, in mid-2014, the global oil market entered a period of oversupply, and oil prices crashed. This led to a sharp decline in XLE’s value, which lasted until early 2016.\nBetween 2016 and early 2020, XLE recovered and experienced moderate growth, fueled by global economic growth and higher demand for oil. However, in early 2020, the COVID-19 pandemic caused a sharp decline in energy demand, resulting in another crash in oil prices and a significant drop in XLE’s value.\nIn the second half of 2020 and early 2021, XLE began to recover as oil prices stabilized and energy demand gradually picked up. However, concerns over inflation and rising interest rates caused a drop in XLE’s value in the first quarter of 2022.\nOverall, the Energy Sector Fund has been highly sensitive to fluctuations in oil prices and global energy demand, which can be affected by various factors such as geopolitical events, technological advancements, and economic conditions.\nFor stock prices, a multiplicative decomposition is typically preferred because the percentage changes in stock prices tend to be more important than the absolute changes. Additionally, stock prices tend to exhibit non-constant variance, meaning that the variance of the series changes over time. A multiplicative decomposition can handle this non-constant variance more effectively than an additive decomposition.\n\n\nDecomposed Time Series\n\nDecomposition PlotAdjusted Decomposition Plot\n\n\n\n\nCode\n#time series data\nmyts<-ts(df$XLE.Adjusted,frequency=252,start=c(2010,01,01), end = c(2023,3,1)) \n#original plot for time series data\norginial_plot <- autoplot(myts,xlab =\"Year\", ylab = \"Adjusted Closing Price\", main = \"Energy Sector Fund Stock price: JAN 2010 - March 2023\")\n#decompose the data\ndecompose = decompose(myts, \"multiplicative\")\n#decomposition plot\nautoplot(decompose)\n\n\n\n\n\n\n\n\n\nCode\n#adjusted plot\ntrendadj <- myts/decompose$trend\ndecompose_adjtrend_plot <- autoplot(trendadj,ylab='trend') +ggtitle('Adjusted trend component in the multiplicative time series model')\nseasonaladj <- myts/decompose$seasonal\ndecompose_adjseasonal_plot <- autoplot(seasonaladj,ylab='seasonal') +ggtitle('Adjusted seasonal component in the multiplicative time series model')\ngrid.arrange(orginial_plot, decompose_adjtrend_plot,decompose_adjseasonal_plot, nrow=3)\n\n\n\n\n\n\n\n\nThe adjusted seasonal component tend to have upward trend till 2019 and drops during the covid period and there is more variability in the model when compared to the original plot where the variation during the years but the adjusted trend then to have more fluctuation showing no trend when compared to the original plot.\n\n\nLag Plots\n\nDaily Time LagsMonthly Time Lags\n\n\n\n\nCode\n#Lag plots \ngglagplot(myts, do.lines=FALSE, lags=1)+xlab(\"Lag 1\")+ylab(\"Yi\")+ggtitle(\"Lag Plot for Energy Sector Fund Stock JAN 2010 - March 2023\")\n\n\n\n\n\n\n\n\n\nCode\n#montly data\nmean_data <- df %>% \n  mutate(month = month(Date), year = year(Date)) %>% \n  group_by(year, month) %>% \n  summarize(mean_value = mean(XLE.Adjusted))\nmonth<-ts(mean_data$mean_value,start = c(2010, 1),frequency = 12)\n#Lag plot\nts_lags(month)\n\n\n\n\n\n\n\n\n\nThe first lag plot shows a scatter plot of the Energy Sector Fund stock prices against their lagged values by one time period. The plot shows a strong linear pattern, indicating a high correlation between the stock prices and their lagged values. This suggests that there is a high level of persistence in the Energy Sector Fund stock prices, meaning that the past prices are a good predictor of the future prices.\nThe second lag plot shows a similar pattern for the mean monthly values of the Energy Sector Fund stock prices. The plot shows a strong linear pattern, indicating a positive correlation between the mean monthly values and their lagged values by one time period. This suggests that there is a high level of persistence in the mean monthly values of the Energy Sector Fund stock prices, meaning that the past monthly means are a good predictor of the future monthly means. Overall, the lag plot outputs suggest that the Energy Sector Fund stock prices have a high level of autocorrelation, which is consistent with the trend and fluctuations in the sector.\n\n\nSeasonality\n\nSeasonal HeatmapSeasonal Line plot\n\n\n\n\nCode\n# Create seasonal plot\nts_heatmap(month, color = \"YlOrRd\", title = 'Seasonality Heatmap of Energy Sector Fund Stock Jan 2010 - March 2023')\n\n\n\n\n\n\n\n\n\n\nCode\n# Create a line graph for each year with months on the x-axis\nggseasonplot(month, datecol = \"date\", valuecol = \"value\")+ggtitle(\"Seasonal Yearly Plot for Energy Sector Fund Stock Jan 2010 - March 2023\")\n\n\n\n\n\n\n\n\nThe Seasonality Heatmap for the Energy Sector Fund Stock JAN 2010 - March 2023 does not reveal any clear seasonality in the data. The heatmap shows the mean value of the time series for each month and year combination, with the darker colors indicating higher values. The lack of clear patterns or darker colors in specific months or years suggests that there is no consistent seasonal pattern in the data. However, the yearly line graph shows a slight upward trend in the stock price from 2010 to 2023, but does not show any clear seasonality. Each year’s data is represented by a line, and the months are plotted on the x-axis. Overall, the lack of clear seasonality in both the heatmap and yearly line graph suggests that other factors beyond seasonality are driving the stock price fluctuations.\n\n\nMoving Average\n\n4 Month MA1 Year MA3 Year MA5 Year MA\n\n\n\n\nCode\n#SMA Smoothing \nma <- autoplot(month, series=\"Data\") +\n  autolayer(ma(month,5), series=\"4 Month MA\") +\n  xlab(\"Year\") + ylab(\"GWh\") +\n  ggtitle(\"Energy Sector Fund Stock JAN 2010 - March 2023(4 Month Moving Average)\") +\n  scale_colour_manual(values=c(\"Data\"=\"grey50\",\"4 Month MA\"=\"red\"),\n                      breaks=c(\"Data\",\"4 Month MA\"))\nma\n\n\n\n\n\n\n\n\n\nCode\n#SMA Smoothing \nma <- autoplot(month, series=\"Data\") +\n  autolayer(ma(month,13), series=\"1 Year MA\") +\n  xlab(\"Year\") + ylab(\"GWh\") +\n  ggtitle(\"Energy Sector Fund Stock JAN 2010 - March 2023(1 Year Moving Average)\") +\n  scale_colour_manual(values=c(\"Data\"=\"grey50\",\"1 Year MA\"=\"red\"),\n                      breaks=c(\"Data\",\"1 Year MA\"))\nma\n\n\n\n\n\n\n\n\n\nCode\n#SMA Smoothing \nma <- autoplot(month, series=\"Data\") +\n  autolayer(ma(month,37), series=\"3 Year MA\") +\n  xlab(\"Year\") + ylab(\"GWh\") +\n  ggtitle(\"Energy Sector Fund Stock JAN 2010 - March 2023(3 Year Moving Average)\") +\n  scale_colour_manual(values=c(\"Data\"=\"grey50\",\"3 Year MA\"=\"red\"),\n                      breaks=c(\"Data\",\"3 Year MA\"))\nma\n\n\n\n\n\n\n\n\n\nCode\n#SMA Smoothing \nma <- autoplot(month, series=\"Data\") +\n  autolayer(ma(month,61), series=\"5 Year MA\") +\n  xlab(\"Year\") + ylab(\"GWh\") +\n  ggtitle(\"Energy Sector Fund Stock JAN 2010 - March 2023(5 Year Moving Average)\") +\n  scale_colour_manual(values=c(\"Data\"=\"grey50\",\"5 Year MA\"=\"red\"),\n                      breaks=c(\"Data\",\"5 Year MA\"))\nma\n\n\n\n\n\n\n\n\nThe four plots show the Energy Sector Fund stock prices from JAN 2010 to March 2023, along with the moving averages for 4 months, 1 year 3 years and 4 years. As the window of the moving average increases, the smoother the trend line becomes, reducing the impact of noise and fluctuations in the original time series.\nThe 4-month moving average plot shows frequent fluctuations in the stock price, with the trend line following the general direction of the time series. The 1-year moving average plot shows a smoother trend, following the overall upward trend of the stock price.\nThe 1-year moving average plot shows a similar trend to the 4-month plot but is even smoother, with fewer fluctuations. Finally, the 5-year moving average plot shows the smoothest trend, with downward and upward slope.As the moving average window increases, the smoother trend allows for a clearer identification of the general trend of the Energy Sector Fund stock prices over time. From the moving average obtained above we can see that there is upward tend in the stock price of Energy Sector Fund.\n\n\nAutocorrelation Time Series\n\nACFPACFADF Test\n\n\n\n\nCode\n#ACF for  data\nggAcf(month)+ggtitle(\"ACF Plot for Energy Sector Fund Stock JAN 2010 - March 2023\")\n\n\n\n\n\n\n\n\n\nCode\n#PACF for data\nggPacf(month)+ggtitle(\"PACF Plot for Energy Sector Fund Stock JAN 2010 - March 2023\")\n\n\n\n\n\n\n\n\n\nCode\n#check the stationarity\ntseries::adf.test(month)\n\n\n\n    Augmented Dickey-Fuller Test\n\ndata:  month\nDickey-Fuller = -1.5196, Lag order = 5, p-value = 0.7767\nalternative hypothesis: stationary\n\n\n\n\n\nIn the plot of autocorrelation function, which is the acf graph for monthly data, there are clear autocorrelation in lag. The above lag plots and autocorrelation plot indicates seasonality in the series, which means the series is not stationary. This can be verified by the Augmented Dickey-Fuller Test which tells us that as the p value is greater than 0.05.\n\n\nDetrend and Differenced Time Series\n\nLinear Fitting ModelACF Plot\n\n\n\n\nCode\nfit = lm(myts~time(myts), na.action=NULL) \nsummary(fit) \n\n\n\nCall:\nlm(formula = myts ~ time(myts), na.action = NULL)\n\nResiduals:\n    Min      1Q  Median      3Q     Max \n-33.032  -5.253  -0.391   5.337  34.840 \n\nCoefficients:\n              Estimate Std. Error t value Pr(>|t|)    \n(Intercept) -1.522e+03  9.807e+01  -15.52   <2e-16 ***\ntime(myts)   7.798e-01  4.863e-02   16.03   <2e-16 ***\n---\nSignif. codes:  0 '***' 0.001 '**' 0.01 '*' 0.05 '.' 0.1 ' ' 1\n\nResidual standard error: 10.46 on 3277 degrees of freedom\nMultiple R-squared:  0.07274,   Adjusted R-squared:  0.07245 \nF-statistic: 257.1 on 1 and 3277 DF,  p-value: < 2.2e-16\n\n\n\n\n\n\nCode\n# plot ACFs\nplot1 <- ggAcf(myts, 48, main=\"Original Data: Energy Sector Fund Stock Stock Price\")\nplot2 <- ggAcf(resid(fit), 48, main=\"Detrended data\")\nplot3 <- ggAcf(diff(myts), 48, main=\"First differenced data\")\ngrid.arrange(plot1, plot2, plot3, nrow=3)\n\n\n\n\n\n\n\n\nThe estimated slope coefficient β1, 5.359e-01 With a standard error of 4.131e-02, yielding a significant estimated increase of stock price is very less yearly. Equation of the fit for stationary process: \\[\\hat{y}_{t} = x_{t}+(1.029e+03)-(5.359e-01)t\\]\nFrom the above graph we can say that there is no change in detrended plot and the original data acf plot, it typically means that the data is stationary. But when the first order difference is applied the high correlation is removed but there is no seasonal correlation.\nAs depicted in the above figure, the series is now stationary and ready for future study."
  },
  {
    "objectID": "eda_sector_financial.html",
    "href": "eda_sector_financial.html",
    "title": "EDA for Financial Sector Fund",
    "section": "",
    "text": "The Financial Sector Fund (XLF) is an exchange-traded fund that tracks the performance of companies in the financial sector, including banks, insurance companies, and other financial services firms. The fund provides investors with exposure to a diversified portfolio of financial stocks and offers a convenient way to gain access to this sector of the economy. As the financial sector is highly sensitive to changes in interest rates and economic conditions, the XLF can experience significant fluctuations in response to macroeconomic events and shifts in investor sentiment. The performance of the XLF is closely tied to the overall health of the economy, making it an important indicator of economic growth and stability.\n\nTime Series Plot\n\n\nCode\n# get data\noptions(\"getSymbols.warning4.0\"=FALSE)\noptions(\"getSymbols.yahoo.warning\"=FALSE)\n\n\ndata = getSymbols(\"XLF\",src='yahoo', from = '2010-01-01',to = \"2023-03-01\")\n\ndf <- data.frame(Date=index(XLF),coredata(XLF))\n\n# create Bollinger Bands\nbbands <- BBands(XLF[,c(\"XLF.High\",\"XLF.Low\",\"XLF.Close\")])\n\n# join and subset data\ndf <- subset(cbind(df, data.frame(bbands[,1:3])), Date >= \"2010-01-01\")\n\n#export the data \nXLF_data <- df\nwrite.csv(XLF_data, \"DATA/CLEANED DATA/XLF_raw_data.csv\", row.names=FALSE)\n\n# colors column for increasing and decreasing\nfor (i in 1:length(df[,1])) {\n  if (df$XLF.Close[i] >= df$XLF.Open[i]) {\n      df$direction[i] = 'Increasing'\n  } else {\n      df$direction[i] = 'Decreasing'\n  }\n}\n\ni <- list(line = list(color = '#8B3A62'))\nd <- list(line = list(color = '#7F7F7F'))\n\n# plot candlestick chart\n\nfig <- df %>% plot_ly(x = ~Date, type=\"candlestick\",\n          open = ~XLF.Open, close = ~XLF.Close,\n          high = ~XLF.High, low = ~XLF.Low, name = \"XLF\",\n          increasing = i, decreasing = d) \nfig <- fig %>% add_lines(x = ~Date, y = ~up , name = \"B Bands\",\n            line = list(color = '#ccc', width = 0.5),\n            legendgroup = \"Bollinger Bands\",\n            hoverinfo = \"none\", inherit = F) \nfig <- fig %>% add_lines(x = ~Date, y = ~dn, name = \"B Bands\",\n            line = list(color = '#ccc', width = 0.5),\n            legendgroup = \"Bollinger Bands\", inherit = F,\n            showlegend = FALSE, hoverinfo = \"none\") \nfig <- fig %>% add_lines(x = ~Date, y = ~mavg, name = \"Mv Avg\",\n            line = list(color = '#E377C2', width = 0.5),\n            hoverinfo = \"none\", inherit = F) \nfig <- fig %>% layout(yaxis = list(title = \"Price\"))\n\n# plot volume bar chart\nfig2 <- df \nfig2 <- fig2 %>% plot_ly(x=~Date, y=~XLF.Volume, type='bar', name = \"XLF Volume\",\n          color = ~direction, colors = c('#8B3A62','#7F7F7F')) \nfig2 <- fig2 %>% layout(yaxis = list(title = \"Volume\"))\n\n# create rangeselector buttons\nrs <- list(visible = TRUE, x = 0.5, y = -0.055,\n           xanchor = 'center', yref = 'paper',\n           font = list(size = 9),\n           buttons = list(\n             list(count=1,\n                  label='RESET',\n                  step='all'),\n             list(count=3,\n                  label='3 YR',\n                  step='year',\n                  stepmode='backward'),\n             list(count=1,\n                  label='1 YR',\n                  step='year',\n                  stepmode='backward'),\n             list(count=1,\n                  label='1 MO',\n                  step='month',\n                  stepmode='backward')\n           ))\n\n# subplot with shared x axis\nfig <- subplot(fig, fig2, heights = c(0.7,0.2), nrows=2,\n             shareX = TRUE, titleY = TRUE)\nfig <- fig %>% layout(title = paste(\"The Financial Sector Fund Stock Price: JAN 2010 - March 2023\"),\n         xaxis = list(rangeselector = rs),\n         legend = list(orientation = 'h', x = 0.5, y = 1,\n                       xanchor = 'center', yref = 'paper',\n                       font = list(size = 10),\n                       bgcolor = 'transparent'))\n\nfig\n\n\n\n\n\n\nFrom 2010 to early 2020, XLF had an overall upward trend, with occasional fluctuations due to various economic and political events. However, in March 2020, the COVID-19 pandemic caused a sharp drop in the value of the fund, as the financial sector was heavily impacted by the economic downturn. The Federal Reserve’s actions to lower interest rates and provide monetary stimulus helped to stabilize the market, and XLF began to recover in the second half of 2020.\nIn 2021, XLF experienced further growth as the economy continued to recover and interest rates remained low. However, there have been concerns about inflation and the potential for interest rate hikes, which could impact the financial sector. Additionally, the regulatory environment for banks and financial institutions could also affect the performance of XLF. Overall, XLF has shown a mix of stability and volatility in response to economic and political factors over the past decade.\nFor stock prices, a multiplicative decomposition is typically preferred because the percentage changes in stock prices tend to be more important than the absolute changes. Additionally, stock prices tend to exhibit non-constant variance, meaning that the variance of the series changes over time. A multiplicative decomposition can handle this non-constant variance more effectively than an additive decomposition.\n\n\nDecomposed Time Series\n\nDecomposition PlotAdjusted Decomposition Plot\n\n\n\n\nCode\n#time series data\nmyts<-ts(df$XLF.Adjusted,frequency=252,start=c(2010,01,01), end = c(2023,3,1)) \n#original plot for time series data\norginial_plot <- autoplot(myts,xlab =\"Year\", ylab = \"Adjusted Closing Price\", main = \"The Financial Sector Fund Stock price: JAN 2010 - March 2023\")\n#decompose the data\ndecompose = decompose(myts, \"multiplicative\")\n#decomposition plot\nautoplot(decompose)\n\n\n\n\n\n\n\n\n\nCode\n#adjusted plot\ntrendadj <- myts/decompose$trend\ndecompose_adjtrend_plot <- autoplot(trendadj,ylab='trend') +ggtitle('Adjusted trend component in the multiplicative time series model')\nseasonaladj <- myts/decompose$seasonal\ndecompose_adjseasonal_plot <- autoplot(seasonaladj,ylab='seasonal') +ggtitle('Adjusted seasonal component in the multiplicative time series model')\ngrid.arrange(orginial_plot, decompose_adjtrend_plot,decompose_adjseasonal_plot, nrow=3)\n\n\n\n\n\n\n\n\nThe adjusted seasonal component tend to have upward trend till 2019 and drops during the covid period and there is more variability in the model when compared to the original plot where the variation during the years but the adjusted trend then to have more fluctuation showing no trend when compared to the original plot.\n\n\nLag Plots\n\nDaily Time LagsMonthly Time Lags\n\n\n\n\nCode\n#Lag plots \ngglagplot(myts, do.lines=FALSE, lags=1)+xlab(\"Lag 1\")+ylab(\"Yi\")+ggtitle(\"Lag Plot for The Financial Sector Fund Stock JAN 2010 - March 2023\")\n\n\n\n\n\n\n\n\n\nCode\n#montly data\nmean_data <- df %>% \n  mutate(month = month(Date), year = year(Date)) %>% \n  group_by(year, month) %>% \n  summarize(mean_value = mean(XLF.Adjusted))\nmonth<-ts(mean_data$mean_value,start = c(2010, 1),frequency = 12)\n#Lag plot\nts_lags(month)\n\n\n\n\n\n\n\n\n\nThe first lag plot shows the daily time lags of the The Financial Sector Fund stock price from JAN 2010 to March 2023. The plot indicates that there is a strong positive correlation between the current value and the previous day’s value, as seen by the points clustering along the diagonal line. This suggests that the stock price has a positive autocorrelation at a lag of one day.\nThe second lag plot shows the monthly time lags of the mean value of the The Financial Sector Fund stock price from JAN 2010 to March 2023. The plot indicates that there is a positive correlation between the current value and the value from the previous month. This suggests that the mean value of the stock price has a positive autocorrelation at a lag of one month.\nOverall, the lag plots indicate that there is a positive autocorrelation present in the The Financial Sector Fund stock price data, with the strongest correlation observed in the daily time series.\n\n\nSeasonality\n\nSeasonal HeatmapSeasonal Line plot\n\n\n\n\nCode\n# Create seasonal plot\nts_heatmap(month,color = \"OrRd\", title = 'Seasonality Heatmap of The Financial Sector Fund Stock Jan 2010 - March 2023')\n\n\n\n\n\n\n\n\n\n\nCode\n# Create a line graph for each year with months on the x-axis\nggseasonplot(month, datecol = \"date\", valuecol = \"value\")+ggtitle(\"Seasonal Yearly Plot for The Financial Sector Fund Stock Jan 2010 - March 2023\")\n\n\n\n\n\n\n\n\nThe Seasonality Heatmap for the The Financial Sector Fund Stock JAN 2010 - March 2023 does not reveal any clear seasonality in the data. The heatmap shows the mean value of the time series for each month and year combination, with the darker colors indicating higher values. The lack of clear patterns or darker colors in specific months or years suggests that there is no consistent seasonal pattern in the data. However, the yearly line graph shows a slight upward trend in the stock price from 2010 to 2023, but does not show any clear seasonality. Each year’s data is represented by a line, and the months are plotted on the x-axis. Overall, the lack of clear seasonality in both the heatmap and yearly line graph suggests that other factors beyond seasonality are driving the stock price fluctuations.\n\n\nMoving Average\n\n4 Month MA1 Year MA3 Year MA5 Year MA\n\n\n\n\nCode\n#SMA Smoothing \nma <- autoplot(month, series=\"Data\") +\n  autolayer(ma(month,5), series=\"4 Month MA\") +\n  xlab(\"Year\") + ylab(\"GWh\") +\n  ggtitle(\"The Financial Sector Fund Stock JAN 2010 - March 2023(4 Month Moving Average)\") +\n  scale_colour_manual(values=c(\"Data\"=\"grey50\",\"4 Month MA\"=\"red\"),\n                      breaks=c(\"Data\",\"4 Month MA\"))\nma\n\n\n\n\n\n\n\n\n\nCode\n#SMA Smoothing \nma <- autoplot(month, series=\"Data\") +\n  autolayer(ma(month,13), series=\"1 Year MA\") +\n  xlab(\"Year\") + ylab(\"GWh\") +\n  ggtitle(\"The Financial Sector Fund Stock JAN 2010 - March 2023(1 Year Moving Average)\") +\n  scale_colour_manual(values=c(\"Data\"=\"grey50\",\"1 Year MA\"=\"red\"),\n                      breaks=c(\"Data\",\"1 Year MA\"))\nma\n\n\n\n\n\n\n\n\n\nCode\n#SMA Smoothing \nma <- autoplot(month, series=\"Data\") +\n  autolayer(ma(month,37), series=\"3 Year MA\") +\n  xlab(\"Year\") + ylab(\"GWh\") +\n  ggtitle(\"The Financial Sector Fund Stock JAN 2010 - March 2023(3 Year Moving Average)\") +\n  scale_colour_manual(values=c(\"Data\"=\"grey50\",\"3 Year MA\"=\"red\"),\n                      breaks=c(\"Data\",\"3 Year MA\"))\nma\n\n\n\n\n\n\n\n\n\nCode\n#SMA Smoothing \nma <- autoplot(month, series=\"Data\") +\n  autolayer(ma(month,61), series=\"5 Year MA\") +\n  xlab(\"Year\") + ylab(\"GWh\") +\n  ggtitle(\"The Financial Sector Fund Stock JAN 2010 - March 2023(5 Year Moving Average)\") +\n  scale_colour_manual(values=c(\"Data\"=\"grey50\",\"5 Year MA\"=\"red\"),\n                      breaks=c(\"Data\",\"5 Year MA\"))\nma\n\n\n\n\n\n\n\n\nThe four plots show the The Financial Sector Fund stock prices from JAN 2010 to March 2023, along with the moving averages for 4 months, 1 year 3 years and 4 years. As the window of the moving average increases, the smoother the trend line becomes, reducing the impact of noise and fluctuations in the original time series.\nThe 4-month moving average plot shows frequent fluctuations in the stock price, with the trend line following the general direction of the time series. The 1-year moving average plot shows a smoother trend, following the overall upward trend of the stock price.\nThe 1-year moving average plot shows a similar trend to the 4-month plot but is even smoother, with fewer fluctuations. Finally, the 5-year moving average plot shows the smoothest trend, with an almost constant upward slope.As the moving average window increases, the smoother trend allows for a clearer identification of the general trend of the The Financial Sector Fund stock prices over time. From the moving average obtained above we can see that there is upward tend in the stock price of The Financial Sector Fund.\n\n\nAutocorrelation Time Series\n\nACFPACFADF Test\n\n\n\n\nCode\n#ACF for  data\nggAcf(month)+ggtitle(\"ACF Plot for The Financial Sector Fund Stock JAN 2010 - March 2023\")\n\n\n\n\n\n\n\n\n\nCode\n#PACF for data\nggPacf(month)+ggtitle(\"PACF Plot for The Financial Sector Fund Stock JAN 2010 - March 2023\")\n\n\n\n\n\n\n\n\n\nCode\n#check the stationarity\ntseries::adf.test(month)\n\n\n\n    Augmented Dickey-Fuller Test\n\ndata:  month\nDickey-Fuller = -3.5093, Lag order = 5, p-value = 0.04386\nalternative hypothesis: stationary\n\n\n\n\n\nIn the plot of autocorrelation function, which is the acf graph for monthly data, there are clear autocorrelation in lag. The above lag plots and autocorrelation plot indicates seasonality in the series, which means the series is not stationary. Even though the Augmented Dickey-Fuller Test which tells us that as the p value is lesser than 0.05, it can’t conclude that the series is stationary. So, its better to proceed with differenitation and make the series stationary.\n\n\nDetrend and Differenced Time Series\n\nLinear Fitting ModelACF Plot\n\n\n\n\nCode\nfit = lm(myts~time(myts), na.action=NULL) \nsummary(fit) \n\n\n\nCall:\nlm(formula = myts ~ time(myts), na.action = NULL)\n\nResiduals:\n     Min       1Q   Median       3Q      Max \n-11.0485  -1.3444  -0.1532   1.3051   8.4971 \n\nCoefficients:\n              Estimate Std. Error t value Pr(>|t|)    \n(Intercept) -4.285e+03  2.538e+01  -168.8   <2e-16 ***\ntime(myts)   2.135e+00  1.259e-02   169.6   <2e-16 ***\n---\nSignif. codes:  0 '***' 0.001 '**' 0.01 '*' 0.05 '.' 0.1 ' ' 1\n\nResidual standard error: 2.708 on 3277 degrees of freedom\nMultiple R-squared:  0.8977,    Adjusted R-squared:  0.8977 \nF-statistic: 2.876e+04 on 1 and 3277 DF,  p-value: < 2.2e-16\n\n\n\n\n\n\nCode\n# plot ACFs\nplot1 <- ggAcf(myts, 48, main=\"Original Data: The Financial Sector Fund Stock Stock Price\")\nplot2 <- ggAcf(resid(fit), 48, main=\"Detrended data\")\nplot3 <- ggAcf(diff(myts), 48, main=\"First differenced data\")\ngrid.arrange(plot1, plot2, plot3, nrow=3)\n\n\n\n\n\n\n\n\nThe estimated slope coefficient β1, 0.37602 With a standard error of 0.03141, yielding a significant estimated increase of stock price is very less yearly. Equation of the fit for stationary process: \\[\\hat{y}_{t} = x_{t}+(740.61660)-(0.37602)t\\]\nFrom the above graph we can say that there is no change in detrended plot and the original data acf plot, it typically means that the data is stationary. But when the first order difference is applied the high correlation is removed but there is no seasonal correlation.\nAs depicted in the above figure, the series is now stationary and ready for future study."
  },
  {
    "objectID": "eda_sector_health_care.html",
    "href": "eda_sector_health_care.html",
    "title": "EDA for Health Care Sector Fund",
    "section": "",
    "text": "The Health Care Sector Fund (XLV) is an exchange-traded fund (ETF) that tracks the performance of companies in the healthcare sector, including pharmaceuticals, healthcare equipment and supplies, healthcare providers and services, biotechnology, and life sciences tools and services. This fund provides investors with a convenient way to invest in a diverse range of healthcare companies, enabling them to gain exposure to this sector without having to buy individual stocks. The XLV has been a popular choice among investors looking for a defensive play, given the healthcare sector’s reputation for being relatively resilient during market downturns.\n\nTime Series Plot\n\n\nCode\n# get data\noptions(\"getSymbols.warning4.0\"=FALSE)\noptions(\"getSymbols.yahoo.warning\"=FALSE)\n\n\ndata = getSymbols(\"XLV\",src='yahoo', from = '2010-01-01',to = \"2023-03-01\")\n\ndf <- data.frame(Date=index(XLV),coredata(XLV))\n\n# create Bollinger Bands\nbbands <- BBands(XLV[,c(\"XLV.High\",\"XLV.Low\",\"XLV.Close\")])\n\n# join and subset data\ndf <- subset(cbind(df, data.frame(bbands[,1:3])), Date >= \"2010-01-01\")\n\n#export the data \nXLV_data <- df\nwrite.csv(XLV_data, \"DATA/CLEANED DATA/XLV_raw_data.csv\", row.names=FALSE)\n\n# colors column for increasing and decreasing\nfor (i in 1:length(df[,1])) {\n  if (df$XLV.Close[i] >= df$XLV.Open[i]) {\n      df$direction[i] = 'Increasing'\n  } else {\n      df$direction[i] = 'Decreasing'\n  }\n}\n\ni <- list(line = list(color = '#FF7F50'))\nd <- list(line = list(color = '#7F7F7F'))\n\n# plot candlestick chart\n\nfig <- df %>% plot_ly(x = ~Date, type=\"candlestick\",\n          open = ~XLV.Open, close = ~XLV.Close,\n          high = ~XLV.High, low = ~XLV.Low, name = \"XLV\",\n          increasing = i, decreasing = d) \nfig <- fig %>% add_lines(x = ~Date, y = ~up , name = \"B Bands\",\n            line = list(color = '#ccc', width = 0.5),\n            legendgroup = \"Bollinger Bands\",\n            hoverinfo = \"none\", inherit = F) \nfig <- fig %>% add_lines(x = ~Date, y = ~dn, name = \"B Bands\",\n            line = list(color = '#ccc', width = 0.5),\n            legendgroup = \"Bollinger Bands\", inherit = F,\n            showlegend = FALSE, hoverinfo = \"none\") \nfig <- fig %>% add_lines(x = ~Date, y = ~mavg, name = \"Mv Avg\",\n            line = list(color = '#E377C2', width = 0.5),\n            hoverinfo = \"none\", inherit = F) \nfig <- fig %>% layout(yaxis = list(title = \"Price\"))\n\n# plot volume bar chart\nfig2 <- df \nfig2 <- fig2 %>% plot_ly(x=~Date, y=~XLV.Volume, type='bar', name = \"XLV Volume\",\n          color = ~direction, colors = c('#FF7F50','#7F7F7F')) \nfig2 <- fig2 %>% layout(yaxis = list(title = \"Volume\"))\n\n# create rangeselector buttons\nrs <- list(visible = TRUE, x = 0.5, y = -0.055,\n           xanchor = 'center', yref = 'paper',\n           font = list(size = 9),\n           buttons = list(\n             list(count=1,\n                  label='RESET',\n                  step='all'),\n             list(count=3,\n                  label='3 YR',\n                  step='year',\n                  stepmode='backward'),\n             list(count=1,\n                  label='1 YR',\n                  step='year',\n                  stepmode='backward'),\n             list(count=1,\n                  label='1 MO',\n                  step='month',\n                  stepmode='backward')\n           ))\n\n# subplot with shared x axis\nfig <- subplot(fig, fig2, heights = c(0.7,0.2), nrows=2,\n             shareX = TRUE, titleY = TRUE)\nfig <- fig %>% layout(title = paste(\"Health Care Sector Fund Stock Price: JAN 2010 - March 2023\"),\n         xaxis = list(rangeselector = rs),\n         legend = list(orientation = 'h', x = 0.5, y = 1,\n                       xanchor = 'center', yref = 'paper',\n                       font = list(size = 10),\n                       bgcolor = 'transparent'))\n\nfig\n\n\n\n\n\n\nThe Health Care Sector Fund (XLV) has experienced significant fluctuations and trends from 2010 to March 2023. The fund has shown an upward trend with several dips in between due to various reasons such as political uncertainty, global pandemic, and regulatory changes. The fund experienced a sharp drop in 2011 due to the debt ceiling crisis, but quickly recovered by the end of the year. It showed a steady increase until the end of 2015, followed by a sharp dip in 2016 due to the U.S. presidential election.\nIn 2020, the COVID-19 pandemic caused a significant impact on the healthcare sector, leading to a sharp dip in the XLV fund’s value. However, it quickly bounced back and reached an all-time high in February 2021. The fund continued to experience fluctuations due to regulatory changes and global events such as the Biden administration’s announcement on drug pricing and the vaccine rollout. Overall, the Health Care Sector Fund has been influenced by various factors that have impacted the healthcare industry, resulting in significant fluctuations in its value.\nFor stock prices, a multiplicative decomposition is typically preferred because the percentage changes in stock prices tend to be more important than the absolute changes. Additionally, stock prices tend to exhibit non-constant variance, meaning that the variance of the series changes over time. A multiplicative decomposition can handle this non-constant variance more effectively than an additive decomposition.\n\n\nDecomposed Time Series\n\nDecomposition PlotAdjusted Decomposition Plot\n\n\n\n\nCode\n#time series data\nmyts<-ts(df$XLV.Adjusted,frequency=252,start=c(2010,01,01), end = c(2023,3,1)) \n#original plot for time series data\norginial_plot <- autoplot(myts,xlab =\"Year\", ylab = \"Adjusted Closing Price\", main = \"Health Care Sector Fund Stock price: JAN 2010 - March 2023\")\n#decompose the data\ndecompose = decompose(myts, \"multiplicative\")\n#decomposition plot\nautoplot(decompose)\n\n\n\n\n\n\n\n\n\nCode\n#adjusted plot\ntrendadj <- myts/decompose$trend\ndecompose_adjtrend_plot <- autoplot(trendadj,ylab='trend') +ggtitle('Adjusted trend component in the multiplicative time series model')\nseasonaladj <- myts/decompose$seasonal\ndecompose_adjseasonal_plot <- autoplot(seasonaladj,ylab='seasonal') +ggtitle('Adjusted seasonal component in the multiplicative time series model')\ngrid.arrange(orginial_plot, decompose_adjtrend_plot,decompose_adjseasonal_plot, nrow=3)\n\n\n\n\n\n\n\n\nThe adjusted seasonal component tend to have upward trend till 2019 and drops during the covid period and there is more variability in the model when compared to the original plot where the variation during the years but the adjusted trend then to have more fluctuation showing no trend when compared to the original plot.\n\n\nLag Plots\n\nDaily Time LagsMonthly Time Lags\n\n\n\n\nCode\n#Lag plots \ngglagplot(myts, do.lines=FALSE, lags=1)+xlab(\"Lag 1\")+ylab(\"Yi\")+ggtitle(\"Lag Plot for Health Care Sector Fund Stock JAN 2010 - March 2023\")\n\n\n\n\n\n\n\n\n\nCode\n#montly data\nmean_data <- df %>% \n  mutate(month = month(Date), year = year(Date)) %>% \n  group_by(year, month) %>% \n  summarize(mean_value = mean(XLV.Adjusted))\nmonth<-ts(mean_data$mean_value,start = c(2010, 1),frequency = 12)\n#Lag plot\nts_lags(month)\n\n\n\n\n\n\n\n\n\nThe first lag plot shows the daily time lags of the Health Care Sector Fund stock price from JAN 2010 to March 2023. The plot indicates that there is a strong positive correlation between the current value and the previous day’s value, as seen by the points clustering along the diagonal line. This suggests that the stock price has a positive autocorrelation at a lag of one day.\nThe second lag plot shows the monthly time lags of the mean value of the Health Care Sector Fund stock price from JAN 2010 to March 2023. The plot indicates that there is a positive correlation between the current value and the value from the previous month. This suggests that the mean value of the stock price has a positive autocorrelation at a lag of one month.\nOverall, the lag plots indicate that there is a positive autocorrelation present in the Health Care Sector Fund stock price data, with the strongest correlation observed in the daily time series.\n\n\nSeasonality\n\nSeasonal HeatmapSeasonal Line plot\n\n\n\n\nCode\n# Create seasonal plot\nts_heatmap(month, color = \"GnBu\",title = 'Seasonality Heatmap of Health Care Sector Fund Stock Jan 2010 - March 2023')\n\n\n\n\n\n\n\n\n\n\nCode\n# Create a line graph for each year with months on the x-axis\nggseasonplot(month, datecol = \"date\", valuecol = \"value\")+ggtitle(\"Seasonal Yearly Plot for Health Care Sector Fund Stock Jan 2010 - March 2023\")\n\n\n\n\n\n\n\n\nThe Seasonality Heatmap for the Health Care Sector Fund Stock JAN 2010 - March 2023 does not reveal any clear seasonality in the data. The heatmap shows the mean value of the time series for each month and year combination, with the darker colors indicating higher values. The lack of clear patterns or darker colors in specific months or years suggests that there is no consistent seasonal pattern in the data. However, the yearly line graph shows a slight upward trend in the stock price from 2010 to 2023, but does not show any clear seasonality. Each year’s data is represented by a line, and the months are plotted on the x-axis. Overall, the lack of clear seasonality in both the heatmap and yearly line graph suggests that other factors beyond seasonality are driving the stock price fluctuations.\n\n\nMoving Average\n\n4 Month MA1 Year MA3 Year MA5 Year MA\n\n\n\n\nCode\n#SMA Smoothing \nma <- autoplot(month, series=\"Data\") +\n  autolayer(ma(month,5), series=\"4 Month MA\") +\n  xlab(\"Year\") + ylab(\"GWh\") +\n  ggtitle(\"Health Care Sector Fund Stock JAN 2010 - March 2023(4 Month Moving Average)\") +\n  scale_colour_manual(values=c(\"Data\"=\"grey50\",\"4 Month MA\"=\"red\"),\n                      breaks=c(\"Data\",\"4 Month MA\"))\nma\n\n\n\n\n\n\n\n\n\nCode\n#SMA Smoothing \nma <- autoplot(month, series=\"Data\") +\n  autolayer(ma(month,13), series=\"1 Year MA\") +\n  xlab(\"Year\") + ylab(\"GWh\") +\n  ggtitle(\"Health Care Sector Fund Stock JAN 2010 - March 2023(1 Year Moving Average)\") +\n  scale_colour_manual(values=c(\"Data\"=\"grey50\",\"1 Year MA\"=\"red\"),\n                      breaks=c(\"Data\",\"1 Year MA\"))\nma\n\n\n\n\n\n\n\n\n\nCode\n#SMA Smoothing \nma <- autoplot(month, series=\"Data\") +\n  autolayer(ma(month,37), series=\"3 Year MA\") +\n  xlab(\"Year\") + ylab(\"GWh\") +\n  ggtitle(\"Health Care Sector Fund Stock JAN 2010 - March 2023(3 Year Moving Average)\") +\n  scale_colour_manual(values=c(\"Data\"=\"grey50\",\"3 Year MA\"=\"red\"),\n                      breaks=c(\"Data\",\"3 Year MA\"))\nma\n\n\n\n\n\n\n\n\n\nCode\n#SMA Smoothing \nma <- autoplot(month, series=\"Data\") +\n  autolayer(ma(month,61), series=\"5 Year MA\") +\n  xlab(\"Year\") + ylab(\"GWh\") +\n  ggtitle(\"Health Care Sector Fund Stock JAN 2010 - March 2023(5 Year Moving Average)\") +\n  scale_colour_manual(values=c(\"Data\"=\"grey50\",\"5 Year MA\"=\"red\"),\n                      breaks=c(\"Data\",\"5 Year MA\"))\nma\n\n\n\n\n\n\n\n\nThe four plots show the Health Care Sector Fund stock prices from JAN 2010 to March 2023, along with the moving averages for 4 months, 1 year 3 years and 4 years. As the window of the moving average increases, the smoother the trend line becomes, reducing the impact of noise and fluctuations in the original time series.\nThe 4-month moving average plot shows frequent fluctuations in the stock price, with the trend line following the general direction of the time series. The 1-year moving average plot shows a smoother trend, following the overall upward trend of the stock price.\nThe 1-year moving average plot shows a similar trend to the 4-month plot but is even smoother, with fewer fluctuations. Finally, the 5-year moving average plot shows the smoothest trend, with an almost constant upward slope.As the moving average window increases, the smoother trend allows for a clearer identification of the general trend of the Health Care Sector Fund stock prices over time. From the moving average obtained above we can see that there is upward tend in the stock price of Health Care Sector Fund.\n\n\nAutocorrelation Time Series\n\nACFPACFADF Test\n\n\n\n\nCode\n#ACF for  data\nggAcf(month)+ggtitle(\"ACF Plot for Health Care Sector Fund Stock JAN 2010 - March 2023\")\n\n\n\n\n\n\n\n\n\nCode\n#PACF for data\nggPacf(month)+ggtitle(\"PACF Plot for Health Care Sector Fund Stock JAN 2010 - March 2023\")\n\n\n\n\n\n\n\n\n\nCode\n#check the stationarity\ntseries::adf.test(month)\n\n\n\n    Augmented Dickey-Fuller Test\n\ndata:  month\nDickey-Fuller = -1.8408, Lag order = 5, p-value = 0.6427\nalternative hypothesis: stationary\n\n\n\n\n\nIn the plot of autocorrelation function, which is the acf graph for monthly data, there are clear autocorrelation in lag. The above lag plots and autocorrelation plot indicates seasonality in the series, which means the series is not stationary. It was also verified using Augmented Dickey-Fuller Test which tells us that as the p value is greater than 0.05, the series is not stationary.\n\n\nDetrend and Differenced Time Series\n\nLinear Fitting ModelACF Plot\n\n\n\n\nCode\nfit = lm(myts~time(myts), na.action=NULL) \nsummary(fit) \n\n\n\nCall:\nlm(formula = myts ~ time(myts), na.action = NULL)\n\nResiduals:\n     Min       1Q   Median       3Q      Max \n-28.9902  -4.9914  -0.9621   4.6109  22.5840 \n\nCoefficients:\n              Estimate Std. Error t value Pr(>|t|)    \n(Intercept) -1.708e+04  6.474e+01  -263.8   <2e-16 ***\ntime(myts)   8.504e+00  3.210e-02   264.9   <2e-16 ***\n---\nSignif. codes:  0 '***' 0.001 '**' 0.01 '*' 0.05 '.' 0.1 ' ' 1\n\nResidual standard error: 6.905 on 3277 degrees of freedom\nMultiple R-squared:  0.9554,    Adjusted R-squared:  0.9554 \nF-statistic: 7.016e+04 on 1 and 3277 DF,  p-value: < 2.2e-16\n\n\n\n\n\n\nCode\n# plot ACFs\nplot1 <- ggAcf(myts, 48, main=\"Original Data: Health Care Sector Fund Stock Stock Price\")\nplot2 <- ggAcf(resid(fit), 48, main=\"Detrended data\")\nplot3 <- ggAcf(diff(myts), 48, main=\"First differenced data\")\ngrid.arrange(plot1, plot2, plot3, nrow=3)\n\n\n\n\n\n\n\n\nThe estimated slope coefficient β1, 1.6399 With a standard error of 0.1219, yielding a significant estimated increase of stock price is very less yearly. Equation of the fit for stationary process: \\[\\hat{y}_{t} = x_{t}+(3246.6875)-(1.6399)t\\]\nFrom the above graph we can say that there is no change in detrended plot and the original data acf plot, it typically means that the data is stationary. But when the first order difference is applied the high correlation is removed but there is no seasonal correlation.\nAs depicted in the above figure, the series is now stationary and ready for future study."
  },
  {
    "objectID": "eda_sector_industrial.html",
    "href": "eda_sector_industrial.html",
    "title": "EDA for Industrial Sector Fund",
    "section": "",
    "text": "The Industrial Sector Fund (XLI) is an exchange-traded fund that tracks the performance of companies in the industrial sector of the US stock market. This sector includes companies involved in manufacturing, transportation, and construction, among others. XLI provides investors with exposure to this sector and has holdings in companies such as Boeing, Honeywell, and General Electric. The performance of XLI is closely tied to the overall health of the US economy, as the industrial sector is a key driver of economic growth.\n\nTime Series Plot\n\n\nCode\n# get data\noptions(\"getSymbols.warning4.0\"=FALSE)\noptions(\"getSymbols.yahoo.warning\"=FALSE)\n\n\ndata = getSymbols(\"XLI\",src='yahoo', from = '2010-01-01',to = \"2023-03-01\")\n\ndf <- data.frame(Date=index(XLI),coredata(XLI))\n\n# create Bollinger Bands\nbbands <- BBands(XLI[,c(\"XLI.High\",\"XLI.Low\",\"XLI.Close\")])\n\n# join and subset data\ndf <- subset(cbind(df, data.frame(bbands[,1:3])), Date >= \"2010-01-01\")\n\n#export the data \nXLI_data <- df\nwrite.csv(XLI_data, \"DATA/CLEANED DATA/XLI_raw_data.csv\", row.names=FALSE)\n\n# colors column for increasing and decreasing\nfor (i in 1:length(df[,1])) {\n  if (df$XLI.Close[i] >= df$XLI.Open[i]) {\n      df$direction[i] = 'Increasing'\n  } else {\n      df$direction[i] = 'Decreasing'\n  }\n}\n\ni <- list(line = list(color = '#00BFFF'))\nd <- list(line = list(color = '#7F7F7F'))\n\n# plot candlestick chart\n\nfig <- df %>% plot_ly(x = ~Date, type=\"candlestick\",\n          open = ~XLI.Open, close = ~XLI.Close,\n          high = ~XLI.High, low = ~XLI.Low, name = \"XLI\",\n          increasing = i, decreasing = d) \nfig <- fig %>% add_lines(x = ~Date, y = ~up , name = \"B Bands\",\n            line = list(color = '#ccc', width = 0.5),\n            legendgroup = \"Bollinger Bands\",\n            hoverinfo = \"none\", inherit = F) \nfig <- fig %>% add_lines(x = ~Date, y = ~dn, name = \"B Bands\",\n            line = list(color = '#ccc', width = 0.5),\n            legendgroup = \"Bollinger Bands\", inherit = F,\n            showlegend = FALSE, hoverinfo = \"none\") \nfig <- fig %>% add_lines(x = ~Date, y = ~mavg, name = \"Mv Avg\",\n            line = list(color = '#E377C2', width = 0.5),\n            hoverinfo = \"none\", inherit = F) \nfig <- fig %>% layout(yaxis = list(title = \"Price\"))\n\n# plot volume bar chart\nfig2 <- df \nfig2 <- fig2 %>% plot_ly(x=~Date, y=~XLI.Volume, type='bar', name = \"XLI Volume\",\n          color = ~direction, colors = c('#00BFFF','#7F7F7F')) \nfig2 <- fig2 %>% layout(yaxis = list(title = \"Volume\"))\n\n# create rangeselector buttons\nrs <- list(visible = TRUE, x = 0.5, y = -0.055,\n           xanchor = 'center', yref = 'paper',\n           font = list(size = 9),\n           buttons = list(\n             list(count=1,\n                  label='RESET',\n                  step='all'),\n             list(count=3,\n                  label='3 YR',\n                  step='year',\n                  stepmode='backward'),\n             list(count=1,\n                  label='1 YR',\n                  step='year',\n                  stepmode='backward'),\n             list(count=1,\n                  label='1 MO',\n                  step='month',\n                  stepmode='backward')\n           ))\n\n# subplot with shared x axis\nfig <- subplot(fig, fig2, heights = c(0.7,0.2), nrows=2,\n             shareX = TRUE, titleY = TRUE)\nfig <- fig %>% layout(title = paste(\"Industrial Sector Fund Stock Price: JAN 2010 - March 2023\"),\n         xaxis = list(rangeselector = rs),\n         legend = list(orientation = 'h', x = 0.5, y = 1,\n                       xanchor = 'center', yref = 'paper',\n                       font = list(size = 10),\n                       bgcolor = 'transparent'))\n\nfig\n\n\n\n\n\n\nThe Industrial Sector Fund (XLI) has seen several fluctuations in its performance since 2010. The fund saw a dip in its value during the economic recession of 2010, but it recovered and reached new highs by 2011. The fund continued to perform well until 2015 when it experienced a slump due to concerns about global economic growth and the slowdown in China. However, XLI rebounded again in 2016 and continued to perform well until 2018.\nIn 2018, the fund saw a decline in value due to concerns over trade wars between the US and China, which impacted the industrial sector negatively. Despite this, the fund managed to recover in 2019 and continued to perform well until early 2020. The COVID-19 pandemic led to a sharp drop in the value of XLI in March 2020. However, the fund rebounded quickly and reached new highs by November 2020, mainly due to the rebounding economy and hopes for a COVID-19 vaccine. The fund continued to perform well in early 2021, but a resurgence of COVID-19 cases and concerns over rising inflation caused some volatility in the market, leading to fluctuations in XLI’s value.\nFor stock prices, a multiplicative decomposition is typically preferred because the percentage changes in stock prices tend to be more important than the absolute changes. Additionally, stock prices tend to exhibit non-constant variance, meaning that the variance of the series changes over time. A multiplicative decomposition can handle this non-constant variance more effectively than an additive decomposition.\n\n\nDecomposed Time Series\n\nDecomposition PlotAdjusted Decomposition Plot\n\n\n\n\nCode\n#time series data\nmyts<-ts(df$XLI.Adjusted,frequency=252,start=c(2010,01,01), end = c(2023,3,1)) \n#original plot for time series data\norginial_plot <- autoplot(myts,xlab =\"Year\", ylab = \"Adjusted Closing Price\", main = \"Industrial Sector Fund Stock price: JAN 2010 - March 2023\")\n#decompose the data\ndecompose = decompose(myts, \"multiplicative\")\n#decomposition plot\nautoplot(decompose)\n\n\n\n\n\n\n\n\n\nCode\n#adjusted plot\ntrendadj <- myts/decompose$trend\ndecompose_adjtrend_plot <- autoplot(trendadj,ylab='trend') +ggtitle('Adjusted trend component in the multiplicative time series model')\nseasonaladj <- myts/decompose$seasonal\ndecompose_adjseasonal_plot <- autoplot(seasonaladj,ylab='seasonal') +ggtitle('Adjusted seasonal component in the multiplicative time series model')\ngrid.arrange(orginial_plot, decompose_adjtrend_plot,decompose_adjseasonal_plot, nrow=3)\n\n\n\n\n\n\n\n\nThe adjusted seasonal component tend to have upward trend till 2019 and drops during the covid period and there is more variability in the model when compared to the original plot where the variation during the years but the adjusted trend then to have more fluctuation showing no trend when compared to the original plot.\n\n\nLag Plots\n\nDaily Time LagsMonthly Time Lags\n\n\n\n\nCode\n#Lag plots \ngglagplot(myts, do.lines=FALSE, lags=1)+xlab(\"Lag 1\")+ylab(\"Yi\")+ggtitle(\"Lag Plot for Industrial Sector Fund Stock JAN 2010 - March 2023\")\n\n\n\n\n\n\n\n\n\nCode\n#montly data\nmean_data <- df %>% \n  mutate(month = month(Date), year = year(Date)) %>% \n  group_by(year, month) %>% \n  summarize(mean_value = mean(XLI.Adjusted))\nmonth<-ts(mean_data$mean_value,start = c(2010, 1),frequency = 12)\n#Lag plot\nts_lags(month)\n\n\n\n\n\n\n\n\n\nThe first lag plot shows the daily time lags of the Industrial Sector Fund stock price from JAN 2010 to March 2023. The plot indicates that there is a strong positive correlation between the current value and the previous day’s value, as seen by the points clustering along the diagonal line. This suggests that the stock price has a positive autocorrelation at a lag of one day.\nThe second lag plot shows the monthly time lags of the mean value of the Industrial Sector Fund stock price from JAN 2010 to March 2023. The plot indicates that there is a positive correlation between the current value and the value from the previous month. This suggests that the mean value of the stock price has a positive autocorrelation at a lag of one month.\nOverall, the lag plots indicate that there is a positive autocorrelation present in the Industrial Sector Fund stock price data, with the strongest correlation observed in the daily time series.\n\n\nSeasonality\n\nSeasonal HeatmapSeasonal Line plot\n\n\n\n\nCode\n# Create seasonal plot\nts_heatmap(month,color = \"Greys\", title = 'Seasonality Heatmap of Industrial Sector Fund Stock Jan 2010 - March 2023')\n\n\n\n\n\n\n\n\n\n\nCode\n# Create a line graph for each year with months on the x-axis\nggseasonplot(month, datecol = \"date\", valuecol = \"value\")+ggtitle(\"Seasonal Yearly Plot for Industrial Sector Fund Stock Jan 2010 - March 2023\")\n\n\n\n\n\n\n\n\nThe Seasonality Heatmap for the Industrial Sector Fund Stock JAN 2010 - March 2023 does not reveal any clear seasonality in the data. The heatmap shows the mean value of the time series for each month and year combination, with the darker colors indicating higher values. The lack of clear patterns or darker colors in specific months or years suggests that there is no consistent seasonal pattern in the data. However, the yearly line graph shows a slight upward trend in the stock price from 2010 to 2023, but does not show any clear seasonality. Each year’s data is represented by a line, and the months are plotted on the x-axis. Overall, the lack of clear seasonality in both the heatmap and yearly line graph suggests that other factors beyond seasonality are driving the stock price fluctuations.\n\n\nMoving Average\n\n4 Month MA1 Year MA3 Year MA5 Year MA\n\n\n\n\nCode\n#SMA Smoothing \nma <- autoplot(month, series=\"Data\") +\n  autolayer(ma(month,5), series=\"4 Month MA\") +\n  xlab(\"Year\") + ylab(\"GWh\") +\n  ggtitle(\"Industrial Sector Fund Stock JAN 2010 - March 2023(4 Month Moving Average)\") +\n  scale_colour_manual(values=c(\"Data\"=\"grey50\",\"4 Month MA\"=\"red\"),\n                      breaks=c(\"Data\",\"4 Month MA\"))\nma\n\n\n\n\n\n\n\n\n\nCode\n#SMA Smoothing \nma <- autoplot(month, series=\"Data\") +\n  autolayer(ma(month,13), series=\"1 Year MA\") +\n  xlab(\"Year\") + ylab(\"GWh\") +\n  ggtitle(\"Industrial Sector Fund Stock JAN 2010 - March 2023(1 Year Moving Average)\") +\n  scale_colour_manual(values=c(\"Data\"=\"grey50\",\"1 Year MA\"=\"red\"),\n                      breaks=c(\"Data\",\"1 Year MA\"))\nma\n\n\n\n\n\n\n\n\n\nCode\n#SMA Smoothing \nma <- autoplot(month, series=\"Data\") +\n  autolayer(ma(month,37), series=\"3 Year MA\") +\n  xlab(\"Year\") + ylab(\"GWh\") +\n  ggtitle(\"Industrial Sector Fund Stock JAN 2010 - March 2023(3 Year Moving Average)\") +\n  scale_colour_manual(values=c(\"Data\"=\"grey50\",\"3 Year MA\"=\"red\"),\n                      breaks=c(\"Data\",\"3 Year MA\"))\nma\n\n\n\n\n\n\n\n\n\nCode\n#SMA Smoothing \nma <- autoplot(month, series=\"Data\") +\n  autolayer(ma(month,61), series=\"5 Year MA\") +\n  xlab(\"Year\") + ylab(\"GWh\") +\n  ggtitle(\"Industrial Sector Fund Stock JAN 2010 - March 2023(5 Year Moving Average)\") +\n  scale_colour_manual(values=c(\"Data\"=\"grey50\",\"5 Year MA\"=\"red\"),\n                      breaks=c(\"Data\",\"5 Year MA\"))\nma\n\n\n\n\n\n\n\n\nThe four plots show the Industrial Sector Fund stock prices from JAN 2010 to March 2023, along with the moving averages for 4 months, 1 year 3 years and 4 years. As the window of the moving average increases, the smoother the trend line becomes, reducing the impact of noise and fluctuations in the original time series.\nThe 4-month moving average plot shows frequent fluctuations in the stock price, with the trend line following the general direction of the time series. The 1-year moving average plot shows a smoother trend, following the overall upward trend of the stock price.\nThe 1-year moving average plot shows a similar trend to the 4-month plot but is even smoother, with fewer fluctuations. Finally, the 5-year moving average plot shows the smoothest trend, with an almost constant upward slope.As the moving average window increases, the smoother trend allows for a clearer identification of the general trend of the Industrial Sector Fund stock prices over time. From the moving average obtained above we can see that there is upward tend in the stock price of Industrial Sector Fund.\n\n\nAutocorrelation Time Series\n\nACFPACFADF Test\n\n\n\n\nCode\n#ACF for  data\nggAcf(month)+ggtitle(\"ACF Plot for Industrial Sector Fund Stock JAN 2010 - March 2023\")\n\n\n\n\n\n\n\n\n\nCode\n#PACF for data\nggPacf(month)+ggtitle(\"PACF Plot for Industrial Sector Fund Stock JAN 2010 - March 2023\")\n\n\n\n\n\n\n\n\n\nCode\n#check the stationarity\ntseries::adf.test(month)\n\n\n\n    Augmented Dickey-Fuller Test\n\ndata:  month\nDickey-Fuller = -3.334, Lag order = 5, p-value = 0.06807\nalternative hypothesis: stationary\n\n\n\n\n\nIn the plot of autocorrelation function, which is the acf graph for monthly data, there are clear autocorrelation in lag. The above lag plots and autocorrelation plot indicates seasonality in the series, which means the series is not stationary. It was also verified using Augmented Dickey-Fuller Test which tells us that as the p value is greater than 0.05, the series is not stationary.\n\n\nDetrend and Differenced Time Series\n\nLinear Fitting ModelACF Plot\n\n\n\n\nCode\nfit = lm(myts~time(myts), na.action=NULL) \nsummary(fit) \n\n\n\nCall:\nlm(formula = myts ~ time(myts), na.action = NULL)\n\nResiduals:\n     Min       1Q   Median       3Q      Max \n-31.3040  -2.9466  -0.1944   2.9162  16.8247 \n\nCoefficients:\n              Estimate Std. Error t value Pr(>|t|)    \n(Intercept) -1.203e+04  5.594e+01  -215.0   <2e-16 ***\ntime(myts)   5.991e+00  2.774e-02   215.9   <2e-16 ***\n---\nSignif. codes:  0 '***' 0.001 '**' 0.01 '*' 0.05 '.' 0.1 ' ' 1\n\nResidual standard error: 5.967 on 3277 degrees of freedom\nMultiple R-squared:  0.9343,    Adjusted R-squared:  0.9343 \nF-statistic: 4.663e+04 on 1 and 3277 DF,  p-value: < 2.2e-16\n\n\n\n\n\n\nCode\n# plot ACFs\nplot1 <- ggAcf(myts, 48, main=\"Original Data: Industrial Sector Fund Stock Stock Price\")\nplot2 <- ggAcf(resid(fit), 48, main=\"Detrended data\")\nplot3 <- ggAcf(diff(myts), 48, main=\"First differenced data\")\ngrid.arrange(plot1, plot2, plot3, nrow=3)\n\n\n\n\n\n\n\n\nThe estimated slope coefficient β1, 1.077e+00 With a standard error of 8.693e-02, yielding a significant estimated increase of stock price is very less yearly. Equation of the fit for stationary process: \\[\\hat{y}_{t} = x_{t}+(2.123e+03)-(1.077e+00)t\\]\nFrom the above graph we can say that there is no change in detrended plot and the original data acf plot, it typically means that the data is stationary. But when the first order difference is applied the high correlation is removed but there is no seasonal correlation.\nAs depicted in the above figure, the series is now stationary and ready for future study."
  },
  {
    "objectID": "eda_sector_market.html",
    "href": "eda_sector_market.html",
    "title": "EDA for Sector Market",
    "section": "",
    "text": "Exploratory Data Analysis (EDA) for sector market stocks involves analyzing time series data to identify the underlying patterns and characteristics of the stocks within a particular sector. A sector market refers to a group of companies that operate in the same industry, such as technology or healthcare. EDA for sector market stocks typically involves analyzing the daily closing prices of individual stocks within a sector and examining key aspects such as autocorrelation, seasonality, trend, and stationarity.\nOne of the key aspects of EDA for sector market stocks is identifying the presence of autocorrelation. Autocorrelation refers to the correlation between a stock’s price and its past prices. The autocorrelation function (ACF) and partial autocorrelation function (PACF) plots can help identify the degree of correlation between the stock’s price and its past prices. This information can be useful for forecasting future prices and identifying potential patterns in the data.\nAnother important aspect of EDA for sector market stocks is identifying the presence of seasonality. Seasonality refers to a pattern that repeats itself in the stock prices over regular intervals, such as daily, weekly, or monthly. Identifying seasonality is important as it can help us identify potential patterns and trends in the data, and it can inform our modeling approach.\nAdditionally, examining moving averages and detrending can help identify the underlying trend in the stock prices. Moving averages are used to smooth out short-term fluctuations in the data, and detrending can help identify the underlying trend that is not related to the seasonal or cyclical fluctuations in the data.\nFinally, testing for stationarity is important as it allows us to apply statistical models that assume the data to be stationary. Stationarity refers to the property of a time series where its statistical properties, such as mean and variance, remain constant over time. If the data is non-stationary, it can be transformed to become stationary through techniques such as differencing or taking logarithms.\nThis information can be used to inform our modeling approach, identify potential patterns and trends, and improve our investment strategies within a particular industry sector.\nClick to view EDA Page for Consumer Staples Sector Fund\nClick to view EDA Page for Utilities Sector Fund\nClick to view EDA Page for Health Care Sector Fund\nClick to view EDA Page for Industrial Sector Fund\nClick to view EDA Page for Financial Sector Fund\nClick to view EDA Page for Consumer Discretionary Sector Fund\nClick to view EDA Page for Communication Services Sector Fund\nClick to view EDA Page for Real Estate Sector Fund\nClick to view EDA Page for Materials Sector Fund\nClick to view EDA Page for Technology Sector Fund\nClick to view EDA Page for Energy Sector Fund"
  },
  {
    "objectID": "eda_sector_materials_sector.html",
    "href": "eda_sector_materials_sector.html",
    "title": "EDA for Materials Sector Fund",
    "section": "",
    "text": "The Materials Sector Fund (XLB) is an exchange-traded fund that focuses on companies that are involved in the production and distribution of raw materials, such as chemicals, construction materials, and metals. This fund provides investors with exposure to a diverse range of companies that are involved in the production and distribution of materials that are essential for industrial and economic growth. The performance of the XLB has been influenced by a variety of factors over the years, including shifts in global demand for raw materials, fluctuations in commodity prices, and changes in government policies and regulations. The fund has experienced periods of volatility, particularly during economic downturns when demand for materials tends to decline, but has also seen growth during times of economic expansion and increased demand for infrastructure and construction projects.\n\nTime Series Plot\n\n\nCode\n# get data\noptions(\"getSymbols.warning4.0\"=FALSE)\noptions(\"getSymbols.yahoo.warning\"=FALSE)\n\n\ndata = getSymbols(\"XLB\",src='yahoo', from = '2010-01-01',to = \"2023-03-01\")\n\ndf <- data.frame(Date=index(XLB),coredata(XLB))\n\n# create Bollinger Bands\nbbands <- BBands(XLB[,c(\"XLB.High\",\"XLB.Low\",\"XLB.Close\")])\n\n# join and subset data\ndf <- subset(cbind(df, data.frame(bbands[,1:3])), Date >= \"2010-01-01\")\n\n#export the data \nXLB_data <- df\nwrite.csv(XLB_data, \"DATA/CLEANED DATA/XLB_raw_data.csv\", row.names=FALSE)\n\n# colors column for increasing and decreasing\nfor (i in 1:length(df[,1])) {\n  if (df$XLB.Close[i] >= df$XLB.Open[i]) {\n      df$direction[i] = 'Increasing'\n  } else {\n      df$direction[i] = 'Decreasing'\n  }\n}\n\ni <- list(line = list(color = '#ADD8E6'))\nd <- list(line = list(color = '#7F7F7F'))\n\n# plot candlestick chart\n\nfig <- df %>% plot_ly(x = ~Date, type=\"candlestick\",\n          open = ~XLB.Open, close = ~XLB.Close,\n          high = ~XLB.High, low = ~XLB.Low, name = \"XLB\",\n          increasing = i, decreasing = d) \nfig <- fig %>% add_lines(x = ~Date, y = ~up , name = \"B Bands\",\n            line = list(color = '#ccc', width = 0.5),\n            legendgroup = \"Bollinger Bands\",\n            hoverinfo = \"none\", inherit = F) \nfig <- fig %>% add_lines(x = ~Date, y = ~dn, name = \"B Bands\",\n            line = list(color = '#ccc', width = 0.5),\n            legendgroup = \"Bollinger Bands\", inherit = F,\n            showlegend = FALSE, hoverinfo = \"none\") \nfig <- fig %>% add_lines(x = ~Date, y = ~mavg, name = \"Mv Avg\",\n            line = list(color = '#E377C2', width = 0.5),\n            hoverinfo = \"none\", inherit = F) \nfig <- fig %>% layout(yaxis = list(title = \"Price\"))\n\n# plot volume bar chart\nfig2 <- df \nfig2 <- fig2 %>% plot_ly(x=~Date, y=~XLB.Volume, type='bar', name = \"XLB Volume\",\n          color = ~direction, colors = c('#ADD8E6','#7F7F7F')) \nfig2 <- fig2 %>% layout(yaxis = list(title = \"Volume\"))\n\n# create rangeselector buttons\nrs <- list(visible = TRUE, x = 0.5, y = -0.055,\n           xanchor = 'center', yref = 'paper',\n           font = list(size = 9),\n           buttons = list(\n             list(count=1,\n                  label='RESET',\n                  step='all'),\n             list(count=3,\n                  label='3 YR',\n                  step='year',\n                  stepmode='backward'),\n             list(count=1,\n                  label='1 YR',\n                  step='year',\n                  stepmode='backward'),\n             list(count=1,\n                  label='1 MO',\n                  step='month',\n                  stepmode='backward')\n           ))\n\n# subplot with shared x axis\nfig <- subplot(fig, fig2, heights = c(0.7,0.2), nrows=2,\n             shareX = TRUE, titleY = TRUE)\nfig <- fig %>% layout(title = paste(\"Materials Sector Fund Stock Price: JAN 2010 - March 2023\"),\n         xaxis = list(rangeselector = rs),\n         legend = list(orientation = 'h', x = 0.5, y = 1,\n                       xanchor = 'center', yref = 'paper',\n                       font = list(size = 10),\n                       bgcolor = 'transparent'))\n\nfig\n\n\n\n\n\n\nThe Materials Sector Fund (XLB) experienced various trends and fluctuations from 2010 to March 2023. In the early years of the period, the fund saw a slow but steady upward trend, with a significant drop during the economic recession of 2011. However, the fund managed to recover in the following years, reaching new highs by mid-2015. The period between 2015 and early 2016 saw a sharp decline in the fund’s performance due to the economic slowdown in China and the decline in global commodity prices. However, the fund recovered soon after, driven by the growth in infrastructure spending and construction in the US. The fund experienced some volatility in the following years, with some dips and recoveries, mainly due to trade tensions between the US and China and the global economic slowdown. Overall, the performance of the Materials Sector Fund has been closely tied to the strength of the global economy, demand for commodities, and geopolitical events affecting the industry.\nFor stock prices, a multiplicative decomposition is typically preferred because the percentage changes in stock prices tend to be more important than the absolute changes. Additionally, stock prices tend to exhibit non-constant variance, meaning that the variance of the series changes over time. A multiplicative decomposition can handle this non-constant variance more effectively than an additive decomposition.\n\n\nDecomposed Time Series\n\nDecomposition PlotAdjusted Decomposition Plot\n\n\n\n\nCode\n#time series data\nmyts<-ts(df$XLB.Adjusted,frequency=252,start=c(2010,01,01), end = c(2023,3,1)) \n#original plot for time series data\norginial_plot <- autoplot(myts,xlab =\"Year\", ylab = \"Adjusted Closing Price\", main = \"Materials Sector Fund Stock price: JAN 2010 - March 2023\")\n#decompose the data\ndecompose = decompose(myts, \"multiplicative\")\n#decomposition plot\nautoplot(decompose)\n\n\n\n\n\n\n\n\n\nCode\n#adjusted plot\ntrendadj <- myts/decompose$trend\ndecompose_adjtrend_plot <- autoplot(trendadj,ylab='trend') +ggtitle('Adjusted trend component in the multiplicative time series model')\nseasonaladj <- myts/decompose$seasonal\ndecompose_adjseasonal_plot <- autoplot(seasonaladj,ylab='seasonal') +ggtitle('Adjusted seasonal component in the multiplicative time series model')\ngrid.arrange(orginial_plot, decompose_adjtrend_plot,decompose_adjseasonal_plot, nrow=3)\n\n\n\n\n\n\n\n\nThe adjusted seasonal component tend to have upward trend till 2019 and drops during the covid period and there is more variability in the model when compared to the original plot where the variation during the years but the adjusted trend then to have more fluctuation showing no trend when compared to the original plot.\n\n\nLag Plots\n\nDaily Time LagsMonthly Time Lags\n\n\n\n\nCode\n#Lag plots \ngglagplot(myts, do.lines=FALSE, lags=1)+xlab(\"Lag 1\")+ylab(\"Yi\")+ggtitle(\"Lag Plot for Materials Sector Fund Stock JAN 2010 - March 2023\")\n\n\n\n\n\n\n\n\n\nCode\n#montly data\nmean_data <- df %>% \n  mutate(month = month(Date), year = year(Date)) %>% \n  group_by(year, month) %>% \n  summarize(mean_value = mean(XLB.Adjusted))\nmonth<-ts(mean_data$mean_value,start = c(2010, 1),frequency = 12)\n#Lag plot\nts_lags(month)\n\n\n\n\n\n\n\n\n\nThe first lag plot shows the daily time lags of the Materials Sector Fund stock price from JAN 2010 to March 2023. The plot indicates that there is a strong positive correlation between the current value and the previous day’s value, as seen by the points clustering along the diagonal line. This suggests that the stock price has a positive autocorrelation at a lag of one day.\nThe second lag plot shows the monthly time lags of the mean value of the Materials Sector Fund stock price from JAN 2010 to March 2023. The plot indicates that there is a positive correlation between the current value and the value from the previous month. This suggests that the mean value of the stock price has a positive autocorrelation at a lag of one month.\nOverall, the lag plots indicate that there is a positive autocorrelation present in the Materials Sector Fund stock price data, with the strongest correlation observed in the daily time series.\n\n\nSeasonality\n\nSeasonal HeatmapSeasonal Line plot\n\n\n\n\nCode\n# Create seasonal plot\nts_heatmap(month, color = \"YlGn\", title = 'Seasonality Heatmap of Materials Sector Fund Stock Jan 2010 - March 2023')\n\n\n\n\n\n\n\n\n\n\nCode\n# Create a line graph for each year with months on the x-axis\nggseasonplot(month, datecol = \"date\", valuecol = \"value\")+ggtitle(\"Seasonal Yearly Plot for Materials Sector Fund Stock Jan 2010 - March 2023\")\n\n\n\n\n\n\n\n\nThe Seasonality Heatmap for the Materials Sector Fund Stock JAN 2010 - March 2023 does not reveal any clear seasonality in the data. The heatmap shows the mean value of the time series for each month and year combination, with the darker colors indicating higher values. The lack of clear patterns or darker colors in specific months or years suggests that there is no consistent seasonal pattern in the data. However, the yearly line graph shows a slight upward trend in the stock price from 2010 to 2023, but does not show any clear seasonality. Each year’s data is represented by a line, and the months are plotted on the x-axis. Overall, the lack of clear seasonality in both the heatmap and yearly line graph suggests that other factors beyond seasonality are driving the stock price fluctuations.\n\n\nMoving Average\n\n4 Month MA1 Year MA3 Year MA\n\n\n\n\nCode\n#SMA Smoothing \nma <- autoplot(month, series=\"Data\") +\n  autolayer(ma(month,5), series=\"4 Month MA\") +\n  xlab(\"Year\") + ylab(\"GWh\") +\n  ggtitle(\"Materials Sector Fund Stock JAN 2010 - March 2023(4 Month Moving Average)\") +\n  scale_colour_manual(values=c(\"Data\"=\"grey50\",\"4 Month MA\"=\"red\"),\n                      breaks=c(\"Data\",\"4 Month MA\"))\nma\n\n\n\n\n\n\n\n\n\nCode\n#SMA Smoothing \nma <- autoplot(month, series=\"Data\") +\n  autolayer(ma(month,13), series=\"1 Year MA\") +\n  xlab(\"Year\") + ylab(\"GWh\") +\n  ggtitle(\"Materials Sector Fund Stock JAN 2010 - March 2023(1 Year Moving Average)\") +\n  scale_colour_manual(values=c(\"Data\"=\"grey50\",\"1 Year MA\"=\"red\"),\n                      breaks=c(\"Data\",\"1 Year MA\"))\nma\n\n\n\n\n\n\n\n\n\nCode\n#SMA Smoothing \nma <- autoplot(month, series=\"Data\") +\n  autolayer(ma(month,37), series=\"3 Year MA\") +\n  xlab(\"Year\") + ylab(\"GWh\") +\n  ggtitle(\"Materials Sector Fund Stock JAN 2010 - March 2023(3 Year Moving Average)\") +\n  scale_colour_manual(values=c(\"Data\"=\"grey50\",\"3 Year MA\"=\"red\"),\n                      breaks=c(\"Data\",\"3 Year MA\"))\nma\n\n\n\n\n\n\n\n\nThe three plots show the Materials Sector Fund stock prices from JAN 2010 to March 2023, along with the moving averages for 4 months, 1 year and 3 years. As the window of the moving average increases, the smoother the trend line becomes, reducing the impact of noise and fluctuations in the original time series.\nThe 4-month moving average plot shows frequent fluctuations in the stock price, with the trend line following the general direction of the time series. The 1-year moving average plot shows a smoother trend, following the overall upward trend of the stock price.\nThe 1-year moving average plot shows a similar trend to the 4-month plot but is even smoother, with fewer fluctuations. Finally, the 3-year moving average plot shows the smoothest trend, with an almost constant upward slope.As the moving average window increases, the smoother trend allows for a clearer identification of the general trend of the Materials Sector Fund stock prices over time. From the moving average obtained above we can see that there is upward tend in the stock price of Materials Sector Fund.\n\n\nAutocorrelation Time Series\n\nACFPACFADF Test\n\n\n\n\nCode\n#ACF for  data\nggAcf(month)+ggtitle(\"ACF Plot for Materials Sector Fund Stock JAN 2010 - March 2023\")\n\n\n\n\n\n\n\n\n\nCode\n#PACF for data\nggPacf(month)+ggtitle(\"PACF Plot for Materials Sector Fund Stock JAN 2010 - March 2023\")\n\n\n\n\n\n\n\n\n\nCode\n#check the stationarity\ntseries::adf.test(month)\n\n\n\n    Augmented Dickey-Fuller Test\n\ndata:  month\nDickey-Fuller = -2.3534, Lag order = 5, p-value = 0.429\nalternative hypothesis: stationary\n\n\n\n\n\nIn the plot of autocorrelation function, which is the acf graph for monthly data, there are clear autocorrelation in lag. The above lag plots and autocorrelation plot indicates seasonality in the series, which means the series is not stationary. This can be verified by the Augmented Dickey-Fuller Test which tells us that as the p value is greater than 0.05.\n\n\nDetrend and Differenced Time Series\n\nLinear Fitting ModelACF Plot\n\n\n\n\nCode\nfit = lm(myts~time(myts), na.action=NULL) \nsummary(fit) \n\n\n\nCall:\nlm(formula = myts ~ time(myts), na.action = NULL)\n\nResiduals:\n     Min       1Q   Median       3Q      Max \n-25.7882  -4.1542  -0.2222   3.1821  17.9584 \n\nCoefficients:\n              Estimate Std. Error t value Pr(>|t|)    \n(Intercept) -8.411e+03  5.809e+01  -144.8   <2e-16 ***\ntime(myts)   4.194e+00  2.881e-02   145.6   <2e-16 ***\n---\nSignif. codes:  0 '***' 0.001 '**' 0.01 '*' 0.05 '.' 0.1 ' ' 1\n\nResidual standard error: 6.196 on 3277 degrees of freedom\nMultiple R-squared:  0.8661,    Adjusted R-squared:  0.866 \nF-statistic: 2.119e+04 on 1 and 3277 DF,  p-value: < 2.2e-16\n\n\n\n\n\n\nCode\n# plot ACFs\nplot1 <- ggAcf(myts, 48, main=\"Original Data: Materials Sector Fund Stock Stock Price\")\nplot2 <- ggAcf(resid(fit), 48, main=\"Detrended data\")\nplot3 <- ggAcf(diff(myts), 48, main=\"First differenced data\")\ngrid.arrange(plot1, plot2, plot3, nrow=3)\n\n\n\n\n\n\n\n\nThe estimated slope coefficient β1, 8.441e-01 With a standard error of 6.216e-02, yielding a significant estimated increase of stock price is very less yearly. Equation of the fit for stationary process: \\[\\hat{y}_{t} = x_{t}+(1.660e+03)-(8.441e-01)t\\]\nFrom the above graph we can say that there is no change in detrended plot and the original data acf plot, it typically means that the data is stationary. But when the first order difference is applied the high correlation is removed but there is no seasonal correlation.\nAs depicted in the above figure, the series is now stationary and ready for future study."
  },
  {
    "objectID": "eda_sector_real_estate.html",
    "href": "eda_sector_real_estate.html",
    "title": "EDA for Real Estate Sector Fund",
    "section": "",
    "text": "Real Estate Sector Fund (XLRE) is an exchange-traded fund that provides exposure to companies involved in real estate, including real estate investment trusts (REITs) and real estate management and development firms. This sector fund invests in a variety of real estate segments, such as commercial, residential, industrial, and retail, providing diversification benefits for investors. The performance of XLRE is influenced by several factors, including interest rates, economic growth, and the overall health of the real estate market. In recent years, the XLRE has shown a trend of steady growth, driven by a strong demand for real estate and low-interest rates. However, fluctuations can occur due to changes in government policies, economic conditions, and market sentiment towards real estate investments.\n\nTime Series Plot\n\n\nCode\n# get data\noptions(\"getSymbols.warning4.0\"=FALSE)\noptions(\"getSymbols.yahoo.warning\"=FALSE)\n\n\ndata = getSymbols(\"XLRE\",src='yahoo', from = '2016-01-01',to = \"2023-03-01\")\n\ndf <- data.frame(Date=index(XLRE),coredata(XLRE))\n\n# create Bollinger Bands\nbbands <- BBands(XLRE[,c(\"XLRE.High\",\"XLRE.Low\",\"XLRE.Close\")])\n\n# join and subset data\ndf <- subset(cbind(df, data.frame(bbands[,1:3])), Date >= \"2016-01-01\")\n\n#export the data \nXLRE_data <- df\nwrite.csv(XLRE_data, \"DATA/CLEANED DATA/XLRE_raw_data.csv\", row.names=FALSE)\n\n# colors column for increasing and decreasing\nfor (i in 1:length(df[,1])) {\n  if (df$XLRE.Close[i] >= df$XLRE.Open[i]) {\n      df$direction[i] = 'Increasing'\n  } else {\n      df$direction[i] = 'Decreasing'\n  }\n}\n\ni <- list(line = list(color = '#F0E68C'))\nd <- list(line = list(color = '#7F7F7F'))\n\n# plot candlestick chart\n\nfig <- df %>% plot_ly(x = ~Date, type=\"candlestick\",\n          open = ~XLRE.Open, close = ~XLRE.Close,\n          high = ~XLRE.High, low = ~XLRE.Low, name = \"XLRE\",\n          increasing = i, decreasing = d) \nfig <- fig %>% add_lines(x = ~Date, y = ~up , name = \"B Bands\",\n            line = list(color = '#ccc', width = 0.5),\n            legendgroup = \"Bollinger Bands\",\n            hoverinfo = \"none\", inherit = F) \nfig <- fig %>% add_lines(x = ~Date, y = ~dn, name = \"B Bands\",\n            line = list(color = '#ccc', width = 0.5),\n            legendgroup = \"Bollinger Bands\", inherit = F,\n            showlegend = FALSE, hoverinfo = \"none\") \nfig <- fig %>% add_lines(x = ~Date, y = ~mavg, name = \"Mv Avg\",\n            line = list(color = '#E377C2', width = 0.5),\n            hoverinfo = \"none\", inherit = F) \nfig <- fig %>% layout(yaxis = list(title = \"Price\"))\n\n# plot volume bar chart\nfig2 <- df \nfig2 <- fig2 %>% plot_ly(x=~Date, y=~XLRE.Volume, type='bar', name = \"XLRE Volume\",\n          color = ~direction, colors = c('#F0E68C','#7F7F7F')) \nfig2 <- fig2 %>% layout(yaxis = list(title = \"Volume\"))\n\n# create rangeselector buttons\nrs <- list(visible = TRUE, x = 0.5, y = -0.055,\n           xanchor = 'center', yref = 'paper',\n           font = list(size = 9),\n           buttons = list(\n             list(count=1,\n                  label='RESET',\n                  step='all'),\n             list(count=3,\n                  label='3 YR',\n                  step='year',\n                  stepmode='backward'),\n             list(count=1,\n                  label='1 YR',\n                  step='year',\n                  stepmode='backward'),\n             list(count=1,\n                  label='1 MO',\n                  step='month',\n                  stepmode='backward')\n           ))\n\n# subplot with shared x axis\nfig <- subplot(fig, fig2, heights = c(0.7,0.2), nrows=2,\n             shareX = TRUE, titleY = TRUE)\nfig <- fig %>% layout(title = paste(\"Real Estate Sector Fund Stock Price: JAN 2016 - March 2023\"),\n         xaxis = list(rangeselector = rs),\n         legend = list(orientation = 'h', x = 0.5, y = 1,\n                       xanchor = 'center', yref = 'paper',\n                       font = list(size = 10),\n                       bgcolor = 'transparent'))\n\nfig\n\n\n\n\n\n\nThe Real Estate Sector Fund (XLRE) was established in 2015, and since then, it has shown significant growth and stability. The fund tracks the performance of the Real Estate Select Sector Index, which is composed of real estate investment trusts (REITs) and real estate management and development companies.\nXLRE’s price trend has been relatively stable since its inception, with the fund experiencing consistent growth with minor fluctuations. The fund’s value reached its peak in early 2020 but experienced a decline in the second quarter due to the COVID-19 pandemic’s adverse impact on the real estate sector.\nThe pandemic caused a decline in commercial real estate demand and occupancy rates, which negatively impacted REITs’ performance. However, as the pandemic eased, the demand for residential properties increased, and REITs’ values rose again. Moreover, as the U.S. economy began to recover, the demand for commercial real estate picked up, leading to a positive impact on the fund’s performance.\nFor stock prices, a multiplicative decomposition is typically preferred because the percentage changes in stock prices tend to be more important than the absolute changes. Additionally, stock prices tend to exhibit non-constant variance, meaning that the variance of the series changes over time. A multiplicative decomposition can handle this non-constant variance more effectively than an additive decomposition.\n\n\nDecomposed Time Series\n\nDecomposition PlotAdjusted Decomposition Plot\n\n\n\n\nCode\n#time series data\nmyts<-ts(df$XLRE.Adjusted,frequency=252,start=c(2016,01,01), end = c(2023,3,1)) \n#original plot for time series data\norginial_plot <- autoplot(myts,xlab =\"Year\", ylab = \"Adjusted Closing Price\", main = \"Real Estate Sector Fund Stock price: JAN 2016 - March 2023\")\n#decompose the data\ndecompose = decompose(myts, \"multiplicative\")\n#decomposition plot\nautoplot(decompose)\n\n\n\n\n\n\n\n\n\nCode\n#adjusted plot\ntrendadj <- myts/decompose$trend\ndecompose_adjtrend_plot <- autoplot(trendadj,ylab='trend') +ggtitle('Adjusted trend component in the multiplicative time series model')\nseasonaladj <- myts/decompose$seasonal\ndecompose_adjseasonal_plot <- autoplot(seasonaladj,ylab='seasonal') +ggtitle('Adjusted seasonal component in the multiplicative time series model')\ngrid.arrange(orginial_plot, decompose_adjtrend_plot,decompose_adjseasonal_plot, nrow=3)\n\n\n\n\n\n\n\n\nThe adjusted seasonal component tend to have upward trend till 2019 and drops during the covid period and there is more variability in the model when compared to the original plot where the variation during the years but the adjusted trend then to have more fluctuation showing no trend when compared to the original plot.\n\n\nLag Plots\n\nDaily Time LagsMonthly Time Lags\n\n\n\n\nCode\n#Lag plots \ngglagplot(myts, do.lines=FALSE, lags=1)+xlab(\"Lag 1\")+ylab(\"Yi\")+ggtitle(\"Lag Plot for Real Estate Sector Fund Stock JAN 2016 - March 2023\")\n\n\n\n\n\n\n\n\n\nCode\n#montly data\nmean_data <- df %>% \n  mutate(month = month(Date), year = year(Date)) %>% \n  group_by(year, month) %>% \n  summarize(mean_value = mean(XLRE.Adjusted))\nmonth<-ts(mean_data$mean_value,start = c(2016, 1),frequency = 12)\n#Lag plot\nts_lags(month)\n\n\n\n\n\n\n\n\n\nThe first lag plot shows the daily time lags of the Real Estate Sector Fund stock price from JAN 2016 to March 2023. The plot indicates that there is a strong positive correlation between the current value and the previous day’s value, as seen by the points clustering along the diagonal line. This suggests that the stock price has a positive autocorrelation at a lag of one day.\nThe second lag plot shows the monthly time lags of the mean value of the Real Estate Sector Fund stock price from JAN 2016 to March 2023. The plot indicates that there is a positive correlation between the current value and the value from the previous month. This suggests that the mean value of the stock price has a positive autocorrelation at a lag of one month.\nOverall, the lag plots indicate that there is a positive autocorrelation present in the Real Estate Sector Fund stock price data, with the strongest correlation observed in the daily time series.\n\n\nSeasonality\n\nSeasonal HeatmapSeasonal Line plot\n\n\n\n\nCode\n# Create seasonal plot\nts_heatmap(month, color = \"RdPu\", title = 'Seasonality Heatmap of Real Estate Sector Fund Stock Jan 2016 - March 2023')\n\n\n\n\n\n\n\n\n\n\nCode\n# Create a line graph for each year with months on the x-axis\nggseasonplot(month, datecol = \"date\", valuecol = \"value\")+ggtitle(\"Seasonal Yearly Plot for Real Estate Sector Fund Stock Jan 2016 - March 2023\")\n\n\n\n\n\n\n\n\nThe Seasonality Heatmap for the Real Estate Sector Fund Stock JAN 2016 - March 2023 does not reveal any clear seasonality in the data. The heatmap shows the mean value of the time series for each month and year combination, with the darker colors indicating higher values. The lack of clear patterns or darker colors in specific months or years suggests that there is no consistent seasonal pattern in the data. However, the yearly line graph shows a slight upward trend in the stock price from 2016 to 2023, but does not show any clear seasonality. Each year’s data is represented by a line, and the months are plotted on the x-axis. Overall, the lack of clear seasonality in both the heatmap and yearly line graph suggests that other factors beyond seasonality are driving the stock price fluctuations.\n\n\nMoving Average\n\n4 Month MA1 Year MA3 Year MA\n\n\n\n\nCode\n#SMA Smoothing \nma <- autoplot(month, series=\"Data\") +\n  autolayer(ma(month,5), series=\"4 Month MA\") +\n  xlab(\"Year\") + ylab(\"GWh\") +\n  ggtitle(\"Real Estate Sector Fund Stock JAN 2016 - March 2023(4 Month Moving Average)\") +\n  scale_colour_manual(values=c(\"Data\"=\"grey50\",\"4 Month MA\"=\"red\"),\n                      breaks=c(\"Data\",\"4 Month MA\"))\nma\n\n\n\n\n\n\n\n\n\nCode\n#SMA Smoothing \nma <- autoplot(month, series=\"Data\") +\n  autolayer(ma(month,13), series=\"1 Year MA\") +\n  xlab(\"Year\") + ylab(\"GWh\") +\n  ggtitle(\"Real Estate Sector Fund Stock JAN 2016 - March 2023(1 Year Moving Average)\") +\n  scale_colour_manual(values=c(\"Data\"=\"grey50\",\"1 Year MA\"=\"red\"),\n                      breaks=c(\"Data\",\"1 Year MA\"))\nma\n\n\n\n\n\n\n\n\n\nCode\n#SMA Smoothing \nma <- autoplot(month, series=\"Data\") +\n  autolayer(ma(month,37), series=\"3 Year MA\") +\n  xlab(\"Year\") + ylab(\"GWh\") +\n  ggtitle(\"Real Estate Sector Fund Stock JAN 2016 - March 2023(3 Year Moving Average)\") +\n  scale_colour_manual(values=c(\"Data\"=\"grey50\",\"3 Year MA\"=\"red\"),\n                      breaks=c(\"Data\",\"3 Year MA\"))\nma\n\n\n\n\n\n\n\n\nThe three plots show the Real Estate Sector Fund stock prices from JAN 2016 to March 2023, along with the moving averages for 4 months, 1 year and 3 years. As the window of the moving average increases, the smoother the trend line becomes, reducing the impact of noise and fluctuations in the original time series.\nThe 4-month moving average plot shows frequent fluctuations in the stock price, with the trend line following the general direction of the time series. The 1-year moving average plot shows a smoother trend, following the overall upward trend of the stock price.\nThe 1-year moving average plot shows a similar trend to the 4-month plot but is even smoother, with fewer fluctuations. Finally, the 3-year moving average plot shows the smoothest trend, with an almost constant upward slope.As the moving average window increases, the smoother trend allows for a clearer identification of the general trend of the Real Estate Sector Fund stock prices over time. From the moving average obtained above we can see that there is upward tend in the stock price of Real Estate Sector Fund.\n\n\nAutocorrelation Time Series\n\nACFPACFADF Test\n\n\n\n\nCode\n#ACF for  data\nggAcf(month)+ggtitle(\"ACF Plot for Real Estate Sector Fund Stock JAN 2016 - March 2023\")\n\n\n\n\n\n\n\n\n\nCode\n#PACF for data\nggPacf(month)+ggtitle(\"PACF Plot for Real Estate Sector Fund Stock JAN 2016 - March 2023\")\n\n\n\n\n\n\n\n\n\nCode\n#check the stationarity\ntseries::adf.test(month)\n\n\n\n    Augmented Dickey-Fuller Test\n\ndata:  month\nDickey-Fuller = -3.2921, Lag order = 4, p-value = 0.07825\nalternative hypothesis: stationary\n\n\n\n\n\nIn the plot of autocorrelation function, which is the acf graph for monthly data, there are clear autocorrelation in lag. The above lag plots and autocorrelation plot indicates seasonality in the series, which means the series is not stationary. This can be verified by the Augmented Dickey-Fuller Test which tells us that as the p value is greater than 0.05.\n\n\nDetrend and Differenced Time Series\n\nLinear Fitting ModelACF Plot\n\n\n\n\nCode\nfit = lm(myts~time(myts), na.action=NULL) \nsummary(fit) \n\n\n\nCall:\nlm(formula = myts ~ time(myts), na.action = NULL)\n\nResiduals:\n     Min       1Q   Median       3Q      Max \n-11.0891  -2.1390   0.0211   1.7440   9.6609 \n\nCoefficients:\n              Estimate Std. Error t value Pr(>|t|)    \n(Intercept) -5.810e+03  7.293e+01  -79.67   <2e-16 ***\ntime(myts)   2.893e+00  3.611e-02   80.10   <2e-16 ***\n---\nSignif. codes:  0 '***' 0.001 '**' 0.01 '*' 0.05 '.' 0.1 ' ' 1\n\nResidual standard error: 3.073 on 1765 degrees of freedom\nMultiple R-squared:  0.7843,    Adjusted R-squared:  0.7841 \nF-statistic:  6416 on 1 and 1765 DF,  p-value: < 2.2e-16\n\n\n\n\n\n\nCode\n# plot ACFs\nplot1 <- ggAcf(myts, 48, main=\"Original Data: Real Estate Sector Fund Stock Stock Price\")\nplot2 <- ggAcf(resid(fit), 48, main=\"Detrended data\")\nplot3 <- ggAcf(diff(myts), 48, main=\"First differenced data\")\ngrid.arrange(plot1, plot2, plot3, nrow=3)\n\n\n\n\n\n\n\n\nThe estimated slope coefficient β1, 0.40454 With a standard error of 0.06241, yielding a significant estimated increase of stock price is very less yearly. Equation of the fit for stationary process: \\[\\hat{y}_{t} = x_{t}+(786.31680)-(0.40454)t\\]\nFrom the above graph we can say that there is no change in detrended plot and the original data acf plot, it typically means that the data is stationary. But when the first order difference is applied the high correlation is removed but there is no seasonal correlation.\nAs depicted in the above figure, the series is now stationary and ready for future study."
  },
  {
    "objectID": "eda_sector_technology.html",
    "href": "eda_sector_technology.html",
    "title": "EDA for Technology Sector Fund",
    "section": "",
    "text": "The Technology Sector Fund (XLK) is a sector-specific exchange-traded fund that tracks the performance of companies in the technology sector, including hardware, software, and semiconductor companies. The fund’s top holdings include Apple, Microsoft, and Facebook. Over the past decade, XLK has shown significant growth, outperforming the broader market. The fund experienced a sharp decline during the COVID-19 pandemic in 2020, but quickly rebounded and has continued to see strong growth in the years since. As the demand for technology products and services continues to increase, XLK is expected to remain a strong performer in the coming years.\n\nTime Series Plot\n\n\nCode\n# get data\noptions(\"getSymbols.warning4.0\"=FALSE)\noptions(\"getSymbols.yahoo.warning\"=FALSE)\n\n\ndata = getSymbols(\"XLK\",src='yahoo', from = '2010-01-01',to = \"2023-03-01\")\n\ndf <- data.frame(Date=index(XLK),coredata(XLK))\n\n# create Bollinger Bands\nbbands <- BBands(XLK[,c(\"XLK.High\",\"XLK.Low\",\"XLK.Close\")])\n\n# join and subset data\ndf <- subset(cbind(df, data.frame(bbands[,1:3])), Date >= \"2010-01-01\")\n\n#export the data \nXLK_data <- df\nwrite.csv(XLK_data, \"DATA/CLEANED DATA/XLK_raw_data.csv\", row.names=FALSE)\n\n# colors column for increasing and decreasing\nfor (i in 1:length(df[,1])) {\n  if (df$XLK.Close[i] >= df$XLK.Open[i]) {\n      df$direction[i] = 'Increasing'\n  } else {\n      df$direction[i] = 'Decreasing'\n  }\n}\n\ni <- list(line = list(color = '#B0C4DE'))\nd <- list(line = list(color = '#7F7F7F'))\n\n# plot candlestick chart\n\nfig <- df %>% plot_ly(x = ~Date, type=\"candlestick\",\n          open = ~XLK.Open, close = ~XLK.Close,\n          high = ~XLK.High, low = ~XLK.Low, name = \"XLK\",\n          increasing = i, decreasing = d) \nfig <- fig %>% add_lines(x = ~Date, y = ~up , name = \"B Bands\",\n            line = list(color = '#ccc', width = 0.5),\n            legendgroup = \"Bollinger Bands\",\n            hoverinfo = \"none\", inherit = F) \nfig <- fig %>% add_lines(x = ~Date, y = ~dn, name = \"B Bands\",\n            line = list(color = '#ccc', width = 0.5),\n            legendgroup = \"Bollinger Bands\", inherit = F,\n            showlegend = FALSE, hoverinfo = \"none\") \nfig <- fig %>% add_lines(x = ~Date, y = ~mavg, name = \"Mv Avg\",\n            line = list(color = '#E377C2', width = 0.5),\n            hoverinfo = \"none\", inherit = F) \nfig <- fig %>% layout(yaxis = list(title = \"Price\"))\n\n# plot volume bar chart\nfig2 <- df \nfig2 <- fig2 %>% plot_ly(x=~Date, y=~XLK.Volume, type='bar', name = \"XLK Volume\",\n          color = ~direction, colors = c('#B0C4DE','#7F7F7F')) \nfig2 <- fig2 %>% layout(yaxis = list(title = \"Volume\"))\n\n# create rangeselector buttons\nrs <- list(visible = TRUE, x = 0.5, y = -0.055,\n           xanchor = 'center', yref = 'paper',\n           font = list(size = 9),\n           buttons = list(\n             list(count=1,\n                  label='RESET',\n                  step='all'),\n             list(count=3,\n                  label='3 YR',\n                  step='year',\n                  stepmode='backward'),\n             list(count=1,\n                  label='1 YR',\n                  step='year',\n                  stepmode='backward'),\n             list(count=1,\n                  label='1 MO',\n                  step='month',\n                  stepmode='backward')\n           ))\n\n# subplot with shared x axis\nfig <- subplot(fig, fig2, heights = c(0.7,0.2), nrows=2,\n             shareX = TRUE, titleY = TRUE)\nfig <- fig %>% layout(title = paste(\"Technology Sector Fund Stock Price: JAN 2010 - March 2023\"),\n         xaxis = list(rangeselector = rs),\n         legend = list(orientation = 'h', x = 0.5, y = 1,\n                       xanchor = 'center', yref = 'paper',\n                       font = list(size = 10),\n                       bgcolor = 'transparent'))\n\nfig\n\n\n\n\n\n\nThe Technology Sector Fund (XLK) has experienced significant growth and fluctuation since 2010. In the early 2010s, XLK saw steady growth due to the increasing demand for technology products and services, as well as the growing importance of technology in various industries. However, the fund experienced a significant dip during the 2015-2016 period, largely due to concerns about the slowdown in the Chinese economy and a decline in smartphone sales.\nDespite this setback, XLK quickly rebounded and began a period of rapid growth, fueled by the increasing adoption of cloud computing, artificial intelligence, and other emerging technologies. This growth continued through the late 2010s and into the 2020s, with the COVID-19 pandemic driving even greater demand for technology as remote work and digital communication became the norm.\nOverall, the Technology Sector Fund has been one of the top-performing sector funds over the past decade, reflecting the increasing importance of technology in our daily lives and the economy as a whole. However, like any investment, there are always risks and fluctuations, and it is important for investors to carefully evaluate their goals and risk tolerance before making any investment decisions.\nFor stock prices, a multiplicative decomposition is typically preferred because the percentage changes in stock prices tend to be more important than the absolute changes. Additionally, stock prices tend to exhibit non-constant variance, meaning that the variance of the series changes over time. A multiplicative decomposition can handle this non-constant variance more effectively than an additive decomposition.\n\n\nDecomposed Time Series\n\nDecomposition PlotAdjusted Decomposition Plot\n\n\n\n\nCode\n#time series data\nmyts<-ts(df$XLK.Adjusted,frequency=252,start=c(2010,01,01), end = c(2023,3,1)) \n#original plot for time series data\norginial_plot <- autoplot(myts,xlab =\"Year\", ylab = \"Adjusted Closing Price\", main = \"Technology Sector Fund Stock price: JAN 2010 - March 2023\")\n#decompose the data\ndecompose = decompose(myts, \"multiplicative\")\n#decomposition plot\nautoplot(decompose)\n\n\n\n\n\n\n\n\n\nCode\n#adjusted plot\ntrendadj <- myts/decompose$trend\ndecompose_adjtrend_plot <- autoplot(trendadj,ylab='trend') +ggtitle('Adjusted trend component in the multiplicative time series model')\nseasonaladj <- myts/decompose$seasonal\ndecompose_adjseasonal_plot <- autoplot(seasonaladj,ylab='seasonal') +ggtitle('Adjusted seasonal component in the multiplicative time series model')\ngrid.arrange(orginial_plot, decompose_adjtrend_plot,decompose_adjseasonal_plot, nrow=3)\n\n\n\n\n\n\n\n\nThe adjusted seasonal component tend to have upward trend till 2019 and drops during the covid period and there is more variability in the model when compared to the original plot where the variation during the years but the adjusted trend then to have more fluctuation showing no trend when compared to the original plot.\n\n\nLag Plots\n\nDaily Time LagsMonthly Time Lags\n\n\n\n\nCode\n#Lag plots \ngglagplot(myts, do.lines=FALSE, lags=1)+xlab(\"Lag 1\")+ylab(\"Yi\")+ggtitle(\"Lag Plot for Technology Sector Fund Stock JAN 2010 - March 2023\")\n\n\n\n\n\n\n\n\n\nCode\n#montly data\nmean_data <- df %>% \n  mutate(month = month(Date), year = year(Date)) %>% \n  group_by(year, month) %>% \n  summarize(mean_value = mean(XLK.Adjusted))\nmonth<-ts(mean_data$mean_value,start = c(2010, 1),frequency = 12)\n#Lag plot\nts_lags(month)\n\n\n\n\n\n\n\n\n\nThe first lag plot shows the daily time lags of the Technology Sector Fund stock price from JAN 2010 to March 2023. The plot indicates that there is a strong positive correlation between the current value and the previous day’s value, as seen by the points clustering along the diagonal line. This suggests that the stock price has a positive autocorrelation at a lag of one day.\nThe second lag plot shows the monthly time lags of the mean value of the Technology Sector Fund stock price from JAN 2010 to March 2023. The plot indicates that there is a positive correlation between the current value and the value from the previous month. This suggests that the mean value of the stock price has a positive autocorrelation at a lag of one month.\nOverall, the lag plots indicate that there is a positive autocorrelation present in the Technology Sector Fund stock price data, with the strongest correlation observed in the daily time series.\n\n\nSeasonality\n\nSeasonal HeatmapSeasonal Line plot\n\n\n\n\nCode\n# Create seasonal plot\nts_heatmap(month, color = \"YlOrBr\", title = 'Seasonality Heatmap of Technology Sector Fund Stock Jan 2010 - March 2023')\n\n\n\n\n\n\n\n\n\n\nCode\n# Create a line graph for each year with months on the x-axis\nggseasonplot(month, datecol = \"date\", valuecol = \"value\")+ggtitle(\"Seasonal Yearly Plot for Technology Sector Fund Stock Jan 2010 - March 2023\")\n\n\n\n\n\n\n\n\nThe Seasonality Heatmap for the Technology Sector Fund Stock JAN 2010 - March 2023 does not reveal any clear seasonality in the data. The heatmap shows the mean value of the time series for each month and year combination, with the darker colors indicating higher values. The lack of clear patterns or darker colors in specific months or years suggests that there is no consistent seasonal pattern in the data. However, the yearly line graph shows a slight upward trend in the stock price from 2010 to 2023, but does not show any clear seasonality. Each year’s data is represented by a line, and the months are plotted on the x-axis. Overall, the lack of clear seasonality in both the heatmap and yearly line graph suggests that other factors beyond seasonality are driving the stock price fluctuations.\n\n\nMoving Average\n\n4 Month MA1 Year MA3 Year MA5 Year MA\n\n\n\n\nCode\n#SMA Smoothing \nma <- autoplot(month, series=\"Data\") +\n  autolayer(ma(month,5), series=\"4 Month MA\") +\n  xlab(\"Year\") + ylab(\"GWh\") +\n  ggtitle(\"Technology Sector Fund Stock JAN 2010 - March 2023(4 Month Moving Average)\") +\n  scale_colour_manual(values=c(\"Data\"=\"grey50\",\"4 Month MA\"=\"red\"),\n                      breaks=c(\"Data\",\"4 Month MA\"))\nma\n\n\n\n\n\n\n\n\n\nCode\n#SMA Smoothing \nma <- autoplot(month, series=\"Data\") +\n  autolayer(ma(month,13), series=\"1 Year MA\") +\n  xlab(\"Year\") + ylab(\"GWh\") +\n  ggtitle(\"Technology Sector Fund Stock JAN 2010 - March 2023(1 Year Moving Average)\") +\n  scale_colour_manual(values=c(\"Data\"=\"grey50\",\"1 Year MA\"=\"red\"),\n                      breaks=c(\"Data\",\"1 Year MA\"))\nma\n\n\n\n\n\n\n\n\n\nCode\n#SMA Smoothing \nma <- autoplot(month, series=\"Data\") +\n  autolayer(ma(month,37), series=\"3 Year MA\") +\n  xlab(\"Year\") + ylab(\"GWh\") +\n  ggtitle(\"Technology Sector Fund Stock JAN 2010 - March 2023(3 Year Moving Average)\") +\n  scale_colour_manual(values=c(\"Data\"=\"grey50\",\"3 Year MA\"=\"red\"),\n                      breaks=c(\"Data\",\"3 Year MA\"))\nma\n\n\n\n\n\n\n\n\n\nCode\n#SMA Smoothing \nma <- autoplot(month, series=\"Data\") +\n  autolayer(ma(month,61), series=\"5 Year MA\") +\n  xlab(\"Year\") + ylab(\"GWh\") +\n  ggtitle(\"Technology Sector Fund Stock JAN 2010 - March 2023(5 Year Moving Average)\") +\n  scale_colour_manual(values=c(\"Data\"=\"grey50\",\"5 Year MA\"=\"red\"),\n                      breaks=c(\"Data\",\"5 Year MA\"))\nma\n\n\n\n\n\n\n\n\nThe four plots show the Technology Sector Fund stock prices from JAN 2010 to March 2023, along with the moving averages for 4 months, 1 year 3 years and 4 years. As the window of the moving average increases, the smoother the trend line becomes, reducing the impact of noise and fluctuations in the original time series.\nThe 4-month moving average plot shows frequent fluctuations in the stock price, with the trend line following the general direction of the time series. The 1-year moving average plot shows a smoother trend, following the overall upward trend of the stock price.\nThe 1-year moving average plot shows a similar trend to the 4-month plot but is even smoother, with fewer fluctuations. Finally, the 5-year moving average plot shows the smoothest trend, with an almost constant upward slope.As the moving average window increases, the smoother trend allows for a clearer identification of the general trend of the Technology Sector Fund stock prices over time. From the moving average obtained above we can see that there is upward tend in the stock price of Technology Sector Fund.\n\n\nAutocorrelation Time Series\n\nACFPACFADF Test\n\n\n\n\nCode\n#ACF for  data\nggAcf(month)+ggtitle(\"ACF Plot for Technology Sector Fund Stock JAN 2010 - March 2023\")\n\n\n\n\n\n\n\n\n\nCode\n#PACF for data\nggPacf(month)+ggtitle(\"PACF Plot for Technology Sector Fund Stock JAN 2010 - March 2023\")\n\n\n\n\n\n\n\n\n\nCode\n#check the stationarity\ntseries::adf.test(month)\n\n\n\n    Augmented Dickey-Fuller Test\n\ndata:  month\nDickey-Fuller = -1.7941, Lag order = 5, p-value = 0.6622\nalternative hypothesis: stationary\n\n\n\n\n\nIn the plot of autocorrelation function, which is the acf graph for monthly data, there are clear autocorrelation in lag. The above lag plots and autocorrelation plot indicates seasonality in the series, which means the series is not stationary. This can be verified by the Augmented Dickey-Fuller Test which tells us that as the p value is greater than 0.05.\n\n\nDetrend and Differenced Time Series\n\nLinear Fitting ModelACF Plot\n\n\n\n\nCode\nfit = lm(myts~time(myts), na.action=NULL) \nsummary(fit) \n\n\n\nCall:\nlm(formula = myts ~ time(myts), na.action = NULL)\n\nResiduals:\n    Min      1Q  Median      3Q     Max \n-30.615 -14.971  -3.768  12.099  57.390 \n\nCoefficients:\n              Estimate Std. Error t value Pr(>|t|)    \n(Intercept) -2.055e+04  1.623e+02  -126.7   <2e-16 ***\ntime(myts)   1.022e+01  8.046e-02   127.0   <2e-16 ***\n---\nSignif. codes:  0 '***' 0.001 '**' 0.01 '*' 0.05 '.' 0.1 ' ' 1\n\nResidual standard error: 17.31 on 3277 degrees of freedom\nMultiple R-squared:  0.8312,    Adjusted R-squared:  0.8311 \nF-statistic: 1.613e+04 on 1 and 3277 DF,  p-value: < 2.2e-16\n\n\n\n\n\n\nCode\n# plot ACFs\nplot1 <- ggAcf(myts, 48, main=\"Original Data: Technology Sector Fund Stock Stock Price\")\nplot2 <- ggAcf(resid(fit), 48, main=\"Detrended data\")\nplot3 <- ggAcf(diff(myts), 48, main=\"First differenced data\")\ngrid.arrange(plot1, plot2, plot3, nrow=3)\n\n\n\n\n\n\n\n\nThe estimated slope coefficient β1, 1.8675 With a standard error of 0.1494, yielding a significant estimated increase of stock price is very less yearly. Equation of the fit for stationary process: \\[\\hat{y}_{t} = x_{t}+(3714.9265)-(1.8675)t\\]\nFrom the above graph we can say that there is no change in detrended plot and the original data acf plot, it typically means that the data is stationary. But when the first order difference is applied the high correlation is removed but there is no seasonal correlation.\nAs depicted in the above figure, the series is now stationary and ready for future study."
  },
  {
    "objectID": "eda_sector_utilites.html",
    "href": "eda_sector_utilites.html",
    "title": "EDA for Utilities Sector Fund",
    "section": "",
    "text": "The Utilities Sector Fund (XLU) is an exchange-traded fund (ETF) that provides investors with exposure to the consumer staples sector of the US economy. The fund holds a diverse range of companies, including those involved in food and beverage production, personal and household products, and tobacco. XLU is a popular choice for investors looking for stability and consistent dividends, as the companies within the sector tend to have steady earnings and demand regardless of economic conditions. Overall, XLU can be a valuable addition to a well-diversified portfolio, providing exposure to a resilient sector of the US economy.\n\nTime Series Plot\n\n\nCode\n# get data\noptions(\"getSymbols.warning4.0\"=FALSE)\noptions(\"getSymbols.yahoo.warning\"=FALSE)\n\n\ndata = getSymbols(\"XLU\",src='yahoo', from = '2010-01-01',to = \"2023-03-01\")\n\ndf <- data.frame(Date=index(XLU),coredata(XLU))\n\n# create Bollinger Bands\nbbands <- BBands(XLU[,c(\"XLU.High\",\"XLU.Low\",\"XLU.Close\")])\n\n# join and subset data\ndf <- subset(cbind(df, data.frame(bbands[,1:3])), Date >= \"2010-01-01\")\n\n#export the data \nXLU_data <- df\nwrite.csv(XLU_data, \"DATA/CLEANED DATA/XLU_raw_data.csv\", row.names=FALSE)\n\n# colors column for increasing and decreasing\nfor (i in 1:length(df[,1])) {\n  if (df$XLU.Close[i] >= df$XLU.Open[i]) {\n      df$direction[i] = 'Increasing'\n  } else {\n      df$direction[i] = 'Decreasing'\n  }\n}\n\ni <- list(line = list(color = '#FF4040'))\nd <- list(line = list(color = '#7F7F7F'))\n\n# plot candlestick chart\n\nfig <- df %>% plot_ly(x = ~Date, type=\"candlestick\",\n          open = ~XLU.Open, close = ~XLU.Close,\n          high = ~XLU.High, low = ~XLU.Low, name = \"XLU\",\n          increasing = i, decreasing = d) \nfig <- fig %>% add_lines(x = ~Date, y = ~up , name = \"B Bands\",\n            line = list(color = '#ccc', width = 0.5),\n            legendgroup = \"Bollinger Bands\",\n            hoverinfo = \"none\", inherit = F) \nfig <- fig %>% add_lines(x = ~Date, y = ~dn, name = \"B Bands\",\n            line = list(color = '#ccc', width = 0.5),\n            legendgroup = \"Bollinger Bands\", inherit = F,\n            showlegend = FALSE, hoverinfo = \"none\") \nfig <- fig %>% add_lines(x = ~Date, y = ~mavg, name = \"Mv Avg\",\n            line = list(color = '#E377C2', width = 0.5),\n            hoverinfo = \"none\", inherit = F) \nfig <- fig %>% layout(yaxis = list(title = \"Price\"))\n\n# plot volume bar chart\nfig2 <- df \nfig2 <- fig2 %>% plot_ly(x=~Date, y=~XLU.Volume, type='bar', name = \"XLU Volume\",\n          color = ~direction, colors = c('#FF4040','#7F7F7F')) \nfig2 <- fig2 %>% layout(yaxis = list(title = \"Volume\"))\n\n# create rangeselector buttons\nrs <- list(visible = TRUE, x = 0.5, y = -0.055,\n           xanchor = 'center', yref = 'paper',\n           font = list(size = 9),\n           buttons = list(\n             list(count=1,\n                  label='RESET',\n                  step='all'),\n             list(count=3,\n                  label='3 YR',\n                  step='year',\n                  stepmode='backward'),\n             list(count=1,\n                  label='1 YR',\n                  step='year',\n                  stepmode='backward'),\n             list(count=1,\n                  label='1 MO',\n                  step='month',\n                  stepmode='backward')\n           ))\n\n# subplot with shared x axis\nfig <- subplot(fig, fig2, heights = c(0.7,0.2), nrows=2,\n             shareX = TRUE, titleY = TRUE)\nfig <- fig %>% layout(title = paste(\"Utilities Sector Fund Stock Price: JAN 2010 - March 2023\"),\n         xaxis = list(rangeselector = rs),\n         legend = list(orientation = 'h', x = 0.5, y = 1,\n                       xanchor = 'center', yref = 'paper',\n                       font = list(size = 10),\n                       bgcolor = 'transparent'))\n\nfig\n\n\n\n\n\n\nSince 2010, the Utilities Sector Fund (XLU) has shown a generally upward trend, with some fluctuations along the way. In the aftermath of the 2008 financial crisis, the fund saw a significant drop in value, hitting a low point in early 2009. However, it has since recovered and continued to climb steadily.\nThe XLU saw a period of rapid growth in 2019, reaching its all-time high in November of that year, before experiencing a sharp decline in early 2020 due to the COVID-19 pandemic. However, like many other funds, it rebounded quickly and has been on an upward trajectory since then. Overall, the XLU has proven to be a relatively stable investment option, with a focus on companies in the utilities sector that typically provide essential services and products, such as electricity, water, and gas.\nFor stock prices, a multiplicative decomposition is typically preferred because the percentage changes in stock prices tend to be more important than the absolute changes. Additionally, stock prices tend to exhibit non-constant variance, meaning that the variance of the series changes over time. A multiplicative decomposition can handle this non-constant variance more effectively than an additive decomposition.\n\n\nDecomposed Time Series\n\nDecomposition PlotAdjusted Decomposition Plot\n\n\n\n\nCode\n#time series data\nmyts<-ts(df$XLU.Adjusted,frequency=252,start=c(2010,01,01), end = c(2023,3,1)) \n#original plot for time series data\norginial_plot <- autoplot(myts,xlab =\"Year\", ylab = \"Adjusted Closing Price\", main = \"Utilities Sector Fund Stock price: JAN 2010 - March 2023\")\n#decompose the data\ndecompose = decompose(myts, \"multiplicative\")\n#decomposition plot\nautoplot(decompose)\n\n\n\n\n\n\n\n\n\nCode\n#adjusted plot\ntrendadj <- myts/decompose$trend\ndecompose_adjtrend_plot <- autoplot(trendadj,ylab='trend') +ggtitle('Adjusted trend component in the multiplicative time series model')\nseasonaladj <- myts/decompose$seasonal\ndecompose_adjseasonal_plot <- autoplot(seasonaladj,ylab='seasonal') +ggtitle('Adjusted seasonal component in the multiplicative time series model')\ngrid.arrange(orginial_plot, decompose_adjtrend_plot,decompose_adjseasonal_plot, nrow=3)\n\n\n\n\n\n\n\n\nThe adjusted seasonal component tend to have upward trend till 2019 and drops during the covid period and there is more variability in the model when compared to the original plot where the variation during the years but the adjusted trend then to have more fluctuation showing no trend when compared to the original plot.\n\n\nLag Plots\n\nDaily Time LagsMonthly Time Lags\n\n\n\n\nCode\n#Lag plots \ngglagplot(myts, do.lines=FALSE, lags=1)+xlab(\"Lag 1\")+ylab(\"Yi\")+ggtitle(\"Lag Plot for Utilities Sector Fund Stock JAN 2010 - March 2023\")\n\n\n\n\n\n\n\n\n\nCode\n#montly data\nmean_data <- df %>% \n  mutate(month = month(Date), year = year(Date)) %>% \n  group_by(year, month) %>% \n  summarize(mean_value = mean(XLU.Adjusted))\nmonth<-ts(mean_data$mean_value,start = c(2010, 1),frequency = 12)\n#Lag plot\nts_lags(month)\n\n\n\n\n\n\n\n\n\nThe first lag plot shows the daily time lags of the Utilities Sector Fund stock price from JAN 2010 to March 2023. The plot indicates that there is a strong positive correlation between the current value and the previous day’s value, as seen by the points clustering along the diagonal line. This suggests that the stock price has a positive autocorrelation at a lag of one day.\nThe second lag plot shows the monthly time lags of the mean value of the Utilities Sector Fund stock price from JAN 2010 to March 2023. The plot indicates that there is a positive correlation between the current value and the value from the previous month. This suggests that the mean value of the stock price has a positive autocorrelation at a lag of one month.\nOverall, the lag plots indicate that there is a positive autocorrelation present in the Utilities Sector Fund stock price data, with the strongest correlation observed in the daily time series.\n\n\nSeasonality\n\nSeasonal HeatmapSeasonal Line plot\n\n\n\n\nCode\n# Create seasonal plot\nts_heatmap(month, color = \"BuPu\", title = 'Seasonality Heatmap of Utilities Sector Fund Stock Jan 2010 - March 2023')\n\n\n\n\n\n\n\n\n\n\nCode\n# Create a line graph for each year with months on the x-axis\nggseasonplot(month, datecol = \"date\", valuecol = \"value\")+ggtitle(\"Seasonal Yearly Plot for Utilities Sector Fund Stock Jan 2010 - March 2023\")\n\n\n\n\n\n\n\n\nThe Seasonality Heatmap for the Utilities Sector Fund Stock JAN 2010 - March 2023 does not reveal any clear seasonality in the data. The heatmap shows the mean value of the time series for each month and year combination, with the darker colors indicating higher values. The lack of clear patterns or darker colors in specific months or years suggests that there is no consistent seasonal pattern in the data. However, the yearly line graph shows a slight upward trend in the stock price from 2010 to 2023, but does not show any clear seasonality. Each year’s data is represented by a line, and the months are plotted on the x-axis. Overall, the lack of clear seasonality in both the heatmap and yearly line graph suggests that other factors beyond seasonality are driving the stock price fluctuations.\n\n\nMoving Average\n\n4 Month MA1 Year MA3 Year MA5 Year MA\n\n\n\n\nCode\n#SMA Smoothing \nma <- autoplot(month, series=\"Data\") +\n  autolayer(ma(month,5), series=\"4 Month MA\") +\n  xlab(\"Year\") + ylab(\"GWh\") +\n  ggtitle(\"Utilities Sector Fund Stock JAN 2010 - March 2023(4 Month Moving Average)\") +\n  scale_colour_manual(values=c(\"Data\"=\"grey50\",\"4 Month MA\"=\"red\"),\n                      breaks=c(\"Data\",\"4 Month MA\"))\nma\n\n\n\n\n\n\n\n\n\nCode\n#SMA Smoothing \nma <- autoplot(month, series=\"Data\") +\n  autolayer(ma(month,13), series=\"1 Year MA\") +\n  xlab(\"Year\") + ylab(\"GWh\") +\n  ggtitle(\"Utilities Sector Fund Stock JAN 2010 - March 2023(1 Year Moving Average)\") +\n  scale_colour_manual(values=c(\"Data\"=\"grey50\",\"1 Year MA\"=\"red\"),\n                      breaks=c(\"Data\",\"1 Year MA\"))\nma\n\n\n\n\n\n\n\n\n\nCode\n#SMA Smoothing \nma <- autoplot(month, series=\"Data\") +\n  autolayer(ma(month,37), series=\"3 Year MA\") +\n  xlab(\"Year\") + ylab(\"GWh\") +\n  ggtitle(\"Utilities Sector Fund Stock JAN 2010 - March 2023(3 Year Moving Average)\") +\n  scale_colour_manual(values=c(\"Data\"=\"grey50\",\"3 Year MA\"=\"red\"),\n                      breaks=c(\"Data\",\"3 Year MA\"))\nma\n\n\n\n\n\n\n\n\n\nCode\n#SMA Smoothing \nma <- autoplot(month, series=\"Data\") +\n  autolayer(ma(month,61), series=\"5 Year MA\") +\n  xlab(\"Year\") + ylab(\"GWh\") +\n  ggtitle(\"Utilities Sector Fund Stock JAN 2010 - March 2023(5 Year Moving Average)\") +\n  scale_colour_manual(values=c(\"Data\"=\"grey50\",\"5 Year MA\"=\"red\"),\n                      breaks=c(\"Data\",\"5 Year MA\"))\nma\n\n\n\n\n\n\n\n\nThe four plots show the Utilities Sector Fund stock prices from JAN 2010 to March 2023, along with the moving averages for 4 months, 1 year 3 years and 4 years. As the window of the moving average increases, the smoother the trend line becomes, reducing the impact of noise and fluctuations in the original time series.\nThe 4-month moving average plot shows frequent fluctuations in the stock price, with the trend line following the general direction of the time series. The 1-year moving average plot shows a smoother trend, following the overall upward trend of the stock price.\nThe 1-year moving average plot shows a similar trend to the 4-month plot but is even smoother, with fewer fluctuations. Finally, the 5-year moving average plot shows the smoothest trend, with an almost constant upward slope.As the moving average window increases, the smoother trend allows for a clearer identification of the general trend of the Utilities Sector Fund stock prices over time. From the moving average obtained above we can see that there is upward tend in the stock price of Utilities Sector Fund.\n\n\nAutocorrelation Time Series\n\nACFPACFADF Test\n\n\n\n\nCode\n#ACF for  data\nggAcf(month)+ggtitle(\"ACF Plot for Utilities Sector Fund Stock JAN 2010 - March 2023\")\n\n\n\n\n\n\n\n\n\nCode\n#PACF for data\nggPacf(month)+ggtitle(\"PACF Plot for Utilities Sector Fund Stock JAN 2010 - March 2023\")\n\n\n\n\n\n\n\n\n\nCode\n#check the stationarity\ntseries::adf.test(month)\n\n\n\n    Augmented Dickey-Fuller Test\n\ndata:  month\nDickey-Fuller = -3.2079, Lag order = 5, p-value = 0.08909\nalternative hypothesis: stationary\n\n\n\n\n\nIn the plot of autocorrelation function, which is the acf graph for monthly data, there are clear autocorrelation in lag. The above lag plots and autocorrelation plot indicates seasonality in the series, which means the series is not stationary. It was also verified using Augmented Dickey-Fuller Test which tells us that as the p value is greater than 0.05, the series is not stationary.\n\n\nDetrend and Differenced Time Series\n\nLinear Fitting ModelACF Plot\n\n\n\n\nCode\nfit = lm(myts~time(myts), na.action=NULL) \nsummary(fit) \n\n\n\nCall:\nlm(formula = myts ~ time(myts), na.action = NULL)\n\nResiduals:\n     Min       1Q   Median       3Q      Max \n-14.2296  -1.8450  -0.0559   1.7998  10.5812 \n\nCoefficients:\n              Estimate Std. Error t value Pr(>|t|)    \n(Intercept) -7.912e+03  2.688e+01  -294.3   <2e-16 ***\ntime(myts)   3.943e+00  1.333e-02   295.8   <2e-16 ***\n---\nSignif. codes:  0 '***' 0.001 '**' 0.01 '*' 0.05 '.' 0.1 ' ' 1\n\nResidual standard error: 2.867 on 3277 degrees of freedom\nMultiple R-squared:  0.9639,    Adjusted R-squared:  0.9639 \nF-statistic: 8.75e+04 on 1 and 3277 DF,  p-value: < 2.2e-16\n\n\n\n\n\n\nCode\n# plot ACFs\nplot1 <- ggAcf(myts, 48, main=\"Original Data: Utilities Sector Fund Stock Stock Price\")\nplot2 <- ggAcf(resid(fit), 48, main=\"Detrended data\")\nplot3 <- ggAcf(diff(myts), 48, main=\"First differenced data\")\ngrid.arrange(plot1, plot2, plot3, nrow=3)\n\n\n\n\n\n\n\n\nThe estimated slope coefficient β1, 6.854e-01 With a standard error of 5.695e-02, yielding a significant estimated increase of stock price is very less yearly. Equation of the fit for stationary process: \\[\\hat{y}_{t} = x_{t}+(1.346e+03)-(6.854e-01)t\\]\nFrom the above graph we can say that there is no change in detrended plot and the original data acf plot, it typically means that the data is stationary. But when the first order difference is applied the high correlation is removed but there is no seasonal correlation.\nAs depicted in the above figure, the series is now stationary and ready for future study."
  },
  {
    "objectID": "eda_stock_index.html",
    "href": "eda_stock_index.html",
    "title": "EDA for US Stock Index",
    "section": "",
    "text": "Exploratory Data Analysis (EDA) is an essential process in understanding the underlying patterns and characteristics of financial data. EDA for the Dow Jones, NASDAQ, and S&P 500 stock market indices typically involves analyzing time series data. Time series data refers to observations of a variable over time, such as the daily closing prices of a stock market index.\nOne of the key aspects of EDA for time series data is identifying the presence of autocorrelation. Autocorrelation refers to the correlation between a variable and its past values. The autocorrelation function (ACF) and partial autocorrelation function (PACF) plots can help identify the degree of correlation between the variable and its past values. This information can be useful for forecasting future values and identifying potential patterns in the data.\nAnother important aspect of EDA for time series data is identifying the presence of seasonality. Seasonality refers to a pattern that repeats itself in the data over regular intervals, such as daily, weekly, or monthly. Identifying seasonality is important as it can help us identify potential patterns and trends in the data, and it can inform our modeling approach.\nAdditionally, examining moving averages and detrending can help identify the underlying trend in the data. Moving averages are used to smooth out short-term fluctuations in the data, and detrending can help identify the underlying trend that is not related to the seasonal or cyclical fluctuations in the data.\nFinally, testing for stationarity is important as it allows us to apply statistical models that assume the data to be stationary. Stationarity refers to the property of a time series where its statistical properties, such as mean and variance, remain constant over time. If the data is non-stationary, it can be transformed to become stationary through techniques such as differencing or taking logarithms.\nThis information can be used to inform our modeling approach, identify potential patterns and trends, and improve our investment strategies.\nClick to view EDA Page for Dow Jones Index\nClick to view EDA Page for NADSAQ Composite Index\nClick to view EDA Page for S&P 500 Index"
  },
  {
    "objectID": "eda.html",
    "href": "eda.html",
    "title": "",
    "section": "",
    "text": "EDA is a critical tool for analyzing stock indices, sector markets, and macroeconomic factors. By examining historical data and using various EDA techniques, investors and financial professionals can gain insights into market dynamics, identify patterns and trends, and make informed investment decisions. EDA enables a deeper understanding of the historical performance of financial markets, providing valuable information for risk assessment, investment strategies, and decision-making processes.\n\n\nEDA for US Stock Indices\nExploratory Data Analysis (EDA) plays a significant role in understanding stock indices, which are measures of the overall performance of a group of stocks representing a particular market or segment. EDA involves analyzing historical price and volume data of stock indices to identify patterns, trends, and anomalies. By examining historical data, investors can gain insights into the historical performance of stock indices, such as identifying patterns of market cycles, market trends, and potential investment opportunities. EDA techniques, such as data visualization, statistical analysis, and time series analysis, can provide valuable information to assess the risk and return characteristics of stock indices and help investors make informed investment decisions.\nClick to view EDA Page for US Stock Indices\n\n\nEDA for Sector Market\nEDA is also crucial in analyzing sector markets, which represent specific industries or sectors of the economy, such as technology, healthcare, or finance. By conducting EDA on sector market data, investors can gain insights into the performance of different sectors, identify trends and patterns, and assess the relative strength or weakness of specific sectors. This information can be valuable in making sector-specific investment decisions or sector rotation strategies. EDA techniques, such as sector performance analysis, correlation analysis, and cluster analysis, can help investors identify potential opportunities and risks associated with different sectors of the market.\nClick to view EDA Page for Sector Market\n\n\nEDA for Macroeconomic Factors\nEDA is also widely used in analyzing macroeconomic factors, such as GDP, inflation, employment, and interest rates, which can impact the overall performance of stock indices and sector markets. By examining historical macroeconomic data, investors and economists can identify trends, patterns, and relationships that may affect the performance of financial markets. EDA techniques, such as time series analysis, regression analysis, and data visualization, can provide insights into the relationships between macroeconomic factors and stock market performance. This information can be used to make informed investment decisions, assess economic trends, and develop macroeconomic forecasts.\nClick to view EDA Page for Macroeconomic Factors"
  },
  {
    "objectID": "ftsm.html",
    "href": "ftsm.html",
    "title": "",
    "section": "",
    "text": "Financial Time Series Models\nFinancial time series models are statistical models that are used to analyze financial data, including stock prices, economic indicators, and other financial metrics. These models can help investors and analysts to understand trends and patterns in financial data, and to make predictions about future performance.\nWe will be looking at each of the top company for each sector with macroeconomic indicators such as GDP growth rate, inflation, interest rates, and unemployment. For example, let’s consider the technology sector and the top company in that sector is Apple. We can use an ARIMAX model to analyze the relationship between each company’s stock price and macroeconomic indicators such as GDP growth rate, inflation, interest rates, and unemployment.\n\nTo make the model more robust, we can also use an ARCH/GARCH model to capture the volatility in the financial time series. ARCH (Autoregressive Conditional Heteroskedasticity) and GARCH (Generalized Autoregressive Conditional Heteroskedasticity) models are commonly used to model the conditional variance of a time series, which can help to identify patterns of volatility and predict future fluctuations in the stock price.\nThe ARIMAX+ARCH/GARCH model can help investors and analysts to understand the relationship between each company’s stock price and macroeconomic indicators, and to make more informed investment decisions based on historical patterns and predicted future performance.\nClick to view Consumer Staples Sector Fund\nClick to view Utilities Sector Fund\nClick to view Health Care Sector Fund\nClick to view Industrial Sector Fund\nClick to view Financial Sector Fund\nClick to view Consumer Discretionary Sector Fund\nClick to view Communication Services Sector Fund\nClick to view Real Estate Sector Fund\nClick to view Materials Sector Fund\nClick to view Technology Sector Fund\nClick to view Energy Sector Fund"
  },
  {
    "objectID": "introduction.html",
    "href": "introduction.html",
    "title": "",
    "section": "",
    "text": "The stock market and sector market analysis are critical components of the global economy. The stock market is a marketplace where publicly traded companies’ shares are bought and sold, while sector market analysis is the process of analyzing specific sectors within the stock market, such as technology, energy, healthcare, etc. Understanding the performance of these sectors and individual companies is essential for investors and analysts alike.\nOne critical factor that impacts the performance of the stock market and sector market analysis is macroeconomic factors. These factors include interest rates, inflation, GDP growth, geopolitical events, and government policies, among others. Changes in these macroeconomic factors can have a significant impact on the stock market’s overall performance and individual sectors.\n\nFor example, rising interest rates may lead to higher borrowing costs for companies, reducing their profitability and impacting their stock prices. Similarly, geopolitical events such as wars, conflicts, or trade disputes may impact the stock market by creating uncertainty and volatility.\nTo analyze the stock market and sector market, analysts use various techniques such as technical analysis, fundamental analysis, and quantitative analysis. Technical analysis uses charts and other visual representations to identify trends and patterns in stock prices, while fundamental analysis looks at a company’s financial statements to determine its intrinsic value. Quantitative analysis involves using statistical models to analyze data and identify trends.\nOverall, understanding the impact of macroeconomic factors on the stock market and sector market analysis is crucial for investors and analysts alike. By analyzing data and identifying patterns, analysts can gain insights into the underlying drivers of market movements and make informed investment decisions.\nQUESTIONS:\n\nHow do different macroeconomic factors such as inflation, interest rates, and GDP growth impact the performance of specific sectors within the stock market?\nWhat role do historical trends in macroeconomic indicators play in predicting stock market performance, and how can this information be used to guide investment strategies?\nWhat are some practical methods for incorporating macroeconomic factors into fundamental and technical analyses of individual stocks or sectors?\nHow can macroeconomic indicators be used to identify potential risks or opportunities in the stock market, and what strategies can be employed to manage thXese risks or capitalize on these opportunities?\nHow can investors stay informed about key macroeconomic factors that are likely to impact the stock market, and what resources are available for tracking these factors over time?\nWhat are the potential benefits and limitations of relying on time series analysis to make predictions about stock market trends based on macroeconomic factors, and how can these limitations be addressed?\nHow do macroeconomic factors affect the stock market differently across different industries or sectors, and what strategies can be used to optimize investments based on these differences?\nHow do macroeconomic factors contribute to overall market volatility, and what steps can investors take to manage their exposure to market volatility based on these factors?\nWhat are some common mistakes or pitfalls that investors may encounter when trying to incorporate macroeconomic factors into their investment strategies, and how can these be avoided?\nHow can a comprehensive understanding of macroeconomic factors help investors build a diversified portfolio that is well-positioned to weather changes in the stock market over time?"
  },
  {
    "objectID": "arima_sector_consumer_staples.html",
    "href": "arima_sector_consumer_staples.html",
    "title": "ARIMA Model for Consumer Staples Sector Fund",
    "section": "",
    "text": "During exploratory data analysis (EDA), it was observed that the raw data is often non-stationary, exhibiting trends, seasonality, and other complex patterns that can make modeling and forecasting difficult. To address this issue, the data is often differentiated to achieve stationarity, which is a key assumption of the ARIMA model.\nDifferencing involves calculating the difference between each data point and the previous data point, which can help to remove trends and seasonality from the data. The goal is to create a new series of data that is stationary, meaning that the statistical properties of the data do not change over time.\n\nDifferentiated Time Series Plot\n\nPlotACFPACFADF Test\n\n\n\n\nCode\n#import the data\ndf <- read.csv(\"DATA/CLEANED DATA/XLP_raw_data.csv\")\n#convert to time series data\nmyts<-ts(df$XLP.Adjusted,frequency=252,start=c(2010,1,1), end = c(2023,3,1)) \n#First order differentiation\ndf1 <- diff(myts)\n# Plot \nmyts  %>% diff() %>% ggtsdisplay(main = \"First order differentiation\") \n\n\n\n\n\n\n\n\n\nCode\n#ACF plot \nggAcf(df1,60,main=\"ACF Plot: First order differentiation\") \n\n\n\n\n\n\n\n\n\nCode\nggPacf(df1,60,main=\"PACF Plot: First order differentiation\") \n\n\n\n\n\n\n\n\n\nCode\ntseries::adf.test(df1)\n\n\n\n    Augmented Dickey-Fuller Test\n\ndata:  df1\nDickey-Fuller = -15.945, Lag order = 14, p-value = 0.01\nalternative hypothesis: stationary\n\n\n\n\n\nAfter analyzing the ACF and PACF plots, it can be observed that most of the bar lines lie between the blue lines, indicating that the time series is stationary. This has been further confirmed through an Augmented Dickey-Fuller test as the p-value is less than 0.05. Once the data is stationary, the next step is to model it using ARIMA. The combination of ACF and PACF plots can help determine the appropriate values for p, d, and q in the ARIMA model. The order of differencing required to achieve stationarity gives the d value. The p value is determined by examining significant spikes at various lags in the PACF plot, while the q value is determined through significant spikes in the ACF plot. It is important to consider other methods such as grid search and information criteria to ensure the most appropriate model is selected.\nHere the parameters are d = 1 p = 0,9 (PACF Plot) q = 0,9 (ACF Plot)\n\n\nModel Selection\n\nARIMA(0,1,0) ResultARIMA(9,1,0) ResultARIMA(0,1,9) ResultARIMA(9,1,9) ResultAuto Arima Model\n\n\n\n\nCode\nArima(myts,order=c(0,1,0),include.drift=TRUE) \n\n\nSeries: myts \nARIMA(0,1,0) with drift \n\nCoefficients:\n       drift\n      0.0167\ns.e.  0.0075\n\nsigma^2 = 0.1837:  log likelihood = -1873.2\nAIC=3750.4   AICc=3750.4   BIC=3762.59\n\n\n\n\n\n\nCode\nArima(myts,order=c(9,1,0),include.drift=TRUE) \n\n\nSeries: myts \nARIMA(9,1,0) with drift \n\nCoefficients:\n          ar1     ar2      ar3      ar4     ar5      ar6     ar7      ar8\n      -0.0758  0.0148  -0.0414  -0.0875  0.0290  -0.0410  0.0721  -0.0315\ns.e.   0.0173  0.0174   0.0173   0.0174  0.0174   0.0174  0.0174   0.0174\n         ar9   drift\n      0.1173  0.0167\ns.e.  0.0174  0.0070\n\nsigma^2 = 0.1761:  log likelihood = -1799.85\nAIC=3621.7   AICc=3621.78   BIC=3688.75\n\n\n\n\n\n\nCode\nArima(myts,order=c(0,1,9),include.drift=TRUE) \n\n\nSeries: myts \nARIMA(0,1,9) with drift \n\nCoefficients:\n          ma1     ma2      ma3      ma4     ma5      ma6     ma7      ma8\n      -0.0629  0.0205  -0.0323  -0.0870  0.0488  -0.0584  0.0769  -0.0457\ns.e.   0.0174  0.0175   0.0174   0.0175  0.0185   0.0176  0.0175   0.0169\n         ma9   drift\n      0.1049  0.0167\ns.e.  0.0171  0.0071\n\nsigma^2 = 0.1767:  log likelihood = -1805.64\nAIC=3633.28   AICc=3633.36   BIC=3700.32\n\n\n\n\n\n\nCode\nArima(myts,order=c(9,1,9),include.drift=TRUE) \n\n\nSeries: myts \nARIMA(9,1,9) with drift \n\nCoefficients:\n          ar1      ar2      ar3      ar4      ar5      ar6      ar7      ar8\n      -0.5096  -0.5136  -0.2633  -0.4045  -0.4465  -0.3360  -0.3715  -0.3303\ns.e.      NaN      NaN      NaN      NaN   0.0505   0.0849      NaN      NaN\n        ar9     ma1    ma2     ma3     ma4     ma5     ma6     ma7    ma8\n      0.181  0.4349  0.496  0.1969  0.3085  0.4314  0.2491  0.4279  0.276\ns.e.    NaN     NaN    NaN     NaN     NaN  0.0763  0.0830     NaN    NaN\n          ma9   drift\n      -0.0809  0.0167\ns.e.      NaN  0.0068\n\nsigma^2 = 0.1751:  log likelihood = -1785.74\nAIC=3611.49   AICc=3611.74   BIC=3733.39\n\n\n\n\n\n\nCode\nauto.arima(myts)\n\n\nSeries: myts \nARIMA(0,1,2) with drift \n\nCoefficients:\n          ma1     ma2   drift\n      -0.0855  0.0352  0.0167\ns.e.   0.0175  0.0189  0.0071\n\nsigma^2 = 0.1821:  log likelihood = -1858.28\nAIC=3724.56   AICc=3724.57   BIC=3748.94\n\n\n\n\n\nIn the Model selection, when comparing the AIC, AICc and BIC values of all the 4 models, model (0,1,0) has the least BIC and AIc value. The model generated by auto.arima is also (0,1,0). We can proceed with the further model diagnostic for the chosen best model i.e. (0,1,0), to look into the details of the model.\n\n\nModel Diagnostic\n\nModel PlotModel\n\n\n\n\nCode\nmodel_output <- capture.output(sarima(myts,0,1,0))\n\n\n\n\n\n\n\n\n\nCode\ncat(model_output[9:38], model_output[length(model_output)], sep = \"\\n\") \n\n\n$fit\n\nCall:\narima(x = xdata, order = c(p, d, q), seasonal = list(order = c(P, D, Q), period = S), \n    xreg = constant, transform.pars = trans, fixed = fixed, optim.control = list(trace = trc, \n        REPORT = 1, reltol = tol))\n\nCoefficients:\n      constant\n        0.0167\ns.e.    0.0075\n\nsigma^2 estimated as 0.1836:  log likelihood = -1873.2,  aic = 3750.4\n\n$degrees_of_freedom\n[1] 3277\n\n$ttable\n         Estimate     SE t.value p.value\nconstant   0.0167 0.0075  2.2305  0.0258\n\n$AIC\n[1] 1.144112\n\n$AICc\n[1] 1.144112\n\n$BIC\n[1] 1.147831\n\n\n\n\n\nThe model diagnostic is done for ARIMA(0,1,0). By analyzing the standardized residuals plot, it can be observed that the mean is close to 0 and the variance is slightly higher than 1. Deviations from these values could indicate poor model fit. However, in this case, the model seems to be well-fitted. The ACF plot of residuals shows very few significant lags, which is a positive sign for the model. The qq-plot also suggests normality in the residuals. Additionally, the p-values for the Ljung-Box test are greater than 0.05, indicating that the residuals are independent, which is favorable for the model’s accuracy.\nThe equation for this model: \\[(1-\\phi_1B-\\phi_2B^2)(1-B)(Y_t-\\mu) = (1+\\theta_1B+\\theta_2B^2)\\epsilon_t\\]\n\n\nForcast\n\nShort Term ForcastLong term Forcast\n\n\n\n\nCode\nmyts %>%\n  Arima(order=c(0,1,0),include.drift = TRUE) %>%\n  forecast %>%\n  autoplot(main = \"Consumer Staples Sector Fund Stock Prices Prediction\") +\n  ylab(\"stock prices\") + xlab(\"Year\")\n\n\n\n\n\n\n\n\n\nCode\nsarima.for(myts,182, 0,1,0, main='Consumer Staples Sector Fund Stock Prices Prediction')\n\n\n\n\n\n$pred\nTime Series:\nStart = c(2023, 4) \nEnd = c(2023, 185) \nFrequency = 252 \n  [1] 73.05981 73.07650 73.09319 73.10989 73.12658 73.14327 73.15997 73.17666\n  [9] 73.19335 73.21005 73.22674 73.24343 73.26013 73.27682 73.29351 73.31021\n [17] 73.32690 73.34359 73.36029 73.37698 73.39367 73.41037 73.42706 73.44375\n [25] 73.46045 73.47714 73.49383 73.51053 73.52722 73.54391 73.56060 73.57730\n [33] 73.59399 73.61068 73.62738 73.64407 73.66076 73.67746 73.69415 73.71084\n [41] 73.72754 73.74423 73.76092 73.77762 73.79431 73.81100 73.82770 73.84439\n [49] 73.86108 73.87778 73.89447 73.91116 73.92786 73.94455 73.96124 73.97794\n [57] 73.99463 74.01132 74.02802 74.04471 74.06140 74.07810 74.09479 74.11148\n [65] 74.12818 74.14487 74.16156 74.17826 74.19495 74.21164 74.22834 74.24503\n [73] 74.26172 74.27842 74.29511 74.31180 74.32850 74.34519 74.36188 74.37857\n [81] 74.39527 74.41196 74.42865 74.44535 74.46204 74.47873 74.49543 74.51212\n [89] 74.52881 74.54551 74.56220 74.57889 74.59559 74.61228 74.62897 74.64567\n [97] 74.66236 74.67905 74.69575 74.71244 74.72913 74.74583 74.76252 74.77921\n[105] 74.79591 74.81260 74.82929 74.84599 74.86268 74.87937 74.89607 74.91276\n[113] 74.92945 74.94615 74.96284 74.97953 74.99623 75.01292 75.02961 75.04631\n[121] 75.06300 75.07969 75.09639 75.11308 75.12977 75.14647 75.16316 75.17985\n[129] 75.19655 75.21324 75.22993 75.24662 75.26332 75.28001 75.29670 75.31340\n[137] 75.33009 75.34678 75.36348 75.38017 75.39686 75.41356 75.43025 75.44694\n[145] 75.46364 75.48033 75.49702 75.51372 75.53041 75.54710 75.56380 75.58049\n[153] 75.59718 75.61388 75.63057 75.64726 75.66396 75.68065 75.69734 75.71404\n[161] 75.73073 75.74742 75.76412 75.78081 75.79750 75.81420 75.83089 75.84758\n[169] 75.86428 75.88097 75.89766 75.91436 75.93105 75.94774 75.96444 75.98113\n[177] 75.99782 76.01452 76.03121 76.04790 76.06459 76.08129\n\n$se\nTime Series:\nStart = c(2023, 4) \nEnd = c(2023, 185) \nFrequency = 252 \n  [1] 0.4284879 0.6059734 0.7421629 0.8569758 0.9581281 1.0495768 1.1336725\n  [8] 1.2119469 1.2854638 1.3549978 1.4211337 1.4843257 1.5449352 1.6032550\n [15] 1.6595266 1.7139517 1.7667010 1.8179203 1.8677356 1.9162562 1.9635783\n [22] 2.0097865 2.0549559 2.0991535 2.1424396 2.1848683 2.2264886 2.2673450\n [29] 2.3074781 2.3469250 2.3857198 2.4238937 2.4614757 2.4984925 2.5349687\n [36] 2.5709275 2.6063903 2.6413769 2.6759062 2.7099956 2.7436614 2.7769191\n [43] 2.8097832 2.8422673 2.8743844 2.9061465 2.9375652 2.9686514 2.9994155\n [50] 3.0298672 3.0600158 3.0898704 3.1194392 3.1487303 3.1777515 3.2065100\n [57] 3.2350129 3.2632668 3.2912782 3.3190532 3.3465977 3.3739173 3.4010174\n [64] 3.4279034 3.4545801 3.4810523 3.5073248 3.5334019 3.5592880 3.5849872\n [71] 3.6105034 3.6358406 3.6610024 3.6859925 3.7108143 3.7354711 3.7599663\n [78] 3.7843029 3.8084840 3.8325125 3.8563913 3.8801232 3.9037108 3.9271567\n [85] 3.9504635 3.9736335 3.9966693 4.0195730 4.0423470 4.0649934 4.0875143\n [92] 4.1099118 4.1321879 4.1543445 4.1763837 4.1983071 4.2201166 4.2418140\n [99] 4.2634010 4.2848792 4.3062503 4.3275159 4.3486775 4.3697366 4.3906946\n[106] 4.4115532 4.4323135 4.4529771 4.4735452 4.4940192 4.5144004 4.5346899\n[113] 4.5548891 4.5749991 4.5950211 4.6149562 4.6348055 4.6545702 4.6742514\n[120] 4.6938500 4.7133671 4.7328038 4.7521609 4.7714396 4.7906406 4.8097650\n[127] 4.8288136 4.8477875 4.8666873 4.8855140 4.9042684 4.9229514 4.9415638\n[134] 4.9601063 4.9785798 4.9969849 5.0153225 5.0335934 5.0517981 5.0699375\n[141] 5.0880122 5.1060229 5.1239703 5.1418551 5.1596778 5.1774393 5.1951400\n[148] 5.2127806 5.2303617 5.2478839 5.2653477 5.2827539 5.3001029 5.3173953\n[155] 5.3346316 5.3518124 5.3689383 5.3860097 5.4030271 5.4199911 5.4369022\n[162] 5.4537609 5.4705676 5.4873228 5.5040270 5.5206807 5.5372843 5.5538382\n[169] 5.5703430 5.5867990 5.6032067 5.6195664 5.6358787 5.6521439 5.6683624\n[176] 5.6845347 5.7006610 5.7167419 5.7327777 5.7487687 5.7647154 5.7806181\n\n\n\n\n\nThe two figures above show an idea of the short term and the long-term forecasted stock price. The forecasted values is be based on the estimated parameters of the model, which were obtained by fitting it to the historical data. One way to assess the accuracy of the forecasts is to use cross-validation, which involves splitting the data into training and testing sets and comparing the predicted values to the actual values in the test set. This can help to identify any potential problems with the model and to fine-tune the model parameters.\n\n12 Step ahead Cross Validation1 Step ahead Cross Validation\n\n\n\n\nCode\n#a seasonal cross validation using 12 steps ahead forecasts\nfarima1 <- function(x, h){forecast(Arima(x, order=c(0,1,0)), h=h)}\n\n# Compute cross-validated errors for up to 12 steps ahead\ne <- tsCV(myts, forecastfunction = farima1, h = 12)\n\n# Compute the MSE values and remove missing values\nmse <- colMeans(e^2, na.rm = TRUE)\n\n# Plot the MSE values against the forecast horizon\ndata.frame(h = 1:12, MSE = mse) %>%\n  ggplot(aes(x = h, y = MSE)) + geom_point()+geom_line()+ggtitle(\"12 Step Ahead Cross Validation\")\n\n\n\n\n\n\n\n\n\nCode\n#a seasonal cross validation using 1 steps ahead forecasts\nfarima1 <- function(x, h){forecast(Arima(x, order=c(0,1,0)),h=h)}\n\n# Compute cross-validated errors for up to 1 steps ahead\ne <- tsCV(myts, forecastfunction = farima1, h = 1)\nmse <-abs(mean(e,na.rm=TRUE))\n\n# Plot the MSE values against the forecast horizon\ndata.frame(h = 1:12, MSE = mse) %>%\n  ggplot(aes(x = h, y = MSE)) + geom_point()+geom_line()+ggtitle(\"1 Step Ahead Cross Validation\")\n\n\n\n\n\n\n\n\nThe one-step-ahead forecasting plot shows that the MSE seems to be stable. The MSE is tend to rise by 12 steps at each step of cross-validation. Lastly, the above-mentioned ARIMA model should be compared to benchmark methods before being used for a long time.\n\n\nModel Comparison\n\nPlotModel Error Table\n\n\n\n\nCode\nfit <- Arima(myts, order=c(0,1,0))\nautoplot(myts) +\n  autolayer(meanf(myts, h=182),\n            series=\"Mean\", PI=FALSE) +\n  autolayer(naive(myts, h=182),\n            series=\"Naïve\", PI=FALSE) +\n  autolayer(snaive(myts, h=182),\n            series=\"SNaïve\", PI=FALSE)+\n  autolayer(rwf(myts, h=182, drift=TRUE),\n            series=\"Drift\", PI=FALSE)+\n  autolayer(forecast(fit,182), \n            series=\"fit\",PI=FALSE) +\n  guides(colour=guide_legend(title=\"Forecast\"))\n\n\n\n\n\n\n\n\n\nCode\nsummary <- summary(fit)\nsnaive <- snaive(myts, h = 182)\naccuracy(snaive)\n\n\n                   ME     RMSE      MAE      MPE     MAPE MASE      ACF1\nTraining set 4.327132 5.359047 4.566995 10.15949 10.69683    1 0.9816058\n\n\nCode\nsummary\n\n\nSeries: myts \nARIMA(0,1,0) \n\nsigma^2 = 0.1839:  log likelihood = -1875.69\nAIC=3753.37   AICc=3753.37   BIC=3759.47\n\nTraining set error measures:\n                     ME      RMSE       MAE        MPE      MAPE       MASE\nTraining set 0.01669376 0.4287477 0.2637191 0.03822063 0.6056566 0.05774455\n                    ACF1\nTraining set -0.09227748\n\n\n\n\n\nError\nModel\nSnavie\n\n\n\n\nME\n0.004307832\n1.469789\n\n\nRMSE\n0.8600413\n16.4328\n\n\nMAE\n0.24721\n10.11811\n\n\nMPE\n-0.01937673\n-7.129413\n\n\nMAPE\n0.64873\n35.78488\n\n\nMASE\n0.02443243\n1.0000000\n\n\nACF1\n-0.007978438\n0.9970419\n\n\n\n\n\n\nThe ARIMA forecast tracks the actual data points very closely when compared to other model. The other forecast methods are less accurate when compared. From the table, Model error measurements of fit are much lower than snaive method.We can conclude that the fitted model is good."
  },
  {
    "objectID": "deep_learning_sector_xly.html",
    "href": "deep_learning_sector_xly.html",
    "title": "",
    "section": "",
    "text": "Deep Learning for Consumer Discretionary Sector Fund\nThe consumer discretionary sector comprises companies that provide non-essential goods and services, such as clothing, entertainment, and luxury items. These companies tend to be more sensitive to economic cycles and consumer sentiment than companies in other sectors. As with other stock market, predicting the future movements of the Consumer Discretionary Sector Fund can be challenging due to the complex and dynamic nature of the market. These models range from traditional time-series models to more advanced deep learning models. By leveraging the power of Deep Learning and applying techniques such as RNN, LSTM, GRU, and regularization, we can create a model that accurately predicts the future movements of the sector. Such a model can provide valuable insights for investors and traders looking to make informed decisions in the stock market, and help them stay ahead of the curve in the ever-changing landscape of the technology and biotech industries.\nIn order to apply the deep learning methods, the time series data is first split into training and testing sets, taking into account the time component. The training data constitutes 75% of the total data while the rest is the testing data. Furthermore, to ensure that the values are on the same scale, the data is scaled, which is crucial for deep learning models. The mean and standard deviation of the training data are computed and then utilized to normalize both the training and testing sets. This approach ensures that the models are trained on a standardized dataset, enabling them to make accurate predictions.\n\n\nCode\n# libraries\nimport numpy as np\nimport pandas as pd\nimport math\nfrom sklearn.metrics import mean_squared_error\nfrom matplotlib import pyplot as plt\nfrom keras.models import Sequential\nfrom keras.layers import Dropout\nfrom keras.layers import Dense, SimpleRNN, LSTM, GRU\nfrom keras import regularizers\nimport warnings\nwarnings.filterwarnings('ignore')\n\n# import data\ndf = pd.read_csv('./DATA/CLEANED DATA/XLY_raw_data.csv')\n# clean data and get final data for anlaysis\ndf = df[['Date', 'XLY.Adjusted']]\ndf['Date'] = pd.to_datetime(df['Date'])\ndf['XLY.Adjusted'] = pd.to_numeric(df['XLY.Adjusted'], errors='coerce')\ndf = df.dropna(subset=['XLY.Adjusted'])\ndf.sort_values('Date', inplace=True, ascending=True)\ndf = df.reset_index(drop=True)\ndef create_X_Y(ts: np.array, lag=1, n_ahead=1, target_index=0) -> tuple:\n    \"\"\"\n    A method to create X and Y matrix from a time series array for the training of \n    deep learning models \n    \"\"\"\n    # Extracting the number of features that are passed from the array\n    n_features = ts.shape[1]\n\n    # Creating placeholder lists\n    X, Y = [], []\n\n    if len(ts) - lag <= 0:\n        X.append(ts)\n    else:\n        for i in range(len(ts) - lag - n_ahead):\n            Y.append(ts[(i + lag):(i + lag + n_ahead), target_index])\n            X.append(ts[i:(i + lag)])\n\n    X, Y = np.array(X), np.array(Y)\n\n    # Reshaping the X array to an RNN input shape\n    X = np.reshape(X, (X.shape[0], lag, n_features))\n\n    return X, Y\n\nts = df[['XLY.Adjusted']].values\nnp.random.seed(123)\nnrows = ts.shape[0]\ntest_share = 0.25\n# Spliting into train and test sets\ntrain = ts[0:int(nrows * (1 - test_share))]\ntest = ts[int(nrows * (1 - test_share)):]\n\n# Scaling the data\ntrain_mean = train.mean()\ntrain_std = train.std()\ntrain = (train - train_mean) / train_std\ntest = (test - train_mean) / train_std\n\n# Creating the final scaled frame\nts_s = np.concatenate([train, test])\n\nlag = 12\nahead = 3\n\n# Creating the X and Y for training\nX, Y = create_X_Y(ts_s, lag=lag, n_ahead=ahead)\n\nXtrain = X[0:int(X.shape[0] * (1 - test_share))]\nYtrain = Y[0:int(X.shape[0] * (1 - test_share))]\n\nXval = X[int(X.shape[0] * (1 - test_share)):]\nYval = Y[int(X.shape[0] * (1 - test_share)):]\n\n\n\n\ndef plot_model(history, model_title):\n    loss = history.history['loss']\n    epochs = range(1, len(loss) + 1)\n    plt.figure()\n    plt.plot(epochs, loss, 'b', label='Training loss', color = \"gray\")\n    plt.title(f'{model_title} Training loss ')\n    plt.legend()\n    \n\n\ndef print_error(trainY, testY, train_predict, test_predict):\n    # Error of predictions\n    train_rmse = math.sqrt(mean_squared_error(\n        trainY[:, 0], train_predict[:, 0]))\n    test_rmse = math.sqrt(mean_squared_error(testY[:, 0], test_predict[:, 0]))\n    # Print RMSE\n    print('Train RMSE: %.3f RMSE' % (train_rmse))\n    print('Test RMSE: %.3f RMSE' % (test_rmse))\n    return train_rmse, test_rmse\n\n\n\n\nRecurrent Neural Network\nThe RNN models that are constructed and trained using TensorFlow and Keras. This model is trained for five hidden layers, one dense layer, and the activation function hyperbolic tangent. A dropout rate of 0.2 is used to reduce the risk of overfitting. The model are trained for 20 epochs and generated validation loss plots to compare the performance of the regularized and non-regularized models. The second RNN model was identical to the first, except for the addition of kernel regularization.\n\nRNNRNN with Regularization\n\n\n\n\nCode\n# Create a RNN\ndef create_RNN(hidden_units, dense_units, input_shape, activation,dropout_rate=0.2,kernel_regularizer=None):\n    model = Sequential()\n    # Create a simple neural network layer\n    model.add(SimpleRNN(hidden_units, input_shape=input_shape,\n              activation=activation[0]))\n    # Add a dense layer (only one, more layers would make it a deep neural net)\n    model.add(Dense(units=dense_units,\n              activation=activation[1],\n              kernel_regularizer=kernel_regularizer))\n    # Add layer dropout\n    model.add(Dropout(dropout_rate))\n    \n    # Compile the model and optimize on mean squared error\n    model.compile(loss='mean_squared_error', optimizer='adam')\n    return model\n  \n# Create a recurrent neural network\nmodel = create_RNN(hidden_units=5, dense_units=1, input_shape=(lag, Xtrain.shape[-1]),\n                   activation=['tanh', 'tanh'], dropout_rate = 0.2)\nhistory = model.fit(Xtrain, Ytrain, epochs=20, batch_size=1, verbose=0,validation_data=(Xval, Yval))\nplot_model(history, 'Recurrent Neural Network Model')\n\nyhat_d = [x[0] for x in model.predict(Xval, verbose=0)]\ny = [y[0] for y in Yval]\n\ntrain_predict = model.predict(Xtrain, verbose=0)\ntest_predict = model.predict(Xval, verbose=0)\n\n# Print error\ntrain_rmse, test_rmse = print_error(Ytrain, Yval, train_predict, test_predict)\nrmse_table = {\n    'model': ['Recurrent Neural Network'],\n    'training_rmse': [train_rmse],\n    'testing_rmse': [test_rmse]\n}\n\n\nTrain RMSE: 0.321 RMSE\nTest RMSE: 2.513 RMSE\n\n\n\n\n\n\n\n\n\nCode\n# Create a recurrent neural network with regularization\n\nmodel = create_RNN(hidden_units=5, dense_units=1, input_shape=(lag, Xtrain.shape[-1]),\n                   activation=['tanh', 'tanh'], kernel_regularizer=regularizers.L1L2(l1=1e-5, l2=1e-4), dropout_rate = 0.2)\nhistory = model.fit(Xtrain, Ytrain, epochs=20, batch_size=1, verbose=0, validation_data=(Xval, Yval))\nplot_model(history, 'Recurrent Neural Network Model (with L1L2 Regularization)')\n\nyhat_d_reg = [x[0] for x in model.predict(Xval, verbose=0)]\n\ntrain_predict = model.predict(Xtrain, verbose=0)\ntest_predict = model.predict(Xval, verbose=0)\n\n# Print error\ntrain_rmse, test_rmse = print_error(Ytrain, Yval, train_predict, test_predict)\nrmse_table['model'].append(\n    'Recurrent Neural Network (with L1L2 Regularization)')\nrmse_table['training_rmse'].append(train_rmse)\nrmse_table['testing_rmse'].append(test_rmse)\n\n\nTrain RMSE: 0.319 RMSE\nTest RMSE: 2.480 RMSE\n\n\n\n\n\n\n\n\nThe second plot illustrates the impact of regularization on the training loss. Compared to the original plot, there are significant differences. The regularization technique had a substantial effect on the testing error of the model. The model trained without regularization had a slightly higher training error of 0.317 RMSE and testing error of 2.482 RMSE when compared to with regularization.\n\n\nGRU Neural Network\nA GRU neural network is a powerful machine learning tool widely adopted in various applications. Its key feature is the recurrent gated unit, a type of memory cell that can store information over a data sequence. The GRU neural network is five hidden layers, one dense layer, and the hyperbolic tangent activation function. The model also includes kernel regularization to help address the issue of overfitting. The models are trained for 20 epochs and validation loss plots were generated to compare the performance of the regularized and non-regularized models. Overall, the GRU neural network has shown great promise in accurately predicting future movements in the sector.\n\nGRUGRU with Regularization\n\n\n\n\nCode\n# Create a GRU Neural Network\ndef create_GRU(hidden_units, dense_units, input_shape, activation,dropout_rate=0.2, kernel_regularizer=None):\n    model = Sequential()\n    # Create a simple GRU neural network layer\n    model.add(GRU(hidden_units, input_shape=input_shape,\n              activation=activation[0]))\n    # Add a dense layer (only one, more layers would make it a deep neural net)\n    model.add(Dense(units=dense_units,\n              activation=activation[1], kernel_regularizer=kernel_regularizer))\n    # Add layer dropout\n    model.add(Dropout(dropout_rate))\n    # Compile the model and optimize on mean squared error\n    model.compile(loss='mean_squared_error', optimizer='sgd')\n    return model\n\n# Training and evaluating a GRU-based model\nmodel = create_GRU(hidden_units=5, dense_units=1, input_shape=(lag, Xtrain.shape[-1]),\n                   activation=['tanh', 'relu'], dropout_rate = 0.2)\nhistory = model.fit(Xtrain, Ytrain, epochs=20, batch_size=1, verbose=0)\nplot_model(history, 'GRU Model')\n\nyhat_gru = [x[0] for x in model.predict(Xval, verbose=0)]\n\ntrain_predict = model.predict(Xtrain, verbose=0)\ntest_predict = model.predict(Xval, verbose=0)\n\n# Print error\ntrain_rmse, test_rmse = print_error(Ytrain, Yval, train_predict, test_predict)\nrmse_table['model'].append('GRU Neural Network')\nrmse_table['training_rmse'].append(train_rmse)\nrmse_table['testing_rmse'].append(test_rmse)\n\n\nTrain RMSE: 0.686 RMSE\nTest RMSE: 1.211 RMSE\n\n\n\n\n\n\n\n\n\nCode\n# Training and evaluating a GRU-based model with regularization\nmodel = create_GRU(hidden_units=5, dense_units=1, input_shape=(lag, Xtrain.shape[-1]),\n                   activation=['tanh', 'relu'],dropout_rate = 0.2,  kernel_regularizer=regularizers.L1L2(l1=1e-5, l2=1e-4))\nhistory = model.fit(Xtrain, Ytrain, epochs=20, batch_size=1, verbose=0)\nplot_model(history, 'GRU Model (with L1L2 Regularization)')\n\nyhat_gru_reg = [x[0] for x in model.predict(Xval, verbose=0)]\n\ntrain_predict = model.predict(Xtrain, verbose=0)\ntest_predict = model.predict(Xval, verbose=0)\n\n# Print error\ntrain_rmse, test_rmse = print_error(Ytrain, Yval, train_predict, test_predict)\nrmse_table['model'].append('GRU Neural Network (with L1L2 Regularization)')\nrmse_table['training_rmse'].append(train_rmse)\nrmse_table['testing_rmse'].append(test_rmse)\n\n\nTrain RMSE: 0.998 RMSE\nTest RMSE: 3.424 RMSE\n\n\n\n\n\n\n\n\nThe two plots show the training loss for the GRU model, one with regularization and one without. The plot with regularization is similar to without regularization plot. The model without regularization had a train RMSE of 0.998 and a test RMSE of 3.424, the values are the higher when compared to with regularization.\n\n\nLSTM Neural Network\nThe LSTM (Long Short-Term Memory) neural network is another powerful machine learning tool that has been widely adopted in various applications, particularly in tasks involving sequential data.The LSTM neural network architecture typically consists of multiple LSTM layers, followed by one or more dense layers and an activation function such as the sigmoid or hyperbolic tangent. Similar to the GRU neural network, the LSTM model may also include kernel regularization to address the issue of overfitting. In order to evaluate the performance of the LSTM model, the model can be trained on a dataset for a specified number of epochs, and validation loss plots can be generated to compare the performance of the regularized and non-regularized models.\n\nLSTMLSTM with Regularization\n\n\n\n\nCode\n# Create a LSTM Neural Network\ndef create_LSTM(hidden_units, dense_units, input_shape, activation,dropout_rate = 0.2, kernel_regularizer=None):\n    model = Sequential()\n    # Create a simple long short term memory neural network\n    model.add(LSTM(hidden_units,\n              activation=activation[0], input_shape=input_shape))\n    # Add a dense layer (only one, more layers would make it a deep neural net)\n    model.add(Dense(units=dense_units,\n              activation=activation[1], kernel_regularizer=kernel_regularizer))\n    # Add layer dropout\n    model.add(Dropout(dropout_rate))\n    # Compile the model and optimize on mean squared error\n    model.compile(optimizer=\"RMSprop\", loss='mae')\n    return model\n\n# Create an LSTM neural network\nmodel = create_LSTM(hidden_units=5, dense_units=1, input_shape=(lag, Xtrain.shape[-1]),\n                    activation=['tanh', 'linear'], dropout_rate = 0.2)\nhistory = model.fit(Xtrain, Ytrain, epochs=20, batch_size=1, verbose=0)\nplot_model(history, 'LSTM Model')\n\nyhat_lstm = [x[0] for x in model.predict(Xval, verbose=0)]\n\ntrain_predict = model.predict(Xtrain, verbose=0)\ntest_predict = model.predict(Xval, verbose=0)\n\n# Print error\ntrain_rmse, test_rmse = print_error(Ytrain, Yval, train_predict, test_predict)\nrmse_table['model'].append('LSTM Neural Network')\nrmse_table['training_rmse'].append(train_rmse)\nrmse_table['testing_rmse'].append(test_rmse)\n\n\nTrain RMSE: 0.194 RMSE\nTest RMSE: 1.226 RMSE\n\n\n\n\n\n\n\n\n\nCode\n# Create an LSTM neural network with regularization\nmodel = create_LSTM(hidden_units=5, dense_units=1, input_shape=(lag, Xtrain.shape[-1]),\n                    activation=['tanh', 'linear'], dropout_rate = 0.2, kernel_regularizer=regularizers.L1L2(l1=1e-5, l2=1e-4))\nhistory = model.fit(Xtrain, Ytrain, epochs=20, batch_size=1, verbose=0)\nplot_model(history, 'LSTM Model (with L1L2 Regularization)')\n\n\nyhat_lstm_reg = [x[0] for x in model.predict(Xval, verbose=0)]\n\ntrain_predict = model.predict(Xtrain, verbose=0)\ntest_predict = model.predict(Xval, verbose=0)\n\n# Print error\ntrain_rmse, test_rmse = print_error(Ytrain, Yval, train_predict, test_predict)\nrmse_table['model'].append('LSTM Neural Network (with L1L2 Regularization)')\nrmse_table['training_rmse'].append(train_rmse)\nrmse_table['testing_rmse'].append(test_rmse)\n\n\nTrain RMSE: 0.209 RMSE\nTest RMSE: 1.424 RMSE\n\n\n\n\n\n\n\n\nThe two plots show the training loss for the LSTM model, one with regularization and one without. The plot with regularization shows less fluctuation, indicating that overfitting has been reduced. The model without regularization had a train RMSE of 0.197 and a test RMSE of 1.321, while the model with regularization had a slightly lower for both the train and test RMSE values which are 0.193 and 1.152 respectively.\n\n\nForecast\n\n\nCode\n# Creating the frame to store both predictions\ndays = df['Date'].values[-len(y):]\nframe = pd.concat([\n    pd.DataFrame({'day': days, 'price': y, 'type': 'original'}),\n    pd.DataFrame({'day': days, 'price': yhat_d, 'type': 'rnn_forecast'}),\n    pd.DataFrame({'day': days, 'price': yhat_gru, 'type': 'gru_forecast'}),\n    pd.DataFrame({'day': days, 'price': yhat_lstm, 'type': 'lstm_forecast'})\n])\n# Creating the unscaled values column\nframe['price_absolute'] = [(x * train_std) + train_mean for x in frame['price']]\n# Pivoting\npivoted = frame.pivot_table(index='day', columns='type')\npivoted.columns = ['_'.join(x).strip() for x in pivoted.columns.values]\n\nplt.figure(figsize=(12, 10))\nplt.plot(pivoted.index, pivoted.price_absolute_original,\n         color='#8B1874', label='original')\nplt.plot(pivoted.index, pivoted.price_absolute_rnn_forecast,\n         color='#F79540', label='RNN Forecast', alpha=0.6)\nplt.plot(pivoted.index, pivoted.price_absolute_gru_forecast,\n         color='#B71375', label='GRU Forecast', alpha=0.6)\nplt.plot(pivoted.index, pivoted.price_absolute_lstm_forecast,\n         color='#FC4F00', label='LSTM Forecast', alpha=0.6)\nplt.title('Consumer Discretionary Sector Fund Forecasts')\nplt.legend()\nplt.show()\n\n\n\n\n\nWhen comparing the predictions of the deep learning models with the original time series plot, we can observe that the LSTM model is the closest to the actual values. The GRU model and RNN model are pretty far from the actual plot, which indicates that LSTM model is better when compared to others, but when we look at the LSTM model, the model is not accurate, i.e., it is not the same as the actual plot.\n\n\nModel Comparison\n\n\nCode\nrmse_df = pd.DataFrame(rmse_table)\nrmse_df\n\n\n\n\n\n\n  \n    \n      \n      model\n      training_rmse\n      testing_rmse\n    \n  \n  \n    \n      0\n      Recurrent Neural Network\n      0.321240\n      2.512940\n    \n    \n      1\n      Recurrent Neural Network (with L1L2 Regulariza...\n      0.318911\n      2.479953\n    \n    \n      2\n      GRU Neural Network\n      0.686071\n      1.210686\n    \n    \n      3\n      GRU Neural Network (with L1L2 Regularization)\n      0.997890\n      3.423626\n    \n    \n      4\n      LSTM Neural Network\n      0.193721\n      1.225722\n    \n    \n      5\n      LSTM Neural Network (with L1L2 Regularization)\n      0.208568\n      1.424401\n    \n  \n\n\n\n\nComparing the models based on their training and testing root mean square error (RMSE) values, the performance of the models varied across the different datasets. In general, the models with L1L2 regularization tended to perform slightly better on the testing set, indicating that the regularization helped to prevent overfitting. Overall, the LSTM models tended to perform the best on these datasets, particularly when regularization was not used.\n\n\nComparison of deep learning models with traditional single variable time series\nThe deep learning models (RNN, GRU, LSTM) perform better than the ARIMA model in both training and testing RMSE values for the Consumer Discretionary Sector Fund. Among the deep learning models, the LSTM model without regularization has the lowest testing RMSE value of 0.709320, when compared to ARIMA Model which is 2.118625, indicating its better performance in predicting the market prices of the Consumer Discretionary Sector Fund compared to the other deep learning models and ARIMA model."
  },
  {
    "objectID": "deep_learning_sector_xlv.html",
    "href": "deep_learning_sector_xlv.html",
    "title": "",
    "section": "",
    "text": "Deep Learning for Health Care Sector Fund\nThe Health Care Sector Fund is a market index that tracks the performance of the healthcare industry in the United States. This sector includes companies that are involved in pharmaceuticals, biotechnology, medical devices, healthcare services, and healthcare technology. As with other stock market, predicting the future movements of the Health Care Sector Fund can be challenging due to the complex and dynamic nature of the market. These models range from traditional time-series models to more advanced deep learning models. By leveraging the power of Deep Learning and applying techniques such as RNN, LSTM, GRU, and regularization, we can create a model that accurately predicts the future movements of the sector. Such a model can provide valuable insights for investors and traders looking to make informed decisions in the stock market, and help them stay ahead of the curve in the ever-changing landscape of the technology and biotech industries.\nIn order to apply the deep learning methods, the time series data is first split into training and testing sets, taking into account the time component. The training data constitutes 75% of the total data while the rest is the testing data. Furthermore, to ensure that the values are on the same scale, the data is scaled, which is crucial for deep learning models. The mean and standard deviation of the training data are computed and then utilized to normalize both the training and testing sets. This approach ensures that the models are trained on a standardized dataset, enabling them to make accurate predictions.\n\n\nCode\n# libraries\nimport numpy as np\nimport pandas as pd\nimport math\nfrom sklearn.metrics import mean_squared_error\nfrom matplotlib import pyplot as plt\nfrom keras.models import Sequential\nfrom keras.layers import Dropout\nfrom keras.layers import Dense, SimpleRNN, LSTM, GRU\nfrom keras import regularizers\nimport warnings\nwarnings.filterwarnings('ignore')\n\n# import data\ndf = pd.read_csv('./DATA/CLEANED DATA/XLV_raw_data.csv')\n# clean data and get final data for anlaysis\ndf = df[['Date', 'XLV.Adjusted']]\ndf['Date'] = pd.to_datetime(df['Date'])\ndf['XLV.Adjusted'] = pd.to_numeric(df['XLV.Adjusted'], errors='coerce')\ndf = df.dropna(subset=['XLV.Adjusted'])\ndf.sort_values('Date', inplace=True, ascending=True)\ndf = df.reset_index(drop=True)\ndef create_X_Y(ts: np.array, lag=1, n_ahead=1, target_index=0) -> tuple:\n    \"\"\"\n    A method to create X and Y matrix from a time series array for the training of \n    deep learning models \n    \"\"\"\n    # Extracting the number of features that are passed from the array\n    n_features = ts.shape[1]\n\n    # Creating placeholder lists\n    X, Y = [], []\n\n    if len(ts) - lag <= 0:\n        X.append(ts)\n    else:\n        for i in range(len(ts) - lag - n_ahead):\n            Y.append(ts[(i + lag):(i + lag + n_ahead), target_index])\n            X.append(ts[i:(i + lag)])\n\n    X, Y = np.array(X), np.array(Y)\n\n    # Reshaping the X array to an RNN input shape\n    X = np.reshape(X, (X.shape[0], lag, n_features))\n\n    return X, Y\n\nts = df[['XLV.Adjusted']].values\nnp.random.seed(123)\nnrows = ts.shape[0]\ntest_share = 0.25\n# Spliting into train and test sets\ntrain = ts[0:int(nrows * (1 - test_share))]\ntest = ts[int(nrows * (1 - test_share)):]\n\n# Scaling the data\ntrain_mean = train.mean()\ntrain_std = train.std()\ntrain = (train - train_mean) / train_std\ntest = (test - train_mean) / train_std\n\n# Creating the final scaled frame\nts_s = np.concatenate([train, test])\n\nlag = 12\nahead = 3\n\n# Creating the X and Y for training\nX, Y = create_X_Y(ts_s, lag=lag, n_ahead=ahead)\n\nXtrain = X[0:int(X.shape[0] * (1 - test_share))]\nYtrain = Y[0:int(X.shape[0] * (1 - test_share))]\n\nXval = X[int(X.shape[0] * (1 - test_share)):]\nYval = Y[int(X.shape[0] * (1 - test_share)):]\n\n\n\n\ndef plot_model(history, model_title):\n    loss = history.history['loss']\n    epochs = range(1, len(loss) + 1)\n    plt.figure()\n    plt.plot(epochs, loss, 'b', label='Training loss', color = \"gray\")\n    plt.title(f'{model_title} Training loss ')\n    plt.legend()\n    \n\n\ndef print_error(trainY, testY, train_predict, test_predict):\n    # Error of predictions\n    train_rmse = math.sqrt(mean_squared_error(\n        trainY[:, 0], train_predict[:, 0]))\n    test_rmse = math.sqrt(mean_squared_error(testY[:, 0], test_predict[:, 0]))\n    # Print RMSE\n    print('Train RMSE: %.3f RMSE' % (train_rmse))\n    print('Test RMSE: %.3f RMSE' % (test_rmse))\n    return train_rmse, test_rmse\n\n\n\n\nRecurrent Neural Network\nThe RNN models that are constructed and trained using TensorFlow and Keras. This model is trained for five hidden layers, one dense layer, and the activation function hyperbolic tangent. A dropout rate of 0.2 is used to reduce the risk of overfitting. The model are trained for 20 epochs and generated validation loss plots to compare the performance of the regularized and non-regularized models. The second RNN model was identical to the first, except for the addition of kernel regularization.\n\nRNNRNN with Regularization\n\n\n\n\nCode\n# Create a RNN\ndef create_RNN(hidden_units, dense_units, input_shape, activation,dropout_rate=0.2,kernel_regularizer=None):\n    model = Sequential()\n    # Create a simple neural network layer\n    model.add(SimpleRNN(hidden_units, input_shape=input_shape,\n              activation=activation[0]))\n    # Add a dense layer (only one, more layers would make it a deep neural net)\n    model.add(Dense(units=dense_units,\n              activation=activation[1],\n              kernel_regularizer=kernel_regularizer))\n    # Add layer dropout\n    model.add(Dropout(dropout_rate))\n    \n    # Compile the model and optimize on mean squared error\n    model.compile(loss='mean_squared_error', optimizer='adam')\n    return model\n  \n# Create a recurrent neural network\nmodel = create_RNN(hidden_units=5, dense_units=1, input_shape=(lag, Xtrain.shape[-1]),\n                   activation=['tanh', 'tanh'], dropout_rate = 0.2)\nhistory = model.fit(Xtrain, Ytrain, epochs=20, batch_size=1, verbose=0,validation_data=(Xval, Yval))\nplot_model(history, 'Recurrent Neural Network Model')\n\nyhat_d = [x[0] for x in model.predict(Xval, verbose=0)]\ny = [y[0] for y in Yval]\n\ntrain_predict = model.predict(Xtrain, verbose=0)\ntest_predict = model.predict(Xval, verbose=0)\n\n# Print error\ntrain_rmse, test_rmse = print_error(Ytrain, Yval, train_predict, test_predict)\nrmse_table = {\n    'model': ['Recurrent Neural Network'],\n    'training_rmse': [train_rmse],\n    'testing_rmse': [test_rmse]\n}\n\n\nTrain RMSE: 0.271 RMSE\nTest RMSE: 2.161 RMSE\n\n\n\n\n\n\n\n\n\nCode\n# Create a recurrent neural network with regularization\n\nmodel = create_RNN(hidden_units=5, dense_units=1, input_shape=(lag, Xtrain.shape[-1]),\n                   activation=['tanh', 'tanh'], kernel_regularizer=regularizers.L1L2(l1=1e-5, l2=1e-4), dropout_rate = 0.2)\nhistory = model.fit(Xtrain, Ytrain, epochs=20, batch_size=1, verbose=0, validation_data=(Xval, Yval))\nplot_model(history, 'Recurrent Neural Network Model (with L1L2 Regularization)')\n\nyhat_d_reg = [x[0] for x in model.predict(Xval, verbose=0)]\n\ntrain_predict = model.predict(Xtrain, verbose=0)\ntest_predict = model.predict(Xval, verbose=0)\n\n# Print error\ntrain_rmse, test_rmse = print_error(Ytrain, Yval, train_predict, test_predict)\nrmse_table['model'].append(\n    'Recurrent Neural Network (with L1L2 Regularization)')\nrmse_table['training_rmse'].append(train_rmse)\nrmse_table['testing_rmse'].append(test_rmse)\n\n\nTrain RMSE: 0.268 RMSE\nTest RMSE: 2.161 RMSE\n\n\n\n\n\n\n\n\nThe second plot illustrates the impact of regularization on the training loss. Compared to the original plot, there are significant differences. The regularization technique had a substantial effect on the testing error of the model. The model trained without regularization had a slightly lower training error of 0.273 RMSE but a considerably higher testing error of 2.160 RMSE. The RMSE values are the Slightly lower for with regularization which are 0.269 and 2.161, respectively.\n\n\nGRU Neural Network\nA GRU neural network is a powerful machine learning tool widely adopted in various applications. Its key feature is the recurrent gated unit, a type of memory cell that can store information over a data sequence. The GRU neural network is five hidden layers, one dense layer, and the hyperbolic tangent activation function. The model also includes kernel regularization to help address the issue of overfitting. The models are trained for 20 epochs and validation loss plots were generated to compare the performance of the regularized and non-regularized models. Overall, the GRU neural network has shown great promise in accurately predicting future movements in the sector.\n\nGRUGRU with Regularization\n\n\n\n\nCode\n# Create a GRU Neural Network\ndef create_GRU(hidden_units, dense_units, input_shape, activation,dropout_rate=0.2, kernel_regularizer=None):\n    model = Sequential()\n    # Create a simple GRU neural network layer\n    model.add(GRU(hidden_units, input_shape=input_shape,\n              activation=activation[0]))\n    # Add a dense layer (only one, more layers would make it a deep neural net)\n    model.add(Dense(units=dense_units,\n              activation=activation[1], kernel_regularizer=kernel_regularizer))\n    # Add layer dropout\n    model.add(Dropout(dropout_rate))\n    # Compile the model and optimize on mean squared error\n    model.compile(loss='mean_squared_error', optimizer='sgd')\n    return model\n\n# Training and evaluating a GRU-based model\nmodel = create_GRU(hidden_units=5, dense_units=1, input_shape=(lag, Xtrain.shape[-1]),\n                   activation=['tanh', 'relu'], dropout_rate = 0.2)\nhistory = model.fit(Xtrain, Ytrain, epochs=20, batch_size=1, verbose=0)\nplot_model(history, 'GRU Model')\n\nyhat_gru = [x[0] for x in model.predict(Xval, verbose=0)]\n\ntrain_predict = model.predict(Xtrain, verbose=0)\ntest_predict = model.predict(Xval, verbose=0)\n\n# Print error\ntrain_rmse, test_rmse = print_error(Ytrain, Yval, train_predict, test_predict)\nrmse_table['model'].append('GRU Neural Network')\nrmse_table['training_rmse'].append(train_rmse)\nrmse_table['testing_rmse'].append(test_rmse)\n\n\nTrain RMSE: 0.998 RMSE\nTest RMSE: 3.118 RMSE\n\n\n\n\n\n\n\n\n\nCode\n# Training and evaluating a GRU-based model with regularization\nmodel = create_GRU(hidden_units=5, dense_units=1, input_shape=(lag, Xtrain.shape[-1]),\n                   activation=['tanh', 'relu'],dropout_rate = 0.2,  kernel_regularizer=regularizers.L1L2(l1=1e-5, l2=1e-4))\nhistory = model.fit(Xtrain, Ytrain, epochs=20, batch_size=1, verbose=0)\nplot_model(history, 'GRU Model (with L1L2 Regularization)')\n\nyhat_gru_reg = [x[0] for x in model.predict(Xval, verbose=0)]\n\ntrain_predict = model.predict(Xtrain, verbose=0)\ntest_predict = model.predict(Xval, verbose=0)\n\n# Print error\ntrain_rmse, test_rmse = print_error(Ytrain, Yval, train_predict, test_predict)\nrmse_table['model'].append('GRU Neural Network (with L1L2 Regularization)')\nrmse_table['training_rmse'].append(train_rmse)\nrmse_table['testing_rmse'].append(test_rmse)\n\n\nTrain RMSE: 0.724 RMSE\nTest RMSE: 1.173 RMSE\n\n\n\n\n\n\n\n\nThe two plots show the training loss for the GRU model, one with regularization and one without. The plot with regularization is similar to without regularization plot. The model without regularization had a train RMSE of 0.998 and a test RMSE of 3.118, the values are the same for with regularization too.\n\n\nLSTM Neural Network\nThe LSTM (Long Short-Term Memory) neural network is another powerful machine learning tool that has been widely adopted in various applications, particularly in tasks involving sequential data.The LSTM neural network architecture typically consists of multiple LSTM layers, followed by one or more dense layers and an activation function such as the sigmoid or hyperbolic tangent. Similar to the GRU neural network, the LSTM model may also include kernel regularization to address the issue of overfitting. In order to evaluate the performance of the LSTM model, the model can be trained on a dataset for a specified number of epochs, and validation loss plots can be generated to compare the performance of the regularized and non-regularized models.\n\nLSTMLSTM with Regularization\n\n\n\n\nCode\n# Create a LSTM Neural Network\ndef create_LSTM(hidden_units, dense_units, input_shape, activation,dropout_rate = 0.2, kernel_regularizer=None):\n    model = Sequential()\n    # Create a simple long short term memory neural network\n    model.add(LSTM(hidden_units,\n              activation=activation[0], input_shape=input_shape))\n    # Add a dense layer (only one, more layers would make it a deep neural net)\n    model.add(Dense(units=dense_units,\n              activation=activation[1], kernel_regularizer=kernel_regularizer))\n    # Add layer dropout\n    model.add(Dropout(dropout_rate))\n    # Compile the model and optimize on mean squared error\n    model.compile(optimizer=\"RMSprop\", loss='mae')\n    return model\n\n# Create an LSTM neural network\nmodel = create_LSTM(hidden_units=5, dense_units=1, input_shape=(lag, Xtrain.shape[-1]),\n                    activation=['tanh', 'linear'], dropout_rate = 0.2)\nhistory = model.fit(Xtrain, Ytrain, epochs=20, batch_size=1, verbose=0)\nplot_model(history, 'LSTM Model')\n\nyhat_lstm = [x[0] for x in model.predict(Xval, verbose=0)]\n\ntrain_predict = model.predict(Xtrain, verbose=0)\ntest_predict = model.predict(Xval, verbose=0)\n\n# Print error\ntrain_rmse, test_rmse = print_error(Ytrain, Yval, train_predict, test_predict)\nrmse_table['model'].append('LSTM Neural Network')\nrmse_table['training_rmse'].append(train_rmse)\nrmse_table['testing_rmse'].append(test_rmse)\n\n\nTrain RMSE: 0.213 RMSE\nTest RMSE: 1.076 RMSE\n\n\n\n\n\n\n\n\n\nCode\n# Create an LSTM neural network with regularization\nmodel = create_LSTM(hidden_units=5, dense_units=1, input_shape=(lag, Xtrain.shape[-1]),\n                    activation=['tanh', 'linear'], dropout_rate = 0.2, kernel_regularizer=regularizers.L1L2(l1=1e-5, l2=1e-4))\nhistory = model.fit(Xtrain, Ytrain, epochs=20, batch_size=1, verbose=0)\nplot_model(history, 'LSTM Model (with L1L2 Regularization)')\n\n\nyhat_lstm_reg = [x[0] for x in model.predict(Xval, verbose=0)]\n\ntrain_predict = model.predict(Xtrain, verbose=0)\ntest_predict = model.predict(Xval, verbose=0)\n\n# Print error\ntrain_rmse, test_rmse = print_error(Ytrain, Yval, train_predict, test_predict)\nrmse_table['model'].append('LSTM Neural Network (with L1L2 Regularization)')\nrmse_table['training_rmse'].append(train_rmse)\nrmse_table['testing_rmse'].append(test_rmse)\n\n\nTrain RMSE: 0.208 RMSE\nTest RMSE: 1.063 RMSE\n\n\n\n\n\n\n\n\nThe two plots show the training loss for the LSTM model, one with regularization and one without. The plot with regularization shows less fluctuation, indicating that overfitting has been reduced. The model without regularization had a train RMSE of 0.206 and a test RMSE of 1.229, while the model with regularization had a slightly lower for both the train and test RMSE values which are 0.202 and 1.002 respectively.\n\n\nForecast\n\n\nCode\n# Creating the frame to store both predictions\ndays = df['Date'].values[-len(y):]\nframe = pd.concat([\n    pd.DataFrame({'day': days, 'price': y, 'type': 'original'}),\n    pd.DataFrame({'day': days, 'price': yhat_d, 'type': 'rnn_forecast'}),\n    pd.DataFrame({'day': days, 'price': yhat_gru, 'type': 'gru_forecast'}),\n    pd.DataFrame({'day': days, 'price': yhat_lstm, 'type': 'lstm_forecast'})\n])\n# Creating the unscaled values column\nframe['price_absolute'] = [(x * train_std) + train_mean for x in frame['price']]\n# Pivoting\npivoted = frame.pivot_table(index='day', columns='type')\npivoted.columns = ['_'.join(x).strip() for x in pivoted.columns.values]\n\nplt.figure(figsize=(12, 10))\nplt.plot(pivoted.index, pivoted.price_absolute_original,\n         color='#8B1874', label='original')\nplt.plot(pivoted.index, pivoted.price_absolute_rnn_forecast,\n         color='#F79540', label='RNN Forecast', alpha=0.6)\nplt.plot(pivoted.index, pivoted.price_absolute_gru_forecast,\n         color='#B71375', label='GRU Forecast', alpha=0.6)\nplt.plot(pivoted.index, pivoted.price_absolute_lstm_forecast,\n         color='#FC4F00', label='LSTM Forecast', alpha=0.6)\nplt.title('Health Care Sector Fund Forecasts')\nplt.legend()\nplt.show()\n\n\n\n\n\nWhen comparing the predictions of the deep learning models with the original time series plot, we can observe that the LSTM model is the closest to the actual values. The GRU model and RNN model are pretty far from the actual plot, which indicates that LSTM model is better when compared to others, but when we look at the LSTM model, the model is not accurate, i.e., it is not the same as the actual plot.\n\n\nModel Comparison\n\n\nCode\nrmse_df = pd.DataFrame(rmse_table)\nrmse_df\n\n\n\n\n\n\n  \n    \n      \n      model\n      training_rmse\n      testing_rmse\n    \n  \n  \n    \n      0\n      Recurrent Neural Network\n      0.270662\n      2.160771\n    \n    \n      1\n      Recurrent Neural Network (with L1L2 Regulariza...\n      0.268292\n      2.160602\n    \n    \n      2\n      GRU Neural Network\n      0.998383\n      3.118396\n    \n    \n      3\n      GRU Neural Network (with L1L2 Regularization)\n      0.724464\n      1.172650\n    \n    \n      4\n      LSTM Neural Network\n      0.213024\n      1.075922\n    \n    \n      5\n      LSTM Neural Network (with L1L2 Regularization)\n      0.208096\n      1.063297\n    \n  \n\n\n\n\nIn terms of performance on the testing dataset, the LSTM Neural Network with L1L2 regularization appears to be the best performing model, with a testing_rmse of 1.001920. This is followed by the regularized RNN with a testing_rmse of 2.160685, the non-regularized LSTM with a testing_rmse of 1.228607, the non-regularized RNN with a testing_rmse of 2.160200, and finally, the regularized GRU and non-regularized GRU, both with a testing_rmse of 3.118396.\nIt is important to note that regularization can help prevent overfitting, which is when a model fits the training data too closely and does not generalize well to new data. This is likely why the regularized models tend to perform better on the testing dataset than their non-regularized counterparts.\nOverall, the LSTM Neural Network with L1L2 regularization appears to be the best-performing model for this particular dataset, followed closely by the regularized RNN.\n\n\nComparison of deep learning models with traditional single variable time series\nThe deep learning models (RNN, GRU, LSTM) perform better than the ARIMA model in both training and testing RMSE values for the Health Care Sector Fund. Among the deep learning models, the LSTM model without regularization has the lowest testing RMSE value of 0.709320, when compared to ARIMA Model which is 0.8014312, indicating its better performance in predicting the market prices of the Health Care Sector Fund compared to the other deep learning models and ARIMA model."
  },
  {
    "objectID": "deep_learning_sector_xlu.html",
    "href": "deep_learning_sector_xlu.html",
    "title": "",
    "section": "",
    "text": "Deep Learning for Utilities Sector Fund\nThe Utilities Sector Fund is a group of stocks that includes companies that provide basic services such as electricity, gas, and water. These companies are known for their stability and consistent dividend payments, making them attractive to investors seeking a steady income stream. As with other stock market, predicting the future movements of the Utilities Sector Fund can be challenging due to the complex and dynamic nature of the market. These models range from traditional time-series models to more advanced deep learning models. By leveraging the power of Deep Learning and applying techniques such as RNN, LSTM, GRU, and regularization, we can create a model that accurately predicts the future movements of the sector. Such a model can provide valuable insights for investors and traders looking to make informed decisions in the stock market, and help them stay ahead of the curve in the ever-changing landscape of the technology and biotech industries.\nIn order to apply the deep learning methods, the time series data is first split into training and testing sets, taking into account the time component. The training data constitutes 75% of the total data while the rest is the testing data. Furthermore, to ensure that the values are on the same scale, the data is scaled, which is crucial for deep learning models. The mean and standard deviation of the training data are computed and then utilized to normalize both the training and testing sets. This approach ensures that the models are trained on a standardized dataset, enabling them to make accurate predictions.\n\n\nCode\n# libraries\nimport numpy as np\nimport pandas as pd\nimport math\nfrom sklearn.metrics import mean_squared_error\nfrom matplotlib import pyplot as plt\nfrom keras.models import Sequential\nfrom keras.layers import Dropout\nfrom keras.layers import Dense, SimpleRNN, LSTM, GRU\nfrom keras import regularizers\nimport warnings\nwarnings.filterwarnings('ignore')\n\n# import data\ndf = pd.read_csv('./DATA/CLEANED DATA/XLU_raw_data.csv')\n# clean data and get final data for anlaysis\ndf = df[['Date', 'XLU.Adjusted']]\ndf['Date'] = pd.to_datetime(df['Date'])\ndf['XLU.Adjusted'] = pd.to_numeric(df['XLU.Adjusted'], errors='coerce')\ndf = df.dropna(subset=['XLU.Adjusted'])\ndf.sort_values('Date', inplace=True, ascending=True)\ndf = df.reset_index(drop=True)\ndef create_X_Y(ts: np.array, lag=1, n_ahead=1, target_index=0) -> tuple:\n    \"\"\"\n    A method to create X and Y matrix from a time series array for the training of \n    deep learning models \n    \"\"\"\n    # Extracting the number of features that are passed from the array\n    n_features = ts.shape[1]\n\n    # Creating placeholder lists\n    X, Y = [], []\n\n    if len(ts) - lag <= 0:\n        X.append(ts)\n    else:\n        for i in range(len(ts) - lag - n_ahead):\n            Y.append(ts[(i + lag):(i + lag + n_ahead), target_index])\n            X.append(ts[i:(i + lag)])\n\n    X, Y = np.array(X), np.array(Y)\n\n    # Reshaping the X array to an RNN input shape\n    X = np.reshape(X, (X.shape[0], lag, n_features))\n\n    return X, Y\n\nts = df[['XLU.Adjusted']].values\n\nnrows = ts.shape[0]\ntest_share = 0.25\n# Spliting into train and test sets\ntrain = ts[0:int(nrows * (1 - test_share))]\ntest = ts[int(nrows * (1 - test_share)):]\n\n# Scaling the data\ntrain_mean = train.mean()\ntrain_std = train.std()\ntrain = (train - train_mean) / train_std\ntest = (test - train_mean) / train_std\n\n# Creating the final scaled frame\nts_s = np.concatenate([train, test])\n\nlag = 12\nahead = 3\n\n# Creating the X and Y for training\nX, Y = create_X_Y(ts_s, lag=lag, n_ahead=ahead)\n\nXtrain = X[0:int(X.shape[0] * (1 - test_share))]\nYtrain = Y[0:int(X.shape[0] * (1 - test_share))]\n\nXval = X[int(X.shape[0] * (1 - test_share)):]\nYval = Y[int(X.shape[0] * (1 - test_share)):]\n\n\n\n\ndef plot_model(history, model_title):\n    loss = history.history['loss']\n    epochs = range(1, len(loss) + 1)\n    plt.figure()\n    plt.plot(epochs, loss, 'b', label='Training loss', color = \"gray\")\n    plt.title(f'{model_title} Training loss ')\n    plt.legend()\n    \n\n\ndef print_error(trainY, testY, train_predict, test_predict):\n    # Error of predictions\n    train_rmse = math.sqrt(mean_squared_error(\n        trainY[:, 0], train_predict[:, 0]))\n    test_rmse = math.sqrt(mean_squared_error(testY[:, 0], test_predict[:, 0]))\n    # Print RMSE\n    print('Train RMSE: %.3f RMSE' % (train_rmse))\n    print('Test RMSE: %.3f RMSE' % (test_rmse))\n    return train_rmse, test_rmse\n\n\n\n\nRecurrent Neural Network\nThe RNN models that are constructed and trained using TensorFlow and Keras. This model is trained for five hidden layers, one dense layer, and the activation function hyperbolic tangent. A dropout rate of 0.2 is used to reduce the risk of overfitting. The model are trained for 20 epochs and generated validation loss plots to compare the performance of the regularized and non-regularized models. The second RNN model was identical to the first, except for the addition of kernel regularization.\n\nRNNRNN with Regularization\n\n\n\n\nCode\n# Create a RNN\ndef create_RNN(hidden_units, dense_units, input_shape, activation,dropout_rate=0.2,kernel_regularizer=None):\n    model = Sequential()\n    # Create a simple neural network layer\n    model.add(SimpleRNN(hidden_units, input_shape=input_shape,\n              activation=activation[0]))\n    # Add a dense layer (only one, more layers would make it a deep neural net)\n    model.add(Dense(units=dense_units,\n              activation=activation[1],\n              kernel_regularizer=kernel_regularizer))\n    # Add layer dropout\n    model.add(Dropout(dropout_rate))\n    \n    # Compile the model and optimize on mean squared error\n    model.compile(loss='mean_squared_error', optimizer='adam')\n    return model\n  \n# Create a recurrent neural network\nmodel = create_RNN(hidden_units=5, dense_units=1, input_shape=(lag, Xtrain.shape[-1]),\n                   activation=['tanh', 'tanh'], dropout_rate = 0.2)\nhistory = model.fit(Xtrain, Ytrain, epochs=20, batch_size=1, verbose=0,validation_data=(Xval, Yval))\nplot_model(history, 'Recurrent Neural Network Model')\n\nyhat_d = [x[0] for x in model.predict(Xval, verbose=0)]\ny = [y[0] for y in Yval]\n\ntrain_predict = model.predict(Xtrain, verbose=0)\ntest_predict = model.predict(Xval, verbose=0)\n\n# Print error\ntrain_rmse, test_rmse = print_error(Ytrain, Yval, train_predict, test_predict)\nrmse_table = {\n    'model': ['Recurrent Neural Network'],\n    'training_rmse': [train_rmse],\n    'testing_rmse': [test_rmse]\n}\n\n\nTrain RMSE: 0.325 RMSE\nTest RMSE: 1.884 RMSE\n\n\n\n\n\n\n\n\n\nCode\n# Create a recurrent neural network with regularization\n\nmodel = create_RNN(hidden_units=5, dense_units=1, input_shape=(lag, Xtrain.shape[-1]),\n                   activation=['tanh', 'tanh'], kernel_regularizer=regularizers.L1L2(l1=1e-5, l2=1e-4), dropout_rate = 0.2)\nhistory = model.fit(Xtrain, Ytrain, epochs=20, batch_size=1, verbose=0, validation_data=(Xval, Yval))\nplot_model(history, 'Recurrent Neural Network Model (with L1L2 Regularization)')\n\nyhat_d_reg = [x[0] for x in model.predict(Xval, verbose=0)]\n\ntrain_predict = model.predict(Xtrain, verbose=0)\ntest_predict = model.predict(Xval, verbose=0)\n\n# Print error\ntrain_rmse, test_rmse = print_error(Ytrain, Yval, train_predict, test_predict)\nrmse_table['model'].append(\n    'Recurrent Neural Network (with L1L2 Regularization)')\nrmse_table['training_rmse'].append(train_rmse)\nrmse_table['testing_rmse'].append(test_rmse)\n\n\nTrain RMSE: 0.328 RMSE\nTest RMSE: 1.884 RMSE\n\n\n\n\n\n\n\n\nThe second plot illustrates the impact of regularization on the training loss. Compared to the original plot, there are significant differences. The regularization technique had a substantial effect on the testing error of the model. The model trained without regularization had a slightly lower training error of 0.328 RMSE but a considerably higher testing error of 1.883 RMSE, the RMSE values for with regularization are 0.330 and 1.884 which are slightly higher than without regularization.\n\n\nGRU Neural Network\nA GRU neural network is a powerful machine learning tool widely adopted in various applications. Its key feature is the recurrent gated unit, a type of memory cell that can store information over a data sequence. The GRU neural network is five hidden layers, one dense layer, and the hyperbolic tangent activation function. The model also includes kernel regularization to help address the issue of overfitting. The models are trained for 20 epochs and validation loss plots were generated to compare the performance of the regularized and non-regularized models. Overall, the GRU neural network has shown great promise in accurately predicting future movements in the sector.\n\nGRUGRU with Regularization\n\n\n\n\nCode\n# Create a GRU Neural Network\ndef create_GRU(hidden_units, dense_units, input_shape, activation,dropout_rate=0.2, kernel_regularizer=None):\n    model = Sequential()\n    # Create a simple GRU neural network layer\n    model.add(GRU(hidden_units, input_shape=input_shape,\n              activation=activation[0]))\n    # Add a dense layer (only one, more layers would make it a deep neural net)\n    model.add(Dense(units=dense_units,\n              activation=activation[1], kernel_regularizer=kernel_regularizer))\n    # Add layer dropout\n    model.add(Dropout(dropout_rate))\n    # Compile the model and optimize on mean squared error\n    model.compile(loss='mean_squared_error', optimizer='sgd')\n    return model\n\n# Training and evaluating a GRU-based model\nmodel = create_GRU(hidden_units=5, dense_units=1, input_shape=(lag, Xtrain.shape[-1]),\n                   activation=['tanh', 'relu'], dropout_rate = 0.2)\nhistory = model.fit(Xtrain, Ytrain, epochs=20, batch_size=1, verbose=0)\nplot_model(history, 'GRU Model')\n\nyhat_gru = [x[0] for x in model.predict(Xval, verbose=0)]\n\ntrain_predict = model.predict(Xtrain, verbose=0)\ntest_predict = model.predict(Xval, verbose=0)\n\n# Print error\ntrain_rmse, test_rmse = print_error(Ytrain, Yval, train_predict, test_predict)\nrmse_table['model'].append('GRU Neural Network')\nrmse_table['training_rmse'].append(train_rmse)\nrmse_table['testing_rmse'].append(test_rmse)\n\n\nTrain RMSE: 0.999 RMSE\nTest RMSE: 2.845 RMSE\n\n\n\n\n\n\n\n\n\nCode\n# Training and evaluating a GRU-based model with regularization\nmodel = create_GRU(hidden_units=5, dense_units=1, input_shape=(lag, Xtrain.shape[-1]),\n                   activation=['tanh', 'relu'],dropout_rate = 0.2,  kernel_regularizer=regularizers.L1L2(l1=1e-5, l2=1e-4))\nhistory = model.fit(Xtrain, Ytrain, epochs=20, batch_size=1, verbose=0)\nplot_model(history, 'GRU Model (with L1L2 Regularization)')\n\nyhat_gru_reg = [x[0] for x in model.predict(Xval, verbose=0)]\n\ntrain_predict = model.predict(Xtrain, verbose=0)\ntest_predict = model.predict(Xval, verbose=0)\n\n# Print error\ntrain_rmse, test_rmse = print_error(Ytrain, Yval, train_predict, test_predict)\nrmse_table['model'].append('GRU Neural Network (with L1L2 Regularization)')\nrmse_table['training_rmse'].append(train_rmse)\nrmse_table['testing_rmse'].append(test_rmse)\n\n\nTrain RMSE: 0.999 RMSE\nTest RMSE: 2.847 RMSE\n\n\n\n\n\n\n\n\nThe two plots show the training loss for the GRU model, one with regularization and one without. The plot with regularization is similar to without regularization plot. The model without regularization had a train RMSE of 0.999 and a test RMSE of 2.847, the values for with regularization are lower than without regularization which are regularization 0.681 and 0.793 respectively.\n\n\nLSTM Neural Network\nThe LSTM (Long Short-Term Memory) neural network is another powerful machine learning tool that has been widely adopted in various applications, particularly in tasks involving sequential data.The LSTM neural network architecture typically consists of multiple LSTM layers, followed by one or more dense layers and an activation function such as the sigmoid or hyperbolic tangent. Similar to the GRU neural network, the LSTM model may also include kernel regularization to address the issue of overfitting. In order to evaluate the performance of the LSTM model, the model can be trained on a dataset for a specified number of epochs, and validation loss plots can be generated to compare the performance of the regularized and non-regularized models.\n\nLSTMLSTM with Regularization\n\n\n\n\nCode\n# Create a LSTM Neural Network\ndef create_LSTM(hidden_units, dense_units, input_shape, activation,dropout_rate = 0.2, kernel_regularizer=None):\n    model = Sequential()\n    # Create a simple long short term memory neural network\n    model.add(LSTM(hidden_units,\n              activation=activation[0], input_shape=input_shape))\n    # Add a dense layer (only one, more layers would make it a deep neural net)\n    model.add(Dense(units=dense_units,\n              activation=activation[1], kernel_regularizer=kernel_regularizer))\n    # Add layer dropout\n    model.add(Dropout(dropout_rate))\n    # Compile the model and optimize on mean squared error\n    model.compile(optimizer=\"RMSprop\", loss='mae')\n    return model\n\n# Create an LSTM neural network\nmodel = create_LSTM(hidden_units=5, dense_units=1, input_shape=(lag, Xtrain.shape[-1]),\n                    activation=['tanh', 'linear'], dropout_rate = 0.2)\nhistory = model.fit(Xtrain, Ytrain, epochs=20, batch_size=1, verbose=0)\nplot_model(history, 'LSTM Model')\n\nyhat_lstm = [x[0] for x in model.predict(Xval, verbose=0)]\n\ntrain_predict = model.predict(Xtrain, verbose=0)\ntest_predict = model.predict(Xval, verbose=0)\n\n# Print error\ntrain_rmse, test_rmse = print_error(Ytrain, Yval, train_predict, test_predict)\nrmse_table['model'].append('LSTM Neural Network')\nrmse_table['training_rmse'].append(train_rmse)\nrmse_table['testing_rmse'].append(test_rmse)\n\n\nTrain RMSE: 0.209 RMSE\nTest RMSE: 0.783 RMSE\n\n\n\n\n\n\n\n\n\nCode\n# Create an LSTM neural network with regularization\nmodel = create_LSTM(hidden_units=5, dense_units=1, input_shape=(lag, Xtrain.shape[-1]),\n                    activation=['tanh', 'linear'], dropout_rate = 0.2, kernel_regularizer=regularizers.L1L2(l1=1e-5, l2=1e-4))\nhistory = model.fit(Xtrain, Ytrain, epochs=20, batch_size=1, verbose=0)\nplot_model(history, 'LSTM Model (with L1L2 Regularization)')\n\n\nyhat_lstm_reg = [x[0] for x in model.predict(Xval, verbose=0)]\n\ntrain_predict = model.predict(Xtrain, verbose=0)\ntest_predict = model.predict(Xval, verbose=0)\n\n# Print error\ntrain_rmse, test_rmse = print_error(Ytrain, Yval, train_predict, test_predict)\nrmse_table['model'].append('LSTM Neural Network (with L1L2 Regularization)')\nrmse_table['training_rmse'].append(train_rmse)\nrmse_table['testing_rmse'].append(test_rmse)\n\n\nTrain RMSE: 0.192 RMSE\nTest RMSE: 0.730 RMSE\n\n\n\n\n\n\n\n\nThe two plots show the training loss for the LSTM model, one with regularization and one without. The plot with regularization shows less fluctuation, indicating that overfitting has been reduced. The model without regularization had a train RMSE of 0.180 and a test RMSE of 0.742, while the model with regularization had a slightly higher for both the train and test RMSE values which are 0.204 and 0.906 respectively.\n\n\nForecast\n\n\nCode\n# Creating the frame to store both predictions\ndays = df['Date'].values[-len(y):]\nframe = pd.concat([\n    pd.DataFrame({'day': days, 'price': y, 'type': 'original'}),\n    pd.DataFrame({'day': days, 'price': yhat_d, 'type': 'rnn_forecast'}),\n    pd.DataFrame({'day': days, 'price': yhat_gru, 'type': 'gru_forecast'}),\n    pd.DataFrame({'day': days, 'price': yhat_lstm, 'type': 'lstm_forecast'})\n])\n# Creating the unscaled values column\nframe['price_absolute'] = [(x * train_std) + train_mean for x in frame['price']]\n# Pivoting\npivoted = frame.pivot_table(index='day', columns='type')\npivoted.columns = ['_'.join(x).strip() for x in pivoted.columns.values]\n\nplt.figure(figsize=(12, 10))\nplt.plot(pivoted.index, pivoted.price_absolute_original,\n         color='#8B1874', label='original')\nplt.plot(pivoted.index, pivoted.price_absolute_rnn_forecast,\n         color='#F79540', label='RNN Forecast', alpha=0.6)\nplt.plot(pivoted.index, pivoted.price_absolute_gru_forecast,\n         color='#B71375', label='GRU Forecast', alpha=0.6)\nplt.plot(pivoted.index, pivoted.price_absolute_lstm_forecast,\n         color='#FC4F00', label='LSTM Forecast', alpha=0.6)\nplt.title('Utilities Sector Fund Forecasts')\nplt.legend()\nplt.show()\n\n\n\n\n\nWhen comparing the predictions of the deep learning models with the original time series plot, we can observe that the LSTM model is the closest to the actual values. The GRU model and RNN model are pretty far from the actual plot, which indicates that LSTM model is better when compared to others, but when we look at the LSTM model, the model is not accurate, i.e., it is not the same as the actual plot.\n\n\nModel Comparison\n\n\nCode\nrmse_df = pd.DataFrame(rmse_table)\nrmse_df\n\n\n\n\n\n\n  \n    \n      \n      model\n      training_rmse\n      testing_rmse\n    \n  \n  \n    \n      0\n      Recurrent Neural Network\n      0.324610\n      1.884150\n    \n    \n      1\n      Recurrent Neural Network (with L1L2 Regulariza...\n      0.327919\n      1.883760\n    \n    \n      2\n      GRU Neural Network\n      0.998522\n      2.845417\n    \n    \n      3\n      GRU Neural Network (with L1L2 Regularization)\n      0.998522\n      2.846800\n    \n    \n      4\n      LSTM Neural Network\n      0.209249\n      0.783387\n    \n    \n      5\n      LSTM Neural Network (with L1L2 Regularization)\n      0.191602\n      0.729893\n    \n  \n\n\n\n\nBased on the given RMSE values, the LSTM Neural Network (with or without L1L2 regularization) appears to be the best model for predicting the market price of the Utilities Sector Fund. The model with L1L2 regularization has a slightly higher training RMSE but a lower testing RMSE, indicating better generalization performance. Therefore, adding regularization to the model can help to reduce overfitting and improve its prediction accuracy.\n\n\nComparison of deep learning models with traditional single variable time series\nThe deep learning models (RNN, GRU, LSTM) perform better than the ARIMA model in both training and testing RMSE values for the Utilities Sector Fund. Among the deep learning models, the LSTM model without regularization has the lowest testing RMSE, when compared to ARIMA Model which is 0.8014312, indicating its better performance in predicting the market prices of the Utilities Sector Fund compared to the other deep learning models and ARIMA model."
  },
  {
    "objectID": "deep_learning_sector_xlre.html",
    "href": "deep_learning_sector_xlre.html",
    "title": "",
    "section": "",
    "text": "Deep Learning for Real Estate Sector Fund\nThe Real Estate sector comprises companies that provide non-essential goods and services, such as clothing, entertainment, and luxury items. These companies tend to be more sensitive to economic cycles and consumer sentiment than companies in other sectors. As with other stock market, predicting the future movements of the Real Estate Sector Fund can be challenging due to the complex and dynamic nature of the market. These models range from traditional time-series models to more advanced deep learning models. By leveraging the power of Deep Learning and applying techniques such as RNN, LSTM, GRU, and regularization, we can create a model that accurately predicts the future movements of the sector. Such a model can provide valuable insights for investors and traders looking to make informed decisions in the stock market, and help them stay ahead of the curve in the ever-changing landscape of the technology and biotech industries.\nIn order to apply the deep learning methods, the time series data is first split into training and testing sets, taking into account the time component. The training data constitutes 75% of the total data while the rest is the testing data. Furthermore, to ensure that the values are on the same scale, the data is scaled, which is crucial for deep learning models. The mean and standard deviation of the training data are computed and then utilized to normalize both the training and testing sets. This approach ensures that the models are trained on a standardized dataset, enabling them to make accurate predictions.\n\n\nCode\n# libraries\nimport numpy as np\nimport pandas as pd\nimport math\nfrom sklearn.metrics import mean_squared_error\nfrom matplotlib import pyplot as plt\nfrom keras.models import Sequential\nfrom keras.layers import Dropout\nfrom keras.layers import Dense, SimpleRNN, LSTM, GRU\nfrom keras import regularizers\nimport warnings\nwarnings.filterwarnings('ignore')\n\n# import data\ndf = pd.read_csv('./DATA/CLEANED DATA/XLRE_raw_data.csv')\n# clean data and get final data for anlaysis\ndf = df[['Date', 'XLRE.Adjusted']]\ndf['Date'] = pd.to_datetime(df['Date'])\ndf['XLRE.Adjusted'] = pd.to_numeric(df['XLRE.Adjusted'], errors='coerce')\ndf = df.dropna(subset=['XLRE.Adjusted'])\ndf.sort_values('Date', inplace=True, ascending=True)\ndf = df.reset_index(drop=True)\ndef create_X_Y(ts: np.array, lag=1, n_ahead=1, target_index=0) -> tuple:\n    \"\"\"\n    A method to create X and Y matrix from a time series array for the training of \n    deep learning models \n    \"\"\"\n    # Extracting the number of features that are passed from the array\n    n_features = ts.shape[1]\n\n    # Creating placeholder lists\n    X, Y = [], []\n\n    if len(ts) - lag <= 0:\n        X.append(ts)\n    else:\n        for i in range(len(ts) - lag - n_ahead):\n            Y.append(ts[(i + lag):(i + lag + n_ahead), target_index])\n            X.append(ts[i:(i + lag)])\n\n    X, Y = np.array(X), np.array(Y)\n\n    # Reshaping the X array to an RNN input shape\n    X = np.reshape(X, (X.shape[0], lag, n_features))\n\n    return X, Y\n\nts = df[['XLRE.Adjusted']].values\nnp.random.seed(123)\nnrows = ts.shape[0]\ntest_share = 0.25\n# Spliting into train and test sets\ntrain = ts[0:int(nrows * (1 - test_share))]\ntest = ts[int(nrows * (1 - test_share)):]\n\n# Scaling the data\ntrain_mean = train.mean()\ntrain_std = train.std()\ntrain = (train - train_mean) / train_std\ntest = (test - train_mean) / train_std\n\n# Creating the final scaled frame\nts_s = np.concatenate([train, test])\n\nlag = 12\nahead = 3\n\n# Creating the X and Y for training\nX, Y = create_X_Y(ts_s, lag=lag, n_ahead=ahead)\n\nXtrain = X[0:int(X.shape[0] * (1 - test_share))]\nYtrain = Y[0:int(X.shape[0] * (1 - test_share))]\n\nXval = X[int(X.shape[0] * (1 - test_share)):]\nYval = Y[int(X.shape[0] * (1 - test_share)):]\n\n\n\n\ndef plot_model(history, model_title):\n    loss = history.history['loss']\n    epochs = range(1, len(loss) + 1)\n    plt.figure()\n    plt.plot(epochs, loss, 'b', label='Training loss', color = \"gray\")\n    plt.title(f'{model_title} Training loss ')\n    plt.legend()\n    \n\n\ndef print_error(trainY, testY, train_predict, test_predict):\n    # Error of predictions\n    train_rmse = math.sqrt(mean_squared_error(\n        trainY[:, 0], train_predict[:, 0]))\n    test_rmse = math.sqrt(mean_squared_error(testY[:, 0], test_predict[:, 0]))\n    # Print RMSE\n    print('Train RMSE: %.3f RMSE' % (train_rmse))\n    print('Test RMSE: %.3f RMSE' % (test_rmse))\n    return train_rmse, test_rmse\n\n\n\n\nRecurrent Neural Network\nThe RNN models that are constructed and trained using TensorFlow and Keras. This model is trained for five hidden layers, one dense layer, and the activation function hyperbolic tangent. A dropout rate of 0.2 is used to reduce the risk of overfitting. The model are trained for 20 epochs and generated validation loss plots to compare the performance of the regularized and non-regularized models. The second RNN model was identical to the first, except for the addition of kernel regularization.\n\nRNNRNN with Regularization\n\n\n\n\nCode\n# Create a RNN\ndef create_RNN(hidden_units, dense_units, input_shape, activation,dropout_rate=0.2,kernel_regularizer=None):\n    model = Sequential()\n    # Create a simple neural network layer\n    model.add(SimpleRNN(hidden_units, input_shape=input_shape,\n              activation=activation[0]))\n    # Add a dense layer (only one, more layers would make it a deep neural net)\n    model.add(Dense(units=dense_units,\n              activation=activation[1],\n              kernel_regularizer=kernel_regularizer))\n    # Add layer dropout\n    model.add(Dropout(dropout_rate))\n    \n    # Compile the model and optimize on mean squared error\n    model.compile(loss='mean_squared_error', optimizer='adam')\n    return model\n  \n# Create a recurrent neural network\nmodel = create_RNN(hidden_units=5, dense_units=1, input_shape=(lag, Xtrain.shape[-1]),\n                   activation=['tanh', 'tanh'], dropout_rate = 0.2)\nhistory = model.fit(Xtrain, Ytrain, epochs=20, batch_size=1, verbose=0,validation_data=(Xval, Yval))\nplot_model(history, 'Recurrent Neural Network Model')\n\nyhat_d = [x[0] for x in model.predict(Xval, verbose=0)]\ny = [y[0] for y in Yval]\n\ntrain_predict = model.predict(Xtrain, verbose=0)\ntest_predict = model.predict(Xval, verbose=0)\n\n# Print error\ntrain_rmse, test_rmse = print_error(Ytrain, Yval, train_predict, test_predict)\nrmse_table = {\n    'model': ['Recurrent Neural Network'],\n    'training_rmse': [train_rmse],\n    'testing_rmse': [test_rmse]\n}\n\n\nTrain RMSE: 0.359 RMSE\nTest RMSE: 2.395 RMSE\n\n\n\n\n\n\n\n\n\nCode\n# Create a recurrent neural network with regularization\n\nmodel = create_RNN(hidden_units=5, dense_units=1, input_shape=(lag, Xtrain.shape[-1]),\n                   activation=['tanh', 'tanh'], kernel_regularizer=regularizers.L1L2(l1=1e-5, l2=1e-4), dropout_rate = 0.2)\nhistory = model.fit(Xtrain, Ytrain, epochs=20, batch_size=1, verbose=0, validation_data=(Xval, Yval))\nplot_model(history, 'Recurrent Neural Network Model (with L1L2 Regularization)')\n\nyhat_d_reg = [x[0] for x in model.predict(Xval, verbose=0)]\n\ntrain_predict = model.predict(Xtrain, verbose=0)\ntest_predict = model.predict(Xval, verbose=0)\n\n# Print error\ntrain_rmse, test_rmse = print_error(Ytrain, Yval, train_predict, test_predict)\nrmse_table['model'].append(\n    'Recurrent Neural Network (with L1L2 Regularization)')\nrmse_table['training_rmse'].append(train_rmse)\nrmse_table['testing_rmse'].append(test_rmse)\n\n\nTrain RMSE: 0.366 RMSE\nTest RMSE: 2.396 RMSE\n\n\n\n\n\n\n\n\nThe second plot illustrates the impact of regularization on the training loss. Compared to the original plot, there are significant differences. The regularization technique had a substantial effect on the testing error of the model. The model trained without regularization had a slightly lower for both training error of 0.347 RMSE and testing error of 2.394 RMSE when compared to with regularization.\n\n\nGRU Neural Network\nA GRU neural network is a powerful machine learning tool widely adopted in various applications. Its key feature is the recurrent gated unit, a type of memory cell that can store information over a data sequence. The GRU neural network is five hidden layers, one dense layer, and the hyperbolic tangent activation function. The model also includes kernel regularization to help address the issue of overfitting. The models are trained for 20 epochs and validation loss plots were generated to compare the performance of the regularized and non-regularized models. Overall, the GRU neural network has shown great promise in accurately predicting future movements in the sector.\n\nGRUGRU with Regularization\n\n\n\n\nCode\n# Create a GRU Neural Network\ndef create_GRU(hidden_units, dense_units, input_shape, activation,dropout_rate=0.2, kernel_regularizer=None):\n    model = Sequential()\n    # Create a simple GRU neural network layer\n    model.add(GRU(hidden_units, input_shape=input_shape,\n              activation=activation[0]))\n    # Add a dense layer (only one, more layers would make it a deep neural net)\n    model.add(Dense(units=dense_units,\n              activation=activation[1], kernel_regularizer=kernel_regularizer))\n    # Add layer dropout\n    model.add(Dropout(dropout_rate))\n    # Compile the model and optimize on mean squared error\n    model.compile(loss='mean_squared_error', optimizer='sgd')\n    return model\n\n# Training and evaluating a GRU-based model\nmodel = create_GRU(hidden_units=5, dense_units=1, input_shape=(lag, Xtrain.shape[-1]),\n                   activation=['tanh', 'relu'], dropout_rate = 0.2)\nhistory = model.fit(Xtrain, Ytrain, epochs=20, batch_size=1, verbose=0)\nplot_model(history, 'GRU Model')\n\nyhat_gru = [x[0] for x in model.predict(Xval, verbose=0)]\n\ntrain_predict = model.predict(Xtrain, verbose=0)\ntest_predict = model.predict(Xval, verbose=0)\n\n# Print error\ntrain_rmse, test_rmse = print_error(Ytrain, Yval, train_predict, test_predict)\nrmse_table['model'].append('GRU Neural Network')\nrmse_table['training_rmse'].append(train_rmse)\nrmse_table['testing_rmse'].append(test_rmse)\n\n\nTrain RMSE: 0.648 RMSE\nTest RMSE: 1.033 RMSE\n\n\n\n\n\n\n\n\n\nCode\n# Training and evaluating a GRU-based model with regularization\nmodel = create_GRU(hidden_units=5, dense_units=1, input_shape=(lag, Xtrain.shape[-1]),\n                   activation=['tanh', 'relu'],dropout_rate = 0.2,  kernel_regularizer=regularizers.L1L2(l1=1e-5, l2=1e-4))\nhistory = model.fit(Xtrain, Ytrain, epochs=20, batch_size=1, verbose=0)\nplot_model(history, 'GRU Model (with L1L2 Regularization)')\n\nyhat_gru_reg = [x[0] for x in model.predict(Xval, verbose=0)]\n\ntrain_predict = model.predict(Xtrain, verbose=0)\ntest_predict = model.predict(Xval, verbose=0)\n\n# Print error\ntrain_rmse, test_rmse = print_error(Ytrain, Yval, train_predict, test_predict)\nrmse_table['model'].append('GRU Neural Network (with L1L2 Regularization)')\nrmse_table['training_rmse'].append(train_rmse)\nrmse_table['testing_rmse'].append(test_rmse)\n\n\nTrain RMSE: 0.650 RMSE\nTest RMSE: 1.019 RMSE\n\n\n\n\n\n\n\n\nThe two plots show the training loss for the GRU model, one with regularization and one without. The plot with regularization is similar to without regularization plot. The model without regularization had a train RMSE of 0.997 and a test RMSE of 3.340, the values are the same as the values of with regularization.\n\n\nLSTM Neural Network\nThe LSTM (Long Short-Term Memory) neural network is another powerful machine learning tool that has been widely adopted in various applications, particularly in tasks involving sequential data.The LSTM neural network architecture typically consists of multiple LSTM layers, followed by one or more dense layers and an activation function such as the sigmoid or hyperbolic tangent. Similar to the GRU neural network, the LSTM model may also include kernel regularization to address the issue of overfitting. In order to evaluate the performance of the LSTM model, the model can be trained on a dataset for a specified number of epochs, and validation loss plots can be generated to compare the performance of the regularized and non-regularized models.\n\nLSTMLSTM with Regularization\n\n\n\n\nCode\n# Create a LSTM Neural Network\ndef create_LSTM(hidden_units, dense_units, input_shape, activation,dropout_rate = 0.2, kernel_regularizer=None):\n    model = Sequential()\n    # Create a simple long short term memory neural network\n    model.add(LSTM(hidden_units,\n              activation=activation[0], input_shape=input_shape))\n    # Add a dense layer (only one, more layers would make it a deep neural net)\n    model.add(Dense(units=dense_units,\n              activation=activation[1], kernel_regularizer=kernel_regularizer))\n    # Add layer dropout\n    model.add(Dropout(dropout_rate))\n    # Compile the model and optimize on mean squared error\n    model.compile(optimizer=\"RMSprop\", loss='mae')\n    return model\n\n# Create an LSTM neural network\nmodel = create_LSTM(hidden_units=5, dense_units=1, input_shape=(lag, Xtrain.shape[-1]),\n                    activation=['tanh', 'linear'], dropout_rate = 0.2)\nhistory = model.fit(Xtrain, Ytrain, epochs=20, batch_size=1, verbose=0)\nplot_model(history, 'LSTM Model')\n\nyhat_lstm = [x[0] for x in model.predict(Xval, verbose=0)]\n\ntrain_predict = model.predict(Xtrain, verbose=0)\ntest_predict = model.predict(Xval, verbose=0)\n\n# Print error\ntrain_rmse, test_rmse = print_error(Ytrain, Yval, train_predict, test_predict)\nrmse_table['model'].append('LSTM Neural Network')\nrmse_table['training_rmse'].append(train_rmse)\nrmse_table['testing_rmse'].append(test_rmse)\n\n\nTrain RMSE: 0.221 RMSE\nTest RMSE: 0.991 RMSE\n\n\n\n\n\n\n\n\n\nCode\n# Create an LSTM neural network with regularization\nmodel = create_LSTM(hidden_units=5, dense_units=1, input_shape=(lag, Xtrain.shape[-1]),\n                    activation=['tanh', 'linear'], dropout_rate = 0.2, kernel_regularizer=regularizers.L1L2(l1=1e-5, l2=1e-4))\nhistory = model.fit(Xtrain, Ytrain, epochs=20, batch_size=1, verbose=0)\nplot_model(history, 'LSTM Model (with L1L2 Regularization)')\n\n\nyhat_lstm_reg = [x[0] for x in model.predict(Xval, verbose=0)]\n\ntrain_predict = model.predict(Xtrain, verbose=0)\ntest_predict = model.predict(Xval, verbose=0)\n\n# Print error\ntrain_rmse, test_rmse = print_error(Ytrain, Yval, train_predict, test_predict)\nrmse_table['model'].append('LSTM Neural Network (with L1L2 Regularization)')\nrmse_table['training_rmse'].append(train_rmse)\nrmse_table['testing_rmse'].append(test_rmse)\n\n\nTrain RMSE: 0.240 RMSE\nTest RMSE: 1.143 RMSE\n\n\n\n\n\n\n\n\nThe two plots show the training loss for the LSTM model, one with regularization and one without. The plot with regularization shows less fluctuation, indicating that overfitting has been reduced. The model without regularization had a train RMSE of 0.205 and a test RMSE of 0.994, while the model with regularization had a slightly lower for both the train and test RMSE values which are 0.204 and 0.925 respectively.\n\n\nForecast\n\n\nCode\n# Creating the frame to store both predictions\ndays = df['Date'].values[-len(y):]\nframe = pd.concat([\n    pd.DataFrame({'day': days, 'price': y, 'type': 'original'}),\n    pd.DataFrame({'day': days, 'price': yhat_d, 'type': 'rnn_forecast'}),\n    pd.DataFrame({'day': days, 'price': yhat_gru, 'type': 'gru_forecast'}),\n    pd.DataFrame({'day': days, 'price': yhat_lstm, 'type': 'lstm_forecast'})\n])\n# Creating the unscaled values column\nframe['price_absolute'] = [(x * train_std) + train_mean for x in frame['price']]\n# Pivoting\npivoted = frame.pivot_table(index='day', columns='type')\npivoted.columns = ['_'.join(x).strip() for x in pivoted.columns.values]\n\nplt.figure(figsize=(12, 10))\nplt.plot(pivoted.index, pivoted.price_absolute_original,\n         color='#8B1874', label='original')\nplt.plot(pivoted.index, pivoted.price_absolute_rnn_forecast,\n         color='#F79540', label='RNN Forecast', alpha=0.6)\nplt.plot(pivoted.index, pivoted.price_absolute_gru_forecast,\n         color='#B71375', label='GRU Forecast', alpha=0.6)\nplt.plot(pivoted.index, pivoted.price_absolute_lstm_forecast,\n         color='#FC4F00', label='LSTM Forecast', alpha=0.6)\nplt.title('Real Estate Sector Fund Forecasts')\nplt.legend()\nplt.show()\n\n\n\n\n\nWhen comparing the predictions of the deep learning models with the original time series plot, we can observe that the LSTM model is the closest to the actual values. The GRU model and RNN model are pretty far from the actual plot, which indicates that LSTM model is better when compared to others, but when we look at the LSTM model, the model is not accurate, i.e., it is not the same as the actual plot.\n\n\nModel Comparison\n\n\nCode\nrmse_df = pd.DataFrame(rmse_table)\nrmse_df\n\n\n\n\n\n\n  \n    \n      \n      model\n      training_rmse\n      testing_rmse\n    \n  \n  \n    \n      0\n      Recurrent Neural Network\n      0.358503\n      2.395009\n    \n    \n      1\n      Recurrent Neural Network (with L1L2 Regulariza...\n      0.365557\n      2.395731\n    \n    \n      2\n      GRU Neural Network\n      0.648459\n      1.033215\n    \n    \n      3\n      GRU Neural Network (with L1L2 Regularization)\n      0.649795\n      1.018676\n    \n    \n      4\n      LSTM Neural Network\n      0.221415\n      0.991301\n    \n    \n      5\n      LSTM Neural Network (with L1L2 Regularization)\n      0.239752\n      1.142754\n    \n  \n\n\n\n\nBased on the evaluation metrics, the LSTM Neural Network with L1L2 Regularization performs the best on most datasets, followed by the GRU Neural Network and Recurrent Neural Network with L1L2 Regularization. However, the large difference between training and testing RMSE values suggests overfitting to the training data.\n\n\nComparison of deep learning models with traditional single variable time series\nThe deep learning models (RNN, GRU, LSTM) perform better than the ARIMA model in both training and testing RMSE values for the Real Estate Sector Fund. Among the deep learning models, the LSTM model without regularization has the lowest testing RMSE value of 0.4838699, when compared to ARIMA Model which is 2.118625, indicating its better performance in predicting the market prices of the Real Estate Sector Fund compared to the other deep learning models and ARIMA model."
  },
  {
    "objectID": "deep_learning_sector_xlp.html",
    "href": "deep_learning_sector_xlp.html",
    "title": "",
    "section": "",
    "text": "Deep Learning for Consumer Staples Sector Fund\nThe Consumer Staples Sector Fund is an exchange-traded fund that tracks the performance of the consumer staples sector of the US stock market. This sector includes companies that produce essential household goods such as food, beverages, and personal care products. As with other stock market, predicting the future movements of the Consumer Staples Sector Fund can be challenging due to the complex and dynamic nature of the market. These models range from traditional time-series models to more advanced deep learning models. By leveraging the power of Deep Learning and applying techniques such as RNN, LSTM, GRU, and regularization, we can create a model that accurately predicts the future movements of the sector. Such a model can provide valuable insights for investors and traders looking to make informed decisions in the stock market, and help them stay ahead of the curve in the ever-changing landscape of the technology and biotech industries.\nIn order to apply the deep learning methods, the time series data is first split into training and testing sets, taking into account the time component. The training data constitutes 75% of the total data while the rest is the testing data. Furthermore, to ensure that the values are on the same scale, the data is scaled, which is crucial for deep learning models. The mean and standard deviation of the training data are computed and then utilized to normalize both the training and testing sets. This approach ensures that the models are trained on a standardized dataset, enabling them to make accurate predictions.\n\n\nCode\n# libraries\nimport numpy as np\nimport pandas as pd\nimport math\nfrom sklearn.metrics import mean_squared_error\nfrom matplotlib import pyplot as plt\nfrom keras.models import Sequential\nfrom keras.layers import Dropout\nfrom keras.layers import Dense, SimpleRNN, LSTM, GRU\nfrom keras import regularizers\nimport warnings\nwarnings.filterwarnings('ignore')\n\n# import data\ndf = pd.read_csv('./DATA/CLEANED DATA/xlp_raw_data.csv')\n# clean data and get final data for anlaysis\ndf = df[['Date', 'XLP.Adjusted']]\ndf['Date'] = pd.to_datetime(df['Date'])\ndf['XLP.Adjusted'] = pd.to_numeric(df['XLP.Adjusted'], errors='coerce')\ndf = df.dropna(subset=['XLP.Adjusted'])\ndf.sort_values('Date', inplace=True, ascending=True)\ndf = df.reset_index(drop=True)\ndef create_X_Y(ts: np.array, lag=1, n_ahead=1, target_index=0) -> tuple:\n    \"\"\"\n    A method to create X and Y matrix from a time series array for the training of \n    deep learning models \n    \"\"\"\n    # Extracting the number of features that are passed from the array\n    n_features = ts.shape[1]\n\n    # Creating placeholder lists\n    X, Y = [], []\n\n    if len(ts) - lag <= 0:\n        X.append(ts)\n    else:\n        for i in range(len(ts) - lag - n_ahead):\n            Y.append(ts[(i + lag):(i + lag + n_ahead), target_index])\n            X.append(ts[i:(i + lag)])\n\n    X, Y = np.array(X), np.array(Y)\n\n    # Reshaping the X array to an RNN input shape\n    X = np.reshape(X, (X.shape[0], lag, n_features))\n\n    return X, Y\n\nts = df[['XLP.Adjusted']].values\nnp.random.seed(123)\nnrows = ts.shape[0]\ntest_share = 0.25\n# Spliting into train and test sets\ntrain = ts[0:int(nrows * (1 - test_share))]\ntest = ts[int(nrows * (1 - test_share)):]\n\n# Scaling the data\ntrain_mean = train.mean()\ntrain_std = train.std()\ntrain = (train - train_mean) / train_std\ntest = (test - train_mean) / train_std\n\n# Creating the final scaled frame\nts_s = np.concatenate([train, test])\n\nlag = 12\nahead = 3\n\n# Creating the X and Y for training\nX, Y = create_X_Y(ts_s, lag=lag, n_ahead=ahead)\n\nXtrain = X[0:int(X.shape[0] * (1 - test_share))]\nYtrain = Y[0:int(X.shape[0] * (1 - test_share))]\n\nXval = X[int(X.shape[0] * (1 - test_share)):]\nYval = Y[int(X.shape[0] * (1 - test_share)):]\n\n\n\n\ndef plot_model(history, model_title):\n    loss = history.history['loss']\n    epochs = range(1, len(loss) + 1)\n    plt.figure()\n    plt.plot(epochs, loss, 'b', label='Training loss', color = \"gray\")\n    plt.title(f'{model_title} Training loss ')\n    plt.legend()\n    \n\n\ndef print_error(trainY, testY, train_predict, test_predict):\n    # Error of predictions\n    train_rmse = math.sqrt(mean_squared_error(\n        trainY[:, 0], train_predict[:, 0]))\n    test_rmse = math.sqrt(mean_squared_error(testY[:, 0], test_predict[:, 0]))\n    # Print RMSE\n    print('Train RMSE: %.3f RMSE' % (train_rmse))\n    print('Test RMSE: %.3f RMSE' % (test_rmse))\n    return train_rmse, test_rmse\n\n\n\n\nRecurrent Neural Network\nThe RNN models that are constructed and trained using TensorFlow and Keras. This model is trained for five hidden layers, one dense layer, and the activation function hyperbolic tangent. A dropout rate of 0.2 is used to reduce the risk of overfitting. The model are trained for 20 epochs and generated validation loss plots to compare the performance of the regularized and non-regularized models. The second RNN model was identical to the first, except for the addition of kernel regularization.\n\nRNNRNN with Regularization\n\n\n\n\nCode\n# Create a RNN\ndef create_RNN(hidden_units, dense_units, input_shape, activation,dropout_rate=0.2,kernel_regularizer=None):\n    model = Sequential()\n    # Create a simple neural network layer\n    model.add(SimpleRNN(hidden_units, input_shape=input_shape,\n              activation=activation[0]))\n    # Add a dense layer (only one, more layers would make it a deep neural net)\n    model.add(Dense(units=dense_units,\n              activation=activation[1],\n              kernel_regularizer=kernel_regularizer))\n    # Add layer dropout\n    model.add(Dropout(dropout_rate))\n    \n    # Compile the model and optimize on mean squared error\n    model.compile(loss='mean_squared_error', optimizer='adam')\n    return model\n  \n# Create a recurrent neural network\nmodel = create_RNN(hidden_units=5, dense_units=1, input_shape=(lag, Xtrain.shape[-1]),\n                   activation=['tanh', 'tanh'], dropout_rate = 0.2)\nhistory = model.fit(Xtrain, Ytrain, epochs=20, batch_size=1, verbose=0,validation_data=(Xval, Yval))\nplot_model(history, 'Recurrent Neural Network Model')\n\nyhat_d = [x[0] for x in model.predict(Xval, verbose=0)]\ny = [y[0] for y in Yval]\n\ntrain_predict = model.predict(Xtrain, verbose=0)\ntest_predict = model.predict(Xval, verbose=0)\n\n# Print error\ntrain_rmse, test_rmse = print_error(Ytrain, Yval, train_predict, test_predict)\nrmse_table = {\n    'model': ['Recurrent Neural Network'],\n    'training_rmse': [train_rmse],\n    'testing_rmse': [test_rmse]\n}\n\n\nTrain RMSE: 0.286 RMSE\nTest RMSE: 1.854 RMSE\n\n\n\n\n\n\n\n\n\nCode\n# Create a recurrent neural network with regularization\n\nmodel = create_RNN(hidden_units=5, dense_units=1, input_shape=(lag, Xtrain.shape[-1]),\n                   activation=['tanh', 'tanh'], kernel_regularizer=regularizers.L1L2(l1=1e-5, l2=1e-4), dropout_rate = 0.2)\nhistory = model.fit(Xtrain, Ytrain, epochs=20, batch_size=1, verbose=0, validation_data=(Xval, Yval))\nplot_model(history, 'Recurrent Neural Network Model (with L1L2 Regularization)')\n\nyhat_d_reg = [x[0] for x in model.predict(Xval, verbose=0)]\n\ntrain_predict = model.predict(Xtrain, verbose=0)\ntest_predict = model.predict(Xval, verbose=0)\n\n# Print error\ntrain_rmse, test_rmse = print_error(Ytrain, Yval, train_predict, test_predict)\nrmse_table['model'].append(\n    'Recurrent Neural Network (with L1L2 Regularization)')\nrmse_table['training_rmse'].append(train_rmse)\nrmse_table['testing_rmse'].append(test_rmse)\n\n\nTrain RMSE: 0.291 RMSE\nTest RMSE: 1.853 RMSE\n\n\n\n\n\n\n\n\nThe second plot illustrates the impact of regularization on the training loss. Compared to the original plot, there are significant differences. The regularization technique had a substantial effect on the testing error of the model. The model trained without regularization had a slightly lower training error of 0.291 RMSE but a considerably higher testing error of 1.853 RMSE. In contrast, the model trained with regularization had a slightly lower for both training and testing RMSE which are 0.286 and 1.852 respectively.\n\n\nGRU Neural Network\nA GRU neural network is a powerful machine learning tool widely adopted in various applications. Its key feature is the recurrent gated unit, a type of memory cell that can store information over a data sequence. The GRU neural network is five hidden layers, one dense layer, and the hyperbolic tangent activation function. The model also includes kernel regularization to help address the issue of overfitting. The models are trained for 20 epochs and validation loss plots were generated to compare the performance of the regularized and non-regularized models. Overall, the GRU neural network has shown great promise in accurately predicting future movements in the sector.\n\nGRUGRU with Regularization\n\n\n\n\nCode\n# Create a GRU Neural Network\ndef create_GRU(hidden_units, dense_units, input_shape, activation,dropout_rate=0.2, kernel_regularizer=None):\n    model = Sequential()\n    # Create a simple GRU neural network layer\n    model.add(GRU(hidden_units, input_shape=input_shape,\n              activation=activation[0]))\n    # Add a dense layer (only one, more layers would make it a deep neural net)\n    model.add(Dense(units=dense_units,\n              activation=activation[1], kernel_regularizer=kernel_regularizer))\n    # Add layer dropout\n    model.add(Dropout(dropout_rate))\n    # Compile the model and optimize on mean squared error\n    model.compile(loss='mean_squared_error', optimizer='sgd')\n    return model\n\n# Training and evaluating a GRU-based model\nmodel = create_GRU(hidden_units=5, dense_units=1, input_shape=(lag, Xtrain.shape[-1]),\n                   activation=['tanh', 'relu'], dropout_rate = 0.2)\nhistory = model.fit(Xtrain, Ytrain, epochs=20, batch_size=1, verbose=0)\nplot_model(history, 'GRU Model')\n\nyhat_gru = [x[0] for x in model.predict(Xval, verbose=0)]\n\ntrain_predict = model.predict(Xtrain, verbose=0)\ntest_predict = model.predict(Xval, verbose=0)\n\n# Print error\ntrain_rmse, test_rmse = print_error(Ytrain, Yval, train_predict, test_predict)\nrmse_table['model'].append('GRU Neural Network')\nrmse_table['training_rmse'].append(train_rmse)\nrmse_table['testing_rmse'].append(test_rmse)\n\n\nTrain RMSE: 0.997 RMSE\nTest RMSE: 2.810 RMSE\n\n\n\n\n\n\n\n\n\nCode\n# Training and evaluating a GRU-based model with regularization\nmodel = create_GRU(hidden_units=5, dense_units=1, input_shape=(lag, Xtrain.shape[-1]),\n                   activation=['tanh', 'relu'],dropout_rate = 0.2,  kernel_regularizer=regularizers.L1L2(l1=1e-5, l2=1e-4))\nhistory = model.fit(Xtrain, Ytrain, epochs=20, batch_size=1, verbose=0)\nplot_model(history, 'GRU Model (with L1L2 Regularization)')\n\nyhat_gru_reg = [x[0] for x in model.predict(Xval, verbose=0)]\n\ntrain_predict = model.predict(Xtrain, verbose=0)\ntest_predict = model.predict(Xval, verbose=0)\n\n# Print error\ntrain_rmse, test_rmse = print_error(Ytrain, Yval, train_predict, test_predict)\nrmse_table['model'].append('GRU Neural Network (with L1L2 Regularization)')\nrmse_table['training_rmse'].append(train_rmse)\nrmse_table['testing_rmse'].append(test_rmse)\n\n\nTrain RMSE: 0.733 RMSE\nTest RMSE: 0.920 RMSE\n\n\n\n\n\n\n\n\nThe two plots show the training loss for the GRU model, one with regularization and one without. The plot with regularization is similar to without regularization plot. The model without regularization had a train RMSE of 0.997 and a test RMSE of 2.810, the values for with regularization are slightly lower then without regularization which are regularization 0.733 and 0.836 respectively.\n\n\nLSTM Neural Network\nThe LSTM (Long Short-Term Memory) neural network is another powerful machine learning tool that has been widely adopted in various applications, particularly in tasks involving sequential data.The LSTM neural network architecture typically consists of multiple LSTM layers, followed by one or more dense layers and an activation function such as the sigmoid or hyperbolic tangent. Similar to the GRU neural network, the LSTM model may also include kernel regularization to address the issue of overfitting. In order to evaluate the performance of the LSTM model, the model can be trained on a dataset for a specified number of epochs, and validation loss plots can be generated to compare the performance of the regularized and non-regularized models.\n\nLSTMLSTM with Regularization\n\n\n\n\nCode\n# Create a LSTM Neural Network\ndef create_LSTM(hidden_units, dense_units, input_shape, activation,dropout_rate = 0.2, kernel_regularizer=None):\n    model = Sequential()\n    # Create a simple long short term memory neural network\n    model.add(LSTM(hidden_units,\n              activation=activation[0], input_shape=input_shape))\n    # Add a dense layer (only one, more layers would make it a deep neural net)\n    model.add(Dense(units=dense_units,\n              activation=activation[1], kernel_regularizer=kernel_regularizer))\n    # Add layer dropout\n    model.add(Dropout(dropout_rate))\n    # Compile the model and optimize on mean squared error\n    model.compile(optimizer=\"RMSprop\", loss='mae')\n    return model\n\n# Create an LSTM neural network\nmodel = create_LSTM(hidden_units=5, dense_units=1, input_shape=(lag, Xtrain.shape[-1]),\n                    activation=['tanh', 'linear'], dropout_rate = 0.2)\nhistory = model.fit(Xtrain, Ytrain, epochs=20, batch_size=1, verbose=0)\nplot_model(history, 'LSTM Model')\n\nyhat_lstm = [x[0] for x in model.predict(Xval, verbose=0)]\n\ntrain_predict = model.predict(Xtrain, verbose=0)\ntest_predict = model.predict(Xval, verbose=0)\n\n# Print error\ntrain_rmse, test_rmse = print_error(Ytrain, Yval, train_predict, test_predict)\nrmse_table['model'].append('LSTM Neural Network')\nrmse_table['training_rmse'].append(train_rmse)\nrmse_table['testing_rmse'].append(test_rmse)\n\n\nTrain RMSE: 0.212 RMSE\nTest RMSE: 0.781 RMSE\n\n\n\n\n\n\n\n\n\nCode\n# Create an LSTM neural network with regularization\nmodel = create_LSTM(hidden_units=5, dense_units=1, input_shape=(lag, Xtrain.shape[-1]),\n                    activation=['tanh', 'linear'], dropout_rate = 0.2, kernel_regularizer=regularizers.L1L2(l1=1e-5, l2=1e-4))\nhistory = model.fit(Xtrain, Ytrain, epochs=20, batch_size=1, verbose=0)\nplot_model(history, 'LSTM Model (with L1L2 Regularization)')\n\n\nyhat_lstm_reg = [x[0] for x in model.predict(Xval, verbose=0)]\n\ntrain_predict = model.predict(Xtrain, verbose=0)\ntest_predict = model.predict(Xval, verbose=0)\n\n# Print error\ntrain_rmse, test_rmse = print_error(Ytrain, Yval, train_predict, test_predict)\nrmse_table['model'].append('LSTM Neural Network (with L1L2 Regularization)')\nrmse_table['training_rmse'].append(train_rmse)\nrmse_table['testing_rmse'].append(test_rmse)\n\n\nTrain RMSE: 0.201 RMSE\nTest RMSE: 0.852 RMSE\n\n\n\n\n\n\n\n\nThe two plots show the training loss for the LSTM model, one with regularization and one without. The plot with regularization shows less fluctuation, indicating that overfitting has been reduced. The model without regularization had a train RMSE of 0.216 and a test RMSE of 0.858, while the model with regularization has slightly lower RMSE for train but the test is higher than without regularization.\n\n\nForecast\n\n\nCode\n# Creating the frame to store both predictions\ndays = df['Date'].values[-len(y):]\nframe = pd.concat([\n    pd.DataFrame({'day': days, 'price': y, 'type': 'original'}),\n    pd.DataFrame({'day': days, 'price': yhat_d, 'type': 'rnn_forecast'}),\n    pd.DataFrame({'day': days, 'price': yhat_gru, 'type': 'gru_forecast'}),\n    pd.DataFrame({'day': days, 'price': yhat_lstm, 'type': 'lstm_forecast'})\n])\n# Creating the unscaled values column\nframe['price_absolute'] = [(x * train_std) + train_mean for x in frame['price']]\n# Pivoting\npivoted = frame.pivot_table(index='day', columns='type')\npivoted.columns = ['_'.join(x).strip() for x in pivoted.columns.values]\n\nplt.figure(figsize=(12, 10))\nplt.plot(pivoted.index, pivoted.price_absolute_original,\n         color='#8B1874', label='original')\nplt.plot(pivoted.index, pivoted.price_absolute_rnn_forecast,\n         color='#F79540', label='RNN Forecast', alpha=0.6)\nplt.plot(pivoted.index, pivoted.price_absolute_gru_forecast,\n         color='#B71375', label='GRU Forecast', alpha=0.6)\nplt.plot(pivoted.index, pivoted.price_absolute_lstm_forecast,\n         color='#FC4F00', label='LSTM Forecast', alpha=0.6)\nplt.title('Consumer Staples Sector Fund Forecasts')\nplt.legend()\nplt.show()\n\n\n\n\n\nWhen comparing the predictions of the deep learning models with the original time series plot, we can observe that the LSTM model is the closest to the actual values. The GRU model and RNN model are pretty far from the actual plot, which indicates that LSTM model is better when compared to others, but when we look at the LSTM model, the model is not accurate, i.e., it is not the same as the actual plot.\n\n\nModel Comparison\n\n\nCode\nrmse_df = pd.DataFrame(rmse_table)\nrmse_df\n\n\n\n\n\n\n  \n    \n      \n      model\n      training_rmse\n      testing_rmse\n    \n  \n  \n    \n      0\n      Recurrent Neural Network\n      0.285830\n      1.853679\n    \n    \n      1\n      Recurrent Neural Network (with L1L2 Regulariza...\n      0.291398\n      1.852842\n    \n    \n      2\n      GRU Neural Network\n      0.996594\n      2.810339\n    \n    \n      3\n      GRU Neural Network (with L1L2 Regularization)\n      0.732948\n      0.919913\n    \n    \n      4\n      LSTM Neural Network\n      0.212167\n      0.781455\n    \n    \n      5\n      LSTM Neural Network (with L1L2 Regularization)\n      0.200554\n      0.851835\n    \n  \n\n\n\n\nIn terms of performance on the testing dataset, the LSTM Neural Network without regularization appears to be the best performing model, with a testing_rmse of 0.858168. This is followed by the regularized RNN with a testing_rmse of 1.852495, the regularized GRU with a testing_rmse of 0.835960, the non-regularized LSTM with a testing_rmse of 2.49750, the non-regularized RNN with a testing_rmse of 1.853256, and finally, the regularized LSTM with a testing_rmse of 2.49750.\nIt is important to note that the results can be influenced by the specific dataset and the problem at hand. However, in general, LSTMs are known to be effective in tasks that require capturing long-term dependencies in sequential data. GRUs, on the other hand, are a simpler variant of LSTMs and can be used as an alternative if the dataset is relatively small or if computational resources are limited. Regularization can help to prevent overfitting, which is a common problem in deep learning models, and can improve generalization performance.\nOverall, the results suggest that for this particular dataset, the LSTM Neural Network without regularization appears to be the best-performing model, followed by the regularized RNN and the regularized GRU.\n\n\nComparison of deep learning models with traditional single variable time series\nThe deep learning models have significantly lower RMSE values than the ARIMA model, indicating better performance in predicting the future movements of the Consumer Staples Sector Fund. Among the deep learning models, the LSTM neural network with regularization performed the best with a testing RMSE. In contrast, the ARIMA model had a RMSE of 0.8600413, which is higher than all of the deep learning models. Therefore, we can conclude that the deep learning models, especially the LSTM neural network with regularization, outperformed the traditional ARIMA model in predicting the future movements of the Consumer Staples Sector Fund."
  },
  {
    "objectID": "deep_learning_sector_xlk.html",
    "href": "deep_learning_sector_xlk.html",
    "title": "",
    "section": "",
    "text": "Deep Learning for Technology Sector Fund\nThe technology sector fund comprises companies that develop and provide technology-related products and services, such as software, hardware, and electronics. These companies are often at the forefront of innovation and can offer investors significant growth potential. As with other stock market, predicting the future movements of the Technology Sector Fund can be challenging due to the complex and dynamic nature of the market. These models range from traditional time-series models to more advanced deep learning models. By leveraging the power of Deep Learning and applying techniques such as RNN, LSTM, GRU, and regularization, we can create a model that accurately predicts the future movements of the sector. Such a model can provide valuable insights for investors and traders looking to make informed decisions in the stock market, and help them stay ahead of the curve in the ever-changing landscape of the technology and biotech industries.\nIn order to apply the deep learning methods, the time series data is first split into training and testing sets, taking into account the time component. The training data constitutes 75% of the total data while the rest is the testing data. Furthermore, to ensure that the values are on the same scale, the data is scaled, which is crucial for deep learning models. The mean and standard deviation of the training data are computed and then utilized to normalize both the training and testing sets. This approach ensures that the models are trained on a standardized dataset, enabling them to make accurate predictions.\n\n\nCode\n# libraries\nimport numpy as np\nimport pandas as pd\nimport math\nfrom sklearn.metrics import mean_squared_error\nfrom matplotlib import pyplot as plt\nfrom keras.models import Sequential\nfrom keras.layers import Dropout\nfrom keras.layers import Dense, SimpleRNN, LSTM, GRU\nfrom keras import regularizers\nimport warnings\nwarnings.filterwarnings('ignore')\n\n# import data\ndf = pd.read_csv('./DATA/CLEANED DATA/XLK_raw_data.csv')\n# clean data and get final data for anlaysis\ndf = df[['Date', 'XLK.Adjusted']]\ndf['Date'] = pd.to_datetime(df['Date'])\ndf['XLK.Adjusted'] = pd.to_numeric(df['XLK.Adjusted'], errors='coerce')\ndf = df.dropna(subset=['XLK.Adjusted'])\ndf.sort_values('Date', inplace=True, ascending=True)\ndf = df.reset_index(drop=True)\ndef create_X_Y(ts: np.array, lag=1, n_ahead=1, target_index=0) -> tuple:\n    \"\"\"\n    A method to create X and Y matrix from a time series array for the training of \n    deep learning models \n    \"\"\"\n    # Extracting the number of features that are passed from the array\n    n_features = ts.shape[1]\n\n    # Creating placeholder lists\n    X, Y = [], []\n\n    if len(ts) - lag <= 0:\n        X.append(ts)\n    else:\n        for i in range(len(ts) - lag - n_ahead):\n            Y.append(ts[(i + lag):(i + lag + n_ahead), target_index])\n            X.append(ts[i:(i + lag)])\n\n    X, Y = np.array(X), np.array(Y)\n\n    # Reshaping the X array to an RNN input shape\n    X = np.reshape(X, (X.shape[0], lag, n_features))\n\n    return X, Y\n\nts = df[['XLK.Adjusted']].values\nnp.random.seed(123)\nnrows = ts.shape[0]\ntest_share = 0.25\n# Spliting into train and test sets\ntrain = ts[0:int(nrows * (1 - test_share))]\ntest = ts[int(nrows * (1 - test_share)):]\n\n# Scaling the data\ntrain_mean = train.mean()\ntrain_std = train.std()\ntrain = (train - train_mean) / train_std\ntest = (test - train_mean) / train_std\n\n# Creating the final scaled frame\nts_s = np.concatenate([train, test])\n\nlag = 12\nahead = 3\n\n# Creating the X and Y for training\nX, Y = create_X_Y(ts_s, lag=lag, n_ahead=ahead)\n\nXtrain = X[0:int(X.shape[0] * (1 - test_share))]\nYtrain = Y[0:int(X.shape[0] * (1 - test_share))]\n\nXval = X[int(X.shape[0] * (1 - test_share)):]\nYval = Y[int(X.shape[0] * (1 - test_share)):]\n\n\n\n\ndef plot_model(history, model_title):\n    loss = history.history['loss']\n    epochs = range(1, len(loss) + 1)\n    plt.figure()\n    plt.plot(epochs, loss, 'b', label='Training loss', color = \"gray\")\n    plt.title(f'{model_title} Training loss ')\n    plt.legend()\n    \n\n\ndef print_error(trainY, testY, train_predict, test_predict):\n    # Error of predictions\n    train_rmse = math.sqrt(mean_squared_error(\n        trainY[:, 0], train_predict[:, 0]))\n    test_rmse = math.sqrt(mean_squared_error(testY[:, 0], test_predict[:, 0]))\n    # Print RMSE\n    print('Train RMSE: %.3f RMSE' % (train_rmse))\n    print('Test RMSE: %.3f RMSE' % (test_rmse))\n    return train_rmse, test_rmse\n\n\n\n\nRecurrent Neural Network\nThe RNN models that are constructed and trained using TensorFlow and Keras. This model is trained for five hidden layers, one dense layer, and the activation function hyperbolic tangent. A dropout rate of 0.2 is used to reduce the risk of overfitting. The model are trained for 20 epochs and generated validation loss plots to compare the performance of the regularized and non-regularized models. The second RNN model was identical to the first, except for the addition of kernel regularization.\n\nRNNRNN with Regularization\n\n\n\n\nCode\n# Create a RNN\ndef create_RNN(hidden_units, dense_units, input_shape, activation,dropout_rate=0.2,kernel_regularizer=None):\n    model = Sequential()\n    # Create a simple neural network layer\n    model.add(SimpleRNN(hidden_units, input_shape=input_shape,\n              activation=activation[0]))\n    # Add a dense layer (only one, more layers would make it a deep neural net)\n    model.add(Dense(units=dense_units,\n              activation=activation[1],\n              kernel_regularizer=kernel_regularizer))\n    # Add layer dropout\n    model.add(Dropout(dropout_rate))\n    \n    # Compile the model and optimize on mean squared error\n    model.compile(loss='mean_squared_error', optimizer='adam')\n    return model\n  \n# Create a recurrent neural network\nmodel = create_RNN(hidden_units=5, dense_units=1, input_shape=(lag, Xtrain.shape[-1]),\n                   activation=['tanh', 'tanh'], dropout_rate = 0.2)\nhistory = model.fit(Xtrain, Ytrain, epochs=20, batch_size=1, verbose=0,validation_data=(Xval, Yval))\nplot_model(history, 'Recurrent Neural Network Model')\n\nyhat_d = [x[0] for x in model.predict(Xval, verbose=0)]\ny = [y[0] for y in Yval]\n\ntrain_predict = model.predict(Xtrain, verbose=0)\ntest_predict = model.predict(Xval, verbose=0)\n\n# Print error\ntrain_rmse, test_rmse = print_error(Ytrain, Yval, train_predict, test_predict)\nrmse_table = {\n    'model': ['Recurrent Neural Network'],\n    'training_rmse': [train_rmse],\n    'testing_rmse': [test_rmse]\n}\n\n\nTrain RMSE: 0.358 RMSE\nTest RMSE: 4.227 RMSE\n\n\n\n\n\n\n\n\n\nCode\n# Create a recurrent neural network with regularization\n\nmodel = create_RNN(hidden_units=5, dense_units=1, input_shape=(lag, Xtrain.shape[-1]),\n                   activation=['tanh', 'tanh'], kernel_regularizer=regularizers.L1L2(l1=1e-5, l2=1e-4), dropout_rate = 0.2)\nhistory = model.fit(Xtrain, Ytrain, epochs=20, batch_size=1, verbose=0, validation_data=(Xval, Yval))\nplot_model(history, 'Recurrent Neural Network Model (with L1L2 Regularization)')\n\nyhat_d_reg = [x[0] for x in model.predict(Xval, verbose=0)]\n\ntrain_predict = model.predict(Xtrain, verbose=0)\ntest_predict = model.predict(Xval, verbose=0)\n\n# Print error\ntrain_rmse, test_rmse = print_error(Ytrain, Yval, train_predict, test_predict)\nrmse_table['model'].append(\n    'Recurrent Neural Network (with L1L2 Regularization)')\nrmse_table['training_rmse'].append(train_rmse)\nrmse_table['testing_rmse'].append(test_rmse)\n\n\nTrain RMSE: 0.362 RMSE\nTest RMSE: 4.227 RMSE\n\n\n\n\n\n\n\n\nThe second plot illustrates the impact of regularization on the training loss. Compared to the original plot, there are significant differences. The regularization technique had a substantial effect on the testing error of the model. The model trained without regularization had a slightly higher training error of 0.366 RMSE but the testing error is the same 4.227 RMSE when compared to with regularization.\n\n\nGRU Neural Network\nA GRU neural network is a powerful machine learning tool widely adopted in various applications. Its key feature is the recurrent gated unit, a type of memory cell that can store information over a data sequence. The GRU neural network is five hidden layers, one dense layer, and the hyperbolic tangent activation function. The model also includes kernel regularization to help address the issue of overfitting. The models are trained for 20 epochs and validation loss plots were generated to compare the performance of the regularized and non-regularized models. Overall, the GRU neural network has shown great promise in accurately predicting future movements in the sector.\n\nGRUGRU with Regularization\n\n\n\n\nCode\n# Create a GRU Neural Network\ndef create_GRU(hidden_units, dense_units, input_shape, activation,dropout_rate=0.2, kernel_regularizer=None):\n    model = Sequential()\n    # Create a simple GRU neural network layer\n    model.add(GRU(hidden_units, input_shape=input_shape,\n              activation=activation[0]))\n    # Add a dense layer (only one, more layers would make it a deep neural net)\n    model.add(Dense(units=dense_units,\n              activation=activation[1], kernel_regularizer=kernel_regularizer))\n    # Add layer dropout\n    model.add(Dropout(dropout_rate))\n    # Compile the model and optimize on mean squared error\n    model.compile(loss='mean_squared_error', optimizer='sgd')\n    return model\n\n# Training and evaluating a GRU-based model\nmodel = create_GRU(hidden_units=5, dense_units=1, input_shape=(lag, Xtrain.shape[-1]),\n                   activation=['tanh', 'relu'], dropout_rate = 0.2)\nhistory = model.fit(Xtrain, Ytrain, epochs=20, batch_size=1, verbose=0)\nplot_model(history, 'GRU Model')\n\nyhat_gru = [x[0] for x in model.predict(Xval, verbose=0)]\n\ntrain_predict = model.predict(Xtrain, verbose=0)\ntest_predict = model.predict(Xval, verbose=0)\n\n# Print error\ntrain_rmse, test_rmse = print_error(Ytrain, Yval, train_predict, test_predict)\nrmse_table['model'].append('GRU Neural Network')\nrmse_table['training_rmse'].append(train_rmse)\nrmse_table['testing_rmse'].append(test_rmse)\n\n\nTrain RMSE: 1.000 RMSE\nTest RMSE: 5.184 RMSE\n\n\n\n\n\n\n\n\n\nCode\n# Training and evaluating a GRU-based model with regularization\nmodel = create_GRU(hidden_units=5, dense_units=1, input_shape=(lag, Xtrain.shape[-1]),\n                   activation=['tanh', 'relu'],dropout_rate = 0.2,  kernel_regularizer=regularizers.L1L2(l1=1e-5, l2=1e-4))\nhistory = model.fit(Xtrain, Ytrain, epochs=20, batch_size=1, verbose=0)\nplot_model(history, 'GRU Model (with L1L2 Regularization)')\n\nyhat_gru_reg = [x[0] for x in model.predict(Xval, verbose=0)]\n\ntrain_predict = model.predict(Xtrain, verbose=0)\ntest_predict = model.predict(Xval, verbose=0)\n\n# Print error\ntrain_rmse, test_rmse = print_error(Ytrain, Yval, train_predict, test_predict)\nrmse_table['model'].append('GRU Neural Network (with L1L2 Regularization)')\nrmse_table['training_rmse'].append(train_rmse)\nrmse_table['testing_rmse'].append(test_rmse)\n\n\nTrain RMSE: 1.000 RMSE\nTest RMSE: 5.184 RMSE\n\n\n\n\n\n\n\n\nThe two plots show the training loss for the GRU model, one with regularization and one without. The plot with regularization is similar to without regularization plot. The model without regularization had a train RMSE of 1.000 and a test RMSE of 5.184, the values are the higher when compared to with regularization.\n\n\nLSTM Neural Network\nThe LSTM (Long Short-Term Memory) neural network is another powerful machine learning tool that has been widely adopted in various applications, particularly in tasks involving sequential data.The LSTM neural network architecture typically consists of multiple LSTM layers, followed by one or more dense layers and an activation function such as the sigmoid or hyperbolic tangent. Similar to the GRU neural network, the LSTM model may also include kernel regularization to address the issue of overfitting. In order to evaluate the performance of the LSTM model, the model can be trained on a dataset for a specified number of epochs, and validation loss plots can be generated to compare the performance of the regularized and non-regularized models.\n\nLSTMLSTM with Regularization\n\n\n\n\nCode\n# Create a LSTM Neural Network\ndef create_LSTM(hidden_units, dense_units, input_shape, activation,dropout_rate = 0.2, kernel_regularizer=None):\n    model = Sequential()\n    # Create a simple long short term memory neural network\n    model.add(LSTM(hidden_units,\n              activation=activation[0], input_shape=input_shape))\n    # Add a dense layer (only one, more layers would make it a deep neural net)\n    model.add(Dense(units=dense_units,\n              activation=activation[1], kernel_regularizer=kernel_regularizer))\n    # Add layer dropout\n    model.add(Dropout(dropout_rate))\n    # Compile the model and optimize on mean squared error\n    model.compile(optimizer=\"RMSprop\", loss='mae')\n    return model\n\n# Create an LSTM neural network\nmodel = create_LSTM(hidden_units=5, dense_units=1, input_shape=(lag, Xtrain.shape[-1]),\n                    activation=['tanh', 'linear'], dropout_rate = 0.2)\nhistory = model.fit(Xtrain, Ytrain, epochs=20, batch_size=1, verbose=0)\nplot_model(history, 'LSTM Model')\n\nyhat_lstm = [x[0] for x in model.predict(Xval, verbose=0)]\n\ntrain_predict = model.predict(Xtrain, verbose=0)\ntest_predict = model.predict(Xval, verbose=0)\n\n# Print error\ntrain_rmse, test_rmse = print_error(Ytrain, Yval, train_predict, test_predict)\nrmse_table['model'].append('LSTM Neural Network')\nrmse_table['training_rmse'].append(train_rmse)\nrmse_table['testing_rmse'].append(test_rmse)\n\n\nTrain RMSE: 0.194 RMSE\nTest RMSE: 2.334 RMSE\n\n\n\n\n\n\n\n\n\nCode\n# Create an LSTM neural network with regularization\nmodel = create_LSTM(hidden_units=5, dense_units=1, input_shape=(lag, Xtrain.shape[-1]),\n                    activation=['tanh', 'linear'], dropout_rate = 0.2, kernel_regularizer=regularizers.L1L2(l1=1e-5, l2=1e-4))\nhistory = model.fit(Xtrain, Ytrain, epochs=20, batch_size=1, verbose=0)\nplot_model(history, 'LSTM Model (with L1L2 Regularization)')\n\n\nyhat_lstm_reg = [x[0] for x in model.predict(Xval, verbose=0)]\n\ntrain_predict = model.predict(Xtrain, verbose=0)\ntest_predict = model.predict(Xval, verbose=0)\n\n# Print error\ntrain_rmse, test_rmse = print_error(Ytrain, Yval, train_predict, test_predict)\nrmse_table['model'].append('LSTM Neural Network (with L1L2 Regularization)')\nrmse_table['training_rmse'].append(train_rmse)\nrmse_table['testing_rmse'].append(test_rmse)\n\n\nTrain RMSE: 0.193 RMSE\nTest RMSE: 2.709 RMSE\n\n\n\n\n\n\n\n\nThe two plots show the training loss for the LSTM model, one with regularization and one without. The plot with regularization shows less fluctuation, indicating that overfitting has been reduced. The model without regularization had a train RMSE of 0.196 and a test RMSE of 2.927, while the model with regularization had a slightly lower for both the train and test RMSE values which are 0.193 and 2.551 respectively.\n\n\nForecast\n\n\nCode\n# Creating the frame to store both predictions\ndays = df['Date'].values[-len(y):]\nframe = pd.concat([\n    pd.DataFrame({'day': days, 'price': y, 'type': 'original'}),\n    pd.DataFrame({'day': days, 'price': yhat_d, 'type': 'rnn_forecast'}),\n    pd.DataFrame({'day': days, 'price': yhat_gru, 'type': 'gru_forecast'}),\n    pd.DataFrame({'day': days, 'price': yhat_lstm, 'type': 'lstm_forecast'})\n])\n# Creating the unscaled values column\nframe['price_absolute'] = [(x * train_std) + train_mean for x in frame['price']]\n# Pivoting\npivoted = frame.pivot_table(index='day', columns='type')\npivoted.columns = ['_'.join(x).strip() for x in pivoted.columns.values]\n\nplt.figure(figsize=(12, 10))\nplt.plot(pivoted.index, pivoted.price_absolute_original,\n         color='#8B1874', label='original')\nplt.plot(pivoted.index, pivoted.price_absolute_rnn_forecast,\n         color='#F79540', label='RNN Forecast', alpha=0.6)\nplt.plot(pivoted.index, pivoted.price_absolute_gru_forecast,\n         color='#B71375', label='GRU Forecast', alpha=0.6)\nplt.plot(pivoted.index, pivoted.price_absolute_lstm_forecast,\n         color='#FC4F00', label='LSTM Forecast', alpha=0.6)\nplt.title('Technology Sector Fund Forecasts')\nplt.legend()\nplt.show()\n\n\n\n\n\nWhen comparing the predictions of the deep learning models with the original time series plot, we can observe that the LSTM model is the closest to the actual values. The GRU model and RNN model are pretty far from the actual plot, which indicates that LSTM model is better when compared to others, but when we look at the LSTM model, the model is not accurate, i.e., it is not the same as the actual plot.\n\n\nModel Comparison\n\n\nCode\nrmse_df = pd.DataFrame(rmse_table)\nrmse_df\n\n\n\n\n\n\n  \n    \n      \n      model\n      training_rmse\n      testing_rmse\n    \n  \n  \n    \n      0\n      Recurrent Neural Network\n      0.358245\n      4.226730\n    \n    \n      1\n      Recurrent Neural Network (with L1L2 Regulariza...\n      0.361511\n      4.226827\n    \n    \n      2\n      GRU Neural Network\n      1.000264\n      5.183701\n    \n    \n      3\n      GRU Neural Network (with L1L2 Regularization)\n      1.000264\n      5.183701\n    \n    \n      4\n      LSTM Neural Network\n      0.194319\n      2.334342\n    \n    \n      5\n      LSTM Neural Network (with L1L2 Regularization)\n      0.193185\n      2.709011\n    \n  \n\n\n\n\nThe Recurrent Neural Network (RNN) model and the RNN with L1L2 Regularization have similar training and testing RMSE values, with the regularized version performing slightly better on the testing data.\nThe GRU Neural Network has a high training and testing RMSE, while the GRU with L1L2 Regularization has a much lower testing RMSE, indicating better performance on unseen data.\nThe LSTM Neural Network and the LSTM with L1L2 Regularization both have low training and testing RMSE values, with the regularized version performing slightly better on the testing data.\nOverall, the LSTM Neural Network with L1L2 Regularization appears to perform the best on this dataset, with the lowest testing RMSE value. The GRU with L1L2 Regularization also performs well on the testing data, but has a higher training RMSE value, indicating potential overfitting to the training data.\n\n\nComparison of deep learning models with traditional single variable time series\nThe deep learning models (RNN, GRU, LSTM) perform better than the ARIMA model in both training and testing RMSE values for the Technology Sector Fund. Among the deep learning models, the LSTM model without regularization has the lowest testing RMSE value of 1.980341, when compared to ARIMA Model which is 2.118625, indicating its better performance in predicting the market prices of the Technology Sector Fund compared to the other deep learning models and ARIMA model."
  },
  {
    "objectID": "deep_learning_sector_xli.html",
    "href": "deep_learning_sector_xli.html",
    "title": "",
    "section": "",
    "text": "Deep Learning for Industrial Sector Fund\nAn industrial sector fund is a type of mutual fund or exchange-traded fund (ETF) that focuses on investing in companies within the industrial sector. These funds may hold stocks of companies involved in manufacturing, transportation, energy, construction, and other related industries. As with other stock market, predicting the future movements of the Industrial Sector Fund can be challenging due to the complex and dynamic nature of the market. These models range from traditional time-series models to more advanced deep learning models. By leveraging the power of Deep Learning and applying techniques such as RNN, LSTM, GRU, and regularization, we can create a model that accurately predicts the future movements of the sector. Such a model can provide valuable insights for investors and traders looking to make informed decisions in the stock market, and help them stay ahead of the curve in the ever-changing landscape of the technology and biotech industries.\nIn order to apply the deep learning methods, the time series data is first split into training and testing sets, taking into account the time component. The training data constitutes 75% of the total data while the rest is the testing data. Furthermore, to ensure that the values are on the same scale, the data is scaled, which is crucial for deep learning models. The mean and standard deviation of the training data are computed and then utilized to normalize both the training and testing sets. This approach ensures that the models are trained on a standardized dataset, enabling them to make accurate predictions.\n\n\nCode\n# libraries\nimport numpy as np\nimport pandas as pd\nimport math\nfrom sklearn.metrics import mean_squared_error\nfrom matplotlib import pyplot as plt\nfrom keras.models import Sequential\nfrom keras.layers import Dropout\nfrom keras.layers import Dense, SimpleRNN, LSTM, GRU\nfrom keras import regularizers\nimport warnings\nwarnings.filterwarnings('ignore')\n\n# import data\ndf = pd.read_csv('./DATA/CLEANED DATA/XLI_raw_data.csv')\n# clean data and get final data for anlaysis\ndf = df[['Date', 'XLI.Adjusted']]\ndf['Date'] = pd.to_datetime(df['Date'])\ndf['XLI.Adjusted'] = pd.to_numeric(df['XLI.Adjusted'], errors='coerce')\ndf = df.dropna(subset=['XLI.Adjusted'])\ndf.sort_values('Date', inplace=True, ascending=True)\ndf = df.reset_index(drop=True)\ndef create_X_Y(ts: np.array, lag=1, n_ahead=1, target_index=0) -> tuple:\n    \"\"\"\n    A method to create X and Y matrix from a time series array for the training of \n    deep learning models \n    \"\"\"\n    # Extracting the number of features that are passed from the array\n    n_features = ts.shape[1]\n\n    # Creating placeholder lists\n    X, Y = [], []\n\n    if len(ts) - lag <= 0:\n        X.append(ts)\n    else:\n        for i in range(len(ts) - lag - n_ahead):\n            Y.append(ts[(i + lag):(i + lag + n_ahead), target_index])\n            X.append(ts[i:(i + lag)])\n\n    X, Y = np.array(X), np.array(Y)\n\n    # Reshaping the X array to an RNN input shape\n    X = np.reshape(X, (X.shape[0], lag, n_features))\n\n    return X, Y\n\nts = df[['XLI.Adjusted']].values\nnp.random.seed(123)\nnrows = ts.shape[0]\ntest_share = 0.25\n# Spliting into train and test sets\ntrain = ts[0:int(nrows * (1 - test_share))]\ntest = ts[int(nrows * (1 - test_share)):]\n\n# Scaling the data\ntrain_mean = train.mean()\ntrain_std = train.std()\ntrain = (train - train_mean) / train_std\ntest = (test - train_mean) / train_std\n\n# Creating the final scaled frame\nts_s = np.concatenate([train, test])\n\nlag = 12\nahead = 3\n\n# Creating the X and Y for training\nX, Y = create_X_Y(ts_s, lag=lag, n_ahead=ahead)\n\nXtrain = X[0:int(X.shape[0] * (1 - test_share))]\nYtrain = Y[0:int(X.shape[0] * (1 - test_share))]\n\nXval = X[int(X.shape[0] * (1 - test_share)):]\nYval = Y[int(X.shape[0] * (1 - test_share)):]\n\n\n\n\ndef plot_model(history, model_title):\n    loss = history.history['loss']\n    epochs = range(1, len(loss) + 1)\n    plt.figure()\n    plt.plot(epochs, loss, 'b', label='Training loss', color = \"gray\")\n    plt.title(f'{model_title} Training loss ')\n    plt.legend()\n    \n\n\ndef print_error(trainY, testY, train_predict, test_predict):\n    # Error of predictions\n    train_rmse = math.sqrt(mean_squared_error(\n        trainY[:, 0], train_predict[:, 0]))\n    test_rmse = math.sqrt(mean_squared_error(testY[:, 0], test_predict[:, 0]))\n    # Print RMSE\n    print('Train RMSE: %.3f RMSE' % (train_rmse))\n    print('Test RMSE: %.3f RMSE' % (test_rmse))\n    return train_rmse, test_rmse\n\n\n\n\nRecurrent Neural Network\nThe RNN models that are constructed and trained using TensorFlow and Keras. This model is trained for five hidden layers, one dense layer, and the activation function hyperbolic tangent. A dropout rate of 0.2 is used to reduce the risk of overfitting. The model are trained for 20 epochs and generated validation loss plots to compare the performance of the regularized and non-regularized models. The second RNN model was identical to the first, except for the addition of kernel regularization.\n\nRNNRNN with Regularization\n\n\n\n\nCode\n# Create a RNN\ndef create_RNN(hidden_units, dense_units, input_shape, activation,dropout_rate=0.2,kernel_regularizer=None):\n    model = Sequential()\n    # Create a simple neural network layer\n    model.add(SimpleRNN(hidden_units, input_shape=input_shape,\n              activation=activation[0]))\n    # Add a dense layer (only one, more layers would make it a deep neural net)\n    model.add(Dense(units=dense_units,\n              activation=activation[1],\n              kernel_regularizer=kernel_regularizer))\n    # Add layer dropout\n    model.add(Dropout(dropout_rate))\n    \n    # Compile the model and optimize on mean squared error\n    model.compile(loss='mean_squared_error', optimizer='adam')\n    return model\n  \n# Create a recurrent neural network\nmodel = create_RNN(hidden_units=5, dense_units=1, input_shape=(lag, Xtrain.shape[-1]),\n                   activation=['tanh', 'tanh'], dropout_rate = 0.2)\nhistory = model.fit(Xtrain, Ytrain, epochs=20, batch_size=1, verbose=0,validation_data=(Xval, Yval))\nplot_model(history, 'Recurrent Neural Network Model')\n\nyhat_d = [x[0] for x in model.predict(Xval, verbose=0)]\ny = [y[0] for y in Yval]\n\ntrain_predict = model.predict(Xtrain, verbose=0)\ntest_predict = model.predict(Xval, verbose=0)\n\n# Print error\ntrain_rmse, test_rmse = print_error(Ytrain, Yval, train_predict, test_predict)\nrmse_table = {\n    'model': ['Recurrent Neural Network'],\n    'training_rmse': [train_rmse],\n    'testing_rmse': [test_rmse]\n}\n\n\nTrain RMSE: 0.293 RMSE\nTest RMSE: 1.839 RMSE\n\n\n\n\n\n\n\n\n\nCode\n# Create a recurrent neural network with regularization\n\nmodel = create_RNN(hidden_units=5, dense_units=1, input_shape=(lag, Xtrain.shape[-1]),\n                   activation=['tanh', 'tanh'], kernel_regularizer=regularizers.L1L2(l1=1e-5, l2=1e-4), dropout_rate = 0.2)\nhistory = model.fit(Xtrain, Ytrain, epochs=20, batch_size=1, verbose=0, validation_data=(Xval, Yval))\nplot_model(history, 'Recurrent Neural Network Model (with L1L2 Regularization)')\n\nyhat_d_reg = [x[0] for x in model.predict(Xval, verbose=0)]\n\ntrain_predict = model.predict(Xtrain, verbose=0)\ntest_predict = model.predict(Xval, verbose=0)\n\n# Print error\ntrain_rmse, test_rmse = print_error(Ytrain, Yval, train_predict, test_predict)\nrmse_table['model'].append(\n    'Recurrent Neural Network (with L1L2 Regularization)')\nrmse_table['training_rmse'].append(train_rmse)\nrmse_table['testing_rmse'].append(test_rmse)\n\n\nTrain RMSE: 0.291 RMSE\nTest RMSE: 1.839 RMSE\n\n\n\n\n\n\n\n\nThe second plot illustrates the impact of regularization on the training loss. Compared to the original plot, there are significant differences. The regularization technique had a substantial effect on the testing error of the model. The model trained without regularization had a slightly lower training error of 0.291 RMSE but a considerably higher testing error of 1.839 RMSE. The RMSE values are the Slightly higher for with regularization in terms of testing error, but for the training error the RMSE values are the same.\n\n\nGRU Neural Network\nA GRU neural network is a powerful machine learning tool widely adopted in various applications. Its key feature is the recurrent gated unit, a type of memory cell that can store information over a data sequence. The GRU neural network is five hidden layers, one dense layer, and the hyperbolic tangent activation function. The model also includes kernel regularization to help address the issue of overfitting. The models are trained for 20 epochs and validation loss plots were generated to compare the performance of the regularized and non-regularized models. Overall, the GRU neural network has shown great promise in accurately predicting future movements in the sector.\n\nGRUGRU with Regularization\n\n\n\n\nCode\n# Create a GRU Neural Network\ndef create_GRU(hidden_units, dense_units, input_shape, activation,dropout_rate=0.2, kernel_regularizer=None):\n    model = Sequential()\n    # Create a simple GRU neural network layer\n    model.add(GRU(hidden_units, input_shape=input_shape,\n              activation=activation[0]))\n    # Add a dense layer (only one, more layers would make it a deep neural net)\n    model.add(Dense(units=dense_units,\n              activation=activation[1], kernel_regularizer=kernel_regularizer))\n    # Add layer dropout\n    model.add(Dropout(dropout_rate))\n    # Compile the model and optimize on mean squared error\n    model.compile(loss='mean_squared_error', optimizer='sgd')\n    return model\n\n# Training and evaluating a GRU-based model\nmodel = create_GRU(hidden_units=5, dense_units=1, input_shape=(lag, Xtrain.shape[-1]),\n                   activation=['tanh', 'relu'], dropout_rate = 0.2)\nhistory = model.fit(Xtrain, Ytrain, epochs=20, batch_size=1, verbose=0)\nplot_model(history, 'GRU Model')\n\nyhat_gru = [x[0] for x in model.predict(Xval, verbose=0)]\n\ntrain_predict = model.predict(Xtrain, verbose=0)\ntest_predict = model.predict(Xval, verbose=0)\n\n# Print error\ntrain_rmse, test_rmse = print_error(Ytrain, Yval, train_predict, test_predict)\nrmse_table['model'].append('GRU Neural Network')\nrmse_table['training_rmse'].append(train_rmse)\nrmse_table['testing_rmse'].append(test_rmse)\n\n\nTrain RMSE: 0.998 RMSE\nTest RMSE: 2.768 RMSE\n\n\n\n\n\n\n\n\n\nCode\n# Training and evaluating a GRU-based model with regularization\nmodel = create_GRU(hidden_units=5, dense_units=1, input_shape=(lag, Xtrain.shape[-1]),\n                   activation=['tanh', 'relu'],dropout_rate = 0.2,  kernel_regularizer=regularizers.L1L2(l1=1e-5, l2=1e-4))\nhistory = model.fit(Xtrain, Ytrain, epochs=20, batch_size=1, verbose=0)\nplot_model(history, 'GRU Model (with L1L2 Regularization)')\n\nyhat_gru_reg = [x[0] for x in model.predict(Xval, verbose=0)]\n\ntrain_predict = model.predict(Xtrain, verbose=0)\ntest_predict = model.predict(Xval, verbose=0)\n\n# Print error\ntrain_rmse, test_rmse = print_error(Ytrain, Yval, train_predict, test_predict)\nrmse_table['model'].append('GRU Neural Network (with L1L2 Regularization)')\nrmse_table['training_rmse'].append(train_rmse)\nrmse_table['testing_rmse'].append(test_rmse)\n\n\nTrain RMSE: 0.688 RMSE\nTest RMSE: 0.919 RMSE\n\n\n\n\n\n\n\n\nThe two plots show the training loss for the GRU model, one with regularization and one without. The plot with regularization is similar to without regularization plot. The model without regularization had a train RMSE of 0.998 and a test RMSE of 2.768, the values are the same for with regularization too.\n\n\nLSTM Neural Network\nThe LSTM (Long Short-Term Memory) neural network is another powerful machine learning tool that has been widely adopted in various applications, particularly in tasks involving sequential data.The LSTM neural network architecture typically consists of multiple LSTM layers, followed by one or more dense layers and an activation function such as the sigmoid or hyperbolic tangent. Similar to the GRU neural network, the LSTM model may also include kernel regularization to address the issue of overfitting. In order to evaluate the performance of the LSTM model, the model can be trained on a dataset for a specified number of epochs, and validation loss plots can be generated to compare the performance of the regularized and non-regularized models.\n\nLSTMLSTM with Regularization\n\n\n\n\nCode\n# Create a LSTM Neural Network\ndef create_LSTM(hidden_units, dense_units, input_shape, activation,dropout_rate = 0.2, kernel_regularizer=None):\n    model = Sequential()\n    # Create a simple long short term memory neural network\n    model.add(LSTM(hidden_units,\n              activation=activation[0], input_shape=input_shape))\n    # Add a dense layer (only one, more layers would make it a deep neural net)\n    model.add(Dense(units=dense_units,\n              activation=activation[1], kernel_regularizer=kernel_regularizer))\n    # Add layer dropout\n    model.add(Dropout(dropout_rate))\n    # Compile the model and optimize on mean squared error\n    model.compile(optimizer=\"RMSprop\", loss='mae')\n    return model\n\n# Create an LSTM neural network\nmodel = create_LSTM(hidden_units=5, dense_units=1, input_shape=(lag, Xtrain.shape[-1]),\n                    activation=['tanh', 'linear'], dropout_rate = 0.2)\nhistory = model.fit(Xtrain, Ytrain, epochs=20, batch_size=1, verbose=0)\nplot_model(history, 'LSTM Model')\n\nyhat_lstm = [x[0] for x in model.predict(Xval, verbose=0)]\n\ntrain_predict = model.predict(Xtrain, verbose=0)\ntest_predict = model.predict(Xval, verbose=0)\n\n# Print error\ntrain_rmse, test_rmse = print_error(Ytrain, Yval, train_predict, test_predict)\nrmse_table['model'].append('LSTM Neural Network')\nrmse_table['training_rmse'].append(train_rmse)\nrmse_table['testing_rmse'].append(test_rmse)\n\n\nTrain RMSE: 0.202 RMSE\nTest RMSE: 0.747 RMSE\n\n\n\n\n\n\n\n\n\nCode\n# Create an LSTM neural network with regularization\nmodel = create_LSTM(hidden_units=5, dense_units=1, input_shape=(lag, Xtrain.shape[-1]),\n                    activation=['tanh', 'linear'], dropout_rate = 0.2, kernel_regularizer=regularizers.L1L2(l1=1e-5, l2=1e-4))\nhistory = model.fit(Xtrain, Ytrain, epochs=20, batch_size=1, verbose=0)\nplot_model(history, 'LSTM Model (with L1L2 Regularization)')\n\n\nyhat_lstm_reg = [x[0] for x in model.predict(Xval, verbose=0)]\n\ntrain_predict = model.predict(Xtrain, verbose=0)\ntest_predict = model.predict(Xval, verbose=0)\n\n# Print error\ntrain_rmse, test_rmse = print_error(Ytrain, Yval, train_predict, test_predict)\nrmse_table['model'].append('LSTM Neural Network (with L1L2 Regularization)')\nrmse_table['training_rmse'].append(train_rmse)\nrmse_table['testing_rmse'].append(test_rmse)\n\n\nTrain RMSE: 0.200 RMSE\nTest RMSE: 0.840 RMSE\n\n\n\n\n\n\n\n\nThe two plots show the training loss for the LSTM model, one with regularization and one without. The plot with regularization shows less fluctuation, indicating that overfitting has been reduced. The model without regularization had a train RMSE of 0.200 and a test RMSE of 0.825, while the model with regularization had a slightly lower for both the train and test RMSE values which are 0.192 and 0.809 respectively.\n\n\nForecast\n\n\nCode\n# Creating the frame to store both predictions\ndays = df['Date'].values[-len(y):]\nframe = pd.concat([\n    pd.DataFrame({'day': days, 'price': y, 'type': 'original'}),\n    pd.DataFrame({'day': days, 'price': yhat_d, 'type': 'rnn_forecast'}),\n    pd.DataFrame({'day': days, 'price': yhat_gru, 'type': 'gru_forecast'}),\n    pd.DataFrame({'day': days, 'price': yhat_lstm, 'type': 'lstm_forecast'})\n])\n# Creating the unscaled values column\nframe['price_absolute'] = [(x * train_std) + train_mean for x in frame['price']]\n# Pivoting\npivoted = frame.pivot_table(index='day', columns='type')\npivoted.columns = ['_'.join(x).strip() for x in pivoted.columns.values]\n\nplt.figure(figsize=(12, 10))\nplt.plot(pivoted.index, pivoted.price_absolute_original,\n         color='#8B1874', label='original')\nplt.plot(pivoted.index, pivoted.price_absolute_rnn_forecast,\n         color='#F79540', label='RNN Forecast', alpha=0.6)\nplt.plot(pivoted.index, pivoted.price_absolute_gru_forecast,\n         color='#B71375', label='GRU Forecast', alpha=0.6)\nplt.plot(pivoted.index, pivoted.price_absolute_lstm_forecast,\n         color='#FC4F00', label='LSTM Forecast', alpha=0.6)\nplt.title('Industrial Sector Fund Forecasts')\nplt.legend()\nplt.show()\n\n\n\n\n\nWhen comparing the predictions of the deep learning models with the original time series plot, we can observe that the LSTM model is the closest to the actual values. The GRU model and RNN model are pretty far from the actual plot, which indicates that LSTM model is better when compared to others, but when we look at the LSTM model, the model is not accurate, i.e., it is not the same as the actual plot.\n\n\nModel Comparison\n\n\nCode\nrmse_df = pd.DataFrame(rmse_table)\nrmse_df\n\n\n\n\n\n\n  \n    \n      \n      model\n      training_rmse\n      testing_rmse\n    \n  \n  \n    \n      0\n      Recurrent Neural Network\n      0.292596\n      1.838883\n    \n    \n      1\n      Recurrent Neural Network (with L1L2 Regulariza...\n      0.291123\n      1.839132\n    \n    \n      2\n      GRU Neural Network\n      0.997856\n      2.767872\n    \n    \n      3\n      GRU Neural Network (with L1L2 Regularization)\n      0.687721\n      0.919074\n    \n    \n      4\n      LSTM Neural Network\n      0.201554\n      0.747094\n    \n    \n      5\n      LSTM Neural Network (with L1L2 Regularization)\n      0.199909\n      0.839552\n    \n  \n\n\n\n\nIn terms of performance on the testing dataset, the LSTM Neural Network with L1L2 regularization appears to be the best performing model, with a testing_rmse of 0.808819. This is followed closely by the non-regularized LSTM with a testing_rmse of 0.824516. The regularized RNN and non-regularized RNN have similar testing_rmse values of 1.840652 and 1.838846, respectively. The non-regularized GRU has a higher testing_rmse of 2.767866, while the regularized GRU has the same testing_rmse as the non-regularized GRU due to the regularization not being effective in this case.\nIt is important to note that the results can be influenced by the specific dataset and the problem at hand. However, in general, LSTMs are known to be effective in tasks that require capturing long-term dependencies in sequential data. GRUs, on the other hand, are a simpler variant of LSTMs and can be used as an alternative if the dataset is relatively small or if computational resources are limited. Regularization can help to prevent overfitting, which is a common problem in deep learning models, and can improve generalization performance.\nOverall, the results suggest that for this particular dataset, the LSTM Neural Network with L1L2 regularization appears to be the best-performing model, followed closely by the non-regularized LSTM. The regularized and non-regularized RNN have similar performance, while the non-regularized GRU appears to perform worse than the other models. It is also worth noting that the regularization did not improve the performance of the GRU in this case, which suggests that the effectiveness of regularization can depend on the specific model and dataset.\n\n\nComparison of deep learning models with traditional single variable time series\nThe deep learning models (RNN, GRU, LSTM) perform better than the ARIMA model in both training and testing RMSE values for the Industrial Sector Fund. Among the deep learning models, the LSTM model without regularization has the lowest testing RMSE value, when compared to ARIMA Model which is 1.317807, indicating its better performance in predicting the market prices of the Industrial Sector Fund compared to the other deep learning models and ARIMA model."
  },
  {
    "objectID": "deep_learning_sector_xlf.html",
    "href": "deep_learning_sector_xlf.html",
    "title": "",
    "section": "",
    "text": "Deep Learning for Financial Sector Fund\nThe financial sector refers to the part of the economy that deals with financial transactions, including banks, investment firms, insurance companies, and other financial institutions. The financial sector plays a crucial role in the economy by providing capital, managing risk, and facilitating the flow of money throughout the economy. As with other stock market, predicting the future movements of the Financial Sector Fund can be challenging due to the complex and dynamic nature of the market. These models range from traditional time-series models to more advanced deep learning models. By leveraging the power of Deep Learning and applying techniques such as RNN, LSTM, GRU, and regularization, we can create a model that accurately predicts the future movements of the sector. Such a model can provide valuable insights for investors and traders looking to make informed decisions in the stock market, and help them stay ahead of the curve in the ever-changing landscape of the technology and biotech industries.\nIn order to apply the deep learning methods, the time series data is first split into training and testing sets, taking into account the time component. The training data constitutes 75% of the total data while the rest is the testing data. Furthermore, to ensure that the values are on the same scale, the data is scaled, which is crucial for deep learning models. The mean and standard deviation of the training data are computed and then utilized to normalize both the training and testing sets. This approach ensures that the models are trained on a standardized dataset, enabling them to make accurate predictions.\n\n\nCode\n# libraries\nimport numpy as np\nimport pandas as pd\nimport math\nfrom sklearn.metrics import mean_squared_error\nfrom matplotlib import pyplot as plt\nfrom keras.models import Sequential\nfrom keras.layers import Dropout\nfrom keras.layers import Dense, SimpleRNN, LSTM, GRU\nfrom keras import regularizers\nimport warnings\nwarnings.filterwarnings('ignore')\n\n# import data\ndf = pd.read_csv('./DATA/CLEANED DATA/XLF_raw_data.csv')\n# clean data and get final data for anlaysis\ndf = df[['Date', 'XLF.Adjusted']]\ndf['Date'] = pd.to_datetime(df['Date'])\ndf['XLF.Adjusted'] = pd.to_numeric(df['XLF.Adjusted'], errors='coerce')\ndf = df.dropna(subset=['XLF.Adjusted'])\ndf.sort_values('Date', inplace=True, ascending=True)\ndf = df.reset_index(drop=True)\ndef create_X_Y(ts: np.array, lag=1, n_ahead=1, target_index=0) -> tuple:\n    \"\"\"\n    A method to create X and Y matrix from a time series array for the training of \n    deep learning models \n    \"\"\"\n    # Extracting the number of features that are passed from the array\n    n_features = ts.shape[1]\n\n    # Creating placeholder lists\n    X, Y = [], []\n\n    if len(ts) - lag <= 0:\n        X.append(ts)\n    else:\n        for i in range(len(ts) - lag - n_ahead):\n            Y.append(ts[(i + lag):(i + lag + n_ahead), target_index])\n            X.append(ts[i:(i + lag)])\n\n    X, Y = np.array(X), np.array(Y)\n\n    # Reshaping the X array to an RNN input shape\n    X = np.reshape(X, (X.shape[0], lag, n_features))\n\n    return X, Y\n\nts = df[['XLF.Adjusted']].values\nnp.random.seed(123)\nnrows = ts.shape[0]\ntest_share = 0.25\n# Spliting into train and test sets\ntrain = ts[0:int(nrows * (1 - test_share))]\ntest = ts[int(nrows * (1 - test_share)):]\n\n# Scaling the data\ntrain_mean = train.mean()\ntrain_std = train.std()\ntrain = (train - train_mean) / train_std\ntest = (test - train_mean) / train_std\n\n# Creating the final scaled frame\nts_s = np.concatenate([train, test])\n\nlag = 12\nahead = 3\n\n# Creating the X and Y for training\nX, Y = create_X_Y(ts_s, lag=lag, n_ahead=ahead)\n\nXtrain = X[0:int(X.shape[0] * (1 - test_share))]\nYtrain = Y[0:int(X.shape[0] * (1 - test_share))]\n\nXval = X[int(X.shape[0] * (1 - test_share)):]\nYval = Y[int(X.shape[0] * (1 - test_share)):]\n\n\n\n\ndef plot_model(history, model_title):\n    loss = history.history['loss']\n    epochs = range(1, len(loss) + 1)\n    plt.figure()\n    plt.plot(epochs, loss, 'b', label='Training loss', color = \"gray\")\n    plt.title(f'{model_title} Training loss ')\n    plt.legend()\n    \n\n\ndef print_error(trainY, testY, train_predict, test_predict):\n    # Error of predictions\n    train_rmse = math.sqrt(mean_squared_error(\n        trainY[:, 0], train_predict[:, 0]))\n    test_rmse = math.sqrt(mean_squared_error(testY[:, 0], test_predict[:, 0]))\n    # Print RMSE\n    print('Train RMSE: %.3f RMSE' % (train_rmse))\n    print('Test RMSE: %.3f RMSE' % (test_rmse))\n    return train_rmse, test_rmse\n\n\n\n\nRecurrent Neural Network\nThe RNN models that are constructed and trained using TensorFlow and Keras. This model is trained for five hidden layers, one dense layer, and the activation function hyperbolic tangent. A dropout rate of 0.2 is used to reduce the risk of overfitting. The model are trained for 20 epochs and generated validation loss plots to compare the performance of the regularized and non-regularized models. The second RNN model was identical to the first, except for the addition of kernel regularization.\n\nRNNRNN with Regularization\n\n\n\n\nCode\n# Create a RNN\ndef create_RNN(hidden_units, dense_units, input_shape, activation,dropout_rate=0.2,kernel_regularizer=None):\n    model = Sequential()\n    # Create a simple neural network layer\n    model.add(SimpleRNN(hidden_units, input_shape=input_shape,\n              activation=activation[0]))\n    # Add a dense layer (only one, more layers would make it a deep neural net)\n    model.add(Dense(units=dense_units,\n              activation=activation[1],\n              kernel_regularizer=kernel_regularizer))\n    # Add layer dropout\n    model.add(Dropout(dropout_rate))\n    \n    # Compile the model and optimize on mean squared error\n    model.compile(loss='mean_squared_error', optimizer='adam')\n    return model\n  \n# Create a recurrent neural network\nmodel = create_RNN(hidden_units=5, dense_units=1, input_shape=(lag, Xtrain.shape[-1]),\n                   activation=['tanh', 'tanh'], dropout_rate = 0.2)\nhistory = model.fit(Xtrain, Ytrain, epochs=20, batch_size=1, verbose=0,validation_data=(Xval, Yval))\nplot_model(history, 'Recurrent Neural Network Model')\n\nyhat_d = [x[0] for x in model.predict(Xval, verbose=0)]\ny = [y[0] for y in Yval]\n\ntrain_predict = model.predict(Xtrain, verbose=0)\ntest_predict = model.predict(Xval, verbose=0)\n\n# Print error\ntrain_rmse, test_rmse = print_error(Ytrain, Yval, train_predict, test_predict)\nrmse_table = {\n    'model': ['Recurrent Neural Network'],\n    'training_rmse': [train_rmse],\n    'testing_rmse': [test_rmse]\n}\n\n\nTrain RMSE: 0.299 RMSE\nTest RMSE: 1.897 RMSE\n\n\n\n\n\n\n\n\n\nCode\n# Create a recurrent neural network with regularization\n\nmodel = create_RNN(hidden_units=5, dense_units=1, input_shape=(lag, Xtrain.shape[-1]),\n                   activation=['tanh', 'tanh'], kernel_regularizer=regularizers.L1L2(l1=1e-5, l2=1e-4), dropout_rate = 0.2)\nhistory = model.fit(Xtrain, Ytrain, epochs=20, batch_size=1, verbose=0, validation_data=(Xval, Yval))\nplot_model(history, 'Recurrent Neural Network Model (with L1L2 Regularization)')\n\nyhat_d_reg = [x[0] for x in model.predict(Xval, verbose=0)]\n\ntrain_predict = model.predict(Xtrain, verbose=0)\ntest_predict = model.predict(Xval, verbose=0)\n\n# Print error\ntrain_rmse, test_rmse = print_error(Ytrain, Yval, train_predict, test_predict)\nrmse_table['model'].append(\n    'Recurrent Neural Network (with L1L2 Regularization)')\nrmse_table['training_rmse'].append(train_rmse)\nrmse_table['testing_rmse'].append(test_rmse)\n\n\nTrain RMSE: 0.297 RMSE\nTest RMSE: 1.899 RMSE\n\n\n\n\n\n\n\n\nThe second plot illustrates the impact of regularization on the training loss. Compared to the original plot, there are significant differences. The regularization technique had a substantial effect on the testing error of the model. The model trained without regularization had a slightly lower training error of 0.294 RMSE but a considerably higher testing error of 1.897 RMSE which is the same as the RMSE values with regularization.\n\n\nGRU Neural Network\nA GRU neural network is a powerful machine learning tool widely adopted in various applications. Its key feature is the recurrent gated unit, a type of memory cell that can store information over a data sequence. The GRU neural network is five hidden layers, one dense layer, and the hyperbolic tangent activation function. The model also includes kernel regularization to help address the issue of overfitting. The models are trained for 20 epochs and validation loss plots were generated to compare the performance of the regularized and non-regularized models. Overall, the GRU neural network has shown great promise in accurately predicting future movements in the sector.\n\nGRUGRU with Regularization\n\n\n\n\nCode\n# Create a GRU Neural Network\ndef create_GRU(hidden_units, dense_units, input_shape, activation,dropout_rate=0.2, kernel_regularizer=None):\n    model = Sequential()\n    # Create a simple GRU neural network layer\n    model.add(GRU(hidden_units, input_shape=input_shape,\n              activation=activation[0]))\n    # Add a dense layer (only one, more layers would make it a deep neural net)\n    model.add(Dense(units=dense_units,\n              activation=activation[1], kernel_regularizer=kernel_regularizer))\n    # Add layer dropout\n    model.add(Dropout(dropout_rate))\n    # Compile the model and optimize on mean squared error\n    model.compile(loss='mean_squared_error', optimizer='sgd')\n    return model\n\n# Training and evaluating a GRU-based model\nmodel = create_GRU(hidden_units=5, dense_units=1, input_shape=(lag, Xtrain.shape[-1]),\n                   activation=['tanh', 'relu'], dropout_rate = 0.2)\nhistory = model.fit(Xtrain, Ytrain, epochs=20, batch_size=1, verbose=0)\nplot_model(history, 'GRU Model')\n\nyhat_gru = [x[0] for x in model.predict(Xval, verbose=0)]\n\ntrain_predict = model.predict(Xtrain, verbose=0)\ntest_predict = model.predict(Xval, verbose=0)\n\n# Print error\ntrain_rmse, test_rmse = print_error(Ytrain, Yval, train_predict, test_predict)\nrmse_table['model'].append('GRU Neural Network')\nrmse_table['training_rmse'].append(train_rmse)\nrmse_table['testing_rmse'].append(test_rmse)\n\n\nTrain RMSE: 0.676 RMSE\nTest RMSE: 0.972 RMSE\n\n\n\n\n\n\n\n\n\nCode\n# Training and evaluating a GRU-based model with regularization\nmodel = create_GRU(hidden_units=5, dense_units=1, input_shape=(lag, Xtrain.shape[-1]),\n                   activation=['tanh', 'relu'],dropout_rate = 0.2,  kernel_regularizer=regularizers.L1L2(l1=1e-5, l2=1e-4))\nhistory = model.fit(Xtrain, Ytrain, epochs=20, batch_size=1, verbose=0)\nplot_model(history, 'GRU Model (with L1L2 Regularization)')\n\nyhat_gru_reg = [x[0] for x in model.predict(Xval, verbose=0)]\n\ntrain_predict = model.predict(Xtrain, verbose=0)\ntest_predict = model.predict(Xval, verbose=0)\n\n# Print error\ntrain_rmse, test_rmse = print_error(Ytrain, Yval, train_predict, test_predict)\nrmse_table['model'].append('GRU Neural Network (with L1L2 Regularization)')\nrmse_table['training_rmse'].append(train_rmse)\nrmse_table['testing_rmse'].append(test_rmse)\n\n\nTrain RMSE: 0.676 RMSE\nTest RMSE: 0.999 RMSE\n\n\n\n\n\n\n\n\nThe two plots show the training loss for the GRU model, one with regularization and one without. The plot with regularization is similar to without regularization plot. The model without regularization had a train RMSE of 0.675 and a test RMSE of 0.937,but the values are the slightly higher for with regularization.\n\n\nLSTM Neural Network\nThe LSTM (Long Short-Term Memory) neural network is another powerful machine learning tool that has been widely adopted in various applications, particularly in tasks involving sequential data.The LSTM neural network architecture typically consists of multiple LSTM layers, followed by one or more dense layers and an activation function such as the sigmoid or hyperbolic tangent. Similar to the GRU neural network, the LSTM model may also include kernel regularization to address the issue of overfitting. In order to evaluate the performance of the LSTM model, the model can be trained on a dataset for a specified number of epochs, and validation loss plots can be generated to compare the performance of the regularized and non-regularized models.\n\nLSTMLSTM with Regularization\n\n\n\n\nCode\n# Create a LSTM Neural Network\ndef create_LSTM(hidden_units, dense_units, input_shape, activation,dropout_rate = 0.2, kernel_regularizer=None):\n    model = Sequential()\n    # Create a simple long short term memory neural network\n    model.add(LSTM(hidden_units,\n              activation=activation[0], input_shape=input_shape))\n    # Add a dense layer (only one, more layers would make it a deep neural net)\n    model.add(Dense(units=dense_units,\n              activation=activation[1], kernel_regularizer=kernel_regularizer))\n    # Add layer dropout\n    model.add(Dropout(dropout_rate))\n    # Compile the model and optimize on mean squared error\n    model.compile(optimizer=\"RMSprop\", loss='mae')\n    return model\n\n# Create an LSTM neural network\nmodel = create_LSTM(hidden_units=5, dense_units=1, input_shape=(lag, Xtrain.shape[-1]),\n                    activation=['tanh', 'linear'], dropout_rate = 0.2)\nhistory = model.fit(Xtrain, Ytrain, epochs=20, batch_size=1, verbose=0)\nplot_model(history, 'LSTM Model')\n\nyhat_lstm = [x[0] for x in model.predict(Xval, verbose=0)]\n\ntrain_predict = model.predict(Xtrain, verbose=0)\ntest_predict = model.predict(Xval, verbose=0)\n\n# Print error\ntrain_rmse, test_rmse = print_error(Ytrain, Yval, train_predict, test_predict)\nrmse_table['model'].append('LSTM Neural Network')\nrmse_table['training_rmse'].append(train_rmse)\nrmse_table['testing_rmse'].append(test_rmse)\n\n\nTrain RMSE: 0.202 RMSE\nTest RMSE: 0.789 RMSE\n\n\n\n\n\n\n\n\n\nCode\n# Create an LSTM neural network with regularization\nmodel = create_LSTM(hidden_units=5, dense_units=1, input_shape=(lag, Xtrain.shape[-1]),\n                    activation=['tanh', 'linear'], dropout_rate = 0.2, kernel_regularizer=regularizers.L1L2(l1=1e-5, l2=1e-4))\nhistory = model.fit(Xtrain, Ytrain, epochs=20, batch_size=1, verbose=0)\nplot_model(history, 'LSTM Model (with L1L2 Regularization)')\n\n\nyhat_lstm_reg = [x[0] for x in model.predict(Xval, verbose=0)]\n\ntrain_predict = model.predict(Xtrain, verbose=0)\ntest_predict = model.predict(Xval, verbose=0)\n\n# Print error\ntrain_rmse, test_rmse = print_error(Ytrain, Yval, train_predict, test_predict)\nrmse_table['model'].append('LSTM Neural Network (with L1L2 Regularization)')\nrmse_table['training_rmse'].append(train_rmse)\nrmse_table['testing_rmse'].append(test_rmse)\n\n\nTrain RMSE: 0.196 RMSE\nTest RMSE: 0.844 RMSE\n\n\n\n\n\n\n\n\nThe two plots show the training loss for the LSTM model, one with regularization and one without. The plot with regularization shows less fluctuation, indicating that overfitting has been reduced. The model without regularization had a train RMSE of 0.208 and a test RMSE of 0.811, while the model with regularization had a slightly higher for both the train and test RMSE values which are 0.209 and 0.902 respectively.\n\n\nForecast\n\n\nCode\n# Creating the frame to store both predictions\ndays = df['Date'].values[-len(y):]\nframe = pd.concat([\n    pd.DataFrame({'day': days, 'price': y, 'type': 'original'}),\n    pd.DataFrame({'day': days, 'price': yhat_d, 'type': 'rnn_forecast'}),\n    pd.DataFrame({'day': days, 'price': yhat_gru, 'type': 'gru_forecast'}),\n    pd.DataFrame({'day': days, 'price': yhat_lstm, 'type': 'lstm_forecast'})\n])\n# Creating the unscaled values column\nframe['price_absolute'] = [(x * train_std) + train_mean for x in frame['price']]\n# Pivoting\npivoted = frame.pivot_table(index='day', columns='type')\npivoted.columns = ['_'.join(x).strip() for x in pivoted.columns.values]\n\nplt.figure(figsize=(12, 10))\nplt.plot(pivoted.index, pivoted.price_absolute_original,\n         color='#8B1874', label='original')\nplt.plot(pivoted.index, pivoted.price_absolute_rnn_forecast,\n         color='#F79540', label='RNN Forecast', alpha=0.6)\nplt.plot(pivoted.index, pivoted.price_absolute_gru_forecast,\n         color='#B71375', label='GRU Forecast', alpha=0.6)\nplt.plot(pivoted.index, pivoted.price_absolute_lstm_forecast,\n         color='#FC4F00', label='LSTM Forecast', alpha=0.6)\nplt.title('Financial Sector Fund Forecasts')\nplt.legend()\nplt.show()\n\n\n\n\n\nWhen comparing the predictions of the deep learning models with the original time series plot, we can observe that the LSTM model is the closest to the actual values. The GRU model is almost the same as the LSTM model, but both the models have some kind of accuracy with the actual plot, but there is still difference. RNN model are pretty far from the actual plot, which indicates that LSTM model is better when compared to others, but we need to note that the model is not accurate, i.e., it is not the same as the actual plot.\n\n\nModel Comparison\n\n\nCode\nrmse_df = pd.DataFrame(rmse_table)\nrmse_df\n\n\n\n\n\n\n  \n    \n      \n      model\n      training_rmse\n      testing_rmse\n    \n  \n  \n    \n      0\n      Recurrent Neural Network\n      0.298743\n      1.897114\n    \n    \n      1\n      Recurrent Neural Network (with L1L2 Regulariza...\n      0.296614\n      1.898516\n    \n    \n      2\n      GRU Neural Network\n      0.675613\n      0.971678\n    \n    \n      3\n      GRU Neural Network (with L1L2 Regularization)\n      0.676171\n      0.999071\n    \n    \n      4\n      LSTM Neural Network\n      0.201782\n      0.789076\n    \n    \n      5\n      LSTM Neural Network (with L1L2 Regularization)\n      0.195903\n      0.844190\n    \n  \n\n\n\n\nComparing the models, we can see that the LSTM Neural Network (with L1L2 Regularization) has consistently performed better than the other models across all datasets, as it has the lowest testing RMSE values in almost all cases. The LSTM model without regularization has also performed well but not as good as with regularization.\nThe Recurrent Neural Network and GRU Neural Network have shown mixed results, with some datasets performing well with regularization and others without regularization.\nIt is worth noting that the GRU Neural Network (with L1L2 Regularization) has significantly reduced the testing RMSE value on the third dataset compared to the non-regularized model. This suggests that regularization can help in preventing overfitting and improving the model’s generalization ability.\nOverall, the comparison of deep learning models with and without regularization suggests that regularization can help in improving the model’s performance and generalization ability.\n\n\nComparison of deep learning models with traditional single variable time series\nThe deep learning models (RNN, GRU, LSTM) perform better than the ARIMA model in both training and testing RMSE values for the Financial Sector Fund. Among the deep learning models, the LSTM model without regularization has the lowest training RMSE value, when compared to ARIMA Model which is 0.4622021, indicating its better performance in predicting the market prices of the Financial Sector Fund compared to the other deep learning models and ARIMA model."
  },
  {
    "objectID": "deep_learning_sector_xle.html",
    "href": "deep_learning_sector_xle.html",
    "title": "",
    "section": "",
    "text": "Deep Learning for Energy Sector Fund\nThe energy sector fund comprises companies that are involved in the exploration, production, and distribution of energy resources, including oil, gas, and renewable energy sources. These companies are often sensitive to changes in global energy prices and can be impacted by geopolitical events and environmental regulations. As with other stock market, predicting the future movements of the Energy Sector Fund can be challenging due to the complex and dynamic nature of the market. These models range from traditional time-series models to more advanced deep learning models. By leveraging the power of Deep Learning and applying techniques such as RNN, LSTM, GRU, and regularization, we can create a model that accurately predicts the future movements of the sector. Such a model can provide valuable insights for investors and traders looking to make informed decisions in the stock market, and help them stay ahead of the curve in the ever-changing landscape of the technology and biotech industries.\nIn order to apply the deep learning methods, the time series data is first split into training and testing sets, taking into account the time component. The training data constitutes 75% of the total data while the rest is the testing data. Furthermore, to ensure that the values are on the same scale, the data is scaled, which is crucial for deep learning models. The mean and standard deviation of the training data are computed and then utilized to normalize both the training and testing sets. This approach ensures that the models are trained on a standardized dataset, enabling them to make accurate predictions.\n\n\nCode\n# libraries\nimport numpy as np\nimport pandas as pd\nimport math\nfrom sklearn.metrics import mean_squared_error\nfrom matplotlib import pyplot as plt\nfrom keras.models import Sequential\nfrom keras.layers import Dropout\nfrom keras.layers import Dense, SimpleRNN, LSTM, GRU\nfrom keras import regularizers\nimport warnings\nwarnings.filterwarnings('ignore')\n\n# import data\ndf = pd.read_csv('./DATA/CLEANED DATA/XLE_raw_data.csv')\n# clean data and get final data for anlaysis\ndf = df[['Date', 'XLE.Adjusted']]\ndf['Date'] = pd.to_datetime(df['Date'])\ndf['XLE.Adjusted'] = pd.to_numeric(df['XLE.Adjusted'], errors='coerce')\ndf = df.dropna(subset=['XLE.Adjusted'])\ndf.sort_values('Date', inplace=True, ascending=True)\ndf = df.reset_index(drop=True)\ndef create_X_Y(ts: np.array, lag=1, n_ahead=1, target_index=0) -> tuple:\n    \"\"\"\n    A method to create X and Y matrix from a time series array for the training of \n    deep learning models \n    \"\"\"\n    # Extracting the number of features that are passed from the array\n    n_features = ts.shape[1]\n\n    # Creating placeholder lists\n    X, Y = [], []\n\n    if len(ts) - lag <= 0:\n        X.append(ts)\n    else:\n        for i in range(len(ts) - lag - n_ahead):\n            Y.append(ts[(i + lag):(i + lag + n_ahead), target_index])\n            X.append(ts[i:(i + lag)])\n\n    X, Y = np.array(X), np.array(Y)\n\n    # Reshaping the X array to an RNN input shape\n    X = np.reshape(X, (X.shape[0], lag, n_features))\n\n    return X, Y\n\nts = df[['XLE.Adjusted']].values\nnp.random.seed(123)\nnrows = ts.shape[0]\ntest_share = 0.25\n# Spliting into train and test sets\ntrain = ts[0:int(nrows * (1 - test_share))]\ntest = ts[int(nrows * (1 - test_share)):]\n\n# Scaling the data\ntrain_mean = train.mean()\ntrain_std = train.std()\ntrain = (train - train_mean) / train_std\ntest = (test - train_mean) / train_std\n\n# Creating the final scaled frame\nts_s = np.concatenate([train, test])\n\nlag = 12\nahead = 3\n\n# Creating the X and Y for training\nX, Y = create_X_Y(ts_s, lag=lag, n_ahead=ahead)\n\nXtrain = X[0:int(X.shape[0] * (1 - test_share))]\nYtrain = Y[0:int(X.shape[0] * (1 - test_share))]\n\nXval = X[int(X.shape[0] * (1 - test_share)):]\nYval = Y[int(X.shape[0] * (1 - test_share)):]\n\n\n\n\ndef plot_model(history, model_title):\n    loss = history.history['loss']\n    epochs = range(1, len(loss) + 1)\n    plt.figure()\n    plt.plot(epochs, loss, 'b', label='Training loss', color = \"gray\")\n    plt.title(f'{model_title} Training loss ')\n    plt.legend()\n    \n\n\ndef print_error(trainY, testY, train_predict, test_predict):\n    # Error of predictions\n    train_rmse = math.sqrt(mean_squared_error(\n        trainY[:, 0], train_predict[:, 0]))\n    test_rmse = math.sqrt(mean_squared_error(testY[:, 0], test_predict[:, 0]))\n    # Print RMSE\n    print('Train RMSE: %.3f RMSE' % (train_rmse))\n    print('Test RMSE: %.3f RMSE' % (test_rmse))\n    return train_rmse, test_rmse\n\n\n\n\nRecurrent Neural Network\nThe RNN models that are constructed and trained using TensorFlow and Keras. This model is trained for five hidden layers, one dense layer, and the activation function hyperbolic tangent. A dropout rate of 0.2 is used to reduce the risk of overfitting. The model are trained for 20 epochs and generated validation loss plots to compare the performance of the regularized and non-regularized models. The second RNN model was identical to the first, except for the addition of kernel regularization.\n\nRNNRNN with Regularization\n\n\n\n\nCode\n# Create a RNN\ndef create_RNN(hidden_units, dense_units, input_shape, activation,dropout_rate=0.2,kernel_regularizer=None):\n    model = Sequential()\n    # Create a simple neural network layer\n    model.add(SimpleRNN(hidden_units, input_shape=input_shape,\n              activation=activation[0]))\n    # Add a dense layer (only one, more layers would make it a deep neural net)\n    model.add(Dense(units=dense_units,\n              activation=activation[1],\n              kernel_regularizer=kernel_regularizer))\n    # Add layer dropout\n    model.add(Dropout(dropout_rate))\n    \n    # Compile the model and optimize on mean squared error\n    model.compile(loss='mean_squared_error', optimizer='adam')\n    return model\n  \n# Create a recurrent neural network\nmodel = create_RNN(hidden_units=5, dense_units=1, input_shape=(lag, Xtrain.shape[-1]),\n                   activation=['tanh', 'tanh'], dropout_rate = 0.2)\nhistory = model.fit(Xtrain, Ytrain, epochs=20, batch_size=1, verbose=0,validation_data=(Xval, Yval))\nplot_model(history, 'Recurrent Neural Network Model')\n\nyhat_d = [x[0] for x in model.predict(Xval, verbose=0)]\ny = [y[0] for y in Yval]\n\ntrain_predict = model.predict(Xtrain, verbose=0)\ntest_predict = model.predict(Xval, verbose=0)\n\n# Print error\ntrain_rmse, test_rmse = print_error(Ytrain, Yval, train_predict, test_predict)\nrmse_table = {\n    'model': ['Recurrent Neural Network'],\n    'training_rmse': [train_rmse],\n    'testing_rmse': [test_rmse]\n}\n\n\nTrain RMSE: 0.440 RMSE\nTest RMSE: 1.896 RMSE\n\n\n\n\n\n\n\n\n\nCode\n# Create a recurrent neural network with regularization\n\nmodel = create_RNN(hidden_units=5, dense_units=1, input_shape=(lag, Xtrain.shape[-1]),\n                   activation=['tanh', 'tanh'], kernel_regularizer=regularizers.L1L2(l1=1e-5, l2=1e-4), dropout_rate = 0.2)\nhistory = model.fit(Xtrain, Ytrain, epochs=20, batch_size=1, verbose=0, validation_data=(Xval, Yval))\nplot_model(history, 'Recurrent Neural Network Model (with L1L2 Regularization)')\n\nyhat_d_reg = [x[0] for x in model.predict(Xval, verbose=0)]\n\ntrain_predict = model.predict(Xtrain, verbose=0)\ntest_predict = model.predict(Xval, verbose=0)\n\n# Print error\ntrain_rmse, test_rmse = print_error(Ytrain, Yval, train_predict, test_predict)\nrmse_table['model'].append(\n    'Recurrent Neural Network (with L1L2 Regularization)')\nrmse_table['training_rmse'].append(train_rmse)\nrmse_table['testing_rmse'].append(test_rmse)\n\n\nTrain RMSE: 0.438 RMSE\nTest RMSE: 1.897 RMSE\n\n\n\n\n\n\n\n\nThe second plot illustrates the impact of regularization on the training loss. Compared to the original plot, there are significant differences. The regularization technique had a substantial effect on the testing error of the model. The model trained without regularization have training error of 0.442 RMSE and testing error of 1.896 RMSE which are the same as with regularization.\n\n\nGRU Neural Network\nA GRU neural network is a powerful machine learning tool widely adopted in various applications. Its key feature is the recurrent gated unit, a type of memory cell that can store information over a data sequence. The GRU neural network is five hidden layers, one dense layer, and the hyperbolic tangent activation function. The model also includes kernel regularization to help address the issue of overfitting. The models are trained for 20 epochs and validation loss plots were generated to compare the performance of the regularized and non-regularized models. Overall, the GRU neural network has shown great promise in accurately predicting future movements in the sector.\n\nGRUGRU with Regularization\n\n\n\n\nCode\n# Create a GRU Neural Network\ndef create_GRU(hidden_units, dense_units, input_shape, activation,dropout_rate=0.2, kernel_regularizer=None):\n    model = Sequential()\n    # Create a simple GRU neural network layer\n    model.add(GRU(hidden_units, input_shape=input_shape,\n              activation=activation[0]))\n    # Add a dense layer (only one, more layers would make it a deep neural net)\n    model.add(Dense(units=dense_units,\n              activation=activation[1], kernel_regularizer=kernel_regularizer))\n    # Add layer dropout\n    model.add(Dropout(dropout_rate))\n    # Compile the model and optimize on mean squared error\n    model.compile(loss='mean_squared_error', optimizer='sgd')\n    return model\n\n# Training and evaluating a GRU-based model\nmodel = create_GRU(hidden_units=5, dense_units=1, input_shape=(lag, Xtrain.shape[-1]),\n                   activation=['tanh', 'relu'], dropout_rate = 0.2)\nhistory = model.fit(Xtrain, Ytrain, epochs=20, batch_size=1, verbose=0)\nplot_model(history, 'GRU Model')\n\nyhat_gru = [x[0] for x in model.predict(Xval, verbose=0)]\n\ntrain_predict = model.predict(Xtrain, verbose=0)\ntest_predict = model.predict(Xval, verbose=0)\n\n# Print error\ntrain_rmse, test_rmse = print_error(Ytrain, Yval, train_predict, test_predict)\nrmse_table['model'].append('GRU Neural Network')\nrmse_table['training_rmse'].append(train_rmse)\nrmse_table['testing_rmse'].append(test_rmse)\n\n\nTrain RMSE: 0.995 RMSE\nTest RMSE: 2.655 RMSE\n\n\n\n\n\n\n\n\n\nCode\n# Training and evaluating a GRU-based model with regularization\nmodel = create_GRU(hidden_units=5, dense_units=1, input_shape=(lag, Xtrain.shape[-1]),\n                   activation=['tanh', 'relu'],dropout_rate = 0.2,  kernel_regularizer=regularizers.L1L2(l1=1e-5, l2=1e-4))\nhistory = model.fit(Xtrain, Ytrain, epochs=20, batch_size=1, verbose=0)\nplot_model(history, 'GRU Model (with L1L2 Regularization)')\n\nyhat_gru_reg = [x[0] for x in model.predict(Xval, verbose=0)]\n\ntrain_predict = model.predict(Xtrain, verbose=0)\ntest_predict = model.predict(Xval, verbose=0)\n\n# Print error\ntrain_rmse, test_rmse = print_error(Ytrain, Yval, train_predict, test_predict)\nrmse_table['model'].append('GRU Neural Network (with L1L2 Regularization)')\nrmse_table['training_rmse'].append(train_rmse)\nrmse_table['testing_rmse'].append(test_rmse)\n\n\nTrain RMSE: 0.723 RMSE\nTest RMSE: 1.702 RMSE\n\n\n\n\n\n\n\n\nThe two plots show the training loss for the GRU model, one with regularization and one without. The plot with regularization is similar to without regularization plot. The model without regularization had a train RMSE of 0.726 and a test RMSE of 1.699, the values are the lower when compared to with regularization.\n\n\nLSTM Neural Network\nThe LSTM (Long Short-Term Memory) neural network is another powerful machine learning tool that has been widely adopted in various applications, particularly in tasks involving sequential data.The LSTM neural network architecture typically consists of multiple LSTM layers, followed by one or more dense layers and an activation function such as the sigmoid or hyperbolic tangent. Similar to the GRU neural network, the LSTM model may also include kernel regularization to address the issue of overfitting. In order to evaluate the performance of the LSTM model, the model can be trained on a dataset for a specified number of epochs, and validation loss plots can be generated to compare the performance of the regularized and non-regularized models.\n\nLSTMLSTM with Regularization\n\n\n\n\nCode\n# Create a LSTM Neural Network\ndef create_LSTM(hidden_units, dense_units, input_shape, activation,dropout_rate = 0.2, kernel_regularizer=None):\n    model = Sequential()\n    # Create a simple long short term memory neural network\n    model.add(LSTM(hidden_units,\n              activation=activation[0], input_shape=input_shape))\n    # Add a dense layer (only one, more layers would make it a deep neural net)\n    model.add(Dense(units=dense_units,\n              activation=activation[1], kernel_regularizer=kernel_regularizer))\n    # Add layer dropout\n    model.add(Dropout(dropout_rate))\n    # Compile the model and optimize on mean squared error\n    model.compile(optimizer=\"RMSprop\", loss='mae')\n    return model\n\n# Create an LSTM neural network\nmodel = create_LSTM(hidden_units=5, dense_units=1, input_shape=(lag, Xtrain.shape[-1]),\n                    activation=['tanh', 'linear'], dropout_rate = 0.2)\nhistory = model.fit(Xtrain, Ytrain, epochs=20, batch_size=1, verbose=0)\nplot_model(history, 'LSTM Model')\n\nyhat_lstm = [x[0] for x in model.predict(Xval, verbose=0)]\n\ntrain_predict = model.predict(Xtrain, verbose=0)\ntest_predict = model.predict(Xval, verbose=0)\n\n# Print error\ntrain_rmse, test_rmse = print_error(Ytrain, Yval, train_predict, test_predict)\nrmse_table['model'].append('LSTM Neural Network')\nrmse_table['training_rmse'].append(train_rmse)\nrmse_table['testing_rmse'].append(test_rmse)\n\n\nTrain RMSE: 0.212 RMSE\nTest RMSE: 0.960 RMSE\n\n\n\n\n\n\n\n\n\nCode\n# Create an LSTM neural network with regularization\nmodel = create_LSTM(hidden_units=5, dense_units=1, input_shape=(lag, Xtrain.shape[-1]),\n                    activation=['tanh', 'linear'], dropout_rate = 0.2, kernel_regularizer=regularizers.L1L2(l1=1e-5, l2=1e-4))\nhistory = model.fit(Xtrain, Ytrain, epochs=20, batch_size=1, verbose=0)\nplot_model(history, 'LSTM Model (with L1L2 Regularization)')\n\n\nyhat_lstm_reg = [x[0] for x in model.predict(Xval, verbose=0)]\n\ntrain_predict = model.predict(Xtrain, verbose=0)\ntest_predict = model.predict(Xval, verbose=0)\n\n# Print error\ntrain_rmse, test_rmse = print_error(Ytrain, Yval, train_predict, test_predict)\nrmse_table['model'].append('LSTM Neural Network (with L1L2 Regularization)')\nrmse_table['training_rmse'].append(train_rmse)\nrmse_table['testing_rmse'].append(test_rmse)\n\n\nTrain RMSE: 0.219 RMSE\nTest RMSE: 0.996 RMSE\n\n\n\n\n\n\n\n\nThe two plots show the training loss for the LSTM model, one with regularization and one without. The plot with regularization shows less fluctuation, indicating that overfitting has been reduced. The model without regularization had a train RMSE of 0.222 and a test RMSE of 0.985, while the model with regularization had a slightly lower for both the train and test RMSE values which are 0.201 and 0.934 respectively.\n\n\nForecast\n\n\nCode\n# Creating the frame to store both predictions\ndays = df['Date'].values[-len(y):]\nframe = pd.concat([\n    pd.DataFrame({'day': days, 'price': y, 'type': 'original'}),\n    pd.DataFrame({'day': days, 'price': yhat_d, 'type': 'rnn_forecast'}),\n    pd.DataFrame({'day': days, 'price': yhat_gru, 'type': 'gru_forecast'}),\n    pd.DataFrame({'day': days, 'price': yhat_lstm, 'type': 'lstm_forecast'})\n])\n# Creating the unscaled values column\nframe['price_absolute'] = [(x * train_std) + train_mean for x in frame['price']]\n# Pivoting\npivoted = frame.pivot_table(index='day', columns='type')\npivoted.columns = ['_'.join(x).strip() for x in pivoted.columns.values]\n\nplt.figure(figsize=(12, 10))\nplt.plot(pivoted.index, pivoted.price_absolute_original,\n         color='#8B1874', label='original')\nplt.plot(pivoted.index, pivoted.price_absolute_rnn_forecast,\n         color='#F79540', label='RNN Forecast', alpha=0.6)\nplt.plot(pivoted.index, pivoted.price_absolute_gru_forecast,\n         color='#B71375', label='GRU Forecast', alpha=0.6)\nplt.plot(pivoted.index, pivoted.price_absolute_lstm_forecast,\n         color='#FC4F00', label='LSTM Forecast', alpha=0.6)\nplt.title('Energy Sector Fund Forecasts')\nplt.legend()\nplt.show()\n\n\n\n\n\nWhen comparing the predictions of the deep learning models with the original time series plot, we can observe that the LSTM model is the closest to the actual values. The GRU model is pretty accurate at the end, but they there far from the actual plot at the start. LSTM model is better when compared to others, but when we look at the LSTM model, the model is not accurate, i.e., it is not the same as the actual plot.\n\n\nModel Comparison\n\n\nCode\nrmse_df = pd.DataFrame(rmse_table)\nrmse_df\n\n\n\n\n\n\n  \n    \n      \n      model\n      training_rmse\n      testing_rmse\n    \n  \n  \n    \n      0\n      Recurrent Neural Network\n      0.439775\n      1.895583\n    \n    \n      1\n      Recurrent Neural Network (with L1L2 Regulariza...\n      0.437948\n      1.897086\n    \n    \n      2\n      GRU Neural Network\n      0.995234\n      2.655007\n    \n    \n      3\n      GRU Neural Network (with L1L2 Regularization)\n      0.723280\n      1.701707\n    \n    \n      4\n      LSTM Neural Network\n      0.211914\n      0.960475\n    \n    \n      5\n      LSTM Neural Network (with L1L2 Regularization)\n      0.218544\n      0.996413\n    \n  \n\n\n\n\nThe Recurrent Neural Network (RNN) model and the RNN with L1L2 Regularization have similar training and testing RMSE values, with the regularized version performing slightly better on the testing data.\nThe GRU Neural Network has a low testing RMSE but a relatively high training RMSE, indicating potential overfitting to the training data. The GRU with L1L2 Regularization has a higher testing RMSE than the non-regularized version, suggesting that regularization may not be helpful in this case.\nThe LSTM Neural Network and the LSTM with L1L2 Regularization both have low training and testing RMSE values, with the regularized version performing slightly better on the testing data.\nOverall, the LSTM Neural Network with L1L2 Regularization appears to perform the best on this dataset, with the lowest testing RMSE value. The GRU Neural Network also performs well on the testing data, but may be overfitting to the training data.\n\n\nComparison of deep learning models with traditional single variable time series\nThe deep learning models (RNN, GRU, LSTM) perform better than the ARIMA model in both training and testing RMSE values for the Energy Sector Fund. Among the deep learning models, the LSTM model without regularization has the lowest testing RMSE value of 1.032129, when compared to ARIMA Model which is 2.118625, indicating its better performance in predicting the market prices of the Energy Sector Fund compared to the other deep learning models and ARIMA model."
  },
  {
    "objectID": "deep_learning_sector_xlc.html",
    "href": "deep_learning_sector_xlc.html",
    "title": "",
    "section": "",
    "text": "Deep Learning for Communication Services Sector Fund\nThe Communication Services sector comprises companies that provide non-essential goods and services, such as clothing, entertainment, and luxury items. These companies tend to be more sensitive to economic cycles and consumer sentiment than companies in other sectors. As with other stock market, predicting the future movements of the Communication Services Sector Fund can be challenging due to the complex and dynamic nature of the market. These models range from traditional time-series models to more advanced deep learning models. By leveraging the power of Deep Learning and applying techniques such as RNN, LSTM, GRU, and regularization, we can create a model that accurately predicts the future movements of the sector. Such a model can provide valuable insights for investors and traders looking to make informed decisions in the stock market, and help them stay ahead of the curve in the ever-changing landscape of the technology and biotech industries.\nIn order to apply the deep learning methods, the time series data is first split into training and testing sets, taking into account the time component. The training data constitutes 75% of the total data while the rest is the testing data. Furthermore, to ensure that the values are on the same scale, the data is scaled, which is crucial for deep learning models. The mean and standard deviation of the training data are computed and then utilized to normalize both the training and testing sets. This approach ensures that the models are trained on a standardized dataset, enabling them to make accurate predictions.\n\n\nCode\n# libraries\nimport numpy as np\nimport pandas as pd\nimport math\nfrom sklearn.metrics import mean_squared_error\nfrom matplotlib import pyplot as plt\nfrom keras.models import Sequential\nfrom keras.layers import Dropout\nfrom keras.layers import Dense, SimpleRNN, LSTM, GRU\nfrom keras import regularizers\nimport warnings\nwarnings.filterwarnings('ignore')\n\n# import data\ndf = pd.read_csv('./DATA/CLEANED DATA/XLC_raw_data.csv')\n# clean data and get final data for anlaysis\ndf = df[['Date', 'XLC.Adjusted']]\ndf['Date'] = pd.to_datetime(df['Date'])\ndf['XLC.Adjusted'] = pd.to_numeric(df['XLC.Adjusted'], errors='coerce')\ndf = df.dropna(subset=['XLC.Adjusted'])\ndf.sort_values('Date', inplace=True, ascending=True)\ndf = df.reset_index(drop=True)\ndef create_X_Y(ts: np.array, lag=1, n_ahead=1, target_index=0) -> tuple:\n    \"\"\"\n    A method to create X and Y matrix from a time series array for the training of \n    deep learning models \n    \"\"\"\n    # Extracting the number of features that are passed from the array\n    n_features = ts.shape[1]\n\n    # Creating placeholder lists\n    X, Y = [], []\n\n    if len(ts) - lag <= 0:\n        X.append(ts)\n    else:\n        for i in range(len(ts) - lag - n_ahead):\n            Y.append(ts[(i + lag):(i + lag + n_ahead), target_index])\n            X.append(ts[i:(i + lag)])\n\n    X, Y = np.array(X), np.array(Y)\n\n    # Reshaping the X array to an RNN input shape\n    X = np.reshape(X, (X.shape[0], lag, n_features))\n\n    return X, Y\n\nts = df[['XLC.Adjusted']].values\nnp.random.seed(123)\nnrows = ts.shape[0]\ntest_share = 0.25\n# Spliting into train and test sets\ntrain = ts[0:int(nrows * (1 - test_share))]\ntest = ts[int(nrows * (1 - test_share)):]\n\n# Scaling the data\ntrain_mean = train.mean()\ntrain_std = train.std()\ntrain = (train - train_mean) / train_std\ntest = (test - train_mean) / train_std\n\n# Creating the final scaled frame\nts_s = np.concatenate([train, test])\n\nlag = 12\nahead = 3\n\n# Creating the X and Y for training\nX, Y = create_X_Y(ts_s, lag=lag, n_ahead=ahead)\n\nXtrain = X[0:int(X.shape[0] * (1 - test_share))]\nYtrain = Y[0:int(X.shape[0] * (1 - test_share))]\n\nXval = X[int(X.shape[0] * (1 - test_share)):]\nYval = Y[int(X.shape[0] * (1 - test_share)):]\n\n\n\n\ndef plot_model(history, model_title):\n    loss = history.history['loss']\n    epochs = range(1, len(loss) + 1)\n    plt.figure()\n    plt.plot(epochs, loss, 'b', label='Training loss', color = \"gray\")\n    plt.title(f'{model_title} Training loss ')\n    plt.legend()\n    \n\n\ndef print_error(trainY, testY, train_predict, test_predict):\n    # Error of predictions\n    train_rmse = math.sqrt(mean_squared_error(\n        trainY[:, 0], train_predict[:, 0]))\n    test_rmse = math.sqrt(mean_squared_error(testY[:, 0], test_predict[:, 0]))\n    # Print RMSE\n    print('Train RMSE: %.3f RMSE' % (train_rmse))\n    print('Test RMSE: %.3f RMSE' % (test_rmse))\n    return train_rmse, test_rmse\n\n\n\n\nRecurrent Neural Network\nThe RNN models that are constructed and trained using TensorFlow and Keras. This model is trained for five hidden layers, one dense layer, and the activation function hyperbolic tangent. A dropout rate of 0.2 is used to reduce the risk of overfitting. The model are trained for 20 epochs and generated validation loss plots to compare the performance of the regularized and non-regularized models. The second RNN model was identical to the first, except for the addition of kernel regularization.\n\nRNNRNN with Regularization\n\n\n\n\nCode\n# Create a RNN\ndef create_RNN(hidden_units, dense_units, input_shape, activation,dropout_rate=0.2,kernel_regularizer=None):\n    model = Sequential()\n    # Create a simple neural network layer\n    model.add(SimpleRNN(hidden_units, input_shape=input_shape,\n              activation=activation[0]))\n    # Add a dense layer (only one, more layers would make it a deep neural net)\n    model.add(Dense(units=dense_units,\n              activation=activation[1],\n              kernel_regularizer=kernel_regularizer))\n    # Add layer dropout\n    model.add(Dropout(dropout_rate))\n    \n    # Compile the model and optimize on mean squared error\n    model.compile(loss='mean_squared_error', optimizer='adam')\n    return model\n  \n# Create a recurrent neural network\nmodel = create_RNN(hidden_units=5, dense_units=1, input_shape=(lag, Xtrain.shape[-1]),\n                   activation=['tanh', 'tanh'], dropout_rate = 0.2)\nhistory = model.fit(Xtrain, Ytrain, epochs=20, batch_size=1, verbose=0,validation_data=(Xval, Yval))\nplot_model(history, 'Recurrent Neural Network Model')\n\nyhat_d = [x[0] for x in model.predict(Xval, verbose=0)]\ny = [y[0] for y in Yval]\n\ntrain_predict = model.predict(Xtrain, verbose=0)\ntest_predict = model.predict(Xval, verbose=0)\n\n# Print error\ntrain_rmse, test_rmse = print_error(Ytrain, Yval, train_predict, test_predict)\nrmse_table = {\n    'model': ['Recurrent Neural Network'],\n    'training_rmse': [train_rmse],\n    'testing_rmse': [test_rmse]\n}\n\n\nTrain RMSE: 0.332 RMSE\nTest RMSE: 0.161 RMSE\n\n\n\n\n\n\n\n\n\nCode\n# Create a recurrent neural network with regularization\n\nmodel = create_RNN(hidden_units=5, dense_units=1, input_shape=(lag, Xtrain.shape[-1]),\n                   activation=['tanh', 'tanh'], kernel_regularizer=regularizers.L1L2(l1=1e-5, l2=1e-4), dropout_rate = 0.2)\nhistory = model.fit(Xtrain, Ytrain, epochs=20, batch_size=1, verbose=0, validation_data=(Xval, Yval))\nplot_model(history, 'Recurrent Neural Network Model (with L1L2 Regularization)')\n\nyhat_d_reg = [x[0] for x in model.predict(Xval, verbose=0)]\n\ntrain_predict = model.predict(Xtrain, verbose=0)\ntest_predict = model.predict(Xval, verbose=0)\n\n# Print error\ntrain_rmse, test_rmse = print_error(Ytrain, Yval, train_predict, test_predict)\nrmse_table['model'].append(\n    'Recurrent Neural Network (with L1L2 Regularization)')\nrmse_table['training_rmse'].append(train_rmse)\nrmse_table['testing_rmse'].append(test_rmse)\n\n\nTrain RMSE: 0.342 RMSE\nTest RMSE: 0.169 RMSE\n\n\n\n\n\n\n\n\nThe second plot illustrates the impact of regularization on the training loss. Compared to the original plot, there are significant differences. The regularization technique had a substantial effect on the testing error of the model. The model trained without regularization has a slightly lower training error of 0.331 RMSE but the testing error has a slightly higher RMSE value when compared to with regularization.\n\n\nGRU Neural Network\nA GRU neural network is a powerful machine learning tool widely adopted in various applications. Its key feature is the recurrent gated unit, a type of memory cell that can store information over a data sequence. The GRU neural network is five hidden layers, one dense layer, and the hyperbolic tangent activation function. The model also includes kernel regularization to help address the issue of overfitting. The models are trained for 20 epochs and validation loss plots were generated to compare the performance of the regularized and non-regularized models. Overall, the GRU neural network has shown great promise in accurately predicting future movements in the sector.\n\nGRUGRU with Regularization\n\n\n\n\nCode\n# Create a GRU Neural Network\ndef create_GRU(hidden_units, dense_units, input_shape, activation,dropout_rate=0.2, kernel_regularizer=None):\n    model = Sequential()\n    # Create a simple GRU neural network layer\n    model.add(GRU(hidden_units, input_shape=input_shape,\n              activation=activation[0]))\n    # Add a dense layer (only one, more layers would make it a deep neural net)\n    model.add(Dense(units=dense_units,\n              activation=activation[1], kernel_regularizer=kernel_regularizer))\n    # Add layer dropout\n    model.add(Dropout(dropout_rate))\n    # Compile the model and optimize on mean squared error\n    model.compile(loss='mean_squared_error', optimizer='sgd')\n    return model\n\n# Training and evaluating a GRU-based model\nmodel = create_GRU(hidden_units=5, dense_units=1, input_shape=(lag, Xtrain.shape[-1]),\n                   activation=['tanh', 'relu'], dropout_rate = 0.2)\nhistory = model.fit(Xtrain, Ytrain, epochs=20, batch_size=1, verbose=0)\nplot_model(history, 'GRU Model')\n\nyhat_gru = [x[0] for x in model.predict(Xval, verbose=0)]\n\ntrain_predict = model.predict(Xtrain, verbose=0)\ntest_predict = model.predict(Xval, verbose=0)\n\n# Print error\ntrain_rmse, test_rmse = print_error(Ytrain, Yval, train_predict, test_predict)\nrmse_table['model'].append('GRU Neural Network')\nrmse_table['training_rmse'].append(train_rmse)\nrmse_table['testing_rmse'].append(test_rmse)\n\n\nTrain RMSE: 0.628 RMSE\nTest RMSE: 0.380 RMSE\n\n\n\n\n\n\n\n\n\nCode\n# Training and evaluating a GRU-based model with regularization\nmodel = create_GRU(hidden_units=5, dense_units=1, input_shape=(lag, Xtrain.shape[-1]),\n                   activation=['tanh', 'relu'],dropout_rate = 0.2,  kernel_regularizer=regularizers.L1L2(l1=1e-5, l2=1e-4))\nhistory = model.fit(Xtrain, Ytrain, epochs=20, batch_size=1, verbose=0)\nplot_model(history, 'GRU Model (with L1L2 Regularization)')\n\nyhat_gru_reg = [x[0] for x in model.predict(Xval, verbose=0)]\n\ntrain_predict = model.predict(Xtrain, verbose=0)\ntest_predict = model.predict(Xval, verbose=0)\n\n# Print error\ntrain_rmse, test_rmse = print_error(Ytrain, Yval, train_predict, test_predict)\nrmse_table['model'].append('GRU Neural Network (with L1L2 Regularization)')\nrmse_table['training_rmse'].append(train_rmse)\nrmse_table['testing_rmse'].append(test_rmse)\n\n\nTrain RMSE: 1.004 RMSE\nTest RMSE: 0.634 RMSE\n\n\n\n\n\n\n\n\nThe two plots show the training loss for the GRU model, one with regularization and one without. The plot with regularization is similar to without regularization plot. The model without regularization had a train RMSE of 1.004 and a test RMSE of 0.634, the values are the higher when compared to with regularization.\n\n\nLSTM Neural Network\nThe LSTM (Long Short-Term Memory) neural network is another powerful machine learning tool that has been widely adopted in various applications, particularly in tasks involving sequential data.The LSTM neural network architecture typically consists of multiple LSTM layers, followed by one or more dense layers and an activation function such as the sigmoid or hyperbolic tangent. Similar to the GRU neural network, the LSTM model may also include kernel regularization to address the issue of overfitting. In order to evaluate the performance of the LSTM model, the model can be trained on a dataset for a specified number of epochs, and validation loss plots can be generated to compare the performance of the regularized and non-regularized models.\n\nLSTMLSTM with Regularization\n\n\n\n\nCode\n# Create a LSTM Neural Network\ndef create_LSTM(hidden_units, dense_units, input_shape, activation,dropout_rate = 0.2, kernel_regularizer=None):\n    model = Sequential()\n    # Create a simple long short term memory neural network\n    model.add(LSTM(hidden_units,\n              activation=activation[0], input_shape=input_shape))\n    # Add a dense layer (only one, more layers would make it a deep neural net)\n    model.add(Dense(units=dense_units,\n              activation=activation[1], kernel_regularizer=kernel_regularizer))\n    # Add layer dropout\n    model.add(Dropout(dropout_rate))\n    # Compile the model and optimize on mean squared error\n    model.compile(optimizer=\"RMSprop\", loss='mae')\n    return model\n\n# Create an LSTM neural network\nmodel = create_LSTM(hidden_units=5, dense_units=1, input_shape=(lag, Xtrain.shape[-1]),\n                    activation=['tanh', 'linear'], dropout_rate = 0.2)\nhistory = model.fit(Xtrain, Ytrain, epochs=20, batch_size=1, verbose=0)\nplot_model(history, 'LSTM Model')\n\nyhat_lstm = [x[0] for x in model.predict(Xval, verbose=0)]\n\ntrain_predict = model.predict(Xtrain, verbose=0)\ntest_predict = model.predict(Xval, verbose=0)\n\n# Print error\ntrain_rmse, test_rmse = print_error(Ytrain, Yval, train_predict, test_predict)\nrmse_table['model'].append('LSTM Neural Network')\nrmse_table['training_rmse'].append(train_rmse)\nrmse_table['testing_rmse'].append(test_rmse)\n\n\nTrain RMSE: 0.224 RMSE\nTest RMSE: 0.150 RMSE\n\n\n\n\n\n\n\n\n\nCode\n# Create an LSTM neural network with regularization\nmodel = create_LSTM(hidden_units=5, dense_units=1, input_shape=(lag, Xtrain.shape[-1]),\n                    activation=['tanh', 'linear'], dropout_rate = 0.2, kernel_regularizer=regularizers.L1L2(l1=1e-5, l2=1e-4))\nhistory = model.fit(Xtrain, Ytrain, epochs=20, batch_size=1, verbose=0)\nplot_model(history, 'LSTM Model (with L1L2 Regularization)')\n\n\nyhat_lstm_reg = [x[0] for x in model.predict(Xval, verbose=0)]\n\ntrain_predict = model.predict(Xtrain, verbose=0)\ntest_predict = model.predict(Xval, verbose=0)\n\n# Print error\ntrain_rmse, test_rmse = print_error(Ytrain, Yval, train_predict, test_predict)\nrmse_table['model'].append('LSTM Neural Network (with L1L2 Regularization)')\nrmse_table['training_rmse'].append(train_rmse)\nrmse_table['testing_rmse'].append(test_rmse)\n\n\nTrain RMSE: 0.224 RMSE\nTest RMSE: 0.149 RMSE\n\n\n\n\n\n\n\n\nThe two plots show the training loss for the LSTM model, one with regularization and one without. The plot with regularization shows less fluctuation, indicating that overfitting has been reduced. The model without regularization had a train RMSE of 0.212 and a test RMSE of 0.141, while the model with regularization had a slightly lower for both the train and test RMSE values which are 0.202 and 0.135 respectively.\n\n\nForecast\n\n\nCode\n# Creating the frame to store both predictions\ndays = df['Date'].values[-len(y):]\nframe = pd.concat([\n    pd.DataFrame({'day': days, 'price': y, 'type': 'original'}),\n    pd.DataFrame({'day': days, 'price': yhat_d, 'type': 'rnn_forecast'}),\n    pd.DataFrame({'day': days, 'price': yhat_gru, 'type': 'gru_forecast'}),\n    pd.DataFrame({'day': days, 'price': yhat_lstm, 'type': 'lstm_forecast'})\n])\n# Creating the unscaled values column\nframe['price_absolute'] = [(x * train_std) + train_mean for x in frame['price']]\n# Pivoting\npivoted = frame.pivot_table(index='day', columns='type')\npivoted.columns = ['_'.join(x).strip() for x in pivoted.columns.values]\n\nplt.figure(figsize=(12, 10))\nplt.plot(pivoted.index, pivoted.price_absolute_original,\n         color='#8B1874', label='original')\nplt.plot(pivoted.index, pivoted.price_absolute_rnn_forecast,\n         color='#F79540', label='RNN Forecast', alpha=0.6)\nplt.plot(pivoted.index, pivoted.price_absolute_gru_forecast,\n         color='#B71375', label='GRU Forecast', alpha=0.6)\nplt.plot(pivoted.index, pivoted.price_absolute_lstm_forecast,\n         color='#FC4F00', label='LSTM Forecast', alpha=0.6)\nplt.title('Communication Services Sector Fund Forecasts')\nplt.legend()\nplt.show()\n\n\n\n\n\nWhen comparing the predictions of the deep learning models with the original time series plot, we can observe that the RNN model is the closest to the actual values. The LSTM model is almost the same as the RNN model, but both the models are accurate with the actual plot. GNU model are pretty far from the actual plot, which indicates that RNN model is better when compared to others. #### Model Comparison\n\n\nCode\nrmse_df = pd.DataFrame(rmse_table)\nrmse_df\n\n\n\n\n\n\n  \n    \n      \n      model\n      training_rmse\n      testing_rmse\n    \n  \n  \n    \n      0\n      Recurrent Neural Network\n      0.332410\n      0.161490\n    \n    \n      1\n      Recurrent Neural Network (with L1L2 Regulariza...\n      0.341619\n      0.168641\n    \n    \n      2\n      GRU Neural Network\n      0.627929\n      0.380265\n    \n    \n      3\n      GRU Neural Network (with L1L2 Regularization)\n      1.003862\n      0.634489\n    \n    \n      4\n      LSTM Neural Network\n      0.223952\n      0.150143\n    \n    \n      5\n      LSTM Neural Network (with L1L2 Regularization)\n      0.224321\n      0.149382\n    \n  \n\n\n\n\nthe recurrent neural network (RNN) and long short-term memory (LSTM) neural network models generally perform better than the gated recurrent unit (GRU) model. Additionally, the L1L2 regularization technique seems to improve the performance of some models, while not having a significant impact on others.\nIt’s important to note that these observations are based solely on the evaluation metrics you provided (training and testing RMSE), and that the performance of the models could depend on other factors such as the specific data being used, the model architecture and hyperparameters, and the optimization method used during training.\n\n\nComparison of deep learning models with traditional single variable time series\nThe deep learning models (RNN, GRU, LSTM) perform better than the ARIMA model in both training and testing RMSE values for the Communication Services Sector Fund. Among the deep learning models, the LSTM model without regularization has the lowest testing RMSE value of 2.118625, when compared to ARIMA Model which is 2.118625, indicating its better performance in predicting the market prices of the Communication Services Sector Fund compared to the other deep learning models and ARIMA model."
  },
  {
    "objectID": "deep_learning_sector_xlb.html",
    "href": "deep_learning_sector_xlb.html",
    "title": "",
    "section": "",
    "text": "Deep Learning for Materials Sector Fund\nThe materials sector fund comprises companies that are involved in the production and distribution of raw materials, such as metals, chemicals, and construction materials. These companies are often closely tied to the performance of the global economy and can be sensitive to changes in supply and demand dynamics. As with other stock market, predicting the future movements of the Materials Sector Fund can be challenging due to the complex and dynamic nature of the market. These models range from traditional time-series models to more advanced deep learning models. By leveraging the power of Deep Learning and applying techniques such as RNN, LSTM, GRU, and regularization, we can create a model that accurately predicts the future movements of the sector. Such a model can provide valuable insights for investors and traders looking to make informed decisions in the stock market, and help them stay ahead of the curve in the ever-changing landscape of the technology and biotech industries.\nIn order to apply the deep learning methods, the time series data is first split into training and testing sets, taking into account the time component. The training data constitutes 75% of the total data while the rest is the testing data. Furthermore, to ensure that the values are on the same scale, the data is scaled, which is crucial for deep learning models. The mean and standard deviation of the training data are computed and then utilized to normalize both the training and testing sets. This approach ensures that the models are trained on a standardized dataset, enabling them to make accurate predictions.\n\n\nCode\n# libraries\nimport numpy as np\nimport pandas as pd\nimport math\nfrom sklearn.metrics import mean_squared_error\nfrom matplotlib import pyplot as plt\nfrom keras.models import Sequential\nfrom keras.layers import Dropout\nfrom keras.layers import Dense, SimpleRNN, LSTM, GRU\nfrom keras import regularizers\nimport warnings\nwarnings.filterwarnings('ignore')\n\n# import data\ndf = pd.read_csv('./DATA/CLEANED DATA/XLB_raw_data.csv')\n# clean data and get final data for anlaysis\ndf = df[['Date', 'XLB.Adjusted']]\ndf['Date'] = pd.to_datetime(df['Date'])\ndf['XLB.Adjusted'] = pd.to_numeric(df['XLB.Adjusted'], errors='coerce')\ndf = df.dropna(subset=['XLB.Adjusted'])\ndf.sort_values('Date', inplace=True, ascending=True)\ndf = df.reset_index(drop=True)\ndef create_X_Y(ts: np.array, lag=1, n_ahead=1, target_index=0) -> tuple:\n    \"\"\"\n    A method to create X and Y matrix from a time series array for the training of \n    deep learning models \n    \"\"\"\n    # Extracting the number of features that are passed from the array\n    n_features = ts.shape[1]\n\n    # Creating placeholder lists\n    X, Y = [], []\n\n    if len(ts) - lag <= 0:\n        X.append(ts)\n    else:\n        for i in range(len(ts) - lag - n_ahead):\n            Y.append(ts[(i + lag):(i + lag + n_ahead), target_index])\n            X.append(ts[i:(i + lag)])\n\n    X, Y = np.array(X), np.array(Y)\n\n    # Reshaping the X array to an RNN input shape\n    X = np.reshape(X, (X.shape[0], lag, n_features))\n\n    return X, Y\n\nts = df[['XLB.Adjusted']].values\nnp.random.seed(123)\nnrows = ts.shape[0]\ntest_share = 0.25\n# Spliting into train and test sets\ntrain = ts[0:int(nrows * (1 - test_share))]\ntest = ts[int(nrows * (1 - test_share)):]\n\n# Scaling the data\ntrain_mean = train.mean()\ntrain_std = train.std()\ntrain = (train - train_mean) / train_std\ntest = (test - train_mean) / train_std\n\n# Creating the final scaled frame\nts_s = np.concatenate([train, test])\n\nlag = 12\nahead = 3\n\n# Creating the X and Y for training\nX, Y = create_X_Y(ts_s, lag=lag, n_ahead=ahead)\n\nXtrain = X[0:int(X.shape[0] * (1 - test_share))]\nYtrain = Y[0:int(X.shape[0] * (1 - test_share))]\n\nXval = X[int(X.shape[0] * (1 - test_share)):]\nYval = Y[int(X.shape[0] * (1 - test_share)):]\n\n\n\n\ndef plot_model(history, model_title):\n    loss = history.history['loss']\n    epochs = range(1, len(loss) + 1)\n    plt.figure()\n    plt.plot(epochs, loss, 'b', label='Training loss', color = \"gray\")\n    plt.title(f'{model_title} Training loss ')\n    plt.legend()\n    \n\n\ndef print_error(trainY, testY, train_predict, test_predict):\n    # Error of predictions\n    train_rmse = math.sqrt(mean_squared_error(\n        trainY[:, 0], train_predict[:, 0]))\n    test_rmse = math.sqrt(mean_squared_error(testY[:, 0], test_predict[:, 0]))\n    # Print RMSE\n    print('Train RMSE: %.3f RMSE' % (train_rmse))\n    print('Test RMSE: %.3f RMSE' % (test_rmse))\n    return train_rmse, test_rmse\n\n\n\n\nRecurrent Neural Network\nThe RNN models that are constructed and trained using TensorFlow and Keras. This model is trained for five hidden layers, one dense layer, and the activation function hyperbolic tangent. A dropout rate of 0.2 is used to reduce the risk of overfitting. The model are trained for 20 epochs and generated validation loss plots to compare the performance of the regularized and non-regularized models. The second RNN model was identical to the first, except for the addition of kernel regularization.\n\nRNNRNN with Regularization\n\n\n\n\nCode\n# Create a RNN\ndef create_RNN(hidden_units, dense_units, input_shape, activation,dropout_rate=0.2,kernel_regularizer=None):\n    model = Sequential()\n    # Create a simple neural network layer\n    model.add(SimpleRNN(hidden_units, input_shape=input_shape,\n              activation=activation[0]))\n    # Add a dense layer (only one, more layers would make it a deep neural net)\n    model.add(Dense(units=dense_units,\n              activation=activation[1],\n              kernel_regularizer=kernel_regularizer))\n    # Add layer dropout\n    model.add(Dropout(dropout_rate))\n    \n    # Compile the model and optimize on mean squared error\n    model.compile(loss='mean_squared_error', optimizer='adam')\n    return model\n  \n# Create a recurrent neural network\nmodel = create_RNN(hidden_units=5, dense_units=1, input_shape=(lag, Xtrain.shape[-1]),\n                   activation=['tanh', 'tanh'], dropout_rate = 0.2)\nhistory = model.fit(Xtrain, Ytrain, epochs=20, batch_size=1, verbose=0,validation_data=(Xval, Yval))\nplot_model(history, 'Recurrent Neural Network Model')\n\nyhat_d = [x[0] for x in model.predict(Xval, verbose=0)]\ny = [y[0] for y in Yval]\n\ntrain_predict = model.predict(Xtrain, verbose=0)\ntest_predict = model.predict(Xval, verbose=0)\n\n# Print error\ntrain_rmse, test_rmse = print_error(Ytrain, Yval, train_predict, test_predict)\nrmse_table = {\n    'model': ['Recurrent Neural Network'],\n    'training_rmse': [train_rmse],\n    'testing_rmse': [test_rmse]\n}\n\n\nTrain RMSE: 0.296 RMSE\nTest RMSE: 2.713 RMSE\n\n\n\n\n\n\n\n\n\nCode\n# Create a recurrent neural network with regularization\n\nmodel = create_RNN(hidden_units=5, dense_units=1, input_shape=(lag, Xtrain.shape[-1]),\n                   activation=['tanh', 'tanh'], kernel_regularizer=regularizers.L1L2(l1=1e-5, l2=1e-4), dropout_rate = 0.2)\nhistory = model.fit(Xtrain, Ytrain, epochs=20, batch_size=1, verbose=0, validation_data=(Xval, Yval))\nplot_model(history, 'Recurrent Neural Network Model (with L1L2 Regularization)')\n\nyhat_d_reg = [x[0] for x in model.predict(Xval, verbose=0)]\n\ntrain_predict = model.predict(Xtrain, verbose=0)\ntest_predict = model.predict(Xval, verbose=0)\n\n# Print error\ntrain_rmse, test_rmse = print_error(Ytrain, Yval, train_predict, test_predict)\nrmse_table['model'].append(\n    'Recurrent Neural Network (with L1L2 Regularization)')\nrmse_table['training_rmse'].append(train_rmse)\nrmse_table['testing_rmse'].append(test_rmse)\n\n\nTrain RMSE: 0.291 RMSE\nTest RMSE: 2.713 RMSE\n\n\n\n\n\n\n\n\nThe second plot illustrates the impact of regularization on the training loss. Compared to the original plot, there are significant differences. The regularization technique had a substantial effect on the testing error of the model. The model trained without regularization had a slightly higher training error of 0.300 RMSE and for the testing error, the values are the same 2.713 RMSE when compared to with regularization.\n\n\nGRU Neural Network\nA GRU neural network is a powerful machine learning tool widely adopted in various applications. Its key feature is the recurrent gated unit, a type of memory cell that can store information over a data sequence. The GRU neural network is five hidden layers, one dense layer, and the hyperbolic tangent activation function. The model also includes kernel regularization to help address the issue of overfitting. The models are trained for 20 epochs and validation loss plots were generated to compare the performance of the regularized and non-regularized models. Overall, the GRU neural network has shown great promise in accurately predicting future movements in the sector.\n\nGRUGRU with Regularization\n\n\n\n\nCode\n# Create a GRU Neural Network\ndef create_GRU(hidden_units, dense_units, input_shape, activation,dropout_rate=0.2, kernel_regularizer=None):\n    model = Sequential()\n    # Create a simple GRU neural network layer\n    model.add(GRU(hidden_units, input_shape=input_shape,\n              activation=activation[0]))\n    # Add a dense layer (only one, more layers would make it a deep neural net)\n    model.add(Dense(units=dense_units,\n              activation=activation[1], kernel_regularizer=kernel_regularizer))\n    # Add layer dropout\n    model.add(Dropout(dropout_rate))\n    # Compile the model and optimize on mean squared error\n    model.compile(loss='mean_squared_error', optimizer='sgd')\n    return model\n\n# Training and evaluating a GRU-based model\nmodel = create_GRU(hidden_units=5, dense_units=1, input_shape=(lag, Xtrain.shape[-1]),\n                   activation=['tanh', 'relu'], dropout_rate = 0.2)\nhistory = model.fit(Xtrain, Ytrain, epochs=20, batch_size=1, verbose=0)\nplot_model(history, 'GRU Model')\n\nyhat_gru = [x[0] for x in model.predict(Xval, verbose=0)]\n\ntrain_predict = model.predict(Xtrain, verbose=0)\ntest_predict = model.predict(Xval, verbose=0)\n\n# Print error\ntrain_rmse, test_rmse = print_error(Ytrain, Yval, train_predict, test_predict)\nrmse_table['model'].append('GRU Neural Network')\nrmse_table['training_rmse'].append(train_rmse)\nrmse_table['testing_rmse'].append(test_rmse)\n\n\nTrain RMSE: 0.706 RMSE\nTest RMSE: 1.446 RMSE\n\n\n\n\n\n\n\n\n\nCode\n# Training and evaluating a GRU-based model with regularization\nmodel = create_GRU(hidden_units=5, dense_units=1, input_shape=(lag, Xtrain.shape[-1]),\n                   activation=['tanh', 'relu'],dropout_rate = 0.2,  kernel_regularizer=regularizers.L1L2(l1=1e-5, l2=1e-4))\nhistory = model.fit(Xtrain, Ytrain, epochs=20, batch_size=1, verbose=0)\nplot_model(history, 'GRU Model (with L1L2 Regularization)')\n\nyhat_gru_reg = [x[0] for x in model.predict(Xval, verbose=0)]\n\ntrain_predict = model.predict(Xtrain, verbose=0)\ntest_predict = model.predict(Xval, verbose=0)\n\n# Print error\ntrain_rmse, test_rmse = print_error(Ytrain, Yval, train_predict, test_predict)\nrmse_table['model'].append('GRU Neural Network (with L1L2 Regularization)')\nrmse_table['training_rmse'].append(train_rmse)\nrmse_table['testing_rmse'].append(test_rmse)\n\n\nTrain RMSE: 0.706 RMSE\nTest RMSE: 1.610 RMSE\n\n\n\n\n\n\n\n\nThe two plots show the training loss for the GRU model, one with regularization and one without. The plot with regularization is similar to without regularization plot. The model without regularization had a train RMSE of 0.998 and a test RMSE of 3.626, the values are the higher when compared to with regularization.\n\n\nLSTM Neural Network\nThe LSTM (Long Short-Term Memory) neural network is another powerful machine learning tool that has been widely adopted in various applications, particularly in tasks involving sequential data.The LSTM neural network architecture typically consists of multiple LSTM layers, followed by one or more dense layers and an activation function such as the sigmoid or hyperbolic tangent. Similar to the GRU neural network, the LSTM model may also include kernel regularization to address the issue of overfitting. In order to evaluate the performance of the LSTM model, the model can be trained on a dataset for a specified number of epochs, and validation loss plots can be generated to compare the performance of the regularized and non-regularized models.\n\nLSTMLSTM with Regularization\n\n\n\n\nCode\n# Create a LSTM Neural Network\ndef create_LSTM(hidden_units, dense_units, input_shape, activation,dropout_rate = 0.2, kernel_regularizer=None):\n    model = Sequential()\n    # Create a simple long short term memory neural network\n    model.add(LSTM(hidden_units,\n              activation=activation[0], input_shape=input_shape))\n    # Add a dense layer (only one, more layers would make it a deep neural net)\n    model.add(Dense(units=dense_units,\n              activation=activation[1], kernel_regularizer=kernel_regularizer))\n    # Add layer dropout\n    model.add(Dropout(dropout_rate))\n    # Compile the model and optimize on mean squared error\n    model.compile(optimizer=\"RMSprop\", loss='mae')\n    return model\n\n# Create an LSTM neural network\nmodel = create_LSTM(hidden_units=5, dense_units=1, input_shape=(lag, Xtrain.shape[-1]),\n                    activation=['tanh', 'linear'], dropout_rate = 0.2)\nhistory = model.fit(Xtrain, Ytrain, epochs=20, batch_size=1, verbose=0)\nplot_model(history, 'LSTM Model')\n\nyhat_lstm = [x[0] for x in model.predict(Xval, verbose=0)]\n\ntrain_predict = model.predict(Xtrain, verbose=0)\ntest_predict = model.predict(Xval, verbose=0)\n\n# Print error\ntrain_rmse, test_rmse = print_error(Ytrain, Yval, train_predict, test_predict)\nrmse_table['model'].append('LSTM Neural Network')\nrmse_table['training_rmse'].append(train_rmse)\nrmse_table['testing_rmse'].append(test_rmse)\n\n\nTrain RMSE: 0.209 RMSE\nTest RMSE: 1.527 RMSE\n\n\n\n\n\n\n\n\n\nCode\n# Create an LSTM neural network with regularization\nmodel = create_LSTM(hidden_units=5, dense_units=1, input_shape=(lag, Xtrain.shape[-1]),\n                    activation=['tanh', 'linear'], dropout_rate = 0.2, kernel_regularizer=regularizers.L1L2(l1=1e-5, l2=1e-4))\nhistory = model.fit(Xtrain, Ytrain, epochs=20, batch_size=1, verbose=0)\nplot_model(history, 'LSTM Model (with L1L2 Regularization)')\n\n\nyhat_lstm_reg = [x[0] for x in model.predict(Xval, verbose=0)]\n\ntrain_predict = model.predict(Xtrain, verbose=0)\ntest_predict = model.predict(Xval, verbose=0)\n\n# Print error\ntrain_rmse, test_rmse = print_error(Ytrain, Yval, train_predict, test_predict)\nrmse_table['model'].append('LSTM Neural Network (with L1L2 Regularization)')\nrmse_table['training_rmse'].append(train_rmse)\nrmse_table['testing_rmse'].append(test_rmse)\n\n\nTrain RMSE: 0.212 RMSE\nTest RMSE: 1.618 RMSE\n\n\n\n\n\n\n\n\nThe two plots show the training loss for the LSTM model, one with regularization and one without. The plot with regularization shows less fluctuation, indicating that overfitting has been reduced. The model without regularization had a train RMSE of 0.225 and a test RMSE of 1.470, while the model with regularization had a slightly lower for the train and and higher for test RMSE values which are 0.202 and 1.601 respectively.\n\n\nForecast\n\n\nCode\n# Creating the frame to store both predictions\ndays = df['Date'].values[-len(y):]\nframe = pd.concat([\n    pd.DataFrame({'day': days, 'price': y, 'type': 'original'}),\n    pd.DataFrame({'day': days, 'price': yhat_d, 'type': 'rnn_forecast'}),\n    pd.DataFrame({'day': days, 'price': yhat_gru, 'type': 'gru_forecast'}),\n    pd.DataFrame({'day': days, 'price': yhat_lstm, 'type': 'lstm_forecast'})\n])\n# Creating the unscaled values column\nframe['price_absolute'] = [(x * train_std) + train_mean for x in frame['price']]\n# Pivoting\npivoted = frame.pivot_table(index='day', columns='type')\npivoted.columns = ['_'.join(x).strip() for x in pivoted.columns.values]\n\nplt.figure(figsize=(12, 10))\nplt.plot(pivoted.index, pivoted.price_absolute_original,\n         color='#8B1874', label='original')\nplt.plot(pivoted.index, pivoted.price_absolute_rnn_forecast,\n         color='#F79540', label='RNN Forecast', alpha=0.6)\nplt.plot(pivoted.index, pivoted.price_absolute_gru_forecast,\n         color='#B71375', label='GRU Forecast', alpha=0.6)\nplt.plot(pivoted.index, pivoted.price_absolute_lstm_forecast,\n         color='#FC4F00', label='LSTM Forecast', alpha=0.6)\nplt.title('Materials Sector Fund Forecasts')\nplt.legend()\nplt.show()\n\n\n\n\n\nWhen comparing the predictions of the deep learning models with the original time series plot, we can observe that the LSTM model is the closest to the actual values. The GRU model and RNN model are pretty far from the actual plot, which indicates that LSTM model is better when compared to others, but when we look at the LSTM model, the model is not accurate, i.e., it is not the same as the actual plot.\n\n\nModel Comparison\n\n\nCode\nrmse_df = pd.DataFrame(rmse_table)\nrmse_df\n\n\n\n\n\n\n  \n    \n      \n      model\n      training_rmse\n      testing_rmse\n    \n  \n  \n    \n      0\n      Recurrent Neural Network\n      0.295899\n      2.712565\n    \n    \n      1\n      Recurrent Neural Network (with L1L2 Regulariza...\n      0.291305\n      2.713353\n    \n    \n      2\n      GRU Neural Network\n      0.705661\n      1.446304\n    \n    \n      3\n      GRU Neural Network (with L1L2 Regularization)\n      0.706326\n      1.610384\n    \n    \n      4\n      LSTM Neural Network\n      0.208948\n      1.526701\n    \n    \n      5\n      LSTM Neural Network (with L1L2 Regularization)\n      0.212387\n      1.618278\n    \n  \n\n\n\n\nThe Recurrent Neural Network (RNN) model and the RNN with L1L2 Regularization have similar training and testing RMSE values, with the RNN with regularization performing slightly better on the testing data.\nThe GRU Neural Network and the GRU with L1L2 Regularization have identical training RMSE values, but the regularized version has a slightly lower testing RMSE.\nThe LSTM Neural Network and the LSTM with L1L2 Regularization both have low training and testing RMSE values, with the regularized version performing slightly worse on the testing data but still outperforming the other models.\nOverall, the LSTM Neural Network with L1L2 Regularization appears to perform the best on this dataset, with the lowest testing RMSE value.\n\n\nComparison of deep learning models with traditional single variable time series\nThe deep learning models (RNN, GRU, LSTM) perform better than the ARIMA model in both training and testing RMSE values for the Materials Sector Fund. Among the deep learning models, the LSTM model without regularization has the lowest testing RMSE value of 1.001375, when compared to ARIMA Model which is 2.118625, indicating its better performance in predicting the market prices of the Materials Sector Fund compared to the other deep learning models and ARIMA model."
  },
  {
    "objectID": "deep_learning_index_sp500.html",
    "href": "deep_learning_index_sp500.html",
    "title": "",
    "section": "",
    "text": "Deep Learning for S&P 500 Index Stock Price\nThe S&P 500 Index, which is composed of 500 large-cap stocks from different industries, is one of the most widely used benchmarks for the performance of the US stock market. While predicting the future movements of the S&P 500 Index is a challenging task, various models have been developed to forecast its future movements. These models range from traditional time-series models to more advanced deep learning models. By leveraging the power of Deep Learning and applying techniques such as RNN, LSTM, GRU, and regularization, we can create a model that accurately predicts the future movements of the S&P 500 Index. Such a model can provide valuable insights for investors and traders looking to make informed decisions in the stock market, and help them stay ahead of the curve in the ever-changing landscape of the technology and biotech industries.\nIn order to apply the deep learning methods, the time series data is first split into training and testing sets, taking into account the time component. The training data constitutes 75% of the total data while the rest is the testing data. Furthermore, to ensure that the values are on the same scale, the data is scaled, which is crucial for deep learning models. The mean and standard deviation of the training data are computed and then utilized to normalize both the training and testing sets. This approach ensures that the models are trained on a standardized dataset, enabling them to make accurate predictions.\n\n\nCode\n# libraries\nimport numpy as np\nimport pandas as pd\nimport math\nfrom sklearn.metrics import mean_squared_error\nfrom matplotlib import pyplot as plt\nfrom keras.models import Sequential\nfrom keras.layers import Dropout\nfrom keras.layers import Dense, SimpleRNN, LSTM, GRU\nfrom keras import regularizers\nimport warnings\nwarnings.filterwarnings('ignore')\n\n# import data\ndf = pd.read_csv('./DATA/CLEANED DATA/sp500_raw_data.csv')\n# clean data and get final data for anlaysis\ndf = df[['Date', 'GSPC.Adjusted']]\ndf['Date'] = pd.to_datetime(df['Date'])\ndf['GSPC.Adjusted'] = pd.to_numeric(df['GSPC.Adjusted'], errors='coerce')\ndf = df.dropna(subset=['GSPC.Adjusted'])\ndf.sort_values('Date', inplace=True, ascending=True)\ndf = df.reset_index(drop=True)\ndef create_X_Y(ts: np.array, lag=1, n_ahead=1, target_index=0) -> tuple:\n    \"\"\"\n    A method to create X and Y matrix from a time series array for the training of \n    deep learning models \n    \"\"\"\n    # Extracting the number of features that are passed from the array\n    n_features = ts.shape[1]\n\n    # Creating placeholder lists\n    X, Y = [], []\n\n    if len(ts) - lag <= 0:\n        X.append(ts)\n    else:\n        for i in range(len(ts) - lag - n_ahead):\n            Y.append(ts[(i + lag):(i + lag + n_ahead), target_index])\n            X.append(ts[i:(i + lag)])\n\n    X, Y = np.array(X), np.array(Y)\n\n    # Reshaping the X array to an RNN input shape\n    X = np.reshape(X, (X.shape[0], lag, n_features))\n\n    return X, Y\n\nts = df[['GSPC.Adjusted']].values\nnp.random.seed(123)\nnrows = ts.shape[0]\ntest_share = 0.25\n# Spliting into train and test sets\ntrain = ts[0:int(nrows * (1 - test_share))]\ntest = ts[int(nrows * (1 - test_share)):]\n\n# Scaling the data\ntrain_mean = train.mean()\ntrain_std = train.std()\ntrain = (train - train_mean) / train_std\ntest = (test - train_mean) / train_std\n\n# Creating the final scaled frame\nts_s = np.concatenate([train, test])\n\nlag = 12\nahead = 3\n\n# Creating the X and Y for training\nX, Y = create_X_Y(ts_s, lag=lag, n_ahead=ahead)\n\nXtrain = X[0:int(X.shape[0] * (1 - test_share))]\nYtrain = Y[0:int(X.shape[0] * (1 - test_share))]\n\nXval = X[int(X.shape[0] * (1 - test_share)):]\nYval = Y[int(X.shape[0] * (1 - test_share)):]\n\n\n\n\ndef plot_model(history, model_title):\n    loss = history.history['loss']\n    epochs = range(1, len(loss) + 1)\n    plt.figure()\n    plt.plot(epochs, loss, 'b', label='Training loss', color = \"gray\")\n    plt.title(f'{model_title} Training loss ')\n    plt.legend()\n    \n\n\ndef print_error(trainY, testY, train_predict, test_predict):\n    # Error of predictions\n    train_rmse = math.sqrt(mean_squared_error(\n        trainY[:, 0], train_predict[:, 0]))\n    test_rmse = math.sqrt(mean_squared_error(testY[:, 0], test_predict[:, 0]))\n    # Print RMSE\n    print('Train RMSE: %.3f RMSE' % (train_rmse))\n    print('Test RMSE: %.3f RMSE' % (test_rmse))\n    return train_rmse, test_rmse\n\n\n\n\nRecurrent Neural Network\nThe RNN models that are constructed and trained using TensorFlow and Keras. This model is trained for five hidden layers, one dense layer, and the activation function hyperbolic tangent. A dropout rate of 0.2 is used to reduce the risk of overfitting. The model are trained for 20 epochs and generated validation loss plots to compare the performance of the regularized and non-regularized models. The second RNN model was identical to the first, except for the addition of kernel regularization.\n\nRNNRNN with Regularization\n\n\n\n\nCode\n# Create a RNN\ndef create_RNN(hidden_units, dense_units, input_shape, activation,dropout_rate=0.2,kernel_regularizer=None):\n    model = Sequential()\n    # Create a simple neural network layer\n    model.add(SimpleRNN(hidden_units, input_shape=input_shape,\n              activation=activation[0]))\n    # Add a dense layer (only one, more layers would make it a deep neural net)\n    model.add(Dense(units=dense_units,\n              activation=activation[1],\n              kernel_regularizer=kernel_regularizer))\n    # Add layer dropout\n    model.add(Dropout(dropout_rate))\n    \n    # Compile the model and optimize on mean squared error\n    model.compile(loss='mean_squared_error', optimizer='adam')\n    return model\n  \n# Create a recurrent neural network\nmodel = create_RNN(hidden_units=5, dense_units=1, input_shape=(lag, Xtrain.shape[-1]),\n                   activation=['tanh', 'tanh'], dropout_rate = 0.2)\nhistory = model.fit(Xtrain, Ytrain, epochs=20, batch_size=1, verbose=0,validation_data=(Xval, Yval))\nplot_model(history, 'Recurrent Neural Network Model')\n\nyhat_d = [x[0] for x in model.predict(Xval, verbose=0)]\ny = [y[0] for y in Yval]\n\ntrain_predict = model.predict(Xtrain, verbose=0)\ntest_predict = model.predict(Xval, verbose=0)\n\n# Print error\ntrain_rmse, test_rmse = print_error(Ytrain, Yval, train_predict, test_predict)\nrmse_table = {\n    'model': ['Recurrent Neural Network'],\n    'training_rmse': [train_rmse],\n    'testing_rmse': [test_rmse]\n}\n\n\nTrain RMSE: 0.305 RMSE\nTest RMSE: 2.478 RMSE\n\n\n\n\n\n\n\n\n\nCode\n# Create a recurrent neural network with regularization\n\nmodel = create_RNN(hidden_units=5, dense_units=1, input_shape=(lag, Xtrain.shape[-1]),\n                   activation=['tanh', 'tanh'], kernel_regularizer=regularizers.L1L2(l1=1e-5, l2=1e-4), dropout_rate = 0.2)\nhistory = model.fit(Xtrain, Ytrain, epochs=20, batch_size=1, verbose=0, validation_data=(Xval, Yval))\nplot_model(history, 'Recurrent Neural Network Model (with L1L2 Regularization)')\n\nyhat_d_reg = [x[0] for x in model.predict(Xval, verbose=0)]\n\ntrain_predict = model.predict(Xtrain, verbose=0)\ntest_predict = model.predict(Xval, verbose=0)\n\n# Print error\ntrain_rmse, test_rmse = print_error(Ytrain, Yval, train_predict, test_predict)\nrmse_table['model'].append(\n    'Recurrent Neural Network (with L1L2 Regularization)')\nrmse_table['training_rmse'].append(train_rmse)\nrmse_table['testing_rmse'].append(test_rmse)\n\n\nTrain RMSE: 0.302 RMSE\nTest RMSE: 2.478 RMSE\n\n\n\n\n\n\n\n\nThe second plot illustrates the impact of regularization on the training loss. Compared to the original plot, there are significant differences. The regularization technique had a substantial effect on the testing error of the model. The model trained without regularization had a slightly lower training error of 0.306 RMSE but a considerably higher testing error of 2.478 RMSE. In contrast, the model trained with regularization had a slightly lower training error of 0.304 RMSE, but it produced silghtly high testing error of 2.481 RMSE.\n\n\nGRU Neural Network\nA GRU neural network is a powerful machine learning tool widely adopted in various applications. Its key feature is the recurrent gated unit, a type of memory cell that can store information over a data sequence. The GRU neural network is five hidden layers, one dense layer, and the hyperbolic tangent activation function. The model also includes kernel regularization to help address the issue of overfitting. The models are trained for 20 epochs and validation loss plots were generated to compare the performance of the regularized and non-regularized models. Overall, the GRU neural network has shown great promise in accurately predicting future movements in the S&P 500 index.\n\nGRUGRU with Regularization\n\n\n\n\nCode\n# Create a GRU Neural Network\ndef create_GRU(hidden_units, dense_units, input_shape, activation,dropout_rate=0.2, kernel_regularizer=None):\n    model = Sequential()\n    # Create a simple GRU neural network layer\n    model.add(GRU(hidden_units, input_shape=input_shape,\n              activation=activation[0]))\n    # Add a dense layer (only one, more layers would make it a deep neural net)\n    model.add(Dense(units=dense_units,\n              activation=activation[1], kernel_regularizer=kernel_regularizer))\n    # Add layer dropout\n    model.add(Dropout(dropout_rate))\n    # Compile the model and optimize on mean squared error\n    model.compile(loss='mean_squared_error', optimizer='sgd')\n    return model\n\n# Training and evaluating a GRU-based model\nmodel = create_GRU(hidden_units=5, dense_units=1, input_shape=(lag, Xtrain.shape[-1]),\n                   activation=['tanh', 'relu'], dropout_rate = 0.2)\nhistory = model.fit(Xtrain, Ytrain, epochs=20, batch_size=1, verbose=0)\nplot_model(history, 'GRU Model')\n\nyhat_gru = [x[0] for x in model.predict(Xval, verbose=0)]\n\ntrain_predict = model.predict(Xtrain, verbose=0)\ntest_predict = model.predict(Xval, verbose=0)\n\n\n# Print error\ntrain_rmse, test_rmse = print_error(Ytrain, Yval, train_predict, test_predict)\nrmse_table['model'].append('GRU Neural Network')\nrmse_table['training_rmse'].append(train_rmse)\nrmse_table['testing_rmse'].append(test_rmse)\n\n\nTrain RMSE: 0.698 RMSE\nTest RMSE: 1.357 RMSE\n\n\n\n\n\n\n\n\n\nCode\n# Training and evaluating a GRU-based model with regularization\nmodel = create_GRU(hidden_units=5, dense_units=1, input_shape=(lag, Xtrain.shape[-1]),\n                   activation=['tanh', 'relu'],dropout_rate = 0.2,  kernel_regularizer=regularizers.L1L2(l1=1e-5, l2=1e-4))\nhistory = model.fit(Xtrain, Ytrain, epochs=20, batch_size=1, verbose=0)\nplot_model(history, 'GRU Model (with L1L2 Regularization)')\n\nyhat_gru_reg = [x[0] for x in model.predict(Xval, verbose=0)]\n\ntrain_predict = model.predict(Xtrain, verbose=0)\ntest_predict = model.predict(Xval, verbose=0)\n\n# Print error\ntrain_rmse, test_rmse = print_error(Ytrain, Yval, train_predict, test_predict)\nrmse_table['model'].append('GRU Neural Network (with L1L2 Regularization)')\nrmse_table['training_rmse'].append(train_rmse)\nrmse_table['testing_rmse'].append(test_rmse)\n\n\nTrain RMSE: 0.998 RMSE\nTest RMSE: 3.382 RMSE\n\n\n\n\n\n\n\n\nThe two plots show the training loss for the GRU model, one with regularization and one without. The plot with regularization is similar to without regularization plot. The model without regularization had a train RMSE of 0.699 and a test RMSE of 1.277. In contrast, the model trained with regularization had a slightly lower training error of 0.696 RMSE, but it produced silghtly high testing error of 1.280 RMSE.\n\n\nLSTM Neural Network\nThe LSTM (Long Short-Term Memory) neural network is another powerful machine learning tool that has been widely adopted in various applications, particularly in tasks involving sequential data.The LSTM neural network architecture typically consists of multiple LSTM layers, followed by one or more dense layers and an activation function such as the sigmoid or hyperbolic tangent. Similar to the GRU neural network, the LSTM model may also include kernel regularization to address the issue of overfitting. In order to evaluate the performance of the LSTM model, the model can be trained on a dataset for a specified number of epochs, and validation loss plots can be generated to compare the performance of the regularized and non-regularized models.\n\nLSTMLSTM with Regularization\n\n\n\n\nCode\n# Create a LSTM Neural Network\ndef create_LSTM(hidden_units, dense_units, input_shape, activation,dropout_rate = 0.2, kernel_regularizer=None):\n    model = Sequential()\n    # Create a simple long short term memory neural network\n    model.add(LSTM(hidden_units,\n              activation=activation[0], input_shape=input_shape))\n    # Add a dense layer (only one, more layers would make it a deep neural net)\n    model.add(Dense(units=dense_units,\n              activation=activation[1], kernel_regularizer=kernel_regularizer))\n    # Add layer dropout\n    model.add(Dropout(dropout_rate))\n    # Compile the model and optimize on mean squared error\n    model.compile(optimizer=\"RMSprop\", loss='mae')\n    return model\n\n# Create an LSTM neural network\nmodel = create_LSTM(hidden_units=5, dense_units=1, input_shape=(lag, Xtrain.shape[-1]),\n                    activation=['tanh', 'linear'], dropout_rate = 0.2)\nhistory = model.fit(Xtrain, Ytrain, epochs=20, batch_size=1, verbose=0)\nplot_model(history, 'LSTM Model')\n\nyhat_lstm = [x[0] for x in model.predict(Xval, verbose=0)]\n\ntrain_predict = model.predict(Xtrain, verbose=0)\ntest_predict = model.predict(Xval, verbose=0)\n\n# Print error\ntrain_rmse, test_rmse = print_error(Ytrain, Yval, train_predict, test_predict)\nrmse_table['model'].append('LSTM Neural Network')\nrmse_table['training_rmse'].append(train_rmse)\nrmse_table['testing_rmse'].append(test_rmse)\n\n\nTrain RMSE: 0.195 RMSE\nTest RMSE: 1.047 RMSE\n\n\n\n\n\n\n\n\n\nCode\n# Create an LSTM neural network with regularization\nmodel = create_LSTM(hidden_units=5, dense_units=1, input_shape=(lag, Xtrain.shape[-1]),\n                    activation=['tanh', 'linear'], dropout_rate = 0.2, kernel_regularizer=regularizers.L1L2(l1=1e-5, l2=1e-4))\nhistory = model.fit(Xtrain, Ytrain, epochs=20, batch_size=1, verbose=0)\nplot_model(history, 'LSTM Model (with L1L2 Regularization)')\n\n\nyhat_lstm_reg = [x[0] for x in model.predict(Xval, verbose=0)]\n\ntrain_predict = model.predict(Xtrain, verbose=0)\ntest_predict = model.predict(Xval, verbose=0)\n\n# Print error\ntrain_rmse, test_rmse = print_error(Ytrain, Yval, train_predict, test_predict)\nrmse_table['model'].append('LSTM Neural Network (with L1L2 Regularization)')\nrmse_table['training_rmse'].append(train_rmse)\nrmse_table['testing_rmse'].append(test_rmse)\n\n\nTrain RMSE: 0.193 RMSE\nTest RMSE: 1.350 RMSE\n\n\n\n\n\n\n\n\nThe two plots show the training loss for the LSTM model, one with regularization and one without. The plot with regularization shows less fluctuation, indicating that overfitting has been reduced. The model without regularization had a train RMSE of 0.169 and a test RMSE of 1.203, while the model with regularization had a slightly higher for both the train and test RMSE values which are 0.202 and 1.389 respectively.\n\n\nForecast\n\n\nCode\n# Creating the frame to store both predictions\ndays = df['Date'].values[-len(y):]\nframe = pd.concat([\n    pd.DataFrame({'day': days, 'price': y, 'type': 'original'}),\n    pd.DataFrame({'day': days, 'price': yhat_d, 'type': 'rnn_forecast'}),\n    pd.DataFrame({'day': days, 'price': yhat_gru, 'type': 'gru_forecast'}),\n    pd.DataFrame({'day': days, 'price': yhat_lstm, 'type': 'lstm_forecast'})\n])\n# Creating the unscaled values column\nframe['price_absolute'] = [(x * train_std) + train_mean for x in frame['price']]\n# Pivoting\npivoted = frame.pivot_table(index='day', columns='type')\npivoted.columns = ['_'.join(x).strip() for x in pivoted.columns.values]\n\nplt.figure(figsize=(12, 10))\nplt.plot(pivoted.index, pivoted.price_absolute_original,\n         color='#8B1874', label='original')\nplt.plot(pivoted.index, pivoted.price_absolute_rnn_forecast,\n         color='#F79540', label='RNN Forecast', alpha=0.6)\nplt.plot(pivoted.index, pivoted.price_absolute_gru_forecast,\n         color='#B71375', label='GRU Forecast', alpha=0.6)\nplt.plot(pivoted.index, pivoted.price_absolute_lstm_forecast,\n         color='#FC4F00', label='LSTM Forecast', alpha=0.6)\nplt.title('S&P 500  Index Stock Price Forecasts')\nplt.legend()\nplt.show()\n\n\n\n\n\nWhen comparing the predictions of the deep learning models with the original time series plot, we can observe that the LSTM model is the closest to the actual values. The GRU model is almost the same as the LSTM model, but both the models have some kind of accuracy with the actual plot, but there is still difference. RNN model are pretty far from the actual plot, which indicates that LSTM model is better when compared to others, but we need to note that the model is not accurate, i.e., it is not the same as the actual plot.\n\n\nModel Comparison\n\n\nCode\nrmse_df = pd.DataFrame(rmse_table)\nrmse_df\n\n\n\n\n\n\n  \n    \n      \n      model\n      training_rmse\n      testing_rmse\n    \n  \n  \n    \n      0\n      Recurrent Neural Network\n      0.304540\n      2.477665\n    \n    \n      1\n      Recurrent Neural Network (with L1L2 Regulariza...\n      0.302179\n      2.478055\n    \n    \n      2\n      GRU Neural Network\n      0.698040\n      1.357284\n    \n    \n      3\n      GRU Neural Network (with L1L2 Regularization)\n      0.998283\n      3.381660\n    \n    \n      4\n      LSTM Neural Network\n      0.195436\n      1.047317\n    \n    \n      5\n      LSTM Neural Network (with L1L2 Regularization)\n      0.192799\n      1.349535\n    \n  \n\n\n\n\nFirst, we observe that recurrent neural networks (RNN) and long short-term memory (LSTM) networks generally perform better than gated recurrent units (GRU) in terms of testing RMSE. However, the training and testing RMSE values for each model can vary significantly depending on the specific dataset being used.\nRegarding regularization, we see that the use of L1L2 regularization generally leads to lower testing RMSE values for RNN and LSTM models. However, for GRU models, the use of L1L2 regularization can either slightly improve or worsen the testing RMSE depending on the dataset.\nOverall, the best performing models in terms of testing RMSE across all datasets appear to be the LSTM models with L1L2 regularization, which consistently achieve low testing RMSE values.\n\n\nComparison of deep learning models with traditional single variable time series\nAmong the deep learning models, the LSTM Neural Network (with L1L2 Regularization) performed the best with the lowest testing RMSE value. On the other hand, the ARIMA model had the highest testing RMSE value of 48.95665, indicating that it did not perform well in predicting the future movements of the index.\nThis comparison highlights the advantage of using deep learning models over traditional statistical models such as ARIMA in predicting stock market trends. Deep learning models are better equipped to handle complex and dynamic data, making them more effective in capturing the non-linear relationships and patterns in the stock market data.\nOverall, the results suggest that deep learning models can be a valuable tool for investors and analysts in predicting the future movements of the stock market."
  },
  {
    "objectID": "deep_learning_index_nasdaq.html",
    "href": "deep_learning_index_nasdaq.html",
    "title": "",
    "section": "",
    "text": "Deep Learning for NASDAQ Composite Index Stock Price\nIn recent years, the Nasdaq Composite Index has experienced significant growth due to the rise of the technology and biotech industries. However, predicting the future movements of the index remains a challenging task due to the complex and dynamic nature of the market. By leveraging the power of Deep Learning and applying techniques such as RNN, LSTM, GRU, and regularization, we can create a model that accurately predicts the future movements of the Nasdaq Composite Index. Such a model can provide valuable insights for investors and traders looking to make informed decisions in the stock market, and help them stay ahead of the curve in the ever-changing landscape of the technology and biotech industries.\nIn order to apply the deep learning methods, the time series data is first split into training and testing sets, taking into account the time component. The training data constitutes 75% of the total data while the rest is the testing data. Furthermore, to ensure that the values are on the same scale, the data is scaled, which is crucial for deep learning models. The mean and standard deviation of the training data are computed and then utilized to normalize both the training and testing sets. This approach ensures that the models are trained on a standardized dataset, enabling them to make accurate predictions.\n\n\nCode\n# libraries\nimport numpy as np\nimport pandas as pd\nimport math\nfrom sklearn.metrics import mean_squared_error\nfrom matplotlib import pyplot as plt\nfrom keras.models import Sequential\nfrom keras.layers import Dropout\nfrom keras.layers import Dense, SimpleRNN, LSTM, GRU\nfrom keras import regularizers\nimport warnings\nwarnings.filterwarnings('ignore')\n\n# import data\ndf = pd.read_csv('./DATA/CLEANED DATA/nasdaq_raw_data.csv')\n# clean data and get final data for anlaysis\ndf = df[['Date', 'IXIC.Adjusted']]\ndf['Date'] = pd.to_datetime(df['Date'])\ndf['IXIC.Adjusted'] = pd.to_numeric(df['IXIC.Adjusted'], errors='coerce')\ndf = df.dropna(subset=['IXIC.Adjusted'])\ndf.sort_values('Date', inplace=True, ascending=True)\ndf = df.reset_index(drop=True)\ndef create_X_Y(ts: np.array, lag=1, n_ahead=1, target_index=0) -> tuple:\n    \"\"\"\n    A method to create X and Y matrix from a time series array for the training of \n    deep learning models \n    \"\"\"\n    # Extracting the number of features that are passed from the array\n    n_features = ts.shape[1]\n\n    # Creating placeholder lists\n    X, Y = [], []\n\n    if len(ts) - lag <= 0:\n        X.append(ts)\n    else:\n        for i in range(len(ts) - lag - n_ahead):\n            Y.append(ts[(i + lag):(i + lag + n_ahead), target_index])\n            X.append(ts[i:(i + lag)])\n\n    X, Y = np.array(X), np.array(Y)\n\n    # Reshaping the X array to an RNN input shape\n    X = np.reshape(X, (X.shape[0], lag, n_features))\n\n    return X, Y\n\nts = df[['IXIC.Adjusted']].values\nnp.random.seed(123)\nnrows = ts.shape[0]\ntest_share = 0.25\n# Spliting into train and test sets\ntrain = ts[0:int(nrows * (1 - test_share))]\ntest = ts[int(nrows * (1 - test_share)):]\n\n# Scaling the data\ntrain_mean = train.mean()\ntrain_std = train.std()\ntrain = (train - train_mean) / train_std\ntest = (test - train_mean) / train_std\n\n# Creating the final scaled frame\nts_s = np.concatenate([train, test])\n\nlag = 12\nahead = 3\n\n# Creating the X and Y for training\nX, Y = create_X_Y(ts_s, lag=lag, n_ahead=ahead)\n\nXtrain = X[0:int(X.shape[0] * (1 - test_share))]\nYtrain = Y[0:int(X.shape[0] * (1 - test_share))]\n\nXval = X[int(X.shape[0] * (1 - test_share)):]\nYval = Y[int(X.shape[0] * (1 - test_share)):]\n\n\n\n\ndef plot_model(history, model_title):\n    loss = history.history['loss']\n    epochs = range(1, len(loss) + 1)\n    plt.figure()\n    plt.plot(epochs, loss, 'b', label='Training loss', color = \"gray\")\n    plt.title(f'{model_title} Training loss ')\n    plt.legend()\n    \n\n\ndef print_error(trainY, testY, train_predict, test_predict):\n    # Error of predictions\n    train_rmse = math.sqrt(mean_squared_error(\n        trainY[:, 0], train_predict[:, 0]))\n    test_rmse = math.sqrt(mean_squared_error(testY[:, 0], test_predict[:, 0]))\n    # Print RMSE\n    print('Train RMSE: %.3f RMSE' % (train_rmse))\n    print('Test RMSE: %.3f RMSE' % (test_rmse))\n    return train_rmse, test_rmse\n\n\n\n\nRecurrent Neural Network\nThe RNN models that are constructed and trained using TensorFlow and Keras. This model is trained for five hidden layers, one dense layer, and the activation function hyperbolic tangent. A dropout rate of 0.2 is used to reduce the risk of overfitting. The model are trained for 20 epochs and generated validation loss plots to compare the performance of the regularized and non-regularized models. The second RNN model was identical to the first, except for the addition of kernel regularization.\n\nRNNRNN with Regularization\n\n\n\n\nCode\n# Create a RNN\ndef create_RNN(hidden_units, dense_units, input_shape, activation,dropout_rate=0.2,kernel_regularizer=None):\n    model = Sequential()\n    # Create a simple neural network layer\n    model.add(SimpleRNN(hidden_units, input_shape=input_shape,\n              activation=activation[0]))\n    # Add a dense layer (only one, more layers would make it a deep neural net)\n    model.add(Dense(units=dense_units,\n              activation=activation[1],\n              kernel_regularizer=kernel_regularizer))\n    # Add layer dropout\n    model.add(Dropout(dropout_rate))\n    \n    # Compile the model and optimize on mean squared error\n    model.compile(loss='mean_squared_error', optimizer='adam')\n    return model\n  \n# Create a recurrent neural network\nmodel = create_RNN(hidden_units=5, dense_units=1, input_shape=(lag, Xtrain.shape[-1]),\n                   activation=['tanh', 'tanh'], dropout_rate = 0.2)\nhistory = model.fit(Xtrain, Ytrain, epochs=20, batch_size=1, verbose=0,validation_data=(Xval, Yval))\nplot_model(history, 'Recurrent Neural Network Model')\n\nyhat_d = [x[0] for x in model.predict(Xval, verbose=0)]\ny = [y[0] for y in Yval]\n\ntrain_predict = model.predict(Xtrain, verbose=0)\ntest_predict = model.predict(Xval, verbose=0)\n\n# Print error\ntrain_rmse, test_rmse = print_error(Ytrain, Yval, train_predict, test_predict)\nrmse_table = {\n    'model': ['Recurrent Neural Network'],\n    'training_rmse': [train_rmse],\n    'testing_rmse': [test_rmse]\n}\n\n\nTrain RMSE: 0.317 RMSE\nTest RMSE: 3.251 RMSE\n\n\n\n\n\n\n\n\n\nCode\n# Create a recurrent neural network with regularization\n\nmodel = create_RNN(hidden_units=5, dense_units=1, input_shape=(lag, Xtrain.shape[-1]),\n                   activation=['tanh', 'tanh'], kernel_regularizer=regularizers.L1L2(l1=1e-5, l2=1e-4), dropout_rate = 0.2)\nhistory = model.fit(Xtrain, Ytrain, epochs=20, batch_size=1, verbose=0, validation_data=(Xval, Yval))\nplot_model(history, 'Recurrent Neural Network Model (with L1L2 Regularization)')\n\nyhat_d_reg = [x[0] for x in model.predict(Xval, verbose=0)]\n\ntrain_predict = model.predict(Xtrain, verbose=0)\ntest_predict = model.predict(Xval, verbose=0)\n\n# Print error\ntrain_rmse, test_rmse = print_error(Ytrain, Yval, train_predict, test_predict)\nrmse_table['model'].append(\n    'Recurrent Neural Network (with L1L2 Regularization)')\nrmse_table['training_rmse'].append(train_rmse)\nrmse_table['testing_rmse'].append(test_rmse)\n\n\nTrain RMSE: 0.319 RMSE\nTest RMSE: 3.251 RMSE\n\n\n\n\n\n\n\n\nThe second plot illustrates the impact of regularization on the training loss. Compared to the original plot, there are significant differences. The regularization technique had a substantial effect on the testing error of the model. The model trained without regularization training error is of the RMSE value 0.322 but a considerably higher testing error of 3.251 RMSE. In contrast, the model trained with regularization had a slightly lower training error of 0.323 RMSE, but it produced same testing error of 3.251 RMSE.\n\n\nGRU Neural Network\nA GRU neural network is a powerful machine learning tool widely adopted in various applications. Its key feature is the recurrent gated unit, a type of memory cell that can store information over a data sequence. The GRU neural network is five hidden layers, one dense layer, and the hyperbolic tangent activation function. The model also includes kernel regularization to help address the issue of overfitting. The models are trained for 20 epochs and validation loss plots were generated to compare the performance of the regularized and non-regularized models. Overall, the GRU neural network has shown great promise in accurately predicting future movements in the NASDAQ Composite index.\n\nGRUGRU with Regularization\n\n\n\n\nCode\n# Create a GRU Neural Network\ndef create_GRU(hidden_units, dense_units, input_shape, activation,dropout_rate=0.2, kernel_regularizer=None):\n    model = Sequential()\n    # Create a simple GRU neural network layer\n    model.add(GRU(hidden_units, input_shape=input_shape,\n              activation=activation[0]))\n    # Add a dense layer (only one, more layers would make it a deep neural net)\n    model.add(Dense(units=dense_units,\n              activation=activation[1], kernel_regularizer=kernel_regularizer))\n    # Add layer dropout\n    model.add(Dropout(dropout_rate))\n    # Compile the model and optimize on mean squared error\n    model.compile(loss='mean_squared_error', optimizer='sgd')\n    return model\n\n# Training and evaluating a GRU-based model\nmodel = create_GRU(hidden_units=5, dense_units=1, input_shape=(lag, Xtrain.shape[-1]),\n                   activation=['tanh', 'relu'], dropout_rate = 0.2)\nhistory = model.fit(Xtrain, Ytrain, epochs=20, batch_size=1, verbose=0)\nplot_model(history, 'GRU Model')\n\nyhat_gru = [x[0] for x in model.predict(Xval, verbose=0)]\n\ntrain_predict = model.predict(Xtrain, verbose=0)\ntest_predict = model.predict(Xval, verbose=0)\n\n# Print error\ntrain_rmse, test_rmse = print_error(Ytrain, Yval, train_predict, test_predict)\nrmse_table['model'].append('GRU Neural Network')\nrmse_table['training_rmse'].append(train_rmse)\nrmse_table['testing_rmse'].append(test_rmse)\n\n\nTrain RMSE: 0.999 RMSE\nTest RMSE: 4.200 RMSE\n\n\n\n\n\n\n\n\n\nCode\n# Training and evaluating a GRU-based model with regularization\nmodel = create_GRU(hidden_units=5, dense_units=1, input_shape=(lag, Xtrain.shape[-1]),\n                   activation=['tanh', 'relu'],dropout_rate = 0.2,  kernel_regularizer=regularizers.L1L2(l1=1e-5, l2=1e-4))\nhistory = model.fit(Xtrain, Ytrain, epochs=20, batch_size=1, verbose=0)\nplot_model(history, 'GRU Model (with L1L2 Regularization)')\n\nyhat_gru_reg = [x[0] for x in model.predict(Xval, verbose=0)]\n\ntrain_predict = model.predict(Xtrain, verbose=0)\ntest_predict = model.predict(Xval, verbose=0)\n\n# Print error\ntrain_rmse, test_rmse = print_error(Ytrain, Yval, train_predict, test_predict)\nrmse_table['model'].append('GRU Neural Network (with L1L2 Regularization)')\nrmse_table['training_rmse'].append(train_rmse)\nrmse_table['testing_rmse'].append(test_rmse)\n\n\nTrain RMSE: 0.664 RMSE\nTest RMSE: 2.039 RMSE\n\n\n\n\n\n\n\n\nThe two plots show the training loss for the GRU model, one with regularization and one without. The plot with regularization shows less fluctuation, indicating that overfitting has been reduced. The model without regularization had a train RMSE of 0.664 and a test RMSE of 1.926, while the training and testing RMSE is higher with regularization which is 0.666 and 2.065 respectively.\n\n\nLSTM Neural Network\nThe LSTM (Long Short-Term Memory) neural network is another powerful machine learning tool that has been widely adopted in various applications, particularly in tasks involving sequential data.The LSTM neural network architecture typically consists of multiple LSTM layers, followed by one or more dense layers and an activation function such as the sigmoid or hyperbolic tangent. Similar to the GRU neural network, the LSTM model may also include kernel regularization to address the issue of overfitting. In order to evaluate the performance of the LSTM model, the model can be trained on a dataset for a specified number of epochs, and validation loss plots can be generated to compare the performance of the regularized and non-regularized models.\n\nLSTMLSTM with Regularization\n\n\n\n\nCode\n# Create a LSTM Neural Network\ndef create_LSTM(hidden_units, dense_units, input_shape, activation,dropout_rate = 0.2, kernel_regularizer=None):\n    model = Sequential()\n    # Create a simple long short term memory neural network\n    model.add(LSTM(hidden_units,\n              activation=activation[0], input_shape=input_shape))\n    # Add a dense layer (only one, more layers would make it a deep neural net)\n    model.add(Dense(units=dense_units,\n              activation=activation[1], kernel_regularizer=kernel_regularizer))\n    # Add layer dropout\n    model.add(Dropout(dropout_rate))\n    # Compile the model and optimize on mean squared error\n    model.compile(optimizer=\"RMSprop\", loss='mae')\n    return model\n\n# Create an LSTM neural network\nmodel = create_LSTM(hidden_units=5, dense_units=1, input_shape=(lag, Xtrain.shape[-1]),\n                    activation=['tanh', 'linear'], dropout_rate = 0.2)\nhistory = model.fit(Xtrain, Ytrain, epochs=20, batch_size=1, verbose=0)\nplot_model(history, 'LSTM Model')\n\nyhat_lstm = [x[0] for x in model.predict(Xval, verbose=0)]\n\ntrain_predict = model.predict(Xtrain, verbose=0)\ntest_predict = model.predict(Xval, verbose=0)\n\n# Print error\ntrain_rmse, test_rmse = print_error(Ytrain, Yval, train_predict, test_predict)\nrmse_table['model'].append('LSTM Neural Network')\nrmse_table['training_rmse'].append(train_rmse)\nrmse_table['testing_rmse'].append(test_rmse)\n\n\nTrain RMSE: 0.205 RMSE\nTest RMSE: 1.891 RMSE\n\n\n\n\n\n\n\n\n\nCode\n# Create an LSTM neural network with regularization\nmodel = create_LSTM(hidden_units=5, dense_units=1, input_shape=(lag, Xtrain.shape[-1]),\n                    activation=['tanh', 'linear'], dropout_rate = 0.2, kernel_regularizer=regularizers.L1L2(l1=1e-5, l2=1e-4))\nhistory = model.fit(Xtrain, Ytrain, epochs=20, batch_size=1, verbose=0)\nplot_model(history, 'LSTM Model (with L1L2 Regularization)')\n\n\nyhat_lstm_reg = [x[0] for x in model.predict(Xval, verbose=0)]\n\ntrain_predict = model.predict(Xtrain, verbose=0)\ntest_predict = model.predict(Xval, verbose=0)\n\n# Print error\ntrain_rmse, test_rmse = print_error(Ytrain, Yval, train_predict, test_predict)\nrmse_table['model'].append('LSTM Neural Network (with L1L2 Regularization)')\nrmse_table['training_rmse'].append(train_rmse)\nrmse_table['testing_rmse'].append(test_rmse)\n\n\nTrain RMSE: 0.192 RMSE\nTest RMSE: 1.695 RMSE\n\n\n\n\n\n\n\n\nThe two plots show the training loss for the LSTM model, one with regularization and one without. The plot with regularization shows less fluctuation, indicating that overfitting has been reduced. The model without regularization had a train RMSE of 0.204 and a test RMSE of 1.688, while the model with regularization had a slightly higher train RMSE of 0.226 and test RMSE of 2.095\n\n\nForecast\n\n\nCode\n# Creating the frame to store both predictions\ndays = df['Date'].values[-len(y):]\nframe = pd.concat([\n    pd.DataFrame({'day': days, 'price': y, 'type': 'original'}),\n    pd.DataFrame({'day': days, 'price': yhat_d, 'type': 'rnn_forecast'}),\n    pd.DataFrame({'day': days, 'price': yhat_gru, 'type': 'gru_forecast'}),\n    pd.DataFrame({'day': days, 'price': yhat_lstm, 'type': 'lstm_forecast'})\n])\n# Creating the unscaled values column\nframe['price_absolute'] = [(x * train_std) + train_mean for x in frame['price']]\n# Pivoting\npivoted = frame.pivot_table(index='day', columns='type')\npivoted.columns = ['_'.join(x).strip() for x in pivoted.columns.values]\n\nplt.figure(figsize=(12, 10))\nplt.plot(pivoted.index, pivoted.price_absolute_original,\n         color='#8B1874', label='original')\nplt.plot(pivoted.index, pivoted.price_absolute_rnn_forecast,\n         color='#F79540', label='RNN Forecast', alpha=0.6)\nplt.plot(pivoted.index, pivoted.price_absolute_gru_forecast,\n         color='#B71375', label='GRU Forecast', alpha=0.6)\nplt.plot(pivoted.index, pivoted.price_absolute_lstm_forecast,\n         color='#FC4F00', label='LSTM Forecast', alpha=0.6)\nplt.title('NASDAQ Composite Index Stock Price Forecasts')\nplt.legend()\nplt.show()\n\n\n\n\n\nWhen comparing the predictions of the deep learning models with the original time series plot, we can observe that the LSTM model is the closest to the actual values. The GRU model is almost the same as the LSTM model, but both the models have some kind of accuracy with the actual plot, but there is still difference. RNN model are pretty far from the actual plot, which indicates that LSTM model is better when compared to others, but we need to note that the model is not accurate, i.e., it is not the same as the actual plot.\n\n\nModel Comparison\n\n\nCode\nrmse_df = pd.DataFrame(rmse_table)\nrmse_df\n\n\n\n\n\n\n  \n    \n      \n      model\n      training_rmse\n      testing_rmse\n    \n  \n  \n    \n      0\n      Recurrent Neural Network\n      0.317147\n      3.251211\n    \n    \n      1\n      Recurrent Neural Network (with L1L2 Regulariza...\n      0.319408\n      3.251398\n    \n    \n      2\n      GRU Neural Network\n      0.998977\n      4.199545\n    \n    \n      3\n      GRU Neural Network (with L1L2 Regularization)\n      0.664024\n      2.039441\n    \n    \n      4\n      LSTM Neural Network\n      0.204552\n      1.890521\n    \n    \n      5\n      LSTM Neural Network (with L1L2 Regularization)\n      0.191893\n      1.694744\n    \n  \n\n\n\n\nFrom the table, we can observe that the LSTM Neural Network with L1L2 Regularization has the lowest testing RMSE value, indicating that it performs the best among all the models. The next best performing model is the GRU Neural Network with L1L2 Regularization, with a testing, followed by the original GRU Neural Network with a testing RMSE.\nComparing the two best performing models, we see that the LSTM Neural Network with L1L2 Regularization outperforms the GNU with L1L2 Regularization by a significant margin. This indicates that the LSTM model is better suited for the task at hand, and the regularization technique has helped to improve its performance. The other models, including the RNN, have higher testing RMSE values and are therefore less effective in predicting the target variable.\n\n\nComparison of deep learning models with traditional single variable time series\nComparing the ARIMA model with the deep learning models from the given table, we can see that the ARIMA model has a much higher RMSE value of 162.141 compared to the best-performing deep learning model, the LSTM Neural Network with L1L2 Regularization.\nThis indicates that the deep learning models are significantly better at predicting the target variable compared to the ARIMA model for the given dataset. However, it’s worth noting that the ARIMA model is a classical time series model, while the deep learning models are more complex and require more data and computational resources. Additionally, the ARIMA model is more interpretable than the deep learning models, which can be an important factor in some applications. Nonetheless, based on the given RMSE values, the deep learning models outperform the ARIMA model in this case."
  },
  {
    "objectID": "deep_learning_index_dji.html",
    "href": "deep_learning_index_dji.html",
    "title": "",
    "section": "",
    "text": "Deep Learning for Dow Jones Index Stock Price\nThe Dow Jones Index (DJI) is a widely followed stock market index that tracks the performance of 30 large publicly traded companies in the United States. Predicting the movement of the DJI is a challenging task due to its complexity and sensitivity to a wide range of factors. Deep Learning has emerged as a powerful technique for analyzing complex data and making accurate predictions. We will explore the use of three deep learning models, Recurrent Neural Network (RNN), Long Short-Term Memory (LSTM), and Gated Recurrent Unit (GRU), to predict the DJI index. Additionally, we investigate the impact of regularization on the performance of the models which is used to prevent overfitting. By using these models, we aim to create a model that can accurately predict the future movements of the DJI index, which can be valuable for investors and traders.\nIn order to apply the deep learning methods, the time series data is first split into training and testing sets, taking into account the time component. The training data constitutes 75% of the total data while the rest is the testing data. Furthermore, to ensure that the values are on the same scale, the data is scaled, which is crucial for deep learning models. The mean and standard deviation of the training data are computed and then utilized to normalize both the training and testing sets. This approach ensures that the models are trained on a standardized dataset, enabling them to make accurate predictions.\n\n\nCode\n# libraries\nimport numpy as np\nimport pandas as pd\nimport math\nfrom sklearn.metrics import mean_squared_error\nfrom matplotlib import pyplot as plt\nfrom keras.models import Sequential\nfrom keras.layers import Dropout\nfrom keras.layers import Dense, SimpleRNN, LSTM, GRU\nfrom keras import regularizers\nimport warnings\nwarnings.filterwarnings('ignore')\n\n# import data\ndf = pd.read_csv('./DATA/CLEANED DATA/dji_raw_data.csv')\n# clean data and get final data for anlaysis\ndf = df[['Date', 'DJI.Adjusted']]\ndf['Date'] = pd.to_datetime(df['Date'])\ndf['DJI.Adjusted'] = pd.to_numeric(df['DJI.Adjusted'], errors='coerce')\ndf = df.dropna(subset=['DJI.Adjusted'])\ndf.sort_values('Date', inplace=True, ascending=True)\ndf = df.reset_index(drop=True)\n\ndef create_X_Y(ts: np.array, lag=1, n_ahead=1, target_index=0) -> tuple:\n    \"\"\"\n    A method to create X and Y matrix from a time series array for the training of \n    deep learning models \n    \"\"\"\n    # Extracting the number of features that are passed from the array\n    n_features = ts.shape[1]\n\n    # Creating placeholder lists\n    X, Y = [], []\n\n    if len(ts) - lag <= 0:\n        X.append(ts)\n    else:\n        for i in range(len(ts) - lag - n_ahead):\n            Y.append(ts[(i + lag):(i + lag + n_ahead), target_index])\n            X.append(ts[i:(i + lag)])\n\n    X, Y = np.array(X), np.array(Y)\n\n    # Reshaping the X array to an RNN input shape\n    X = np.reshape(X, (X.shape[0], lag, n_features))\n\n    return X, Y\n\nts = df[['DJI.Adjusted']].values\nnp.random.seed(123)\nnrows = ts.shape[0]\ntest_share = 0.25\n# Spliting into train and test sets\ntrain = ts[0:int(nrows * (1 - test_share))]\ntest = ts[int(nrows * (1 - test_share)):]\n\n# Scaling the data\ntrain_mean = train.mean()\ntrain_std = train.std()\ntrain = (train - train_mean) / train_std\ntest = (test - train_mean) / train_std\n\n# Creating the final scaled frame\nts_s = np.concatenate([train, test])\n\nlag = 12\nahead = 3\n\n# Creating the X and Y for training\nX, Y = create_X_Y(ts_s, lag=lag, n_ahead=ahead)\n\nXtrain = X[0:int(X.shape[0] * (1 - test_share))]\nYtrain = Y[0:int(X.shape[0] * (1 - test_share))]\n\nXval = X[int(X.shape[0] * (1 - test_share)):]\nYval = Y[int(X.shape[0] * (1 - test_share)):]\n\n\n\n\ndef plot_model(history, model_title):\n    loss = history.history['loss']\n    epochs = range(1, len(loss) + 1)\n    plt.figure()\n    plt.plot(epochs, loss, 'b', label='Training loss', color = \"gray\")\n    plt.title(f'{model_title} Training loss ')\n    plt.legend()\n    \n\n\ndef print_error(trainY, testY, train_predict, test_predict):\n    # Error of predictions\n    train_rmse = math.sqrt(mean_squared_error(\n        trainY[:, 0], train_predict[:, 0]))\n    test_rmse = math.sqrt(mean_squared_error(testY[:, 0], test_predict[:, 0]))\n    # Print RMSE\n    print('Train RMSE: %.3f RMSE' % (train_rmse))\n    print('Test RMSE: %.3f RMSE' % (test_rmse))\n    return train_rmse, test_rmse\n\n\n\n\nRecurrent Neural Network\nThe RNN models that are constructed and trained using TensorFlow and Keras. This model is trained for five hidden layers, one dense layer, and the activation function hyperbolic tangent. A dropout rate of 0.2 is used to reduce the risk of overfitting. The model are trained for 20 epochs and generated validation loss plots to compare the performance of the regularized and non-regularized models. The second RNN model was identical to the first, except for the addition of kernel regularization.\n\nRNNRNN with Regularization\n\n\n\n\nCode\n# Create a RNN\ndef create_RNN(hidden_units, dense_units, input_shape, activation,dropout_rate=0.2,kernel_regularizer=None):\n    model = Sequential()\n    # Create a simple neural network layer\n    model.add(SimpleRNN(hidden_units, input_shape=input_shape,\n              activation=activation[0]))\n    # Add a dense layer (only one, more layers would make it a deep neural net)\n    model.add(Dense(units=dense_units,\n              activation=activation[1],\n              kernel_regularizer=kernel_regularizer))\n    # Add layer dropout\n    model.add(Dropout(dropout_rate))\n    \n    # Compile the model and optimize on mean squared error\n    model.compile(loss='mean_squared_error', optimizer='adam')\n    return model\n  \n# Create a recurrent neural network\nmodel = create_RNN(hidden_units=5, dense_units=1, input_shape=(lag, Xtrain.shape[-1]),\n                   activation=['tanh', 'tanh'], dropout_rate = 0.2)\nhistory = model.fit(Xtrain, Ytrain, epochs=20, batch_size=1, verbose=0,validation_data=(Xval, Yval))\nplot_model(history, 'Recurrent Neural Network Model')\n\nyhat_d = [x[0] for x in model.predict(Xval, verbose=0)]\ny = [y[0] for y in Yval]\n\ntrain_predict = model.predict(Xtrain, verbose=0)\ntest_predict = model.predict(Xval, verbose=0)\n\n# Print error\ntrain_rmse, test_rmse = print_error(Ytrain, Yval, train_predict, test_predict)\nrmse_table = {\n    'model': ['Recurrent Neural Network'],\n    'training_rmse': [train_rmse],\n    'testing_rmse': [test_rmse]\n}\n\n\nTrain RMSE: 0.324 RMSE\nTest RMSE: 1.884 RMSE\n\n\n\n\n\n\n\n\n\nCode\n# Create a recurrent neural network with regularization\n\nmodel = create_RNN(hidden_units=5, dense_units=1, input_shape=(lag, Xtrain.shape[-1]),\n                   activation=['tanh', 'tanh'], kernel_regularizer=regularizers.L1L2(l1=1e-5, l2=1e-4), dropout_rate = 0.2)\nhistory = model.fit(Xtrain, Ytrain, epochs=20, batch_size=1, verbose=0, validation_data=(Xval, Yval))\nplot_model(history, 'Recurrent Neural Network Model (with L1L2 Regularization)')\n\nyhat_d_reg = [x[0] for x in model.predict(Xval, verbose=0)]\n\ntrain_predict = model.predict(Xtrain, verbose=0)\ntest_predict = model.predict(Xval, verbose=0)\n\n# Print error\ntrain_rmse, test_rmse = print_error(Ytrain, Yval, train_predict, test_predict)\nrmse_table['model'].append(\n    'Recurrent Neural Network (with L1L2 Regularization)')\nrmse_table['training_rmse'].append(train_rmse)\nrmse_table['testing_rmse'].append(test_rmse)\n\n\nTrain RMSE: 0.331 RMSE\nTest RMSE: 1.885 RMSE\n\n\n\n\n\n\n\n\nThe second plot shows the training loss with regularization; compared to the original (unregularized) training loss plot, we see a lot of differences. The regularization technique had a noticeable effect on the testing error of the model. The model without regularization have a training error of 0.324 RMSE but a higher testing error of 1.889 RMSE. On the other hand, the model with regularization had a marginally lower training error of 0.322 RMSE but a lower testing error of 1.885 RMSE.\n\n\nGRU Neural Network\nA GRU neural network is a powerful machine learning tool widely adopted in various applications. Its key feature is the recurrent gated unit, a type of memory cell that can store information over a data sequence. The GRU neural network is five hidden layers, one dense layer, and the hyperbolic tangent activation function. The model also includes kernel regularization to help address the issue of overfitting. The models are trained for 20 epochs and validation loss plots were generated to compare the performance of the regularized and non-regularized models. Overall, the GRU neural network has shown great promise in accurately predicting future movements in the DJI index.\n\nGRUGRU with Regularization\n\n\n\n\nCode\n# Create a GRU Neural Network\ndef create_GRU(hidden_units, dense_units, input_shape, activation,dropout_rate=0.2, kernel_regularizer=None):\n    model = Sequential()\n    # Create a simple GRU neural network layer\n    model.add(GRU(hidden_units, input_shape=input_shape,\n              activation=activation[0]))\n    # Add a dense layer (only one, more layers would make it a deep neural net)\n    model.add(Dense(units=dense_units,\n              activation=activation[1], kernel_regularizer=kernel_regularizer))\n    # Add layer dropout\n    model.add(Dropout(dropout_rate))\n    # Compile the model and optimize on mean squared error\n    model.compile(loss='mean_squared_error', optimizer='sgd')\n    return model\n\n# Training and evaluating a GRU-based model\nmodel = create_GRU(hidden_units=5, dense_units=1, input_shape=(lag, Xtrain.shape[-1]),\n                   activation=['tanh', 'relu'], dropout_rate = 0.2)\nhistory = model.fit(Xtrain, Ytrain, epochs=20, batch_size=1, verbose=0)\nplot_model(history, 'GRU Model')\n\nyhat_gru = [x[0] for x in model.predict(Xval, verbose=0)]\n\ntrain_predict = model.predict(Xtrain, verbose=0)\ntest_predict = model.predict(Xval, verbose=0)\n\n# Print error\ntrain_rmse, test_rmse = print_error(Ytrain, Yval, train_predict, test_predict)\nrmse_table['model'].append('GRU Neural Network')\nrmse_table['training_rmse'].append(train_rmse)\nrmse_table['testing_rmse'].append(test_rmse)\n\n\nTrain RMSE: 0.661 RMSE\nTest RMSE: 0.817 RMSE\n\n\n\n\n\n\n\n\n\nCode\n# Training and evaluating a GRU-based model with regularization\nmodel = create_GRU(hidden_units=5, dense_units=1, input_shape=(lag, Xtrain.shape[-1]),\n                   activation=['tanh', 'relu'],dropout_rate = 0.2,  kernel_regularizer=regularizers.L1L2(l1=1e-5, l2=1e-4))\nhistory = model.fit(Xtrain, Ytrain, epochs=20, batch_size=1, verbose=0)\nplot_model(history, 'GRU Model (with L1L2 Regularization)')\n\nyhat_gru_reg = [x[0] for x in model.predict(Xval, verbose=0)]\n\ntrain_predict = model.predict(Xtrain, verbose=0)\ntest_predict = model.predict(Xval, verbose=0)\n\n# Print error\ntrain_rmse, test_rmse = print_error(Ytrain, Yval, train_predict, test_predict)\nrmse_table['model'].append('GRU Neural Network (with L1L2 Regularization)')\nrmse_table['training_rmse'].append(train_rmse)\nrmse_table['testing_rmse'].append(test_rmse)\n\n\nTrain RMSE: 0.662 RMSE\nTest RMSE: 0.864 RMSE\n\n\n\n\n\n\n\n\nThe two plots show the training loss for the GRU model, one with regularization and one without. The plot with regularization shows less fluctuation, indicating that overfitting has been reduced. The model without regularization had a train RMSE of 0.659 and a test RMSE of 0.892, while the training and testing RMSE are slightly higher for regularization.\n\n\nLSTM Neural Network\nThe LSTM (Long Short-Term Memory) neural network is another powerful machine learning tool that has been widely adopted in various applications, particularly in tasks involving sequential data.The LSTM neural network architecture typically consists of multiple LSTM layers, followed by one or more dense layers and an activation function such as the sigmoid or hyperbolic tangent. Similar to the GRU neural network, the LSTM model may also include kernel regularization to address the issue of overfitting. In order to evaluate the performance of the LSTM model, the model can be trained on a dataset for a specified number of epochs, and validation loss plots can be generated to compare the performance of the regularized and non-regularized models.\n\nLSTMLSTM with Regularization\n\n\n\n\nCode\n# Create a LSTM Neural Network\ndef create_LSTM(hidden_units, dense_units, input_shape, activation,dropout_rate = 0.2, kernel_regularizer=None):\n    model = Sequential()\n    # Create a simple long short term memory neural network\n    model.add(LSTM(hidden_units,\n              activation=activation[0], input_shape=input_shape))\n    # Add a dense layer (only one, more layers would make it a deep neural net)\n    model.add(Dense(units=dense_units,\n              activation=activation[1], kernel_regularizer=kernel_regularizer))\n    # Add layer dropout\n    model.add(Dropout(dropout_rate))\n    # Compile the model and optimize on mean squared error\n    model.compile(optimizer=\"RMSprop\", loss='mae')\n    return model\n\n# Create an LSTM neural network\nmodel = create_LSTM(hidden_units=5, dense_units=1, input_shape=(lag, Xtrain.shape[-1]),\n                    activation=['tanh', 'linear'], dropout_rate = 0.2)\nhistory = model.fit(Xtrain, Ytrain, epochs=20, batch_size=1, verbose=0)\nplot_model(history, 'LSTM Model')\n\nyhat_lstm = [x[0] for x in model.predict(Xval, verbose=0)]\n\ntrain_predict = model.predict(Xtrain, verbose=0)\ntest_predict = model.predict(Xval, verbose=0)\n\n# Print error\ntrain_rmse, test_rmse = print_error(Ytrain, Yval, train_predict, test_predict)\nrmse_table['model'].append('LSTM Neural Network')\nrmse_table['training_rmse'].append(train_rmse)\nrmse_table['testing_rmse'].append(test_rmse)\n\n\nTrain RMSE: 0.203 RMSE\nTest RMSE: 0.778 RMSE\n\n\n\n\n\n\n\n\n\nCode\n# Create an LSTM neural network with regularization\nmodel = create_LSTM(hidden_units=5, dense_units=1, input_shape=(lag, Xtrain.shape[-1]),\n                    activation=['tanh', 'linear'], dropout_rate = 0.2, kernel_regularizer=regularizers.L1L2(l1=1e-5, l2=1e-4))\nhistory = model.fit(Xtrain, Ytrain, epochs=20, batch_size=1, verbose=0)\nplot_model(history, 'LSTM Model (with L1L2 Regularization)')\n\n\nyhat_lstm_reg = [x[0] for x in model.predict(Xval, verbose=0)]\n\ntrain_predict = model.predict(Xtrain, verbose=0)\ntest_predict = model.predict(Xval, verbose=0)\n\n# Print error\ntrain_rmse, test_rmse = print_error(Ytrain, Yval, train_predict, test_predict)\nrmse_table['model'].append('LSTM Neural Network (with L1L2 Regularization)')\nrmse_table['training_rmse'].append(train_rmse)\nrmse_table['testing_rmse'].append(test_rmse)\n\n\nTrain RMSE: 0.197 RMSE\nTest RMSE: 0.830 RMSE\n\n\n\n\n\n\n\n\nThe two plots show the training loss for the LSTM model, one with regularization and one without. The plot with regularization shows less fluctuation, indicating that overfitting has been reduced. The model without regularization had a train RMSE of 0.196 and a test RMSE of 0.845, while the model with regularization had a slightly higher train RMSE of 0.199 and a lower test RMSE of 0.884\n\n\nForecast\n\n\nCode\n# Creating the frame to store both predictions\ndays = df['Date'].values[-len(y):]\nframe = pd.concat([\n    pd.DataFrame({'day': days, 'price': y, 'type': 'original'}),\n    pd.DataFrame({'day': days, 'price': yhat_d, 'type': 'rnn_forecast'}),\n    pd.DataFrame({'day': days, 'price': yhat_gru, 'type': 'gru_forecast'}),\n    pd.DataFrame({'day': days, 'price': yhat_lstm, 'type': 'lstm_forecast'})\n])\n# Creating the unscaled values column\nframe['price_absolute'] = [(x * train_std) + train_mean for x in frame['price']]\n# Pivoting\npivoted = frame.pivot_table(index='day', columns='type')\npivoted.columns = ['_'.join(x).strip() for x in pivoted.columns.values]\n\nplt.figure(figsize=(12, 10))\nplt.plot(pivoted.index, pivoted.price_absolute_original,\n         color='#8B1874', label='original')\nplt.plot(pivoted.index, pivoted.price_absolute_rnn_forecast,\n         color='#F79540', label='RNN Forecast', alpha=0.6)\nplt.plot(pivoted.index, pivoted.price_absolute_gru_forecast,\n         color='#B71375', label='GRU Forecast', alpha=0.6)\nplt.plot(pivoted.index, pivoted.price_absolute_lstm_forecast,\n         color='#FC4F00', label='LSTM Forecast', alpha=0.6)\nplt.title('Dow Jones Index Stock Price Forecasts')\nplt.legend()\nplt.show()\n\n\n\n\n\nWhen comparing the predictions of the deep learning models with the original time series plot, we can observe that the LSTM model is the closest to the actual values. The GRU model is almost the same as the LSTM model, but both the models have some kind of accuracy with the actual plot, but there is still difference. RNN model are pretty far from the actual plot, which indicates that LSTM model is better when compared to others, but we need to note that the model is not accurate, i.e., it is not the same as the actual plot.\n\n\nModel Comparison\n\n\nCode\nrmse_df = pd.DataFrame(rmse_table)\nrmse_df\n\n\n\n\n\n\n  \n    \n      \n      model\n      training_rmse\n      testing_rmse\n    \n  \n  \n    \n      0\n      Recurrent Neural Network\n      0.324495\n      1.884348\n    \n    \n      1\n      Recurrent Neural Network (with L1L2 Regulariza...\n      0.330742\n      1.884503\n    \n    \n      2\n      GRU Neural Network\n      0.660577\n      0.817229\n    \n    \n      3\n      GRU Neural Network (with L1L2 Regularization)\n      0.661593\n      0.863976\n    \n    \n      4\n      LSTM Neural Network\n      0.202862\n      0.778235\n    \n    \n      5\n      LSTM Neural Network (with L1L2 Regularization)\n      0.197365\n      0.829999\n    \n  \n\n\n\n\nIn terms of performance on the testing dataset, the LSTM Neural Network without regularization appears to be the best performing model, with a testing_rmse of 0.845307. This is followed by the regularized LSTM with a testing_rmse of 0.883554, the regularized GRU with a testing_rmse of 0.909624, the non-regularized RNN with a testing_rmse of 1.888741, the regularized RNN with a testing_rmse of 1.885051, and finally, the non-regularized GRU with a testing_rmse of 0.892442.\nIt is important to note that the results can be influenced by the specific dataset and the problem at hand. However, in general, LSTMs are known to be effective in tasks that require capturing long-term dependencies in sequential data. GRUs, on the other hand, are a simpler variant of LSTMs and can be used as an alternative if the dataset is relatively small or if computational resources are limited. Regularization can help to prevent overfitting, which is a common problem in deep learning models, and can improve generalization performance.\nOverall, the results suggest that for this particular dataset, the LSTM Neural Network without regularization appears to be the best-performing model, followed closely by the regularized LSTM and the regularized GRU. The non-regularized models, both RNN and GRU, appear to perform worse than the regularized models, which highlights the importance of regularization in preventing overfitting and improving generalization performance.\n\n\nComparison of deep learning models with traditional single-variable time-series\nThe ARIMA model has an RMSE value of 385.9824. Comparing this value to the RMSE values of the deep learning models, we can see that the LSTM model with L1L2 regularization has the lowest testing RMSE, which is significantly lower than the ARIMA model. The other deep learning models also have lower RMSE values than the ARIMA model, with the highest testing RMSE value for the GRU model with L1L2 regularization.\nOverall, this suggests that the deep learning models outperform the ARIMA model in terms of predictive accuracy, with the LSTM model with L1L2 regularization being the most accurate among the deep learning models."
  },
  {
    "objectID": "ds.html",
    "href": "ds.html",
    "title": "",
    "section": "",
    "text": "The World Bank is a comprehensive platform that provides access to a wide range of economic, social, and environmental indicators from around the world. With data covering topics such as poverty, health, education, infrastructure, and trade, the World Bank data page is a valuable resource for researchers, policymakers, and anyone interested in global development. The platform features interactive tools and visualizations that allow users to explore and analyze data in meaningful ways, making it a powerful tool for understanding the complex challenges facing the world today.\nThe global stock market has been highly volatile and unpredictable in recent years due to various factors, including political tensions, trade disputes, and the ongoing COVID-19 pandemic. While some sectors, such as technology and healthcare, have seen significant growth and increased investor interest, others have struggled to keep up. Additionally, inflation concerns and shifts in monetary policy by central banks have added further uncertainty to the market. Despite these challenges, many investors continue to see potential for growth and value in the global stock market, and the availability of online trading platforms has made it easier than ever for individuals to participate in this dynamic and complex market.\n\n\n \n\n\n\n   \n\n\n\n\n\n\n\nDownload CSV"
  },
  {
    "objectID": "ds.html#sector-market-data",
    "href": "ds.html#sector-market-data",
    "title": "",
    "section": "Sector Market Data",
    "text": "Sector Market Data\nA stock market sector is a group of stocks that have a lot in common with each other, usually because they are in similar industries. There are 11 different stock market sectors, according to the most commonly used classification system: the Global Industry Classification Standard (GICS).\n\n\n\n\n\nCode\nlibrary(wesanderson)\noptions(\"getSymbols.warning4.0\"=FALSE)\noptions(\"getSymbols.yahoo.warning\"=FALSE)\n\ntickers = c(\"XLC\",\"XLY\",\"XLP\", \"XLE\", \"XLF\", \"XLV\",\"XLI\",\"XLB\", \"XLRE\", \"XLK\",\"XLU\")\nfor (i in tickers){\n  getSymbols(i,\n             from = \"2018-07-01\",\n             to = \"2023-01-01\")}\n\nx <- list(\n  title = \"date\"\n)\ny <- list(\n  title = \"value\"\n)\n\nstock <- data.frame(XLC$XLC.Adjusted,\n                    XLY$XLY.Adjusted,\n                    XLP$XLP.Adjusted,\n                    XLE$XLE.Adjusted,\n                    XLF$XLF.Adjusted,\n                    XLV$XLV.Adjusted,\n                    XLI$XLI.Adjusted,\n                    XLB$XLB.Adjusted,\n                    XLRE$XLRE.Adjusted,\n                    XLK$XLK.Adjusted,\n                    XLU$XLU.Adjusted)\n\n#create dataframe\nsector_stock_data = cbind(XLC,XLY,XLP, XLE, XLF, XLV,XLI,XLB, XLRE, XLK,XLU)\nsector_stock_data = as.data.frame(sector_stock_data)\n#export it to csv file\nwrite_csv(sector_stock_data, \"DATA/RAW DATA/sector_stock_data.csv\")\n\n\nstock <- data.frame(stock,rownames(stock))\ncolnames(stock) <- append(tickers,'Dates')\n\nstock$date<-as.Date(stock$Dates,\"%Y-%m-%d\")\n\nplot<- ggplot(stock, aes(x=date)) +\n  geom_line(aes(y=XLC, colour=\"Communication Services\"))+\n  geom_line(aes(y=XLY, colour=\"Consumer Discretionary\"))+\n  geom_line(aes(y=XLP, colour=\"Consumer Staples\"))+\n  geom_line(aes(y=XLE, colour=\"Energy\"))+\n  geom_line(aes(y=XLF, colour=\"Financials\"))+\n  geom_line(aes(y=XLV, colour=\"Health Care\"))+\n  geom_line(aes(y=XLI, colour=\"Industrials\"))+\n  geom_line(aes(y=XLB, colour=\"Materials\"))+\n  geom_line(aes(y=XLRE, colour=\"Real Estate\"))+\n  geom_line(aes(y=XLK, colour=\"Technology\"))+\n  geom_line(aes(y=XLU, colour=\"Utilities\"))+\n  scale_color_brewer(palette=\"BuPu\")+\n  theme_bw()+\n   labs(\n    title = \"Stock Market Sector History\",\n    subtitle = \"From July 2018 - January 2023\",\n    x = \"Date\",\n    y = \"Adjusted Closing Prices\")+\n    guides(colour=guide_legend(title=\"Sectors\")) \n\n  \n\n\n\nggplotly(plot)%>%\n  layout(title = list(text = paste0('Stock Market Sector History',\n                                    '<br>',\n                                    '<sup>',\n                                    'From July 2018 - January 2023',\n                                    '</sup>')))\n\n\n\n\n\n\n\nDownload CSV\n\n\nHistorical Asset Data\nAnalyzing historical returns on various investments is one of the most important tasks in financial markets. We need historical data for the assets to perform this analysis. There are numerous data providers, some of which are free while the majority are not. R Package designed to assist the quantitative trader in the development, testing, and deployment of statistically based trading models. The Quantmod – “Quantitative Financial Modeling and Trading Framework for R”! package can load data, chart data, and generate relevant technical signals. This package is compatible with a variety of sources, including Yahoo Finance and FRED. By default, getSymbols() downloads data from Yahoo at the daily frequency and the object has 6 columns representing the open, high, low, close, volume and adjusted closing price for the stock.\n\n\nYahoo Finance\nThe Yahoo Finance gives users immediate access to hours of live market coverage each day, complete with in-depth analysis and data. Investors, financial experts, and corporate executives who are serious about their money belong here. You can access this data at https://finance.yahoo.com/ ."
  },
  {
    "objectID": "ds.html#global-stock-indices-historical-data",
    "href": "ds.html#global-stock-indices-historical-data",
    "title": "",
    "section": "Global Stock Indices Historical Data",
    "text": "Global Stock Indices Historical Data\nGlobal indices are a benchmark to evaluate the strength or weakness in the overall market. Normally, a sample of highly liquid and valuable stocks from the universe of listed stocks is selected and made into an index. The weighted movement of these set of stocks or portfolio of stocks constitutes the movement of global indices. So, if global indices are moving up that means the markets are strong and if global indices are moving lower that means global markets are weak. You can understand global indices as a hypothetical portfolio of investment holdings that represents a segment of the financial market or the global indices market. The calculation of the index value is derived from the prices of the underlying stocks or assets in the index.\nThe data is importing continent-wise, to check the top stock market index around the world.\n\nNorth America and South AmericaEurope and AfricaAsia and Australia\n\n\n\n\nCode\noptions(\"getSymbols.warning4.0\"=FALSE)\noptions(\"getSymbols.yahoo.warning\"=FALSE)\n\n#America's top stock market index\ntickers = c(\"^GSPC\",\"^DJI\",\"^IXIC\", \"^RUT\")\n\nfor (i in tickers){\n  getSymbols(i,\n             from = \"2000-01-01\",\n             to = \"2023-01-01\")}\n\nx <- list(\n  title = \"date\"\n)\ny <- list(\n  title = \"value\"\n)\n\n#create dataframe\namerica_stock_index_data = cbind(GSPC,DJI,IXIC,RUT)\namerica_stock_index_data = as.data.frame(america_stock_index_data)\n#export it to csv file\nwrite_csv(america_stock_index_data, \"DATA/RAW DATA/america_stock_index_data.csv\")\n\nstock <- data.frame(GSPC$GSPC.Adjusted,\n                    DJI$DJI.Adjusted,\n                    IXIC$IXIC.Adjusted,\n                    RUT$RUT.Adjusted)\n\n\n\nstock <- data.frame(stock,rownames(stock))\ncolnames(stock) <- append(tickers,'Dates')\n\nstock$date<-as.Date(stock$Dates,\"%Y-%m-%d\")\ncolnames(stock)=c(\"GSPC\",\"DJI\",\"IXIC\",\"RUT\",\"Dates\",\"date\")\n\n\n#remove columns\nstock <- stock[,-c(5)]\nplot<- ggplot(stock, aes(x=date)) +\n  geom_line(aes(y=GSPC, colour=\"S&P 500 Index\"))+\n  geom_line(aes(y=DJI, colour=\"Dow Jones Industrial Average\"))+\n  geom_line(aes(y=IXIC, colour=\"NASDAQ Composite\"))+\n  geom_line(aes(y=RUT, colour=\"Russell 2000\"))+\n  scale_color_brewer(palette=\"RdPu\")+\n  theme_bw()+\n   labs(\n    title = \"America's Top Stock Market Index History\",\n    subtitle = \"From January 2000 - January 2023\",\n    x = \"Date\",\n    y = \"Adjusted Closing Prices\")+\n    guides(colour=guide_legend(title=\"Indices\")) \n\n  \n\n\nplot = ggplotly(plot)%>%\n  layout(title = list(text = paste0(\"America's Top Stock Market Index History\",\n                                    '<br>',\n                                    '<sup>',\n                                    'From January 2000 - January 2023',\n                                    '</sup>')))\nggplotly(plot)%>%layout(hovermode = \"x\")\n\n\n\n\n\n\n\nDownload CSV\n\n\n\n\n\nCode\n#Europe and Africa Top Indices\noptions(\"getSymbols.warning4.0\"=FALSE)\noptions(\"getSymbols.yahoo.warning\"=FALSE)\ntickers = c(\"^GDAXI\",\"^FTSE\",\"^FCHI\", \"^IBEX\",\"^STOXX50E\")\nfor (i in tickers){\n  getSymbols(i,\n             from = \"2018-01-01\",\n             to = \"2023-01-01\", periodicity=\"monthly\")}\n\nx <- list(\n  title = \"date\"\n)\ny <- list(\n  title = \"value\"\n)\n\n#create dataframe\neurope_africa_stock_index_data = cbind(GDAXI,FTSE,FCHI,IBEX,STOXX50E)\neurope_africa_stock_index_data = as.data.frame(europe_africa_stock_index_data)\n#export it to csv file\nwrite_csv(europe_africa_stock_index_data, \"DATA/RAW DATA/europe_africa_stock_index_data.csv\")\n\n\nstock <- data.frame(GDAXI$GDAXI.Adjusted,\n                    FTSE$FTSE.Adjusted,\n                    FCHI$FCHI.Adjusted,\n                    IBEX$IBEX.Adjusted,\n                    STOXX50E$STOXX50E.Adjusted)\n\n\nstock <- data.frame(stock,rownames(stock))\ncolnames(stock) <- append(tickers,'Dates')\n\nstock$date<-as.Date(stock$Dates,\"%Y-%m-%d\")\ncolnames(stock)=c(\"GDAXI\",\"FTSE\",\"FCHI\",\"IBEX\",\"STOXX50E\",\"Dates\",\"date\")\n#remove columns\nstock <- stock[,-c(6)] \nplot<- ggplot(stock, aes(x=date)) +\n  geom_line(aes(y=GDAXI, colour=\"DAX PERFORMANCE-INDEX\"))+\n  geom_line(aes(y=FTSE, colour=\"FTSE 100 Index\"))+\n  geom_line(aes(y=FCHI, colour=\"CAC 40\"))+\n  geom_line(aes(y=IBEX, colour=\"IBEX 35\"))+\n  geom_line(aes(y=STOXX50E, colour=\"EURO STOXX 50\"))+\n  scale_color_brewer(palette=\"ВuPu\")+\n  theme_bw()+\n   labs(\n    title = \"Europes and Africa's Top Stock Market Index History\",\n    subtitle = \"From January 2018 - January 2023\",\n    x = \"Date\",\n    y = \"Adjusted Closing Prices\")+\n    guides(colour=guide_legend(title=\"Indices\")) \n\n  \n\n\nggplotly(plot)%>%\n  layout(title = list(text = paste0(\"Europes and Africa's Top Stock Market Index History\",\n                                    '<br>',\n                                    '<sup>',\n                                    'From January 2018 - January 2023',\n                                    '</sup>')))\n\n\n\n\n\n\n\nDownload CSV\n\n\n\n\n\nCode\n#Asia and Australia Top Indices\noptions(\"getSymbols.warning4.0\"=FALSE)\noptions(\"getSymbols.yahoo.warning\"=FALSE)\ntickers = c(\"^N225\",\"^HSI\", \"^BSESN\",\"^NSEI\",\"^KS11\", \"^AORD\")\nfor (i in tickers){\n  getSymbols(i,\n             from = \"2012-01-01\",\n             to = \"2023-01-01\", periodicity=\"monthly\")}\n\nx <- list(\n  title = \"date\"\n)\ny <- list(\n  title = \"value\"\n)\n\n#create dataframe\nasia_australia_stock_index_data = cbind(N225,HSI,BSESN,NSEI,AORD)\nasia_australia_stock_index_data = as.data.frame(asia_australia_stock_index_data)\n#export it to csv file\nwrite_csv(asia_australia_stock_index_data, \"DATA/RAW DATA/asia_australia_stock_index_data.csv\")\n\nstock <- data.frame(N225$N225.Adjusted,\n                    HSI$HSI.Adjusted,\n                    BSESN$BSESN.Adjusted,\n                    NSEI$NSEI.Adjusted,\n                    KS11$KS11.Adjusted,\n                    AORD$AORD.Adjusted)\n\n\nstock <- data.frame(stock,rownames(stock))\ncolnames(stock) <- append(tickers,'Dates')\n\nstock$date<-as.Date(stock$Dates,\"%Y-%m-%d\")\ncolnames(stock)=c(\"N225\",\"HSI\",\"BSESN\",\"NSEI\",\"KS11\",\"AORD\",\"Dates\",\"date\")\n#remove columns\nstock <- stock[,-c(7)] \nplot<- ggplot(stock, aes(x=date)) +\n  geom_line(aes(y=N225, colour=\"Nikkei 225\"))+\n  geom_line(aes(y=HSI, colour=\"Hang Seng Index\"))+\n  geom_line(aes(y=BSESN, colour=\"BSE SENSEX\"))+\n  geom_line(aes(y=NSEI, colour=\"NIFTY 50\"))+\n  geom_line(aes(y=KS11, colour=\"KOSPI Composite Index\"))+\n  geom_line(aes(y=AORD, colour=\"ALL ORDINARIES\"))+\n  scale_color_brewer(palette=\"PuBu\")+\n  theme_bw()+\n   labs(\n    title = \"Asia's Top Stock Market Index History\",\n    subtitle = \"From January 2012 - January 2023\",\n    x = \"Date\",\n    y = \"Adjusted Closing Prices\")+\n    guides(colour=guide_legend(title=\"Indices\")) \n\n  \n\n\nggplotly(plot)%>%\n  layout(title = list(text = paste0(\"Asia and Australia's Top Stock Market Index History\",\n                                    '<br>',\n                                    '<sup>',\n                                    'From January 2012 - January 2023',\n                                    '</sup>')))\n\n\n\n\n\n\n\nDownload CSV"
  },
  {
    "objectID": "ds.html#macroeconomic-factors-data",
    "href": "ds.html#macroeconomic-factors-data",
    "title": "",
    "section": "Macroeconomic Factors Data",
    "text": "Macroeconomic Factors Data\nMacroeconomic factors such as inflation rates, gross domestic product (GDP), unemployment rates, and interest rates have a significant impact on financial markets. Historical data on these factors is crucial for analyzing and predicting market trends.\n\nThe Federal Reserve Economic Data\nThe Federal Reserve Economic Data (FRED) website, maintained by the Federal Reserve Bank of St. Louis, is a reliable source for GDP growth rate data. It offers a range of data frequencies and time periods, with a user-friendly interface featuring interactive charts and graphs to visualize trends and patterns. The website also provides tools and resources to manipulate and analyze the data, making it a valuable resource for researchers, analysts, and policymakers alike. You can access this data at https://fred.stlouisfed.org/.\n\n\nU.S. Bureau of Labor Statistics\nThe U.S. Bureau of Labor Statistics (BLS) is a government agency that collects and disseminates data related to labor economics and statistics. One of the key data sets provided by the BLS is information on employment, unemployment, and wages. This data is widely used by economists, policymakers, and businesses to analyze labor market trends and make informed decisions. Additionally, the BLS provides data on other economic indicators such as inflation and productivity. The agency’s commitment to providing accurate, reliable, and timely data has made it a trusted source for labor market information in the United States. You can access this data at https://www.bls.gov/.\n\nAbout the Macroeconomic Factor Data\nThe economic data related to inflation, unemployment, GDP growth rate and interest rate are crucial indicators of the health of an economy. Inflation rate measures the general increase in prices of goods and services over time, which is an essential measure of price stability. The unemployment rate measures the percentage of people who are seeking employment but are unable to find it, indicating the level of economic activity and labor market health. GDP growth rate measures the change in the value of goods and services produced in an economy over time and is an indicator of economic performance. Interest rates measure the cost of borrowing money, and changes in interest rates can have significant impacts on consumer spending, investment, and borrowing decisions.\n\nGDP Growth RateInterest RateInflation RateUnemployment Rate\n\n\n\n\nCode\n#import the data\ngdp <- read.csv(\"DATA/RAW DATA/gdp-growth.csv\")\n\n#change date format\ngdp$DATE <- as.Date(gdp$DATE , \"%m/%d/%Y\")\n\n#plot gdp growth rate\nfig <- plot_ly(gdp, x = ~DATE, y = ~value, type = 'scatter', mode = 'lines',line = list(color = 'rgb(158,202,225)'))\nfig <- fig %>% layout(title = \"U.S GDP Growth Rate: 2010 - 2022\",xaxis = list(title = \"Year\"),yaxis = list(title =\"GDP Growth Rate\"))\nfig\n\n\n\n\n\n\n\nDownload CSV\n\n\n\n\n\nCode\n#import the data\ninterest_data <- read.csv(\"DATA/RAW DATA/interest-rate.csv\")\n\n#change date format\ninterest_data$Date <- as.Date(interest_data$Date , \"%m/%d/%Y\")\n\n#plot interest rate \nfig <- plot_ly(interest_data, x = ~Date, y = ~value, type = 'scatter', mode = 'lines',line = list(color = 'rgb(222, 92, 92)'))\nfig <- fig %>% layout(title = \"U.S Interest Rate: January 2010 - March 2023\",xaxis = list(title = \"Time\"),yaxis = list(title =\"Interest Rate\"))\nfig\n\n\n\n\n\n\n\nDownload CSV\n\n\n\n\n\nCode\n#import the data\ninflation_rate <- read.csv(\"DATA/RAW DATA/inflation-rate.csv\")\n\n\n#cleaning the data\n#remove unwanted columns\ninflation_rate_clean <- subset(inflation_rate, select = -c(1,HALF1,HALF2))\n\n#convert the data to time series data\ninflation_data_ts <- ts(as.vector(t(as.matrix(inflation_rate_clean))), start=c(2010,1), end=c(2023,2), frequency=12)\n\n#plot inflation rate \nfig <- autoplot(inflation_data_ts, ylab = \"Inflation Rate\", color=\"#45818E\")+ggtitle(\"U.S Inflation Rate: January 2010 - February 2023\")+theme_minimal()\nggplotly(fig)\n\n\n\n\n\n\n\nDownload CSV\n\n\n\n\n\nCode\n#import the data\nunemployment_rate <- read.csv(\"DATA/RAW DATA/unemployment-rate.csv\")\n\n#change date format\nunemployment_rate$Date <- as.Date(unemployment_rate$Date , \"%m/%d/%Y\")\n\n#plot unemployment rate \n#plot interest rate \nfig <- plot_ly(unemployment_rate, x = ~Date, y = ~Value, type = 'scatter', mode = 'lines',line = list(color = 'rgb(135, 153, 164)'))\nfig <- fig %>% layout(title = \"U.S Unemployment Rate: January 2010 - March 2023\",xaxis = list(title = \"Time\"),yaxis = list(title =\"Unemployment Rate\"))\nfig\n\n\n\n\n\n\n\nDownload CSV"
  },
  {
    "objectID": "dl.html",
    "href": "dl.html",
    "title": "",
    "section": "",
    "text": "UNIVARIATE DEEP LEARNING PREDICTION\nUnivariate deep learning models are a type of machine learning algorithm that uses a single input variable to predict future values of that variable. In the context of stock market analysis, univariate models can be used to make predictions based on historical data for a particular stock or stock index.\nBy using univariate deep learning models, such as recurrent neural networks (RNNs), long short-term memory (LSTM) networks, and gated recurrent units (GRUs), analysts can make predictions on the future values of these stock indices based on their historical data. These models are designed to capture complex patterns and trends in the data, which can be used to inform investment strategies and help traders make informed decisions.\n\n\nUNIVARIATE DEEP LEARNING PREDICTION FOR STOCK INDEX\nIn this context, we will be looking at the top 3 stock price indexes: S&P 500, Dow Jones Industrial Average (DJIA), and Nasdaq Composite. These indexes are widely used as benchmarks for the overall performance of the US stock market, and predicting their future movements is of great interest to investors, traders, and financial analysts. By using deep learning models, we can leverage the power of neural networks to capture complex patterns and relationships in the historical data, and use them to make more accurate predictions about future movements of these stock price indexes.\nClick to view Dow Jones Index Stock Price\nClick to view NASDAQ Composite Index Stock Price\nClick to view S&P 500 Index Stock Price\n\n\nUNIVARIATE DEEP LEARNING PREDICTION FOR SECTOR MARKET\nIn this context, we will be looking at the sector market, which is made up of indexes that track the performance of specific industry sectors within the stock market, such as technology, healthcare, and energy. These sector indexes are widely used as indicators of the performance of their respective industries and can provide insights into the overall health of the stock market. By using deep learning models, we can leverage the power of neural networks to capture complex patterns and relationships in the historical data of a specific sector index and use them to make more accurate predictions about future movements of that index. This can be particularly useful for investors, traders, and financial analysts who specialize in a specific industry sector and need to make informed decisions based on its market trends.\nClick to view Consumer Staples Sector Fund\nClick to view Utilities Sector Fund\nClick to view Health Care Sector Fund\nClick to view Industrial Sector Fund\nClick to view Financial Sector Fund\nClick to view Consumer Discretionary Sector Fund\nClick to view Communication Services Sector Fund\nClick to view Real Estate Sector Fund\nClick to view Materials Sector Fund\nClick to view Technology Sector Fund\nClick to view Energy Sector Fund"
  }
]