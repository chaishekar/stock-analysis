[
  {
    "objectID": "about.html",
    "href": "about.html",
    "title": "",
    "section": "",
    "text": "Name: Chaitanya Shekar\nNet ID: cs2046\nHello everyone! My name is Chaitanya Shekar pursuing my masters degree in Data Science and Analytics at Georgetown University. I graduated with a bachelor’s degree with a major in statistics, mathematics, and economics. During the intensive programming component of my master’s degree, I’m learning skills and techniques to deal with unique challenges in complex algorithms and scientific problems. My DSAN goals involve strengthening my machine learning and deep learning understanding and programming skills.\nAs I take a quick look about, I see that the utilization of big data for business applications, establishing marketing trends, and forecasting consumer behavior has skyrocketed, and as a direct consequence of this, the demand for both business and data analysts has also considerably increased. I want to deepen my understanding of data science and statistics as well as have a better handle on the intricacies that are present in both areas of study. My high school placed a significant emphasis on mathematics and science, which kindled my interest in data and motivated me to pursue a degree in statistics while I was enrolled in my university studies. As a consequence of this, I aimed my qualifications in the right direction and worked on developing my analytical skills in order to acquire a deeper comprehension of the influence that data can wield.\nI consider myself to be an ambitious person who has the long-term goal of working in the field of data science at some point in the not-too-distant future. I would like to continue developing my talents in this area. In addition to completing the requirements for a conventional graduate degree, I also earned certificates in data science and analysis, machine learning, neural networks, deep learning, programming in Python and R, and all these subjects. Because I have such a great interest in data sciences, I decided to carry this out. To estimate who would emerge triumphant from a game of roulette based on statistical probability and game theory as the underlying concepts, I constructed a model-based project for the inter-collegiate Statistics Festival. During the time that I was an undergraduate student, I took part in a project in the field of data science that concentrated primarily on statistical principles. The study focused on cardiovascular health and the factors that are related with increased risk of cardiovascular disease.\nFollowing the completion of my undergraduate degree, I obtained an internship with Feedback Business Consulting Services Pvt, Bangalore, in the role of research analyst. As part of my responsibilities, I was responsible for managing the data gathering for the Patent Operations B2B benchmark research as well as performing analysis.\nAfter giving my current skill set and the demand for innovative analystâ€™s considerable consideration, I’ve come to the conclusion that I want to initiate the process of converting myself into a global citizen who is capable of thriving in high-pressure work environments. This course will provide me with the right steppingstone to enable me to enter the field of sophisticated technology and play a leadership role in the investigation and development of breakthroughs of this kind. This class will also prepare me to enter the field of sophisticated technology. Because of the meticulous study, analysis, and presentation of needs that the program provides, I am in a position where I can provide ideas and provide professional advice. I am extremely confident that it will assist me in applying the skills by undertaking projects and paving the path for ground-breaking research and development; furthermore, the emphasis on real-world skills, such as problem-solving, legal, ethical, and professional framework throughout the course that would give a holistic view. I am extremely confident that it will assist me in applying the skills by undertaking projects and paving the path for ground-breaking research and development. My head is spinning at the amazing amount of data and breadth that is necessary because I am detail-oriented, enjoys working with a variety of data, and appreciates performing analysis. The development of the capabilities and skills essential to prosper in the industry while also being able to adjust to the continuous changes that are taking place in that sector is the purpose of the program. In addition to my academic interests now, at a time when the global information technology industry is on the edge of a major technological transition, the data sciences are crucial to our attempts to explore the next frontier.\nI am quite proud of my academic achievements, but as a person, I am also aware of the advantages I have enjoyed in life, and I want to help close the gaps by making positive contributions toward creating a more equitable world. It motivated me to get involved in actions for social causes, such as becoming an active member of the core committee of Amnesty International. was an event that left me feeling really fulfilled and got me thinking about how I could put my skills to use in the service of social improvement. When I think back on my path, I realize that it was at that point that I first became aware of the applications of data sciences in a variety of fields, including but not limited to business, artificial intelligence, healthcare, sustainability, and climate change. During that time, I participated in several different events and campaigns in which I pushed for human rights, LGBTQ rights, and the rights of other underrepresented groups in our society. As part of my efforts to further the cause, I went to Home of Hope, a shelter for the disadvantaged, and while there, I became aware of socioeconomic and structural disparities. Not only that, but I also took part in efforts to raise general awareness about cancer and the silent march for AIDS. It was something I desired to investigate, and I dove headfirst into the data.\nIn addition to pursuing a career in academia, one of my passions is traveling and learning about new countries and ways of life. In addition, a significant reason for me to get involved in the fight against climate change and global warming is to promote sustainable living practices in all aspects of human endeavor."
  },
  {
    "objectID": "arch_Consumer_Discretionary_Sector_Fund.html",
    "href": "arch_Consumer_Discretionary_Sector_Fund.html",
    "title": "",
    "section": "",
    "text": "Impact of Macroeconomic Factors on Amazon Stock Price\n\n\n\nThe Consumer Discretionary sector comprises companies that produce non-essential goods and services, such as apparel, entertainment, and luxury items. One of the top companies in this sector is Amazon.com, Inc. (AMZN), which has revolutionized the retail industry by providing customers with an online platform to purchase a wide range of products.\nBy using an ARIMAX+ARCH/GARCH model to analyze the stock price behavior of Amazon, we can gain insights into how macroeconomic factors impact the company’s performance. For example, an increase in GDP growth rate or a decrease in unemployment rate may lead to increased consumer spending and a rise in Amazon’s stock price. Conversely, an increase in inflation or interest rates may lead to a decrease in consumer spending and a decline in Amazon’s stock price.\n\n\nTime series Plot\n\nAmazonDifferentitaed Chart SeriesGDP GrowthInflationInterestUnemployment\n\n\n\n\nCode\n# get data\noptions(\"getSymbols.warning4.0\"=FALSE)\noptions(\"getSymbols.yahoo.warning\"=FALSE)\n\n\ndata.info = getSymbols(\"AMZN\",src='yahoo', from = '2010-01-01',to = \"2023-03-01\",auto.assign = FALSE)\ndata = getSymbols(\"AMZN\",src='yahoo', from = '2010-01-01',to = \"2023-03-01\")\ndf <- data.frame(Date=index(AMZN),coredata(AMZN))\n\n# create Bollinger Bands\nbbands <- BBands(AMZN[,c(\"AMZN.High\",\"AMZN.Low\",\"AMZN.Close\")])\n\n# join and subset data\ndf <- subset(cbind(df, data.frame(bbands[,1:3])), Date >= \"2010-01-01\")\n\n\n# colors column for increasing and decreasing\nfor (i in 1:length(df[,1])) {\n  if (df$AMZN.Close[i] >= df$AMZN.Open[i]) {\n      df$direction[i] = 'Increasing'\n  } else {\n      df$direction[i] = 'Decreasing'\n  }\n}\n\ni <- list(line = list(color = '#7FB3D5'))\nd <- list(line = list(color = '#7F7F7F'))\n\n# plot candlestick chart\n\nfig <- df %>% plot_ly(x = ~Date, type=\"candlestick\",\n          open = ~AMZN.Open, close = ~AMZN.Close,\n          high = ~AMZN.High, low = ~AMZN.Low, name = \"AMZN\",\n          increasing = i, decreasing = d) \nfig <- fig %>% add_lines(x = ~Date, y = ~up , name = \"B Bands\",\n            line = list(color = '#ccc', width = 0.5),\n            legendgroup = \"Bollinger Bands\",\n            hoverinfo = \"none\", inherit = F) \nfig <- fig %>% add_lines(x = ~Date, y = ~dn, name = \"B Bands\",\n            line = list(color = '#ccc', width = 0.5),\n            legendgroup = \"Bollinger Bands\", inherit = F,\n            showlegend = FALSE, hoverinfo = \"none\") \nfig <- fig %>% add_lines(x = ~Date, y = ~mavg, name = \"Mv Avg\",\n            line = list(color = '#C052B3', width = 0.5),\n            hoverinfo = \"none\", inherit = F) \nfig <- fig %>% layout(yaxis = list(title = \"Price\"))\n\n# plot volume bar chart\nfig2 <- df \nfig2 <- fig2 %>% plot_ly(x=~Date, y=~AMZN.Volume, type='bar', name = \"AMZN Volume\",\n          color = ~direction, colors = c('#7FB3D5','#7F7F7F')) \nfig2 <- fig2 %>% layout(yaxis = list(title = \"Volume\"))\n\n# create rangeselector buttons\nrs <- list(visible = TRUE, x = 0.5, y = -0.055,\n           xanchor = 'center', yref = 'paper',\n           font = list(size = 9),\n           buttons = list(\n             list(count=1,\n                  label='RESET',\n                  step='all'),\n             list(count=3,\n                  label='3 YR',\n                  step='year',\n                  stepmode='backward'),\n             list(count=1,\n                  label='1 YR',\n                  step='year',\n                  stepmode='backward'),\n             list(count=1,\n                  label='1 MO',\n                  step='month',\n                  stepmode='backward')\n           ))\n\n# subplot with shared x axis\nfig <- subplot(fig, fig2, heights = c(0.7,0.2), nrows=2,\n             shareX = TRUE, titleY = TRUE)\nfig <- fig %>% layout(title = paste(\"Amazon Stock Price: January 2010 - March 2023\"),\n         xaxis = list(rangeselector = rs),\n         legend = list(orientation = 'h', x = 0.5, y = 1,\n                       xanchor = 'center', yref = 'paper',\n                       font = list(size = 10),\n                       bgcolor = 'transparent'))\n\nfig\n\n\n\n\n\n\n\n\n\n\nCode\nlog(data.info$`AMZN.Adjusted`) %>% diff() %>% chartSeries(theme=chartTheme('white'),up.col='#7FB3D5')\n\n\n\n\n\n\n\n\n\nCode\n#import the data\ngdp <- read.csv(\"DATA/RAW DATA/gdp-growth.csv\")\n\n#change date format\ngdp$Date <- as.Date(gdp$DATE , \"%m/%d/%Y\")\n\n#drop DATE column\ngdp <- subset(gdp, select = -c(1))\n\n#export the cleaned data\ngdp_clean <- gdp\nwrite.csv(gdp_clean, \"DATA/CLEANED DATA/gdp_clean_data.csv\", row.names=FALSE)\n\n#plot gdp growth rate \nfig <- plot_ly(gdp, x = ~Date, y = ~value, type = 'scatter', mode = 'lines',line = list(color = 'rgb(240, 128, 128)'))\nfig <- fig %>% layout(title = \"U.S GPD Growth Rate: 2010 - 2022\",xaxis = list(title = \"Time\"),yaxis = list(title =\"GDP Growth Rate\"))\nfig\n\n\n\n\n\n\n\n\n\n\nCode\n#import the data\ninflation_rate <- read.csv(\"DATA/RAW DATA/inflation-rate.csv\")\n\n#cleaning the data\n#remove unwanted columns\ninflation_rate_clean <- subset(inflation_rate, select = -c(1,HALF1,HALF2))\n\n#convert the data to time series data\ninflation_data_ts <- ts(as.vector(t(as.matrix(inflation_rate_clean))), start=c(2010,1), end=c(2023,2), frequency=12)\n\n#export the data\nwrite.csv(inflation_rate_clean, \"DATA/CLEANED DATA/inflation_rate_clean_data.csv\", row.names=FALSE)\n\n\n#plot inflation rate \nfig <- autoplot(inflation_data_ts, ylab = \"Inflation Rate\", color=\"#FFA07A\")+ggtitle(\"U.S Inflation Rate: January 2010 - February 2023\")+theme_bw()\nggplotly(fig)\n\n\n\n\n\n\n\n\n\n\nCode\n#import the data\ninterest_data <- read.csv(\"DATA/RAW DATA/interest-rate.csv\")\n\n#change date format\ninterest_data$Date <- as.Date(interest_data$Date , \"%m/%d/%Y\")\n\n#export the cleaned data\ninterest_clean_data <- interest_data\nwrite.csv(interest_clean_data, \"DATA/CLEANED DATA/interest_rate_clean_data.csv\", row.names=FALSE)\n\n#plot interest rate \nfig <- plot_ly(interest_data, x = ~Date, y = ~value, type = 'scatter', mode = 'lines',line = list(color='rgb(219, 112, 147)'))\nfig <- fig %>% layout(title = \"U.S Interest Rate: January 2010 - March 2023\",xaxis = list(title = \"Time\"),yaxis = list(title =\"Interest Rate\"))\nfig\n\n\n\n\n\n\n\n\n\n\nCode\n#import the data\nunemployment_rate <- read.csv(\"DATA/RAW DATA/unemployment-rate.csv\")\n\n#change date format\nunemployment_rate$Date <- as.Date(unemployment_rate$Date , \"%m/%d/%Y\")\n\n# export the data\nwrite.csv(unemployment_rate, \"DATA/CLEANED DATA/unemployment_rate_clean_data.csv\", row.names=FALSE)\n\n#plot unemployment rate \nfig <- plot_ly(unemployment_rate, x = ~Date, y = ~Value, type = 'scatter', mode = 'lines',line = list(color = 'rgb(189, 183, 107)'))\nfig <- fig %>% layout(title = \"U.S Unemployment Rate: January 2010 - March 2023\",xaxis = list(title = \"Time\"),yaxis = list(title =\"Unemployment Rate\"))\nfig\n\n\n\n\n\n\n\n\n\nFrom 2010 to 2023, Amazon’s stock experienced significant growth as the company expanded its operations and diversified its offerings. In 2010, Amazon was primarily known as an online retailer of books, electronics, and other consumer goods. However, over the next several years, the company expanded into new markets, including streaming video and music, cloud computing services, and even physical retail stores.\nThroughout this period, Amazon’s stock price saw consistent growth, although there were occasional dips in response to macroeconomic events or company-specific news. For example, the company’s stock price declined in 2014 amid concerns about its profitability and increased competition in the retail industry. However, Amazon’s stock quickly rebounded as the company continued to innovate and expand into new markets.\nIn recent years, Amazon’s stock price has been impacted by a variety of factors, including the COVID-19 pandemic and increased scrutiny from regulators. Nevertheless, the company’s strong position in the e-commerce and cloud computing markets, as well as its continued investment in new technologies and initiatives, have helped to maintain investor confidence in Amazon’s long-term growth potential.\nSince early 2021, Amazon’s stock price has experienced some volatility, likely due to a combination of factors such as global economic uncertainty and fluctuations in consumer demand for the company’s products.\nAs discussed before, the macroeconomic factors of GDP growth, inflation, interest rates, and unemployment rate are closely interrelated and play a crucial role in the overall health and stability of an economy. From 2010 to 2023, the global economy experienced a mix of ups and downs, with periods of strong GDP growth followed by slowdowns and recessions.\nThe second plot shows the first difference of the logarithm of the adjusted Amazon stock price. Taking the first difference removes any long-term trends and transforms the time series into a stationary process. From the plot, we can observe that the first difference of the logarithm of the Amazon stock price appears to be stationary, as the mean and variance are roughly constant over time.\n\n\nEnodogenous and Exogenous Variables\n\nPlotCorrelation HeatmapCCF GDPCCF InterestCCF InflationCCF Unemployment\n\n\n\n\nCode\nnumeric_data <- c(\"AMZN.Adjusted\",\"gdp\", \"interest\", \"inflation\", \"unemployment\")\nnumeric_data <- final[, numeric_data]\nnormalized_data_numeric <- scale(numeric_data)\nnormalized_data <- ts(normalized_data_numeric, start = c(2010, 1), end = c(2021,10),frequency = 4)\nts_plot(normalized_data,\n        title = \"Normalized Time Series Data for AMZN Stock and Macroeconomic Variables\",\n        Ytitle = \"Normalized Values\",\n        Xtitle = \"Year\")\n\n\n\n\n\n\n\n\n\n\nCode\n# Get upper triangle of the correlation matrix\nget_upper_tri <- function(cormat){\n    cormat[lower.tri(cormat)]<- NA\n    return(cormat)\n}\ncormat <- round(cor(normalized_data_numeric),2)\n\nupper_tri <- get_upper_tri(cormat)\n\nmelted_cormat <- melt(upper_tri, na.rm = TRUE)\n# Create a ggheatmap\nggheatmap <- ggplot(melted_cormat, aes(Var2, Var1, fill = value))+\n geom_tile(color = \"white\")+\n scale_fill_gradient2(low = \"blue\", high = \"red\", mid = \"white\", \n   midpoint = 0, limit = c(-1,1), space = \"Lab\", \n    name=\"Pearson\\nCorrelation\") +\n  theme_minimal()+ # minimal theme\n theme(axis.text.x = element_text(angle = 45, vjust = 1, \n    size = 12, hjust = 1))+\n coord_fixed()\n\nggheatmap + \ngeom_text(aes(Var2, Var1, label = value), color = \"black\", size = 4) +\ntheme(\n  axis.title.x = element_blank(),\n  axis.title.y = element_blank(),\n  panel.grid.major = element_blank(),\n  panel.border = element_blank(),\n  panel.background = element_blank(),\n  axis.ticks = element_blank(),\n  legend.justification = c(1, 0),\n  legend.position = c(0.6, 0.7),\n  legend.direction = \"horizontal\")+\n  guides(fill = guide_colorbar(barwidth = 7, barheight = 1,\n                title.position = \"top\", title.hjust = 0.5))\n\n\n\n\n\n\n\n\n\nCode\npar(mfrow=c(1,1))\nccf_result <- ccf(normalized_data[, c(\"AMZN.Adjusted\")], normalized_data[, c(\"gdp\")], \n    lag.max = 300,\n    main = \"Cros-Correlation Plot for AMZN Stock Price and GDP Growth Rate \",\n    ylab = \"CCF\")\n\n\n\n\n\nCode\ncat(\"The sum of cross correlation function is\", sum(abs(ccf_result$acf)))\n\n\nThe sum of cross correlation function is 4.585922\n\n\n\n\n\n\nCode\npar(mfrow=c(1,1))\nccf_result <- ccf(normalized_data[, c(\"AMZN.Adjusted\")], normalized_data[, c(\"interest\")], \n    lag.max = 300,\n    main = \"Cros-Correlation Plot for AMZN Stock Price and Interest Rate\",\n    ylab = \"CCF\")\n\n\n\n\n\nCode\ncat(\"The sum of cross correlation function is\", sum(abs(ccf_result$acf)))\n\n\nThe sum of cross correlation function is 11.29588\n\n\n\n\n\n\nCode\npar(mfrow=c(1,1))\nccf_result <- ccf(normalized_data[, c(\"AMZN.Adjusted\")], normalized_data[, c(\"inflation\")], \n    lag.max = 300,\n    main = \"Cros-Correlation Plot for AMZN Stock Price and Inflation Rate\",\n    ylab = \"CCF\")\n\n\n\n\n\nCode\ncat(\"The sum of cross correlation function is\", sum(abs(ccf_result$acf)))\n\n\nThe sum of cross correlation function is 16.68466\n\n\n\n\n\n\nCode\npar(mfrow=c(1,1))\nccf_result <- ccf(normalized_data[, c(\"AMZN.Adjusted\")], normalized_data[, c(\"unemployment\")], \n    lag.max = 300,\n    main = \"Cros-Correlation Plot for AMZN Stock Priceand Unemployment Rate\",\n    ylab = \"CCF\")\n\n\n\n\n\nCode\ncat(\"The sum of cross correlation function is\", sum(abs(ccf_result$acf)))\n\n\nThe sum of cross correlation function is 20.53876\n\n\n\n\n\nThe Normalized Time Series Data for Stock Price and Macroeconomic Variables plot shows the same variables as the first plot but has been normalized to a common range of 0 to 1 using the scale() function in R, which standardizes the variables to have a mean of 0 and a standard deviation of 1. The heatmap analysis of the normalized data reveals that inflation and unemployment rate exhibit strong positive correlations with the stock price indices, indicating that these variables may significantly influence stock price movements. On the other hand, weaker correlations were observed between the stock price indices and GDP and interest rates, suggesting that these variables may have less impact on stock price fluctuations. The cross-correlation feature plots confirm these findings, indicating that inflation and unemployment rate are more suitable feature variables for the ARIMAX model when predicting Amazon movements.\nFinal Exogenous variables: Macroeconomic indicators: Inflation rate and unemployment rate.\n\n\nEnodogenous and Exogenous Variables Plot\n\nPlotCheck the stationarity\n\n\n\n\nCode\nfinal_data <- final %>%dplyr::select( Date,AMZN.Adjusted, inflation,unemployment)\nnumeric_data <- c(\"AMZN.Adjusted\", \"inflation\",\"unemployment\")\nnumeric_data <- final_data[, numeric_data]\nnormalized_data_numeric <- scale(numeric_data)\nnormalized_numeric_df <- data.frame(normalized_data_numeric)\nnormalized_data_ts <- ts(normalized_data_numeric, start = c(2010, 1), frequency = 4)\n\nautoplot(normalized_data_ts, facets=TRUE) +\n  xlab(\"Year\") + ylab(\"\") +\n  ggtitle(\"Amazon Stock Price, Inflation Rate and Unemployment Rate in USA 2010-2023\")\n\n\n\n\n\n\n\n\n\nCode\n# Convert your multivariate time series data to a matrix\nfinal_data_ts_multivariate <- as.matrix(normalized_data_ts)\n\n# Check for stationarity using Phillips-Perron test\nphillips_perron_test <- ur.pp(final_data_ts_multivariate)  \nsummary(phillips_perron_test)\n\n\n\n################################## \n# Phillips-Perron Unit Root Test # \n################################## \n\nTest regression with intercept \n\n\nCall:\nlm(formula = y ~ y.l1)\n\nResiduals:\n    Min      1Q  Median      3Q     Max \n-1.4360 -0.1440 -0.0602  0.0993  4.5029 \n\nCoefficients:\n             Estimate Std. Error t value Pr(>|t|)    \n(Intercept) 0.0001847  0.0403454   0.005    0.996    \ny.l1        0.8645022  0.0407542  21.213   <2e-16 ***\n---\nSignif. codes:  0 '***' 0.001 '**' 0.01 '*' 0.05 '.' 0.1 ' ' 1\n\nResidual standard error: 0.5023 on 153 degrees of freedom\nMultiple R-squared:  0.7463,    Adjusted R-squared:  0.7446 \nF-statistic:   450 on 1 and 153 DF,  p-value: < 2.2e-16\n\n\nValue of test-statistic, type: Z-alpha  is: -22.0542 \n\n         aux. Z statistics\nZ-tau-mu            0.0055\n\n\n\n\n\nThe results of the Phillips-Perron unit root test indicate strong evidence against the null hypothesis of a unit root, as the p-value for the coefficient of the lagged variable is less than the significance level of 0.05. This suggests that the variable y, which is being tested for stationarity, is likely stationary. Furthermore, the test statistic Z-tau-mu is 0.0055, which is smaller than the critical value of Z-alpha (-22.0542), providing further evidence of stationarity.\nTo determine whether the linear model requires an ARCH model, an ARCH test is conducted. The ACF and PACF plots are also used to identify suitable model values.\n\n\nModel Fitting\n\nPlotARCH TestACF PlotPACF Plot\n\n\n\n\nCode\nnormalized_numeric_df$AMZN.Adjusted<-ts(normalized_numeric_df$AMZN.Adjusted,star=decimal_date(as.Date(\"2010-01-01\",format = \"%Y-%m-%d\")),frequency = 4)\nnormalized_numeric_df$inflation<-ts(normalized_numeric_df$inflation,star=decimal_date(as.Date(\"2010-01-01\",format = \"%Y-%m-%d\")),frequency = 4)\nnormalized_numeric_df$unemployment<-ts(normalized_numeric_df$unemployment,star=decimal_date(as.Date(\"2010-01-01\",format = \"%Y-%m-%d\")),frequency = 4)\n\nfit <- lm(AMZN.Adjusted ~ inflation+unemployment, data=normalized_numeric_df)\nfit.res<-ts(residuals(fit),star=decimal_date(as.Date(\"2010-01-01\",format = \"%Y-%m-%d\")),frequency = 4)\n############## Then look at the residuals ############\nreturns <- fit.res  %>% diff()\nautoplot(returns)+ggtitle(\"Linear Model Returns\")\n\n\n\n\n\n\n\n\n\nCode\nbyd.archTest <- ArchTest(fit.res, lags = 1, demean = TRUE)\nbyd.archTest\n\n\n\n    ARCH LM-test; Null hypothesis: no ARCH effects\n\ndata:  fit.res\nChi-squared = 36.896, df = 1, p-value = 1.246e-09\n\n\n\n\n\n\nCode\nggAcf(returns) +ggtitle(\"ACF for returns\")\n\n\n\n\n\n\n\n\n\nCode\nggPacf(returns) +ggtitle(\"PACF for returns\")\n\n\n\n\n\n\n\n\nThe ARCH LM-test was conducted with the null hypothesis of no ARCH effects. The test resulted in a chi-squared value of 36.896 with one degree of freedom, and a very low p-value of 1.246e-09. This suggests strong evidence against the null hypothesis, indicating the presence of ARCH effects in the data.\nBased on the ACF and PACF plots, it appears that there is some significant autocorrelation and partial autocorrelation at multiple lags, which suggests that an ARIMA model may not be sufficient to capture the time series behavior. Additionally, the values for p and q appear to be relatively high, with p = 1 and q = 1 being suggested by the plots.\n\n\nARIMAX Model\n\nAuto Arima ModelAuto Arima ResidualsARIMA (1,1,1) PlotARIMA (1,1,1) ModelCross Validation\n\n\n\n\nCode\nxreg <- cbind(Inflation = normalized_data_ts[, \"inflation\"],\n              Unemployment = normalized_data_ts[, \"unemployment\"])\nfit.auto <- auto.arima(normalized_data_ts[, \"AMZN.Adjusted\"], xreg = xreg)\nsummary(fit.auto)\n\n\nSeries: normalized_data_ts[, \"AMZN.Adjusted\"] \nRegression with ARIMA(0,1,0) errors \n\nCoefficients:\n      Inflation  Unemployment\n         0.0625       -0.0687\ns.e.     0.0966        0.0438\n\nsigma^2 = 0.04933:  log likelihood = 5.39\nAIC=-4.78   AICc=-4.27   BIC=1.01\n\nTraining set error measures:\n                     ME     RMSE       MAE      MPE     MAPE     MASE      ACF1\nTraining set 0.03057732 0.215598 0.1225672 11.69566 31.46174 0.408043 0.1470963\n\n\n\n\n\n\nCode\ncheckresiduals(fit.auto)\n\n\n\n\n\n\n    Ljung-Box test\n\ndata:  Residuals from Regression with ARIMA(0,1,0) errors\nQ* = 4.4858, df = 8, p-value = 0.8109\n\nModel df: 0.   Total lags used: 8\n\n\n\n\n\n\nCode\nset.seed(1234)\n\nmodel_output <- capture.output(sarima(fit.res, 1,1,1)) \n\n\n\n\n\n\n\n\n\nCode\ncat(model_output[30:61], model_output[length(model_output)], sep = \"\\n\")\n\n\nCall:\narima(x = xdata, order = c(p, d, q), seasonal = list(order = c(P, D, Q), period = S), \n    xreg = constant, transform.pars = trans, fixed = fixed, optim.control = list(trace = trc, \n        REPORT = 1, reltol = tol))\n\nCoefficients:\n         ar1      ma1  constant\n      0.6076  -0.2899   -0.0064\ns.e.  0.2341   0.2628    0.0647\n\nsigma^2 estimated as 0.06782:  log likelihood = -3.83,  aic = 15.66\n\n$degrees_of_freedom\n[1] 48\n\n$ttable\n         Estimate     SE t.value p.value\nar1        0.6076 0.2341  2.5960  0.0125\nma1       -0.2899 0.2628 -1.1031  0.2755\nconstant  -0.0064 0.0647 -0.0984  0.9220\n\n$AIC\n[1] 0.3070239\n\n$AICc\n[1] 0.3170364\n\n$BIC\n[1] 0.4585396\n\nNA\nNA\n\n\n\n\n\n\nCode\nn=length(fit.res)\nk= 51\n \n \nrmse1 <- matrix(NA, (n-k),4)\nrmse2 <- matrix(NA, (n-k),4)\nrmse3 <- matrix(NA, (n-k),4)\n\n\nst <- tsp(fit.res)[1]+(k-5)/4 \n\nfor(i in 1:(n-k))\n{\n  xtrain <- window(fit.res, end=st + i/4)\n  xtest <- window(fit.res, start=st + (i+1)/4, end=st + (i+4)/4)\n  \n  #ARIMA(0,1,0) ARIMA(1,1,1)\n  \n  fit <- Arima(xtrain, order=c(0,1,0),\n                include.drift=TRUE, method=\"ML\")\n  fcast <- forecast(fit, h=4)\n  \n  fit2 <- Arima(xtrain, order=c(1,1,1),\n                include.drift=TRUE, method=\"ML\")\n  fcast2 <- forecast(fit2, h=4)\n\n\n  rmse1[i,1:length(xtest)]   <- sqrt((fcast$mean-xtest)^2)\n  rmse2[i,1:length(xtest)] <- sqrt((fcast2$mean-xtest)^2)\n}\n\nplot(1:4,colMeans(rmse1,na.rm=TRUE), type=\"l\",col=2, xlab=\"horizon\", ylab=\"RMSE\")\nlines(1:4, colMeans(rmse2,na.rm=TRUE), type=\"l\",col=3)\nlegend(\"topleft\",legend=c(\"fit1\",\"fit2\"),col=2:4,lty=1)\n\n\n\n\n\n\n\n\nBased on the results of the auto.arima function, the suggested best model is ARIMA(0,1,0), but the acf and pacf plots suggest a simpler ARIMA(1,1,1) model. To determine the best model, we conduct cross-validation and compare the RMSE values of both models. The results show that ARIMA(1,1,1) has lower RMSE values than ARIMA(0,1,0), indicating that it is the better model.\nWe can then proceed to choose the best GARCH model using ARIMA(1,1,1) as the base model.\n\n\nSquared Residuals\n\nPlotACF PlotPACF PlotGRACH(1,1)\n\n\n\n\nCode\nfit <- lm(AMZN.Adjusted ~ inflation+unemployment, data=normalized_numeric_df)\nfit.res<-ts(residuals(fit),star=decimal_date(as.Date(\"2010-01-01\",format = \"%Y-%m-%d\")),frequency = 4)\nfit <- Arima(fit.res,order=c(1,1,1))\nres=fit$res\nplot(res^2,main='Squared Residuals')\n\n\n\n\n\n\n\n\n\nCode\nacf(res^2,24, main = \"ACF Residuals Square\")\n\n\n\n\n\n\n\n\n\nCode\npacf(res^2,24, main = \"PACF Residuals Square\")\n\n\n\n\n\n\n\n\n\nCode\nsummary(garchFit(~garch(1,1),res, trace=F))\n\n\n\nTitle:\n GARCH Modelling \n\nCall:\n garchFit(formula = ~garch(1, 1), data = res, trace = F) \n\nMean and Variance Equation:\n data ~ garch(1, 1)\n<environment: 0x1557b63a0>\n [data = res]\n\nConditional Distribution:\n norm \n\nCoefficient(s):\n         mu        omega       alpha1        beta1  \n-0.00530411   0.00068953   0.48627960   0.65780719  \n\nStd. Errors:\n based on Hessian \n\nError Analysis:\n         Estimate  Std. Error  t value Pr(>|t|)    \nmu     -0.0053041   0.0155220   -0.342   0.7326    \nomega   0.0006895   0.0011861    0.581   0.5610    \nalpha1  0.4862796   0.2392140    2.033   0.0421 *  \nbeta1   0.6578072   0.1208067    5.445 5.18e-08 ***\n---\nSignif. codes:  0 '***' 0.001 '**' 0.01 '*' 0.05 '.' 0.1 ' ' 1\n\nLog Likelihood:\n 9.169268    normalized:  0.1763321 \n\nDescription:\n Tue Jan  9 20:58:23 2024 by user:  \n\n\nStandardised Residuals Tests:\n                                Statistic p-Value   \n Jarque-Bera Test   R    Chi^2  1.08557   0.5811275 \n Shapiro-Wilk Test  R    W      0.9616717 0.09248575\n Ljung-Box Test     R    Q(10)  5.843556  0.8282312 \n Ljung-Box Test     R    Q(15)  8.260042  0.9129345 \n Ljung-Box Test     R    Q(20)  15.71232  0.7343031 \n Ljung-Box Test     R^2  Q(10)  9.933015  0.4463896 \n Ljung-Box Test     R^2  Q(15)  11.22686  0.7363534 \n Ljung-Box Test     R^2  Q(20)  13.8952   0.8357696 \n LM Arch Test       R    TR^2   13.33945  0.3448583 \n\nInformation Criterion Statistics:\n        AIC         BIC         SIC        HQIC \n-0.19881801 -0.04872234 -0.20956333 -0.14127488 \n\n\n\n\n\nFrom the squared residuals of the best ARIMA model, it can be observed that the ACF plot and PACF plot indicate that the residuals are not autocorrelated and are white noise, indicating a good fit of the model. Based on the squared residuals of the best ARIMA model, we can see that the ACF and PACF plots indicate that most of the values lie between the blue lines. Additionally, the p-value is 1 and q-value is 1. This suggests that the model has a good fit and that there is no significant autocorrelation or partial autocorrelation in the residuals.\nThe bst model is ARIMA(1,1,1) and GARCH(1,1). #### Best Model\n\nARIMA ModelGARCH ModelVolatility\n\n\n\n\nCode\n#fiting an ARIMA model to the Inflation variable\ninflation_fit<-auto.arima(normalized_numeric_df$inflation) \nfinflation<-forecast(inflation_fit)\n\n#fitting an ARIMA model to the Unemployment variable\nunemployment_fit<-auto.arima(normalized_numeric_df$unemployment) \nfunemployment<-forecast(unemployment_fit)\n\n# best model fit for forcasting\nxreg <- cbind(Inflation = normalized_data_ts[, \"inflation\"],\n              Unemployment = normalized_data_ts[, \"unemployment\"])\n\n\n\nsummary(arima.fit<-Arima(normalized_data_ts[, \"AMZN.Adjusted\"],order=c(1,1,1),xreg=xreg),include.drift = TRUE)\n\n\nSeries: normalized_data_ts[, \"AMZN.Adjusted\"] \nRegression with ARIMA(1,1,1) errors \n\nCoefficients:\n          ar1     ma1  Inflation  Unemployment\n      -0.5637  1.0000    -0.0025       -0.0790\ns.e.   0.1390  0.0573     0.0949        0.0336\n\nsigma^2 = 0.0412:  log likelihood = 9.68\nAIC=-9.36   AICc=-8.03   BIC=0.3\n\nTraining set error measures:\n                     ME      RMSE      MAE      MPE     MAPE      MASE\nTraining set 0.02911543 0.1929808 0.114944 11.94347 27.98997 0.3826642\n                    ACF1\nTraining set -0.06750725\n\n\n\n\n\n\nCode\nsummary(final.fit <- garchFit(~garch(1,1), res,trace = F))\n\n\n\nTitle:\n GARCH Modelling \n\nCall:\n garchFit(formula = ~garch(1, 1), data = res, trace = F) \n\nMean and Variance Equation:\n data ~ garch(1, 1)\n<environment: 0x15548d140>\n [data = res]\n\nConditional Distribution:\n norm \n\nCoefficient(s):\n         mu        omega       alpha1        beta1  \n-0.00530411   0.00068953   0.48627960   0.65780719  \n\nStd. Errors:\n based on Hessian \n\nError Analysis:\n         Estimate  Std. Error  t value Pr(>|t|)    \nmu     -0.0053041   0.0155220   -0.342   0.7326    \nomega   0.0006895   0.0011861    0.581   0.5610    \nalpha1  0.4862796   0.2392140    2.033   0.0421 *  \nbeta1   0.6578072   0.1208067    5.445 5.18e-08 ***\n---\nSignif. codes:  0 '***' 0.001 '**' 0.01 '*' 0.05 '.' 0.1 ' ' 1\n\nLog Likelihood:\n 9.169268    normalized:  0.1763321 \n\nDescription:\n Tue Jan  9 20:58:24 2024 by user:  \n\n\nStandardised Residuals Tests:\n                                Statistic p-Value   \n Jarque-Bera Test   R    Chi^2  1.08557   0.5811275 \n Shapiro-Wilk Test  R    W      0.9616717 0.09248575\n Ljung-Box Test     R    Q(10)  5.843556  0.8282312 \n Ljung-Box Test     R    Q(15)  8.260042  0.9129345 \n Ljung-Box Test     R    Q(20)  15.71232  0.7343031 \n Ljung-Box Test     R^2  Q(10)  9.933015  0.4463896 \n Ljung-Box Test     R^2  Q(15)  11.22686  0.7363534 \n Ljung-Box Test     R^2  Q(20)  13.8952   0.8357696 \n LM Arch Test       R    TR^2   13.33945  0.3448583 \n\nInformation Criterion Statistics:\n        AIC         BIC         SIC        HQIC \n-0.19881801 -0.04872234 -0.20956333 -0.14127488 \n\n\n\n\n\n\nCode\nht <- final.fit@h.t #a numeric vector with the conditional variances (h.t = sigma.t^delta)\n\n#############################\ndata=data.frame(final)\ndata$Date<-as.Date(data$Date,\"%Y-%m-%d\")\n\n\ndata2= data.frame(ht,data$Date)\nggplot(data2, aes(y = ht, x = data.Date)) + geom_line(col = '#7FB3D5') + ylab('Conditional Variance') + xlab('Date')\n\n\n\n\n\n\n\n\nFrom the ARIMA(1,1,1), we see that the training set error measures also suggest a good fit, with low mean absolute error, root mean squared error, and autocorrelation of the residuals. GATCH(1,1) model model is used to estimate the volatility of the standardized residuals of the previous regression model. The model includes a mean equation that estimates the mean of the residuals and a variance equation that models the conditional variance of the residuals. The coefficients of the mean equation suggest that the mean of the residuals is close to zero. The variance equation coefficients suggest that the conditional variance of the residuals is dependent on the past conditional variances and the past squared standardized residuals. The model’s the AIC, BIC, SIC, and HQIC values are all relatively low, indicating a good fit of the model. The standardized residuals tests indicate that the residuals are approximately normally distributed and that there is no significant autocorrelation in the residuals.\nThe volatility of the model seems high in 2020 but has decreased gradually in the past few months. This could indicate that the asset’s price was experiencing a lot of fluctuations in 2020, but the market has stabilized recently.\n\n\nModel Diagnostics\n\nResidualsQQ PlotBox Test\n\n\n\n\nCode\nfit2<-garch(res,order=c(1,1),trace=F)\ncheckresiduals(fit2) \n\n\n\n\n\n\n\n\n\nCode\nqqnorm(fit2$residuals, pch = 1)\nqqline(fit2$residuals, col = \"blue\", lwd = 2)\n\n\n\n\n\n\n\n\n\nCode\nBox.test (fit2$residuals, type = \"Ljung\")\n\n\n\n    Box-Ljung test\n\ndata:  fit2$residuals\nX-squared = 2.0187, df = 1, p-value = 0.1554\n\n\nThe ACF plot of the residuals shows all the values between the blue lines, which indicates that the residuals are not significantly autocorrelated. The range of values for the residual plot between -2 and 2 is considered acceptable. Additionally, the QQ plot of the residuals shows a linear plot on the line, which is another good indication that the residuals are normally distributed. The QQ plot is a valuable tool to assess if the residuals follow a normal distribution, and in this case, the plot suggests that the residuals do indeed follow a normal distribution.\nThe Box-Ljung test, a p-value of 0.1554 indicates that the model’s residuals are not significantly autocorrelated, meaning that the model has captured most of the information in the data. This result is good because it suggests that the model is a good fit for the data and has accounted for most of the underlying patterns in the data. Therefore, we can rely on the model’s predictions and use them to make informed decisions.\n\n\n\n\n\nForecast\n\n\nCode\npredict(final.fit, n.ahead = 5, plot=TRUE)\n\n\n\n\n\n  meanForecast meanError standardDeviation lowerInterval upperInterval\n1 -0.005304111 0.5028875         0.5028875    -0.9909454     0.9803372\n2 -0.005304111 0.5385390         0.5385390    -1.0608212     1.0502130\n3 -0.005304111 0.5766303         0.5766303    -1.1354787     1.1248705\n4 -0.005304111 0.6173340         0.6173340    -1.2152565     1.2046483\n5 -0.005304111 0.6608347         0.6608347    -1.3005162     1.2899080\n\n\nThe forecasted plot is based on the best model ARIMAX(1,1,1)+GARCH(1,1). This model takes into account the autoregressive and moving average components of the data, as well as the impact off exogenous variables on the time series. Additionally, the GARCH component of the model accounts for the volatility clustering in the data. Overall, this model is well-suited to make accurate predictions about future values of the time series.\n\n\nEquation of the Model\nThe equation of the ARIMAX(1,1,1) model is:\n\\(Y(t) = c + \\phi_1(Y{(t-1)} - X{(t-1)}) + \\theta_1\\epsilon{(t-1)} + \\epsilon(t)\\)\nwhere, \\(Y(t)\\) is the time series variable, \\(X(t-1)\\) is the exogenous variable, \\(c\\) is a constant, \\(\\phi_1\\) and \\(\\theta_1\\) are the parameters, and \\(\\epsilon(t)\\) is the error term.\nThe equation of the GARCH(1,1) model is:\n\\(\\sigma^2(t) = \\alpha_0+\\alpha_1\\epsilon_t^2(t-1)+ \\beta_1\\sigma^2(t-1)\\)\nwhere \\(\\sigma^2_t\\) is the conditional variance at time \\(t\\), \\(\\alpha_0\\) is a constant, \\(\\alpha_1\\) and \\(\\beta_1\\) are the parameters, and \\(\\epsilon_t\\) is the error term.\nThe combined equation of the ARIMAX(1,1,1)+GARCH(1,1) model is:\n\\(Y(t) = c + \\phi_1(Y(t-1) - X(t-1)) + \\theta_1\\epsilon(t-1) + \\epsilon(t)\\)\n\\(\\epsilon(t) = \\sigma(t) * \\epsilon~(t)\\)\n\\(\\sigma^2(t) = \\alpha_0+\\alpha_1\\epsilon_t^2(t-1)+ \\beta_1\\sigma^2(t-1)\\)"
  },
  {
    "objectID": "arch_Consumer_Staples_Sector_Fund.html",
    "href": "arch_Consumer_Staples_Sector_Fund.html",
    "title": "",
    "section": "",
    "text": "Impact of Macroeconomic Factors on P&G Stock Price\n\n\n\nConsumer staple funds are mutual funds or exchange-traded funds (ETFs) that invest in companies that produce essential goods and services, such as food, beverages, household products, and personal care items. These funds are designed to provide investors with stable and consistent returns, even in uncertain market conditions.\nOne of the top companies in the consumer staple sector is Procter & Gamble Co. (PG), which produces a wide range of products including personal care, cleaning, and food and beverage items. To analyze the stock price behavior of Procter & Gamble, an ARIMAX+ARCH/GARCH model can be employed.\nBy using an ARIMAX+ARCH/GARCH model to analyze the stock price behavior of Procter & Gamble, we can gain insights into how macroeconomic factors impact the company’s performance. For example, an increase in GDP growth rate or a decrease in unemployment rate may lead to increased consumer spending and a rise in Procter & Gamble’s stock price. Conversely, an increase in inflation or interest rates may lead to a decrease in consumer spending and a decline in Procter & Gamble’s stock price.\n\n\nTime series Plot\n\nP&GDifferentitaed Chart SeriesGDP GrowthInflationInterestUnemployment\n\n\n\n\nCode\n# get data\noptions(\"getSymbols.warning4.0\"=FALSE)\noptions(\"getSymbols.yahoo.warning\"=FALSE)\n\n\ndata.info = getSymbols(\"PG\",src='yahoo', from = '2010-01-01',to = \"2023-03-01\",auto.assign = FALSE)\ndata = getSymbols(\"PG\",src='yahoo', from = '2010-01-01',to = \"2023-03-01\")\ndf <- data.frame(Date=index(PG),coredata(PG))\n\n# create Bollinger Bands\nbbands <- BBands(PG[,c(\"PG.High\",\"PG.Low\",\"PG.Close\")])\n\n# join and subset data\ndf <- subset(cbind(df, data.frame(bbands[,1:3])), Date >= \"2010-01-01\")\n\n\n# colors column for increasing and decreasing\nfor (i in 1:length(df[,1])) {\n  if (df$PG.Close[i] >= df$PG.Open[i]) {\n      df$direction[i] = 'Increasing'\n  } else {\n      df$direction[i] = 'Decreasing'\n  }\n}\n\ni <- list(line = list(color = '#43A098'))\nd <- list(line = list(color = '#7F7F7F'))\n\n# plot candlestick chart\n\nfig <- df %>% plot_ly(x = ~Date, type=\"candlestick\",\n          open = ~PG.Open, close = ~PG.Close,\n          high = ~PG.High, low = ~PG.Low, name = \"PG\",\n          increasing = i, decreasing = d) \nfig <- fig %>% add_lines(x = ~Date, y = ~up , name = \"B Bands\",\n            line = list(color = '#ccc', width = 0.5),\n            legendgroup = \"Bollinger Bands\",\n            hoverinfo = \"none\", inherit = F) \nfig <- fig %>% add_lines(x = ~Date, y = ~dn, name = \"B Bands\",\n            line = list(color = '#ccc', width = 0.5),\n            legendgroup = \"Bollinger Bands\", inherit = F,\n            showlegend = FALSE, hoverinfo = \"none\") \nfig <- fig %>% add_lines(x = ~Date, y = ~mavg, name = \"Mv Avg\",\n            line = list(color = '#C052B3', width = 0.5),\n            hoverinfo = \"none\", inherit = F) \nfig <- fig %>% layout(yaxis = list(title = \"Price\"))\n\n# plot volume bar chart\nfig2 <- df \nfig2 <- fig2 %>% plot_ly(x=~Date, y=~PG.Volume, type='bar', name = \"PG Volume\",\n          color = ~direction, colors = c('#43A098','#7F7F7F')) \nfig2 <- fig2 %>% layout(yaxis = list(title = \"Volume\"))\n\n# create rangeselector buttons\nrs <- list(visible = TRUE, x = 0.5, y = -0.055,\n           xanchor = 'center', yref = 'paper',\n           font = list(size = 9),\n           buttons = list(\n             list(count=1,\n                  label='RESET',\n                  step='all'),\n             list(count=3,\n                  label='3 YR',\n                  step='year',\n                  stepmode='backward'),\n             list(count=1,\n                  label='1 YR',\n                  step='year',\n                  stepmode='backward'),\n             list(count=1,\n                  label='1 MO',\n                  step='month',\n                  stepmode='backward')\n           ))\n\n# subplot with shared x axis\nfig <- subplot(fig, fig2, heights = c(0.7,0.2), nrows=2,\n             shareX = TRUE, titleY = TRUE)\nfig <- fig %>% layout(title = paste(\"P&G Stock Price: January 2010 - March 2023\"),\n         xaxis = list(rangeselector = rs),\n         legend = list(orientation = 'h', x = 0.5, y = 1,\n                       xanchor = 'center', yref = 'paper',\n                       font = list(size = 10),\n                       bgcolor = 'transparent'))\n\nfig\n\n\n\n\n\n\n\n\n\n\nCode\nlog(data.info$`PG.Adjusted`) %>% diff() %>% chartSeries(theme=chartTheme('white'),up.col='#43A098')\n\n\n\n\n\n\n\n\n\nCode\n#import the data\ngdp <- read.csv(\"DATA/RAW DATA/gdp-growth.csv\")\n\n#change date format\ngdp$Date <- as.Date(gdp$DATE , \"%m/%d/%Y\")\n\n#drop DATE column\ngdp <- subset(gdp, select = -c(1))\n\n#export the cleaned data\ngdp_clean <- gdp\nwrite.csv(gdp_clean, \"DATA/CLEANED DATA/gdp_clean_data.csv\", row.names=FALSE)\n\n#plot gdp growth rate \nfig <- plot_ly(gdp, x = ~Date, y = ~value, type = 'scatter', mode = 'lines',line = list(color = 'rgb(240, 128, 128)'))\nfig <- fig %>% layout(title = \"U.S GPD Growth Rate: 2010 - 2022\",xaxis = list(title = \"Time\"),yaxis = list(title =\"GDP Growth Rate\"))\nfig\n\n\n\n\n\n\n\n\n\n\nCode\n#import the data\ninflation_rate <- read.csv(\"DATA/RAW DATA/inflation-rate.csv\")\n\n#cleaning the data\n#remove unwanted columns\ninflation_rate_clean <- subset(inflation_rate, select = -c(1,HALF1,HALF2))\n\n#convert the data to time series data\ninflation_data_ts <- ts(as.vector(t(as.matrix(inflation_rate_clean))), start=c(2010,1), end=c(2023,2), frequency=12)\n\n#export the data\nwrite.csv(inflation_rate_clean, \"DATA/CLEANED DATA/inflation_rate_clean_data.csv\", row.names=FALSE)\n\n\n#plot inflation rate \nfig <- autoplot(inflation_data_ts, ylab = \"Inflation Rate\", color=\"#FFA07A\")+ggtitle(\"U.S Inflation Rate: January 2010 - February 2023\")+theme_bw()\nggplotly(fig)\n\n\n\n\n\n\n\n\n\n\nCode\n#import the data\ninterest_data <- read.csv(\"DATA/RAW DATA/interest-rate.csv\")\n\n#change date format\ninterest_data$Date <- as.Date(interest_data$Date , \"%m/%d/%Y\")\n\n#export the cleaned data\ninterest_clean_data <- interest_data\nwrite.csv(interest_clean_data, \"DATA/CLEANED DATA/interest_rate_clean_data.csv\", row.names=FALSE)\n\n#plot interest rate \nfig <- plot_ly(interest_data, x = ~Date, y = ~value, type = 'scatter', mode = 'lines',line = list(color='rgb(219, 112, 147)'))\nfig <- fig %>% layout(title = \"U.S Interest Rate: January 2010 - March 2023\",xaxis = list(title = \"Time\"),yaxis = list(title =\"Interest Rate\"))\nfig\n\n\n\n\n\n\n\n\n\n\nCode\n#import the data\nunemployment_rate <- read.csv(\"DATA/RAW DATA/unemployment-rate.csv\")\n\n#change date format\nunemployment_rate$Date <- as.Date(unemployment_rate$Date , \"%m/%d/%Y\")\n\n# export the data\nwrite.csv(unemployment_rate, \"DATA/CLEANED DATA/unemployment_rate_clean_data.csv\", row.names=FALSE)\n\n#plot unemployment rate \nfig <- plot_ly(unemployment_rate, x = ~Date, y = ~Value, type = 'scatter', mode = 'lines',line = list(color = 'rgb(189, 183, 107)'))\nfig <- fig %>% layout(title = \"U.S Unemployment Rate: January 2010 - March 2023\",xaxis = list(title = \"Time\"),yaxis = list(title =\"Unemployment Rate\"))\nfig\n\n\n\n\n\n\n\n\n\nThe stock price of P&G experienced a steady increase from 2010 to 2014, likely due to the company’s strong financial performance and consistent dividend payouts. However, from 2014 to 2016, P&G’s stock price experienced a decline, which could be attributed to a combination of factors such as slowing sales growth and increased competition in the consumer goods industry.\nFollowing this period of steadiness and decline, P&G’s stock price began to recover from 2016 to 2018, likely due to the company’s efforts to streamline its operations and focus on core brands. This trend continued into 2019, with P&G’s stock price reaching an all-time high in mid-2019.\nHowever, the outbreak of the COVID-19 pandemic in early 2020 caused a brief dip in P&G’s stock price, as investors were uncertain about the impact of the pandemic on the company’s operations and financial performance. Nevertheless, P&G’s strong position in the consumer goods industry and its ability to adapt to changing market conditions helped it to quickly rebound and continue its growth trend throughout 2020 and into early 2021.\nSince early 2021, P&G’s stock price has experienced some volatility, likely due to a combination of factors such as global economic uncertainty and fluctuations in consumer demand for the company’s products.\nAs discussed before, the macroeconomic factors of GDP growth, inflation, interest rates, and unemployment rate are closely interrelated and play a crucial role in the overall health and stability of an economy. From 2010 to 2023, the global economy experienced a mix of ups and downs, with periods of strong GDP growth followed by slowdowns and recessions.\nThe second plot shows the first difference of the logarithm of the adjusted P&G stock price. Taking the first difference removes any long-term trends and transforms the time series into a stationary process. From the plot, we can observe that the first difference of the logarithm of the P&G stock price appears to be stationary, as the mean and variance are roughly constant over time.\n\n\nEnodogenous and Exogenous Variables\n\nPlotCorrelation HeatmapCCF GDPCCF InterestCCF InflationCCF Unemployment\n\n\n\n\nCode\nnumeric_data <- c(\"PG.Adjusted\",\"gdp\", \"interest\", \"inflation\", \"unemployment\")\nnumeric_data <- final[, numeric_data]\nnormalized_data_numeric <- scale(numeric_data)\nnormalized_data <- ts(normalized_data_numeric, start = c(2010, 1), end = c(2021,10),frequency = 4)\nts_plot(normalized_data,\n        title = \"Normalized Time Series Data for PG Stock and Macroeconomic Variables\",\n        Ytitle = \"Normalized Values\",\n        Xtitle = \"Year\")\n\n\n\n\n\n\n\n\n\n\nCode\n# Get upper triangle of the correlation matrix\nget_upper_tri <- function(cormat){\n    cormat[lower.tri(cormat)]<- NA\n    return(cormat)\n}\ncormat <- round(cor(normalized_data_numeric),2)\n\nupper_tri <- get_upper_tri(cormat)\n\nmelted_cormat <- melt(upper_tri, na.rm = TRUE)\n# Create a ggheatmap\nggheatmap <- ggplot(melted_cormat, aes(Var2, Var1, fill = value))+\n geom_tile(color = \"white\")+\n scale_fill_gradient2(low = \"blue\", high = \"red\", mid = \"white\", \n   midpoint = 0, limit = c(-1,1), space = \"Lab\", \n    name=\"Pearson\\nCorrelation\") +\n  theme_minimal()+ # minimal theme\n theme(axis.text.x = element_text(angle = 45, vjust = 1, \n    size = 12, hjust = 1))+\n coord_fixed()\n\nggheatmap + \ngeom_text(aes(Var2, Var1, label = value), color = \"black\", size = 4) +\ntheme(\n  axis.title.x = element_blank(),\n  axis.title.y = element_blank(),\n  panel.grid.major = element_blank(),\n  panel.border = element_blank(),\n  panel.background = element_blank(),\n  axis.ticks = element_blank(),\n  legend.justification = c(1, 0),\n  legend.position = c(0.6, 0.7),\n  legend.direction = \"horizontal\")+\n  guides(fill = guide_colorbar(barwidth = 7, barheight = 1,\n                title.position = \"top\", title.hjust = 0.5))\n\n\n\n\n\n\n\n\n\nCode\npar(mfrow=c(1,1))\nccf_result <- ccf(normalized_data[, c(\"PG.Adjusted\")], normalized_data[, c(\"gdp\")], \n    lag.max = 300,\n    main = \"Cros-Correlation Plot for PG Stock Price and GDP Growth Rate \",\n    ylab = \"CCF\")\n\n\n\n\n\nCode\ncat(\"The sum of cross correlation function is\", sum(abs(ccf_result$acf)))\n\n\nThe sum of cross correlation function is 4.8926\n\n\n\n\n\n\nCode\npar(mfrow=c(1,1))\nccf_result <- ccf(normalized_data[, c(\"PG.Adjusted\")], normalized_data[, c(\"interest\")], \n    lag.max = 300,\n    main = \"Cros-Correlation Plot for PG Stock Price and Interest Rate\",\n    ylab = \"CCF\")\n\n\n\n\n\nCode\ncat(\"The sum of cross correlation function is\", sum(abs(ccf_result$acf)))\n\n\nThe sum of cross correlation function is 12.00986\n\n\n\n\n\n\nCode\npar(mfrow=c(1,1))\nccf_result <- ccf(normalized_data[, c(\"PG.Adjusted\")], normalized_data[, c(\"inflation\")], \n    lag.max = 300,\n    main = \"Cros-Correlation Plot for PG Stock Price and Inflation Rate\",\n    ylab = \"CCF\")\n\n\n\n\n\nCode\ncat(\"The sum of cross correlation function is\", sum(abs(ccf_result$acf)))\n\n\nThe sum of cross correlation function is 15.81772\n\n\n\n\n\n\nCode\npar(mfrow=c(1,1))\nccf_result <- ccf(normalized_data[, c(\"PG.Adjusted\")], normalized_data[, c(\"unemployment\")], \n    lag.max = 300,\n    main = \"Cros-Correlation Plot for PG Stock Priceand Unemployment Rate\",\n    ylab = \"CCF\")\n\n\n\n\n\nCode\ncat(\"The sum of cross correlation function is\", sum(abs(ccf_result$acf)))\n\n\nThe sum of cross correlation function is 19.34548\n\n\n\n\n\nThe Normalized Time Series Data for Stock Price and Macroeconomic Variables plot shows the same variables as the first plot but has been normalized to a common range of 0 to 1 using the scale() function in R, which standardizes the variables to have a mean of 0 and a standard deviation of 1. The heatmap analysis of the normalized data reveals that inflation and unemployment rate exhibit strong positive correlations with the stock price indices, indicating that these variables may significantly influence stock price movements. On the other hand, weaker correlations were observed between the stock price indices and GDP and interest rates, suggesting that these variables may have less impact on stock price fluctuations. The cross-correlation feature plots confirm these findings, indicating that inflation and unemployment rate are more suitable feature variables for the ARIMAX model when predicting P&G movements.\nFinal Exogenous variables: Macroeconomic indicators: Inflation rate and unemployment rate.\n\n\nEnodogenous and Exogenous Variables Plot\n\nPlotCheck the stationarity\n\n\n\n\nCode\nfinal_data <- final %>%dplyr::select( Date,PG.Adjusted, inflation,unemployment)\nnumeric_data <- c(\"PG.Adjusted\", \"inflation\",\"unemployment\")\nnumeric_data <- final_data[, numeric_data]\nnormalized_data_numeric <- scale(numeric_data)\nnormalized_numeric_df <- data.frame(normalized_data_numeric)\nnormalized_data_ts <- ts(normalized_data_numeric, start = c(2010, 1), frequency = 4)\n\nautoplot(normalized_data_ts, facets=TRUE) +\n  xlab(\"Year\") + ylab(\"\") +\n  ggtitle(\"P&G Stock Price, Inflation Rate and Unemployment Rate in USA 2010-2023\")\n\n\n\n\n\n\n\n\n\nCode\n# Convert your multivariate time series data to a matrix\nfinal_data_ts_multivariate <- as.matrix(normalized_data_ts)\n\n# Check for stationarity using Phillips-Perron test\nphillips_perron_test <- ur.pp(final_data_ts_multivariate)  \nsummary(phillips_perron_test)\n\n\n\n################################## \n# Phillips-Perron Unit Root Test # \n################################## \n\nTest regression with intercept \n\n\nCall:\nlm(formula = y ~ y.l1)\n\nResiduals:\n    Min      1Q  Median      3Q     Max \n-1.7824 -0.1562 -0.0531  0.0989  4.4953 \n\nCoefficients:\n            Estimate Std. Error t value Pr(>|t|)    \n(Intercept) 0.001564   0.040861   0.038     0.97    \ny.l1        0.859052   0.041275  20.813   <2e-16 ***\n---\nSignif. codes:  0 '***' 0.001 '**' 0.01 '*' 0.05 '.' 0.1 ' ' 1\n\nResidual standard error: 0.5087 on 153 degrees of freedom\nMultiple R-squared:  0.739, Adjusted R-squared:  0.7373 \nF-statistic: 433.2 on 1 and 153 DF,  p-value: < 2.2e-16\n\n\nValue of test-statistic, type: Z-alpha  is: -23.2156 \n\n         aux. Z statistics\nZ-tau-mu            0.0386\n\n\n\n\n\nThe results of the Phillips-Perron unit root test indicate strong evidence against the null hypothesis of a unit root, as the p-value for the coefficient of the lagged variable is less than the significance level of 0.05. This suggests that the variable y, which is being tested for stationarity, is likely stationary. Furthermore, the test statistic Z-tau-mu is 0.0386, which is smaller than the critical value of Z-alpha (-23.2156), providing further evidence of stationarity.\nTo determine whether the linear model requires an ARCH model, an ARCH test is conducted. The ACF and PACF plots are also used to identify suitable model values.\n\n\nModel Fitting\n\nPlotARCH TestACF PlotPACF Plot\n\n\n\n\nCode\nnormalized_numeric_df$PG.Adjusted<-ts(normalized_numeric_df$PG.Adjusted,star=decimal_date(as.Date(\"2010-01-01\",format = \"%Y-%m-%d\")),frequency = 4)\nnormalized_numeric_df$inflation<-ts(normalized_numeric_df$inflation,star=decimal_date(as.Date(\"2010-01-01\",format = \"%Y-%m-%d\")),frequency = 4)\nnormalized_numeric_df$unemployment<-ts(normalized_numeric_df$unemployment,star=decimal_date(as.Date(\"2010-01-01\",format = \"%Y-%m-%d\")),frequency = 4)\n\nfit <- lm(PG.Adjusted ~ inflation+unemployment, data=normalized_numeric_df)\nfit.res<-ts(residuals(fit),star=decimal_date(as.Date(\"2010-01-01\",format = \"%Y-%m-%d\")),frequency = 4)\n############## Then look at the residuals ############\nreturns <- fit.res  %>% diff()\nautoplot(returns)+ggtitle(\"Linear Model Returns\")\n\n\n\n\n\n\n\n\n\nCode\nbyd.archTest <- ArchTest(fit.res, lags = 1, demean = TRUE)\nbyd.archTest\n\n\n\n    ARCH LM-test; Null hypothesis: no ARCH effects\n\ndata:  fit.res\nChi-squared = 33.303, df = 1, p-value = 7.885e-09\n\n\n\n\n\n\nCode\nggAcf(returns) +ggtitle(\"ACF for returns\")\n\n\n\n\n\n\n\n\n\nCode\nggPacf(returns) +ggtitle(\"PACF for returns\")\n\n\n\n\n\n\n\n\nThe ARCH LM-test was conducted with the null hypothesis of no ARCH effects. The test resulted in a chi-squared value of 33.303 with one degree of freedom, and a very low p-value of 7.885e-09. This suggests strong evidence against the null hypothesis, indicating the presence of ARCH effects in the data.\nBased on the ACF and PACF plots, it appears that there is some significant autocorrelation and partial autocorrelation at multiple lags, which suggests that an ARIMA model may not be sufficient to capture the time series behavior. Additionally, the values for p and q appear to be relatively high, with p = 8 and q = 8 being suggested by the plots.\n\n\nARIMAX Model\n\nAuto Arima ModelAuto Arima ResidualsARIMAX ModelARIMA (1,1,1) PlotARIMA (1,1,1) ModelCross Validation\n\n\n\n\nCode\nxreg <- cbind(Inflation = normalized_data_ts[, \"inflation\"],\n              Unemployment = normalized_data_ts[, \"unemployment\"])\nfit.auto <- auto.arima(normalized_data_ts[, \"PG.Adjusted\"], xreg = xreg)\nsummary(fit.auto)\n\n\nSeries: normalized_data_ts[, \"PG.Adjusted\"] \nRegression with ARIMA(0,1,0) errors \n\nCoefficients:\n       drift  Inflation  Unemployment\n      0.0442     0.0139       -0.1199\ns.e.  0.0271     0.0857        0.0381\n\nsigma^2 = 0.03803:  log likelihood = 12.55\nAIC=-17.1   AICc=-16.23   BIC=-9.38\n\nTraining set error measures:\n                        ME      RMSE       MAE      MPE     MAPE      MASE\nTraining set -1.921084e-05 0.1873552 0.1337598 409.0784 430.9485 0.4244207\n                  ACF1\nTraining set 0.1245825\n\n\n\n\n\n\nCode\ncheckresiduals(fit.auto)\n\n\n\n\n\n\n    Ljung-Box test\n\ndata:  Residuals from Regression with ARIMA(0,1,0) errors\nQ* = 12.832, df = 8, p-value = 0.1177\n\nModel df: 0.   Total lags used: 8\n\n\n\n\n\n\nCode\nARIMA.c=function(p1,q1,q2,data){\ntemp=c()\nd=1\ni=1\ntemp= data.frame()\nls=matrix(rep(NA,6*30),nrow=30)\n\n\nfor (p in p1)#\n{\n  for(q in q1:q2)#\n  {\n    for(d in 0:1)\n    {\n      \n      if(p+d+q<=6)\n      {\n        \n        model<- Arima(data,order=c(p,d,q))\n        ls[i,]= c(p,d,q,model$aic,model$bic,model$aicc)\n        i=i+1\n        #print(i)\n        \n      }\n      \n    }\n  }\n}\n\n\ntemp= as.data.frame(ls)\nnames(temp)= c(\"p\",\"d\",\"q\",\"AIC\",\"BIC\",\"AICc\")\n\ntemp\n}\n\noutput <- ARIMA.c(1,1,8,data=residuals(fit))\n\noutput[which.min(output$AIC),] \n\n\n  p d q      AIC      BIC     AICc\n2 1 1 1 19.41045 25.20592 19.92108\n\n\nCode\noutput[which.min(output$BIC),]\n\n\n  p d q      AIC      BIC     AICc\n2 1 1 1 19.41045 25.20592 19.92108\n\n\nCode\noutput[which.min(output$AICc),]\n\n\n  p d q      AIC      BIC     AICc\n2 1 1 1 19.41045 25.20592 19.92108\n\n\n\n\n\n\nCode\nset.seed(1234)\n\nmodel_output <- capture.output(sarima(fit.res, 1,1,1)) \n\n\n\n\n\n\n\n\n\nCode\ncat(model_output[30:61], model_output[length(model_output)], sep = \"\\n\")\n\n\n$fit\n\nCall:\narima(x = xdata, order = c(p, d, q), seasonal = list(order = c(P, D, Q), period = S), \n    xreg = constant, transform.pars = trans, fixed = fixed, optim.control = list(trace = trc, \n        REPORT = 1, reltol = tol))\n\nCoefficients:\n         ar1      ma1  constant\n      0.5118  -0.2678   -0.0053\ns.e.  0.3621   0.3920    0.0573\n\nsigma^2 estimated as 0.07602:  log likelihood = -6.7,  aic = 21.4\n\n$degrees_of_freedom\n[1] 48\n\n$ttable\n         Estimate     SE t.value p.value\nar1        0.5118 0.3621  1.4137  0.1639\nma1       -0.2678 0.3920 -0.6833  0.4977\nconstant  -0.0053 0.0573 -0.0933  0.9261\n\n$AIC\n[1] 0.4196416\n\n$AICc\n[1] 0.4296541\n\n$BIC\n[1] 0.5711574\n\n\n\n\n\n\nCode\nn=length(fit.res)\nk= 51\n \n \nrmse1 <- matrix(NA, (n-k),4)\nrmse2 <- matrix(NA, (n-k),4)\nrmse3 <- matrix(NA, (n-k),4)\n\n\nst <- tsp(fit.res)[1]+(k-5)/4 \n\nfor(i in 1:(n-k))\n{\n  xtrain <- window(fit.res, end=st + i/4)\n  xtest <- window(fit.res, start=st + (i+1)/4, end=st + (i+4)/4)\n  \n  #ARIMA(0,1,0) ARIMA(1,1,1)\n  \n  fit <- Arima(xtrain, order=c(0,1,0),\n                include.drift=TRUE, method=\"ML\")\n  fcast <- forecast(fit, h=4)\n  \n  fit2 <- Arima(xtrain, order=c(1,1,1),\n                include.drift=TRUE, method=\"ML\")\n  fcast2 <- forecast(fit2, h=4)\n\n\n  rmse1[i,1:length(xtest)]   <- sqrt((fcast$mean-xtest)^2)\n  rmse2[i,1:length(xtest)] <- sqrt((fcast2$mean-xtest)^2)\n}\n\nplot(1:4,colMeans(rmse1,na.rm=TRUE), type=\"l\",col=2, xlab=\"horizon\", ylab=\"RMSE\")\nlines(1:4, colMeans(rmse2,na.rm=TRUE), type=\"l\",col=3)\nlegend(\"topleft\",legend=c(\"fit1\",\"fit2\"),col=2:4,lty=1)\n\n\n\n\n\n\n\n\nBased on the results of the auto.arima function, the suggested best model is ARIMA(0,1,0). However, when we manually test different ARIMA models, we find that ARIMA(1,1,1) has the lowest values for AIC, BIC, and AICC. Additionally, both models have similar standardized residual plots, with means close to 0, indicating a good fit. The ACF plot of residuals also shows no significant lags, further indicating a well-fitted model.\nTo determine the best model, we conduct cross-validation and compare the RMSE values of both models. The results show that ARIMA(1,1,1) has lower RMSE values than ARIMA(0,1,0), indicating that it is the better model.\nWe can then proceed to choose the best GARCH model using ARIMA(1,1,1) as the base model.\n\n\nSquared Residuals\n\nPlotACF PlotPACF Plot\n\n\n\n\nCode\nfit <- lm(PG.Adjusted ~ inflation+unemployment, data=normalized_numeric_df)\nfit.res<-ts(residuals(fit),star=decimal_date(as.Date(\"2010-01-01\",format = \"%Y-%m-%d\")),frequency = 4)\nfit <- Arima(fit.res,order=c(1,1,1))\nres=fit$res\nplot(res^2,main='Squared Residuals')\n\n\n\n\n\n\n\n\n\nCode\nacf(res^2,24, main = \"ACF Residuals Square\")\n\n\n\n\n\n\n\n\n\nCode\npacf(res^2,24, main = \"PACF Residuals Square\")\n\n\n\n\n\n\n\n\nFrom the squared residuals of the best ARIMA model, it can be observed that the ACF plot and PACF plot indicate that the residuals are not autocorrelated and are white noise, indicating a good fit of the model. Based on the squared residuals of the best ARIMA model, we can see that the ACF and PACF plots indicate that most of the values lie between the blue lines. Additionally, the p-value is 2 and q-value is 2. This suggests that the model has a good fit and that there is no significant autocorrelation or partial autocorrelation in the residuals. Now we can proceed by fitting GARCH Model for p and q values.\n\n\nGARCH Model\n\nModelGRACH(1,1)GRACH(2,1)GRACH(1,2)\n\n\n\n\nCode\nmodel <- list() ## set counter\ncc <- 1\nfor (p in 1:2) {\n  for (q in 1:2) {\n  \nmodel[[cc]] <- garch(res,order=c(q,p),trace=F)\ncc <- cc + 1\n}\n} \n\n## get AIC values for model evaluation\nGARCH_AIC <- sapply(model, AIC) ## model with lowest AIC is the best\nwhich(GARCH_AIC == min(GARCH_AIC))\n\n\n[1] 4\n\n\nCode\nmodel[[which(GARCH_AIC == min(GARCH_AIC))]]\n\n\n\nCall:\ngarch(x = res, order = c(q, p), trace = F)\n\nCoefficient(s):\n       a0         a1         a2         b1         b2  \n2.257e-02  3.047e-02  3.867e-01  1.788e-15  1.963e-01  \n\n\n\n\n\n\nCode\nsummary(garchFit(~garch(1,1),res, trace=F))\n\n\n\nTitle:\n GARCH Modelling \n\nCall:\n garchFit(formula = ~garch(1, 1), data = res, trace = F) \n\nMean and Variance Equation:\n data ~ garch(1, 1)\n<environment: 0x121395940>\n [data = res]\n\nConditional Distribution:\n norm \n\nCoefficient(s):\n         mu        omega       alpha1        beta1  \n-0.00046672   0.00001929   0.14787443   0.90106994  \n\nStd. Errors:\n based on Hessian \n\nError Analysis:\n         Estimate  Std. Error  t value Pr(>|t|)    \nmu     -4.667e-04   3.139e-02   -0.015    0.988    \nomega   1.929e-05   4.754e-03    0.004    0.997    \nalpha1  1.479e-01   9.919e-02    1.491    0.136    \nbeta1   9.011e-01   1.030e-01    8.746   <2e-16 ***\n---\nSignif. codes:  0 '***' 0.001 '**' 0.01 '*' 0.05 '.' 0.1 ' ' 1\n\nLog Likelihood:\n -3.885086    normalized:  -0.07471318 \n\nDescription:\n Tue Jan  9 20:59:52 2024 by user:  \n\n\nStandardised Residuals Tests:\n                                Statistic p-Value     \n Jarque-Bera Test   R    Chi^2  1.93484   0.3800624   \n Shapiro-Wilk Test  R    W      0.9777844 0.436524    \n Ljung-Box Test     R    Q(10)  9.637902  0.4728146   \n Ljung-Box Test     R    Q(15)  15.96454  0.3844124   \n Ljung-Box Test     R    Q(20)  27.64276  0.1181294   \n Ljung-Box Test     R^2  Q(10)  27.23887  0.002387006 \n Ljung-Box Test     R^2  Q(15)  45.57051  6.220836e-05\n Ljung-Box Test     R^2  Q(20)  51.54644  0.0001325434\n LM Arch Test       R    TR^2   28.74873  0.004292744 \n\nInformation Criterion Statistics:\n      AIC       BIC       SIC      HQIC \n0.3032725 0.4533682 0.2925272 0.3608157 \n\n\n\n\n\n\nCode\nsummary(garchFit(~garch(2,1),res, trace=F))\n\n\n\nTitle:\n GARCH Modelling \n\nCall:\n garchFit(formula = ~garch(2, 1), data = res, trace = F) \n\nMean and Variance Equation:\n data ~ garch(2, 1)\n<environment: 0x121a3a008>\n [data = res]\n\nConditional Distribution:\n norm \n\nCoefficient(s):\n         mu        omega       alpha1       alpha2        beta1  \n-0.01258591   0.00315889   0.00000001   0.23548224   0.76820639  \n\nStd. Errors:\n based on Hessian \n\nError Analysis:\n         Estimate  Std. Error  t value Pr(>|t|)    \nmu     -1.259e-02   3.345e-02   -0.376  0.70674    \nomega   3.159e-03   6.361e-03    0.497  0.61945    \nalpha1  1.000e-08   1.290e-01    0.000  1.00000    \nalpha2  2.355e-01   1.984e-01    1.187  0.23522    \nbeta1   7.682e-01   2.043e-01    3.760  0.00017 ***\n---\nSignif. codes:  0 '***' 0.001 '**' 0.01 '*' 0.05 '.' 0.1 ' ' 1\n\nLog Likelihood:\n -2.5769    normalized:  -0.04955577 \n\nDescription:\n Tue Jan  9 20:59:52 2024 by user:  \n\n\nStandardised Residuals Tests:\n                                Statistic p-Value     \n Jarque-Bera Test   R    Chi^2  0.889059  0.6411259   \n Shapiro-Wilk Test  R    W      0.9811742 0.57696     \n Ljung-Box Test     R    Q(10)  9.405694  0.4940824   \n Ljung-Box Test     R    Q(15)  15.23843  0.4343829   \n Ljung-Box Test     R    Q(20)  26.4125   0.1526163   \n Ljung-Box Test     R^2  Q(10)  23.65166  0.008580658 \n Ljung-Box Test     R^2  Q(15)  42.63577  0.0001793075\n Ljung-Box Test     R^2  Q(20)  50.08643  0.0002152542\n LM Arch Test       R    TR^2   26.23072  0.009955126 \n\nInformation Criterion Statistics:\n      AIC       BIC       SIC      HQIC \n0.2914192 0.4790388 0.2750022 0.3633482 \n\n\n\n\n\n\nCode\nsummary(garchFit(~garch(1,2),res, trace=F))\n\n\n\nTitle:\n GARCH Modelling \n\nCall:\n garchFit(formula = ~garch(1, 2), data = res, trace = F) \n\nMean and Variance Equation:\n data ~ garch(1, 2)\n<environment: 0x1121e7ed0>\n [data = res]\n\nConditional Distribution:\n norm \n\nCoefficient(s):\n         mu        omega       alpha1        beta1        beta2  \n-0.00103294   0.00049633   0.14694869   0.89300883   0.00000001  \n\nStd. Errors:\n based on Hessian \n\nError Analysis:\n         Estimate  Std. Error  t value Pr(>|t|)\nmu     -1.033e-03   3.265e-02   -0.032    0.975\nomega   4.963e-04   5.285e-03    0.094    0.925\nalpha1  1.469e-01   1.622e-01    0.906    0.365\nbeta1   8.930e-01   1.440e+00    0.620    0.535\nbeta2   1.000e-08   1.394e+00    0.000    1.000\n\nLog Likelihood:\n -4.088505    normalized:  -0.0786251 \n\nDescription:\n Tue Jan  9 20:59:52 2024 by user:  \n\n\nStandardised Residuals Tests:\n                                Statistic p-Value     \n Jarque-Bera Test   R    Chi^2  2.312217  0.3147084   \n Shapiro-Wilk Test  R    W      0.9763827 0.385689    \n Ljung-Box Test     R    Q(10)  9.746995  0.4629638   \n Ljung-Box Test     R    Q(15)  16.04255  0.3792292   \n Ljung-Box Test     R    Q(20)  27.41625  0.1239541   \n Ljung-Box Test     R^2  Q(10)  27.39288  0.002256282 \n Ljung-Box Test     R^2  Q(15)  44.98822  7.6901e-05  \n Ljung-Box Test     R^2  Q(20)  50.7473   0.0001729734\n LM Arch Test       R    TR^2   29.18193  0.0037021   \n\nInformation Criterion Statistics:\n      AIC       BIC       SIC      HQIC \n0.3495579 0.5371775 0.3331409 0.4214868 \n\n\n\n\n\nBased on the analysis of the different GARCH models, it appears that GARCH(1,1) is the optimal choice. Although the AIC values of the different models are relatively similar, we can further evaluate their significance to make a final determination. Upon closer inspection, it appears that GARCH(1,1) has significantly better values than the other models, indicating that it is the most appropriate choice. Therefore, we can conclude that the GARCH(1,1) model is the best fit for the data.\n\n\nBest Model\n\nARIMA ModelGARCH ModelVolatility\n\n\n\n\nCode\n#fiting an ARIMA model to the Inflation variable\ninflation_fit<-auto.arima(normalized_numeric_df$inflation) \nfinflation<-forecast(inflation_fit)\n\n#fitting an ARIMA model to the Unemployment variable\nunemployment_fit<-auto.arima(normalized_numeric_df$unemployment) \nfunemployment<-forecast(unemployment_fit)\n\n# best model fit for forcasting\nxreg <- cbind(Inflation = normalized_data_ts[, \"inflation\"],\n              Unemployment = normalized_data_ts[, \"unemployment\"])\n\n\n\nsummary(arima.fit<-Arima(normalized_data_ts[, \"PG.Adjusted\"],order=c(1,1,1),xreg=xreg),include.drift = TRUE)\n\n\nSeries: normalized_data_ts[, \"PG.Adjusted\"] \nRegression with ARIMA(1,1,1) errors \n\nCoefficients:\n         ar1      ma1  Inflation  Unemployment\n      0.3380  -0.1329     0.0428       -0.1152\ns.e.  0.4362   0.4426     0.0897        0.0383\n\nsigma^2 = 0.03922:  log likelihood = 12.27\nAIC=-14.55   AICc=-13.21   BIC=-4.89\n\nTraining set error measures:\n                     ME      RMSE       MAE      MPE   MAPE      MASE\nTraining set 0.02921537 0.1882886 0.1408886 283.0815 316.01 0.4470403\n                    ACF1\nTraining set -0.03829399\n\n\n\n\n\n\nCode\nsummary(final.fit <- garchFit(~garch(1,1), res,trace = F))\n\n\n\nTitle:\n GARCH Modelling \n\nCall:\n garchFit(formula = ~garch(1, 1), data = res, trace = F) \n\nMean and Variance Equation:\n data ~ garch(1, 1)\n<environment: 0x1110af3f0>\n [data = res]\n\nConditional Distribution:\n norm \n\nCoefficient(s):\n         mu        omega       alpha1        beta1  \n-0.00046672   0.00001929   0.14787443   0.90106994  \n\nStd. Errors:\n based on Hessian \n\nError Analysis:\n         Estimate  Std. Error  t value Pr(>|t|)    \nmu     -4.667e-04   3.139e-02   -0.015    0.988    \nomega   1.929e-05   4.754e-03    0.004    0.997    \nalpha1  1.479e-01   9.919e-02    1.491    0.136    \nbeta1   9.011e-01   1.030e-01    8.746   <2e-16 ***\n---\nSignif. codes:  0 '***' 0.001 '**' 0.01 '*' 0.05 '.' 0.1 ' ' 1\n\nLog Likelihood:\n -3.885086    normalized:  -0.07471318 \n\nDescription:\n Tue Jan  9 20:59:53 2024 by user:  \n\n\nStandardised Residuals Tests:\n                                Statistic p-Value     \n Jarque-Bera Test   R    Chi^2  1.93484   0.3800624   \n Shapiro-Wilk Test  R    W      0.9777844 0.436524    \n Ljung-Box Test     R    Q(10)  9.637902  0.4728146   \n Ljung-Box Test     R    Q(15)  15.96454  0.3844124   \n Ljung-Box Test     R    Q(20)  27.64276  0.1181294   \n Ljung-Box Test     R^2  Q(10)  27.23887  0.002387006 \n Ljung-Box Test     R^2  Q(15)  45.57051  6.220836e-05\n Ljung-Box Test     R^2  Q(20)  51.54644  0.0001325434\n LM Arch Test       R    TR^2   28.74873  0.004292744 \n\nInformation Criterion Statistics:\n      AIC       BIC       SIC      HQIC \n0.3032725 0.4533682 0.2925272 0.3608157 \n\n\n\n\n\n\nCode\nht <- final.fit@h.t #a numeric vector with the conditional variances (h.t = sigma.t^delta)\n\n#############################\ndata=data.frame(final)\ndata$Date<-as.Date(data$Date,\"%Y-%m-%d\")\n\n\ndata2= data.frame(ht,data$Date)\nggplot(data2, aes(y = ht, x = data.Date)) + geom_line(col = '#43A098') + ylab('Conditional Variance') + xlab('Date')\n\n\n\n\n\n\n\n\nFrom the ARIMA(1,1,1), we see that the training set error measures also suggest a good fit, with low mean absolute error, root mean squared error, and autocorrelation of the residuals. GATCH(1,1) model model is used to estimate the volatility of the standardized residuals of the previous regression model. The model includes a mean equation that estimates the mean of the residuals and a variance equation that models the conditional variance of the residuals. The coefficients of the mean equation suggest that the mean of the residuals is close to zero. The variance equation coefficients suggest that the conditional variance of the residuals is dependent on the past conditional variances and the past squared standardized residuals. The model’s log-likelihood value is -3.885, and the AIC, BIC, SIC, and HQIC values are all relatively low, indicating a good fit of the model. The standardized residuals tests indicate that the residuals are approximately normally distributed and that there is no significant autocorrelation in the residuals.\nThe volatility of the model seems high in 2020 but has decreased gradually in the past few months. This could indicate that the asset’s price was experiencing a lot of fluctuations in 2020, but the market has stabilized recently.\n\n\nModel Diagnostics\n\nResidualsQQ PlotBox Test\n\n\n\n\nCode\nfit2<-garch(res,order=c(1,1),trace=F)\ncheckresiduals(fit2) \n\n\n\n\n\n\n\n\n\nCode\nqqnorm(fit2$residuals, pch = 1)\nqqline(fit2$residuals, col = \"blue\", lwd = 2)\n\n\n\n\n\n\n\n\n\nCode\nBox.test (fit2$residuals, type = \"Ljung\")\n\n\n\n    Box-Ljung test\n\ndata:  fit2$residuals\nX-squared = 0.020613, df = 1, p-value = 0.8858\n\n\nThe ACF plot of the residuals shows all the values between the blue lines, which indicates that the residuals are not significantly autocorrelated. The range of values for the residual plot between -2 and 2 is considered acceptable. Additionally, the QQ plot of the residuals shows a linear plot on the line, which is another good indication that the residuals are normally distributed. The QQ plot is a valuable tool to assess if the residuals follow a normal distribution, and in this case, the plot suggests that the residuals do indeed follow a normal distribution.\nThe Box-Ljung test, a p-value of 0.9008 indicates that the model’s residuals are not significantly autocorrelated, meaning that the model has captured most of the information in the data. This result is good because it suggests that the model is a good fit for the data and has accounted for most of the underlying patterns in the data. Therefore, we can rely on the model’s predictions and use them to make informed decisions.\n\n\n\n\n\nForecast\n\n\nCode\npredict(final.fit, n.ahead = 5, plot=TRUE)\n\n\n\n\n\n   meanForecast meanError standardDeviation lowerInterval upperInterval\n1 -0.0004667176 0.4689872         0.4689872    -0.9196648     0.9187314\n2 -0.0004667176 0.4803474         0.4803474    -0.9419302     0.9409968\n3 -0.0004667176 0.4919817         0.4919817    -0.9647331     0.9637997\n4 -0.0004667176 0.5038969         0.5038969    -0.9880864     0.9871530\n5 -0.0004667176 0.5160997         0.5160997    -1.0120036     1.0110701\n\n\nThe forecasted plot is based on the best model ARIMAX(1,1,1)+GARCH(1,1). This model takes into account the autoregressive and moving average components of the data, as well as the impact of exogenous variables on the time series. Additionally, the GARCH component of the model accounts for the volatility clustering in the data. Overall, this model is well-suited to make accurate predictions about future values of the time series.\n\n\nEquation of the Model\nThe equation of the ARIMAX(1,1,1) model is:\n\\(Y(t) = c + \\phi_1(Y{(t-1)} - X{(t-1)}) + \\theta_1\\epsilon{(t-1)} + \\epsilon(t)\\)\nwhere, \\(Y(t)\\) is the time series variable, \\(X(t-1)\\) is the exogenous variable, \\(c\\) is a constant, \\(\\phi_1\\) and \\(\\theta_1\\) are the parameters, and \\(\\epsilon(t)\\) is the error term.\nThe equation of the GARCH(1,1) model is:\n\\(\\sigma^2(t) = \\alpha_0+\\alpha_1\\epsilon_t^2(t-1)+ \\beta_1\\sigma^2(t-1)\\)\nwhere \\(\\sigma^2_t\\) is the conditional variance at time \\(t\\), \\(\\alpha_0\\) is a constant, \\(\\alpha_1\\) and \\(\\beta_1\\) are the parameters, and \\(\\epsilon_t\\) is the error term.\nThe combined equation of the ARIMAX(1,1,1)+GARCH(1,1) model is:\n\\(Y(t) = c + \\phi_1(Y(t-1) - X(t-1)) + \\theta_1\\epsilon(t-1) + \\epsilon(t)\\)\n\\(\\epsilon(t) = \\sigma(t) * \\epsilon~(t)\\)\n\\(\\sigma^2(t) = \\alpha_0+\\alpha_1\\epsilon_t^2(t-1)+ \\beta_1\\sigma^2(t-1)\\)"
  },
  {
    "objectID": "arch_Communication_Services_Sector_Fund.html",
    "href": "arch_Communication_Services_Sector_Fund.html",
    "title": "",
    "section": "",
    "text": "Impact of Macroeconomic Factors on AT&T Stock Price\n\n\n\nAT&T Inc. is a multinational telecommunications conglomerate that provides a range of services including wireless communication, internet, television, and digital entertainment. It is one of the largest communication service providers in the United States and has a significant presence in other countries as well. In recent years, AT&T Inc. has been focusing on expanding its 5G network and acquiring media companies to strengthen its digital entertainment offerings.\nOne of the top companies in the consumer staple sector is Procter & Gamble Co. (T), which produces a wide range of products including personal care, cleaning, and food and beverage items. To analyze the stock price behavior of Procter & Gamble, an ARIMAX+ARCH/GARCH model can be employed.\nBy using an ARIMAX+ARCH/GARCH model to analyze the stock price behavior of AT&T, we can gain insights into how macroeconomic factors impact the company’s performance. For example, an increase in GDP growth rate or a decrease in unemployment rate may lead to increased consumer spending and a rise in AT&T’s stock price. Conversely, an increase in inflation or interest rates may lead to a decrease in consumer spending and a decline in AT&T’s stock price.\n\n\nTime series Plot\n\nAT&TDifferentitaed Chart SeriesGDP GrowthInflationInterestUnemployment\n\n\n\n\nCode\n# get data\noptions(\"getSymbols.warning4.0\"=FALSE)\noptions(\"getSymbols.yahoo.warning\"=FALSE)\n\n\ndata.info = getSymbols(\"T\",src='yahoo', from = '2010-01-01',to = \"2023-03-01\",auto.assign = FALSE)\ndata = getSymbols(\"T\",src='yahoo', from = '2010-01-01',to = \"2023-03-01\")\ndf <- data.frame(Date=index(T),coredata(T))\n\n# create Bollinger Bands\nbbands <- BBands(T[,c(\"T.High\",\"T.Low\",\"T.Close\")])\n\n# join and subset data\ndf <- subset(cbind(df, data.frame(bbands[,1:3])), Date >= \"2010-01-01\")\n\n\n# colors column for increasing and decreasing\nfor (i in 1:length(df[,1])) {\n  if (df$T.Close[i] >= df$T.Open[i]) {\n      df$direction[i] = 'Increasing'\n  } else {\n      df$direction[i] = 'Decreasing'\n  }\n}\n\ni <- list(line = list(color = '#73C6B6'))\nd <- list(line = list(color = '#7F7F7F'))\n\n# plot candlestick chart\n\nfig <- df %>% plot_ly(x = ~Date, type=\"candlestick\",\n          open = ~T.Open, close = ~T.Close,\n          high = ~T.High, low = ~T.Low, name = \"T\",\n          increasing = i, decreasing = d) \nfig <- fig %>% add_lines(x = ~Date, y = ~up , name = \"B Bands\",\n            line = list(color = '#ccc', width = 0.5),\n            legendgroup = \"Bollinger Bands\",\n            hoverinfo = \"none\", inherit = F) \nfig <- fig %>% add_lines(x = ~Date, y = ~dn, name = \"B Bands\",\n            line = list(color = '#ccc', width = 0.5),\n            legendgroup = \"Bollinger Bands\", inherit = F,\n            showlegend = FALSE, hoverinfo = \"none\") \nfig <- fig %>% add_lines(x = ~Date, y = ~mavg, name = \"Mv Avg\",\n            line = list(color = '#C052B3', width = 0.5),\n            hoverinfo = \"none\", inherit = F) \nfig <- fig %>% layout(yaxis = list(title = \"Price\"))\n\n# plot volume bar chart\nfig2 <- df \nfig2 <- fig2 %>% plot_ly(x=~Date, y=~T.Volume, type='bar', name = \"T Volume\",\n          color = ~direction, colors = c('#73C6B6','#7F7F7F')) \nfig2 <- fig2 %>% layout(yaxis = list(title = \"Volume\"))\n\n# create rangeselector buttons\nrs <- list(visible = TRUE, x = 0.5, y = -0.055,\n           xanchor = 'center', yref = 'paper',\n           font = list(size = 9),\n           buttons = list(\n             list(count=1,\n                  label='RESET',\n                  step='all'),\n             list(count=3,\n                  label='3 YR',\n                  step='year',\n                  stepmode='backward'),\n             list(count=1,\n                  label='1 YR',\n                  step='year',\n                  stepmode='backward'),\n             list(count=1,\n                  label='1 MO',\n                  step='month',\n                  stepmode='backward')\n           ))\n\n# subplot with shared x axis\nfig <- subplot(fig, fig2, heights = c(0.7,0.2), nrows=2,\n             shareX = TRUE, titleY = TRUE)\nfig <- fig %>% layout(title = paste(\"AT&T Stock Price: January 2010 - March 2023\"),\n         xaxis = list(rangeselector = rs),\n         legend = list(orientation = 'h', x = 0.5, y = 1,\n                       xanchor = 'center', yref = 'paper',\n                       font = list(size = 10),\n                       bgcolor = 'transparent'))\n\nfig\n\n\n\n\n\n\n\n\n\n\nCode\nlog(data.info$`T.Adjusted`) %>% diff() %>% chartSeries(theme=chartTheme('white'),up.col='#73C6B6')\n\n\n\n\n\n\n\n\n\nCode\n#import the data\ngdp <- read.csv(\"DATA/RAW DATA/gdp-growth.csv\")\n\n#change date format\ngdp$Date <- as.Date(gdp$DATE , \"%m/%d/%Y\")\n\n#drop DATE column\ngdp <- subset(gdp, select = -c(1))\n\n#export the cleaned data\ngdp_clean <- gdp\nwrite.csv(gdp_clean, \"DATA/CLEANED DATA/gdp_clean_data.csv\", row.names=FALSE)\n\n#plot gdp growth rate \nfig <- plot_ly(gdp, x = ~Date, y = ~value, type = 'scatter', mode = 'lines',line = list(color = 'rgb(240, 128, 128)'))\nfig <- fig %>% layout(title = \"U.S GPD Growth Rate: 2010 - 2022\",xaxis = list(title = \"Time\"),yaxis = list(title =\"GDP Growth Rate\"))\nfig\n\n\n\n\n\n\n\n\n\n\nCode\n#import the data\ninflation_rate <- read.csv(\"DATA/RAW DATA/inflation-rate.csv\")\n\n#cleaning the data\n#remove unwanted columns\ninflation_rate_clean <- subset(inflation_rate, select = -c(1,HALF1,HALF2))\n\n#convert the data to time series data\ninflation_data_ts <- ts(as.vector(t(as.matrix(inflation_rate_clean))), start=c(2010,1), end=c(2023,2), frequency=12)\n\n#export the data\nwrite.csv(inflation_rate_clean, \"DATA/CLEANED DATA/inflation_rate_clean_data.csv\", row.names=FALSE)\n\n\n#plot inflation rate \nfig <- autoplot(inflation_data_ts, ylab = \"Inflation Rate\", color=\"#FFA07A\")+ggtitle(\"U.S Inflation Rate: January 2010 - February 2023\")+theme_bw()\nggplotly(fig)\n\n\n\n\n\n\n\n\n\n\nCode\n#import the data\ninterest_data <- read.csv(\"DATA/RAW DATA/interest-rate.csv\")\n\n#change date format\ninterest_data$Date <- as.Date(interest_data$Date , \"%m/%d/%Y\")\n\n#export the cleaned data\ninterest_clean_data <- interest_data\nwrite.csv(interest_clean_data, \"DATA/CLEANED DATA/interest_rate_clean_data.csv\", row.names=FALSE)\n\n#plot interest rate \nfig <- plot_ly(interest_data, x = ~Date, y = ~value, type = 'scatter', mode = 'lines',line = list(color='rgb(219, 112, 147)'))\nfig <- fig %>% layout(title = \"U.S Interest Rate: January 2010 - March 2023\",xaxis = list(title = \"Time\"),yaxis = list(title =\"Interest Rate\"))\nfig\n\n\n\n\n\n\n\n\n\n\nCode\n#import the data\nunemployment_rate <- read.csv(\"DATA/RAW DATA/unemployment-rate.csv\")\n\n#change date format\nunemployment_rate$Date <- as.Date(unemployment_rate$Date , \"%m/%d/%Y\")\n\n# export the data\nwrite.csv(unemployment_rate, \"DATA/CLEANED DATA/unemployment_rate_clean_data.csv\", row.names=FALSE)\n\n#plot unemployment rate \nfig <- plot_ly(unemployment_rate, x = ~Date, y = ~Value, type = 'scatter', mode = 'lines',line = list(color = 'rgb(189, 183, 107)'))\nfig <- fig %>% layout(title = \"U.S Unemployment Rate: January 2010 - March 2023\",xaxis = list(title = \"Time\"),yaxis = list(title =\"Unemployment Rate\"))\nfig\n\n\n\n\n\n\n\n\n\nFrom 2010 to 2014, AT&T’s stock price had a relatively steady increase, likely due to the company’s consistent revenue growth and strong financial performance. However, from 2014 to 2016, AT&T’s stock price experienced some volatility, which could be attributed to various factors such as changes in regulatory policies and increased competition in the telecommunication industry.\nIn 2016, AT&T’s stock price began to recover, likely due to the company’s acquisition of Time Warner, which expanded its portfolio of media and entertainment assets. This trend continued into 2017 and 2018, with AT&T’s stock price reaching an all-time high in mid-2018.\nHowever, from late 2018 to early 2020, AT&T’s stock price experienced some decline, which could be attributed to factors such as the company’s high debt load and increasing competition from new players in the telecommunication industry.\nThe outbreak of the COVID-19 pandemic in early 2020 caused a brief dip in AT&T’s stock price, as investors were uncertain about the impact of the pandemic on the company’s operations and financial performance. Nevertheless, AT&T’s strong position in the telecommunication and media industries helped it to quickly rebound and continue its growth trend throughout 2020 and into early 2021.\nSince early 2021, AT&T’s stock price has experienced some volatility, likely due to a combination of factors such as global economic uncertainty and fluctuations in consumer demand for the company’s products and services.\nSince early 2020, AT&T’s stock price has experienced some volatility, likely due to a combination of factors such as global economic uncertainty and fluctuations in consumer demand for the company’s products.\nAs discussed before, the macroeconomic factors of GDP growth, inflation, interest rates, and unemployment rate are closely interrelated and play a crucial role in the overall health and stability of an economy. From 2010 to 2023, the global economy experienced a mix of ups and downs, with periods of strong GDP growth followed by slowdowns and recessions.\nThe second plot shows the first difference of the logarithm of the adjusted AT&T stock price. Taking the first difference removes any long-term trends and transforms the time series into a stationary process. From the plot, we can observe that the first difference of the logarithm of the AT&T stock price appears to be stationary, as the mean and variance are roughly constant over time.\n\n\nEnodogenous and Exogenous Variables\n\nPlotCorrelation HeatmapCCF GDPCCF InterestCCF InflationCCF Unemployment\n\n\n\n\nCode\nnumeric_data <- c(\"T.Adjusted\",\"gdp\", \"interest\", \"inflation\", \"unemployment\")\nnumeric_data <- final[, numeric_data]\nnormalized_data_numeric <- scale(numeric_data)\nnormalized_data <- ts(normalized_data_numeric, start = c(2010, 1), end = c(2021,10),frequency = 4)\nts_plot(normalized_data,\n        title = \"Normalized Time Series Data for T Stock and Macroeconomic Variables\",\n        Ytitle = \"Normalized Values\",\n        Xtitle = \"Year\")\n\n\n\n\n\n\n\n\n\n\nCode\n# Get upper triangle of the correlation matrix\nget_upper_tri <- function(cormat){\n    cormat[lower.tri(cormat)]<- NA\n    return(cormat)\n}\ncormat <- round(cor(normalized_data_numeric),2)\n\nupper_tri <- get_upper_tri(cormat)\n\nmelted_cormat <- melt(upper_tri, na.rm = TRUE)\n# Create a ggheatmap\nggheatmap <- ggplot(melted_cormat, aes(Var2, Var1, fill = value))+\n geom_tile(color = \"white\")+\n scale_fill_gradient2(low = \"blue\", high = \"red\", mid = \"white\", \n   midpoint = 0, limit = c(-1,1), space = \"Lab\", \n    name=\"Pearson\\nCorrelation\") +\n  theme_minimal()+ # minimal theme\n theme(axis.text.x = element_text(angle = 45, vjust = 1, \n    size = 12, hjust = 1))+\n coord_fixed()\n\nggheatmap + \ngeom_text(aes(Var2, Var1, label = value), color = \"black\", size = 4) +\ntheme(\n  axis.title.x = element_blank(),\n  axis.title.y = element_blank(),\n  panel.grid.major = element_blank(),\n  panel.border = element_blank(),\n  panel.background = element_blank(),\n  axis.ticks = element_blank(),\n  legend.justification = c(1, 0),\n  legend.position = c(0.6, 0.7),\n  legend.direction = \"horizontal\")+\n  guides(fill = guide_colorbar(barwidth = 7, barheight = 1,\n                title.position = \"top\", title.hjust = 0.5))\n\n\n\n\n\n\n\n\n\nCode\npar(mfrow=c(1,1))\nccf_result <- ccf(normalized_data[, c(\"T.Adjusted\")], normalized_data[, c(\"gdp\")], \n    lag.max = 300,\n    main = \"Cros-Correlation Plot for T Stock Price and GDP Growth Rate \",\n    ylab = \"CCF\")\n\n\n\n\n\nCode\ncat(\"The sum of cross correlation function is\", sum(abs(ccf_result$acf)))\n\n\nThe sum of cross correlation function is 5.957981\n\n\n\n\n\n\nCode\npar(mfrow=c(1,1))\nccf_result <- ccf(normalized_data[, c(\"T.Adjusted\")], normalized_data[, c(\"interest\")], \n    lag.max = 300,\n    main = \"Cros-Correlation Plot for T Stock Price and Interest Rate\",\n    ylab = \"CCF\")\n\n\n\n\n\nCode\ncat(\"The sum of cross correlation function is\", sum(abs(ccf_result$acf)))\n\n\nThe sum of cross correlation function is 12.0267\n\n\n\n\n\n\nCode\npar(mfrow=c(1,1))\nccf_result <- ccf(normalized_data[, c(\"T.Adjusted\")], normalized_data[, c(\"inflation\")], \n    lag.max = 300,\n    main = \"Cros-Correlation Plot for T Stock Price and Inflation Rate\",\n    ylab = \"CCF\")\n\n\n\n\n\nCode\ncat(\"The sum of cross correlation function is\", sum(abs(ccf_result$acf)))\n\n\nThe sum of cross correlation function is 16.72642\n\n\n\n\n\n\nCode\npar(mfrow=c(1,1))\nccf_result <- ccf(normalized_data[, c(\"T.Adjusted\")], normalized_data[, c(\"unemployment\")], \n    lag.max = 300,\n    main = \"Cros-Correlation Plot for T Stock Priceand Unemployment Rate\",\n    ylab = \"CCF\")\n\n\n\n\n\nCode\ncat(\"The sum of cross correlation function is\", sum(abs(ccf_result$acf)))\n\n\nThe sum of cross correlation function is 20.3674\n\n\n\n\n\nThe Normalized Time Series Data for Stock Price and Macroeconomic Variables plot shows the same variables as the first plot but has been normalized to a common range of 0 to 1 using the scale() function in R, which standardizes the variables to have a mean of 0 and a standard deviation of 1. The heatmap analysis of the normalized data reveals that inflation and unemployment rate exhibit strong positive correlations with the stock price indices, indicating that these variables may significantly influence stock price movements. On the other hand, weaker correlations were observed between the stock price indices and GDP and interest rates, suggesting that these variables may have less impact on stock price fluctuations. The cross-correlation feature plots confirm these findings, indicating that inflation and unemployment rate are more suitable feature variables for the ARIMAX model when predicting AT&T movements.\nFinal Exogenous variables: Macroeconomic indicators: Inflation rate and unemployment rate.\n\n\nEnodogenous and Exogenous Variables Plot\n\nPlotCheck the stationarity\n\n\n\n\nCode\nfinal_data <- final %>%dplyr::select( Date,T.Adjusted, inflation,unemployment)\nnumeric_data <- c(\"T.Adjusted\", \"inflation\",\"unemployment\")\nnumeric_data <- final_data[, numeric_data]\nnormalized_data_numeric <- scale(numeric_data)\nnormalized_numeric_df <- data.frame(normalized_data_numeric)\nnormalized_data_ts <- ts(normalized_data_numeric, start = c(2010, 1), frequency = 4)\n\nautoplot(normalized_data_ts, facets=TRUE) +\n  xlab(\"Year\") + ylab(\"\") +\n  ggtitle(\"AT&T Stock Price, Inflation Rate and Unemployment Rate in USA 2010-2023\")\n\n\n\n\n\n\n\n\n\nCode\n# Convert your multivariate time series data to a matrix\nfinal_data_ts_multivariate <- as.matrix(normalized_data_ts)\n\n# Check for stationarity using Phillips-Perron test\nphillips_perron_test <- ur.pp(final_data_ts_multivariate)  \nsummary(phillips_perron_test)\n\n\n\n################################## \n# Phillips-Perron Unit Root Test # \n################################## \n\nTest regression with intercept \n\n\nCall:\nlm(formula = y ~ y.l1)\n\nResiduals:\n    Min      1Q  Median      3Q     Max \n-1.3311 -0.1828 -0.0604  0.1125  4.4755 \n\nCoefficients:\n            Estimate Std. Error t value Pr(>|t|)    \n(Intercept) 0.005902   0.041678   0.142    0.888    \ny.l1        0.845352   0.042100  20.079   <2e-16 ***\n---\nSignif. codes:  0 '***' 0.001 '**' 0.01 '*' 0.05 '.' 0.1 ' ' 1\n\nResidual standard error: 0.5189 on 153 degrees of freedom\nMultiple R-squared:  0.7249,    Adjusted R-squared:  0.7231 \nF-statistic: 403.2 on 1 and 153 DF,  p-value: < 2.2e-16\n\n\nValue of test-statistic, type: Z-alpha  is: -22.8539 \n\n         aux. Z statistics\nZ-tau-mu            0.1434\n\n\n\n\n\nThe results of the Phillips-Perron unit root test indicate strong evidence against the null hypothesis of a unit root, as the p-value for the coefficient of the lagged variable is less than the significance level of 0.05. This suggests that the variable y, which is being tested for stationarity, is likely stationary. Furthermore, the test statistic Z-tau-mu is 0.1434, which is smaller than the critical value of Z-alpha (-22.8539), providing further evidence of stationarity.\nTo determine whether the linear model requires an ARCH model, an ARCH test is conducted. The ACF and PACF plots are also used to identify suitable model values.\n\n\nModel Fitting\n\nPlotARCH TestACF PlotPACF Plot\n\n\n\n\nCode\nnormalized_numeric_df$T.Adjusted<-ts(normalized_numeric_df$T.Adjusted,star=decimal_date(as.Date(\"2010-01-01\",format = \"%Y-%m-%d\")),frequency = 4)\nnormalized_numeric_df$inflation<-ts(normalized_numeric_df$inflation,star=decimal_date(as.Date(\"2010-01-01\",format = \"%Y-%m-%d\")),frequency = 4)\nnormalized_numeric_df$unemployment<-ts(normalized_numeric_df$unemployment,star=decimal_date(as.Date(\"2010-01-01\",format = \"%Y-%m-%d\")),frequency = 4)\n\nfit <- lm(T.Adjusted ~ inflation+unemployment, data=normalized_numeric_df)\nfit.res<-ts(residuals(fit),star=decimal_date(as.Date(\"2010-01-01\",format = \"%Y-%m-%d\")),frequency = 4)\n############## Then look at the residuals ############\nreturns <- fit.res  %>% diff()\nautoplot(returns)+ggtitle(\"Linear Model Returns\")\n\n\n\n\n\n\n\n\n\nCode\nbyd.archTest <- ArchTest(fit.res, lags = 1, demean = TRUE)\nbyd.archTest\n\n\n\n    ARCH LM-test; Null hypothesis: no ARCH effects\n\ndata:  fit.res\nChi-squared = 12.762, df = 1, p-value = 0.0003537\n\n\n\n\n\n\nCode\nggAcf(returns) +ggtitle(\"ACF for returns\")\n\n\n\n\n\n\n\n\n\nCode\nggPacf(returns) +ggtitle(\"PACF for returns\")\n\n\n\n\n\n\n\n\nThe ARCH LM-test was conducted with the null hypothesis of no ARCH effects. The test resulted in a chi-squared value of 12.762 with one degree of freedom, and a very low p-value of 0.0003537. This suggests strong evidence against the null hypothesis, indicating the presence of ARCH effects in the data.\nBased on the ACF and PACF plots, it appears that there is some significant autocorrelation and partial autocorrelation at multiple lags, which suggests that an ARIMA model may not be sufficient to capture the time series behavior. Additionally, the values for p and q appear to be relatively high, with p = 0and q = 0 being suggested by the plots.\n\n\nARIMAX Model\n\nAuto Arima ModelAuto Arima Residuals\n\n\n\n\nCode\nxreg <- cbind(Inflation = normalized_data_ts[, \"inflation\"],\n              Unemployment = normalized_data_ts[, \"unemployment\"])\nfit.auto <- auto.arima(normalized_data_ts[, \"T.Adjusted\"], xreg = xreg)\nsummary(fit.auto)\n\n\nSeries: normalized_data_ts[, \"T.Adjusted\"] \nRegression with ARIMA(0,1,0) errors \n\nCoefficients:\n      Inflation  Unemployment\n         -0.156       -0.3069\ns.e.      0.150        0.0681\n\nsigma^2 = 0.119:  log likelihood = -17.07\nAIC=40.14   AICc=40.65   BIC=45.94\n\nTraining set error measures:\n                     ME     RMSE       MAE       MPE     MAPE      MASE\nTraining set 0.03193586 0.334906 0.2596333 -52.59142 81.78774 0.5318549\n                   ACF1\nTraining set -0.1104955\n\n\n\n\n\n\nCode\ncheckresiduals(fit.auto)\n\n\n\n\n\n\n    Ljung-Box test\n\ndata:  Residuals from Regression with ARIMA(0,1,0) errors\nQ* = 1.767, df = 8, p-value = 0.9873\n\nModel df: 0.   Total lags used: 8\n\n\n\n\n\nBased on the results of the auto.arima function, the suggested best model is ARIMA(0,1,0) which is the same as the manual choosen arima model. So, we can then proceed to choose the best GARCH model using ARIMA(0,1,0) as the base model.\n\n\nSquared Residuals\n\nPlotACF PlotPACF PlotACF AbsoluteARCH Model\n\n\n\n\nCode\nfit <- lm(T.Adjusted ~ inflation+unemployment, data=normalized_numeric_df)\nfit.res<-ts(residuals(fit),star=decimal_date(as.Date(\"2010-01-01\",format = \"%Y-%m-%d\")),frequency = 4)\nfit <- Arima(fit.res,order=c(0,1,0))\nres=fit$res\nplot(res^2,main='Squared Residuals')\n\n\n\n\n\n\n\n\n\nCode\nacf(res^2,24, main = \"ACF Residuals Square\")\n\n\n\n\n\n\n\n\n\nCode\npacf(res^2,24, main = \"PACF Residuals Square\")\n\n\n\n\n\n\n\n\n\nCode\nacf(abs(res), main = \"ACF of Absolute Residuals\")\n\n\n\n\n\n\n\nCode\npacf(abs(res), main = \"PACF of Absolute Residuals\")\n\n\n\n\n\n\n\n\n\nCode\nsummary(garchFit(~garch(1,0),res, trace=F))\n\n\n\nTitle:\n GARCH Modelling \n\nCall:\n garchFit(formula = ~garch(1, 0), data = res, trace = F) \n\nMean and Variance Equation:\n data ~ garch(1, 0)\n<environment: 0x133d2e208>\n [data = res]\n\nConditional Distribution:\n norm \n\nCoefficient(s):\n       mu      omega     alpha1  \n-0.014923   0.061675   1.000000  \n\nStd. Errors:\n based on Hessian \n\nError Analysis:\n        Estimate  Std. Error  t value Pr(>|t|)\nmu      -0.01492     0.04410   -0.338    0.735\nomega    0.06168     0.05922    1.041    0.298\nalpha1   1.00000     0.91140    1.097    0.273\n\nLog Likelihood:\n -24.54432    normalized:  -0.4720061 \n\nDescription:\n Tue Jan  9 20:57:55 2024 by user:  \n\n\nStandardised Residuals Tests:\n                                Statistic p-Value     \n Jarque-Bera Test   R    Chi^2  21.09339  2.628022e-05\n Shapiro-Wilk Test  R    W      0.9298323 0.004419964 \n Ljung-Box Test     R    Q(10)  14.55682  0.1490741   \n Ljung-Box Test     R    Q(15)  23.42799  0.075466    \n Ljung-Box Test     R    Q(20)  30.70192  0.05925058  \n Ljung-Box Test     R^2  Q(10)  3.989285  0.9478291   \n Ljung-Box Test     R^2  Q(15)  4.464593  0.9957709   \n Ljung-Box Test     R^2  Q(20)  7.691302  0.993721    \n LM Arch Test       R    TR^2   5.066871  0.955708    \n\nInformation Criterion Statistics:\n     AIC      BIC      SIC     HQIC \n1.059397 1.171969 1.053211 1.102554 \n\n\n\n\n\nFrom the squared residuals of the best ARIMA model, it can be observed that the ACF plot and PACF plot indicate that the residuals are not autocorrelated and are white noise, indicating a good fit of the model. Based on the squared residuals of the best ARIMA model, we can see that the ACF and PACF plots indicate that most of the values lie between the blue lines. Additionally, the p-value is 1 and q-value is 0. This suggests that the model has a good fit and that there is no significant autocorrelation or partial autocorrelation in the residuals.\nThe best models are ARIMA(0,1,0) and ARCH(1)\n\n\nBest Model\n\nARIMA ModelGARCH ModelVolatility\n\n\n\n\nCode\n#fiting an ARIMA model to the Inflation variable\ninflation_fit<-auto.arima(normalized_numeric_df$inflation) \nfinflation<-forecast(inflation_fit)\n\n#fitting an ARIMA model to the Unemployment variable\nunemployment_fit<-auto.arima(normalized_numeric_df$unemployment) \nfunemployment<-forecast(unemployment_fit)\n\n# best model fit for forcasting\nxreg <- cbind(Inflation = normalized_data_ts[, \"inflation\"],\n              Unemployment = normalized_data_ts[, \"unemployment\"])\n\n\n\nsummary(arima.fit<-Arima(normalized_data_ts[, \"T.Adjusted\"],order=c(0,1,0),xreg=xreg),include.drift = TRUE)\n\n\nSeries: normalized_data_ts[, \"T.Adjusted\"] \nRegression with ARIMA(0,1,0) errors \n\nCoefficients:\n      Inflation  Unemployment\n         -0.156       -0.3069\ns.e.      0.150        0.0681\n\nsigma^2 = 0.119:  log likelihood = -17.07\nAIC=40.14   AICc=40.65   BIC=45.94\n\nTraining set error measures:\n                     ME     RMSE       MAE       MPE     MAPE      MASE\nTraining set 0.03193586 0.334906 0.2596333 -52.59142 81.78774 0.5318549\n                   ACF1\nTraining set -0.1104955\n\n\n\n\n\n\nCode\nsummary(final.fit <- garchFit(~garch(1,0), res,trace = F))\n\n\n\nTitle:\n GARCH Modelling \n\nCall:\n garchFit(formula = ~garch(1, 0), data = res, trace = F) \n\nMean and Variance Equation:\n data ~ garch(1, 0)\n<environment: 0x1346afcf8>\n [data = res]\n\nConditional Distribution:\n norm \n\nCoefficient(s):\n       mu      omega     alpha1  \n-0.014923   0.061675   1.000000  \n\nStd. Errors:\n based on Hessian \n\nError Analysis:\n        Estimate  Std. Error  t value Pr(>|t|)\nmu      -0.01492     0.04410   -0.338    0.735\nomega    0.06168     0.05922    1.041    0.298\nalpha1   1.00000     0.91140    1.097    0.273\n\nLog Likelihood:\n -24.54432    normalized:  -0.4720061 \n\nDescription:\n Tue Jan  9 20:57:56 2024 by user:  \n\n\nStandardised Residuals Tests:\n                                Statistic p-Value     \n Jarque-Bera Test   R    Chi^2  21.09339  2.628022e-05\n Shapiro-Wilk Test  R    W      0.9298323 0.004419964 \n Ljung-Box Test     R    Q(10)  14.55682  0.1490741   \n Ljung-Box Test     R    Q(15)  23.42799  0.075466    \n Ljung-Box Test     R    Q(20)  30.70192  0.05925058  \n Ljung-Box Test     R^2  Q(10)  3.989285  0.9478291   \n Ljung-Box Test     R^2  Q(15)  4.464593  0.9957709   \n Ljung-Box Test     R^2  Q(20)  7.691302  0.993721    \n LM Arch Test       R    TR^2   5.066871  0.955708    \n\nInformation Criterion Statistics:\n     AIC      BIC      SIC     HQIC \n1.059397 1.171969 1.053211 1.102554 \n\n\n\n\n\n\nCode\nht <- final.fit@h.t #a numeric vector with the conditional variances (h.t = sigma.t^delta)\n\n#############################\ndata=data.frame(final)\ndata$Date<-as.Date(data$Date,\"%Y-%m-%d\")\n\n\ndata2= data.frame(ht,data$Date)\nggplot(data2, aes(y = ht, x = data.Date)) + geom_line(col = '#73C6B6') + ylab('Conditional Variance') + xlab('Date')\n\n\n\n\n\n\n\n\nFrom the ARIMA(0,1,0), we see that the training set error measures also suggest a good fit, with low mean absolute error, root mean squared error, and autocorrelation of the residuals. GATCH(1,0) model model is used to estimate the volatility of the standardized residuals of the previous regression model. The model includes a mean equation that estimates the mean of the residuals and a variance equation that models the conditional variance of the residuals. The coefficients of the mean equation suggest that the mean of the residuals is close to zero. The variance equation coefficients suggest that the conditional variance of the residuals is dependent on the past conditional variances and the past squared standardized residuals. The model’s the AIC, BIC, SIC, and HQIC values are all relatively low, indicating a good fit of the model. The standardized residuals tests indicate that the residuals are approximately normally distributed and that there is no significant autocorrelation in the residuals.\nThe volatility of the model seems high in 2020 but has decreased gradually in the past few months. This could indicate that the asset’s price was experiencing a lot of fluctuations in 2020, but the market has stabilized recently.\n\n\nModel Diagnostics\n\nResidualsQQ PlotBox Test\n\n\n\n\nCode\nfit2<-garch(res,order=c(1,0),trace=F)\ncheckresiduals(fit2) \n\n\n\n\n\n\n\n\n\nCode\nqqnorm(fit2$residuals, pch = 1)\nqqline(fit2$residuals, col = \"blue\", lwd = 2)\n\n\n\n\n\n\n\n\n\nCode\nBox.test (fit2$residuals, type = \"Ljung\")\n\n\n\n    Box-Ljung test\n\ndata:  fit2$residuals\nX-squared = 0.20159, df = 1, p-value = 0.6534\n\n\nThe ACF plot of the residuals shows all the values between the blue lines, which indicates that the residuals are not significantly autocorrelated. The range of values for the residual plot between -2 and 2 is considered acceptable. Additionally, the QQ plot of the residuals shows a linear plot on the line, which is another good indication that the residuals are normally distributed. The QQ plot is a valuable tool to assess if the residuals follow a normal distribution, and in this case, the plot suggests that the residuals do indeed follow a normal distribution.\nThe Box-Ljung test, a p-value of 0.6534 indicates that the model’s residuals are not significantly autocorrelated, meaning that the model has captured most of the information in the data. This result is good because it suggests that the model is a good fit for the data and has accounted for most of the underlying patterns in the data. Therefore, we can rely on the model’s predictions and use them to make informed decisions.\n\n\n\n\n\nForecast\n\n\nCode\npredict(final.fit, n.ahead = 5, plot=TRUE)\n\n\n\n\n\n  meanForecast meanError standardDeviation lowerInterval upperInterval\n1  -0.01492338  1.240661          1.240661     -2.446574      2.416727\n2  -0.01492338  1.265273          1.265273     -2.494812      2.464966\n3  -0.01492338  1.289415          1.289415     -2.542130      2.512283\n4  -0.01492338  1.313113          1.313113     -2.588577      2.558731\n5  -0.01492338  1.336391          1.336391     -2.634201      2.604355\n\n\nThe forecasted plot is based on the best model ARIMAX(0,1,0)+ARCH(1,0). This model takes into account the autoregressive and moving average components of the data, as well as the impact of exogenous variables on the time series. Additionally, the GARCH component of the model accounts for the volatility clustering in the data. Overall, this model is well-suited to make accurate predictions about future values of the time series.\n\n\nEquation of the Model\nThe equation of the ARIMAX(0,1,0) model is:\n\\(Y(t) = Y(t-1) + \\epsilon(t)\\)\nwhere \\(Y(t)\\) is the time series variable and \\(\\epsilon(t)\\) is the error term.\nThe equation of the ARCH(1) model is:\n\\(\\sigma_{t}^{2}=\\alpha_{0}+\\alpha_{1}\\epsilon_{t-1}^{2}\\)\nwhere \\(\\sigma^2_t\\) is the conditional variance at time \\(t\\), \\(\\alpha_0\\) is a constant, \\(\\alpha_1\\) and \\(\\beta_1\\) are the parameters, and \\(\\epsilon_t\\) is the error term.\nThe combined equation of the ARIMAX(1,1,1)+GARCH(1,1) model is:\n\\(Y(t) = Y(t-1) + \\epsilon(t)\\)\n\\(\\sigma_{t}^{2}=\\alpha_{0}+\\alpha_{1}\\epsilon_{t-1}^{2}\\)"
  },
  {
    "objectID": "arch_Energy_Sector_Fund.html",
    "href": "arch_Energy_Sector_Fund.html",
    "title": "",
    "section": "",
    "text": "Impact of Macroeconomic Factors on ExxonMobil Stock Price\n\n\n\nOne of the top companies in the energy sector is ExxonMobil Corporation (XOM), which is one of the largest publicly traded international oil and gas companies in the world. ExxonMobil engages in exploration, production, transportation, and sale of crude oil, natural gas, and petroleum products. With operations in over 70 countries, ExxonMobil has a significant global presence and is a major player in the energy industry. As of 2021, ExxonMobil also has investments in alternative energy technologies such as biofuels and carbon capture and storage.\nBy using an ARIMAX+ARCH/GARCH model to analyze the stock price behavior of ExxonMobil, we can gain insights into how macroeconomic factors impact the company’s performance. For example, an increase in GDP growth rate or a decrease in unemployment rate may lead to increased consumer spending and a rise in ExxonMobil’s stock price. Conversely, an increase in inflation or interest rates may lead to a decrease in consumer spending and a decline in ExxonMobil’s stock price.\n\n\nTime series Plot\n\nExxonMobilDifferentitaed Chart SeriesGDP GrowthInflationInterestUnemployment\n\n\n\n\nCode\n# get data\noptions(\"getSymbols.warning4.0\"=FALSE)\noptions(\"getSymbols.yahoo.warning\"=FALSE)\n\n\ndata.info = getSymbols(\"XOM\",src='yahoo', from = '2010-01-01',to = \"2023-03-01\",auto.assign = FALSE)\ndata = getSymbols(\"XOM\",src='yahoo', from = '2010-01-01',to = \"2023-03-01\")\ndf <- data.frame(Date=index(XOM),coredata(XOM))\n\n# create Bollinger Bands\nbbands <- BBands(XOM[,c(\"XOM.High\",\"XOM.Low\",\"XOM.Close\")])\n\n# join and subset data\ndf <- subset(cbind(df, data.frame(bbands[,1:3])), Date >= \"2010-01-01\")\n\n\n# colors column for increasing and decreasing\nfor (i in 1:length(df[,1])) {\n  if (df$XOM.Close[i] >= df$XOM.Open[i]) {\n      df$direction[i] = 'Increasing'\n  } else {\n      df$direction[i] = 'Decreasing'\n  }\n}\n\ni <- list(line = list(color = '#F9E79F'))\nd <- list(line = list(color = '#7F7F7F'))\n\n# plot candlestick chart\n\nfig <- df %>% plot_ly(x = ~Date, type=\"candlestick\",\n          open = ~XOM.Open, close = ~XOM.Close,\n          high = ~XOM.High, low = ~XOM.Low, name = \"XOM\",\n          increasing = i, decreasing = d) \nfig <- fig %>% add_lines(x = ~Date, y = ~up , name = \"B Bands\",\n            line = list(color = '#ccc', width = 0.5),\n            legendgroup = \"Bollinger Bands\",\n            hoverinfo = \"none\", inherit = F) \nfig <- fig %>% add_lines(x = ~Date, y = ~dn, name = \"B Bands\",\n            line = list(color = '#ccc', width = 0.5),\n            legendgroup = \"Bollinger Bands\", inherit = F,\n            showlegend = FALSE, hoverinfo = \"none\") \nfig <- fig %>% add_lines(x = ~Date, y = ~mavg, name = \"Mv Avg\",\n            line = list(color = '#C052B3', width = 0.5),\n            hoverinfo = \"none\", inherit = F) \nfig <- fig %>% layout(yaxis = list(title = \"Price\"))\n\n# plot volume bar chart\nfig2 <- df \nfig2 <- fig2 %>% plot_ly(x=~Date, y=~XOM.Volume, type='bar', name = \"XOM Volume\",\n          color = ~direction, colors = c('#F9E79F','#7F7F7F')) \nfig2 <- fig2 %>% layout(yaxis = list(title = \"Volume\"))\n\n# create rangeselector buttons\nrs <- list(visible = TRUE, x = 0.5, y = -0.055,\n           xanchor = 'center', yref = 'paper',\n           font = list(size = 9),\n           buttons = list(\n             list(count=1,\n                  label='RESET',\n                  step='all'),\n             list(count=3,\n                  label='3 YR',\n                  step='year',\n                  stepmode='backward'),\n             list(count=1,\n                  label='1 YR',\n                  step='year',\n                  stepmode='backward'),\n             list(count=1,\n                  label='1 MO',\n                  step='month',\n                  stepmode='backward')\n           ))\n\n# subplot with shared x axis\nfig <- subplot(fig, fig2, heights = c(0.7,0.2), nrows=2,\n             shareX = TRUE, titleY = TRUE)\nfig <- fig %>% layout(title = paste(\"ExxonMobil Stock Price: January 2010 - March 2023\"),\n         xaxis = list(rangeselector = rs),\n         legend = list(orientation = 'h', x = 0.5, y = 1,\n                       xanchor = 'center', yref = 'paper',\n                       font = list(size = 10),\n                       bgcolor = 'transparent'))\n\nfig\n\n\n\n\n\n\n\n\n\n\nCode\nlog(data.info$`XOM.Adjusted`) %>% diff() %>% chartSeries(theme=chartTheme('white'),up.col='#F9E79F')\n\n\n\n\n\n\n\n\n\nCode\n#import the data\ngdp <- read.csv(\"DATA/RAW DATA/gdp-growth.csv\")\n\n#change date format\ngdp$Date <- as.Date(gdp$DATE , \"%m/%d/%Y\")\n\n#drop DATE column\ngdp <- subset(gdp, select = -c(1))\n\n#export the cleaned data\ngdp_clean <- gdp\nwrite.csv(gdp_clean, \"DATA/CLEANED DATA/gdp_clean_data.csv\", row.names=FALSE)\n\n#plot gdp growth rate \nfig <- plot_ly(gdp, x = ~Date, y = ~value, type = 'scatter', mode = 'lines',line = list(color = 'rgb(240, 128, 128)'))\nfig <- fig %>% layout(title = \"U.S GPD Growth Rate: 2010 - 2022\",xaxis = list(title = \"Time\"),yaxis = list(title =\"GDP Growth Rate\"))\nfig\n\n\n\n\n\n\n\n\n\n\nCode\n#import the data\ninflation_rate <- read.csv(\"DATA/RAW DATA/inflation-rate.csv\")\n\n#cleaning the data\n#remove unwanted columns\ninflation_rate_clean <- subset(inflation_rate, select = -c(1,HALF1,HALF2))\n\n#convert the data to time series data\ninflation_data_ts <- ts(as.vector(t(as.matrix(inflation_rate_clean))), start=c(2010,1), end=c(2023,2), frequency=12)\n\n#export the data\nwrite.csv(inflation_rate_clean, \"DATA/CLEANED DATA/inflation_rate_clean_data.csv\", row.names=FALSE)\n\n\n#plot inflation rate \nfig <- autoplot(inflation_data_ts, ylab = \"Inflation Rate\", color=\"#FFA07A\")+ggtitle(\"U.S Inflation Rate: January 2010 - February 2023\")+theme_bw()\nggplotly(fig)\n\n\n\n\n\n\n\n\n\n\nCode\n#import the data\ninterest_data <- read.csv(\"DATA/RAW DATA/interest-rate.csv\")\n\n#change date format\ninterest_data$Date <- as.Date(interest_data$Date , \"%m/%d/%Y\")\n\n#export the cleaned data\ninterest_clean_data <- interest_data\nwrite.csv(interest_clean_data, \"DATA/CLEANED DATA/interest_rate_clean_data.csv\", row.names=FALSE)\n\n#plot interest rate \nfig <- plot_ly(interest_data, x = ~Date, y = ~value, type = 'scatter', mode = 'lines',line = list(color='rgb(219, 112, 147)'))\nfig <- fig %>% layout(title = \"U.S Interest Rate: January 2010 - March 2023\",xaxis = list(title = \"Time\"),yaxis = list(title =\"Interest Rate\"))\nfig\n\n\n\n\n\n\n\n\n\n\nCode\n#import the data\nunemployment_rate <- read.csv(\"DATA/RAW DATA/unemployment-rate.csv\")\n\n#change date format\nunemployment_rate$Date <- as.Date(unemployment_rate$Date , \"%m/%d/%Y\")\n\n# export the data\nwrite.csv(unemployment_rate, \"DATA/CLEANED DATA/unemployment_rate_clean_data.csv\", row.names=FALSE)\n\n#plot unemployment rate \nfig <- plot_ly(unemployment_rate, x = ~Date, y = ~Value, type = 'scatter', mode = 'lines',line = list(color = 'rgb(189, 183, 107)'))\nfig <- fig %>% layout(title = \"U.S Unemployment Rate: January 2010 - March 2023\",xaxis = list(title = \"Time\"),yaxis = list(title =\"Unemployment Rate\"))\nfig\n\n\n\n\n\n\n\n\n\nExxonMobil (XOM) is one of the largest integrated oil and gas companies in the world. From 2010 to 2023, XOM’s stock price experienced some significant fluctuations.\nIn the early part of the decade, XOM’s stock price steadily increased as the global demand for oil and gas remained strong. However, the company’s stock price began to decline in mid-2014, as the oversupply of oil and gas in the global market led to a drop in prices. This decline was further exacerbated by the COVID-19 pandemic in 2020, which caused a sharp drop in global demand for oil and gas.\nDespite these challenges, XOM’s stock price has shown signs of recovery in recent years, as the company has taken steps to reduce its costs and increase its focus on natural gas and chemicals. In addition, the company has made significant investments in renewable energy, which could help it to diversify its revenue streams in the long term.\nSince mid 2020, ExxonMobil’s stock price has experienced some volatility, likely due to a combination of factors such as global economic uncertainty and fluctuations in consumer demand for the company’s products.\nAs discussed before, the macroeconomic factors of GDP growth, inflation, interest rates, and unemployment rate are closely interrelated and play a crucial role in the overall health and stability of an economy. From 2010 to 2023, the global economy experienced a mix of ups and downs, with periods of strong GDP growth followed by slowdowns and recessions.\nThe second plot shows the first difference of the logarithm of the adjusted ExxonMobil stock price. Taking the first difference removes any long-term trends and transforms the time series into a stationary process. From the plot, we can observe that the first difference of the logarithm of the ExxonMobil stock price appears to be stationary, as the mean and variance are roughly constant over time.\n\n\nEnodogenous and Exogenous Variables\n\nPlotCorrelation HeatmapCCF GDPCCF InterestCCF InflationCCF Unemployment\n\n\n\n\nCode\nnumeric_data <- c(\"XOM.Adjusted\",\"gdp\", \"interest\", \"inflation\", \"unemployment\")\nnumeric_data <- final[, numeric_data]\nnormalized_data_numeric <- scale(numeric_data)\nnormalized_data <- ts(normalized_data_numeric, start = c(2010, 1), end = c(2021,10),frequency = 4)\nts_plot(normalized_data,\n        title = \"Normalized Time Series Data for XOM Stock and Macroeconomic Variables\",\n        Ytitle = \"Normalized Values\",\n        Xtitle = \"Year\")\n\n\n\n\n\n\n\n\n\n\nCode\n# Get upper triangle of the correlation matrix\nget_upper_tri <- function(cormat){\n    cormat[lower.tri(cormat)]<- NA\n    return(cormat)\n}\ncormat <- round(cor(normalized_data_numeric),2)\n\nupper_tri <- get_upper_tri(cormat)\n\nmelted_cormat <- melt(upper_tri, na.rm = TRUE)\n# Create a ggheatmap\nggheatmap <- ggplot(melted_cormat, aes(Var2, Var1, fill = value))+\n geom_tile(color = \"white\")+\n scale_fill_gradient2(low = \"blue\", high = \"red\", mid = \"white\", \n   midpoint = 0, limit = c(-1,1), space = \"Lab\", \n    name=\"Pearson\\nCorrelation\") +\n  theme_minimal()+ # minimal theme\n theme(axis.text.x = element_text(angle = 45, vjust = 1, \n    size = 12, hjust = 1))+\n coord_fixed()\n\nggheatmap + \ngeom_text(aes(Var2, Var1, label = value), color = \"black\", size = 4) +\ntheme(\n  axis.title.x = element_blank(),\n  axis.title.y = element_blank(),\n  panel.grid.major = element_blank(),\n  panel.border = element_blank(),\n  panel.background = element_blank(),\n  axis.ticks = element_blank(),\n  legend.justification = c(1, 0),\n  legend.position = c(0.6, 0.7),\n  legend.direction = \"horizontal\")+\n  guides(fill = guide_colorbar(barwidth = 7, barheight = 1,\n                title.position = \"top\", title.hjust = 0.5))\n\n\n\n\n\n\n\n\n\nCode\npar(mfrow=c(1,1))\nccf_result <- ccf(normalized_data[, c(\"XOM.Adjusted\")], normalized_data[, c(\"gdp\")], \n    lag.max = 300,\n    main = \"Cros-Correlation Plot for XOM Stock Price and GDP Growth Rate \",\n    ylab = \"CCF\")\n\n\n\n\n\nCode\ncat(\"The sum of cross correlation function is\", sum(abs(ccf_result$acf)))\n\n\nThe sum of cross correlation function is 8.584101\n\n\n\n\n\n\nCode\npar(mfrow=c(1,1))\nccf_result <- ccf(normalized_data[, c(\"XOM.Adjusted\")], normalized_data[, c(\"interest\")], \n    lag.max = 300,\n    main = \"Cros-Correlation Plot for XOM Stock Price and Interest Rate\",\n    ylab = \"CCF\")\n\n\n\n\n\nCode\ncat(\"The sum of cross correlation function is\", sum(abs(ccf_result$acf)))\n\n\nThe sum of cross correlation function is 13.78063\n\n\n\n\n\n\nCode\npar(mfrow=c(1,1))\nccf_result <- ccf(normalized_data[, c(\"XOM.Adjusted\")], normalized_data[, c(\"inflation\")], \n    lag.max = 300,\n    main = \"Cros-Correlation Plot for XOM Stock Price and Inflation Rate\",\n    ylab = \"CCF\")\n\n\n\n\n\nCode\ncat(\"The sum of cross correlation function is\", sum(abs(ccf_result$acf)))\n\n\nThe sum of cross correlation function is 13.02693\n\n\n\n\n\n\nCode\npar(mfrow=c(1,1))\nccf_result <- ccf(normalized_data[, c(\"XOM.Adjusted\")], normalized_data[, c(\"unemployment\")], \n    lag.max = 300,\n    main = \"Cros-Correlation Plot for XOM Stock Priceand Unemployment Rate\",\n    ylab = \"CCF\")\n\n\n\n\n\nCode\ncat(\"The sum of cross correlation function is\", sum(abs(ccf_result$acf)))\n\n\nThe sum of cross correlation function is 11.6615\n\n\n\n\n\nThe Normalized Time Series Data for Stock Price and Macroeconomic Variables plot shows the same variables as the first plot but has been normalized to a common range of 0 to 1 using the scale() function in R, which standardizes the variables to have a mean of 0 and a standard deviation of 1. The heatmap analysis of the normalized data reveals that inflation and unemployment rate exhibit strong positive correlations with the stock price indices, indicating that these variables may significantly influence stock price movements. On the other hand, weaker correlations were observed between the stock price indices and GDP and interest rates, suggesting that these variables may have less impact on stock price fluctuations. The cross-correlation feature plots confirm these findings, indicating that inflation and unemployment rate are more suitable feature variables for the ARIMAX model when predicting ExxonMobil movements.\nFinal Exogenous variables: Macroeconomic indicators: Inflation rate and unemployment rate.\n\n\nEnodogenous and Exogenous Variables Plot\n\nPlotCheck the stationarity\n\n\n\n\nCode\nfinal_data <- final %>%dplyr::select( Date,XOM.Adjusted, inflation,unemployment)\nnumeric_data <- c(\"XOM.Adjusted\", \"inflation\",\"unemployment\")\nnumeric_data <- final_data[, numeric_data]\nnormalized_data_numeric <- scale(numeric_data)\nnormalized_numeric_df <- data.frame(normalized_data_numeric)\nnormalized_data_ts <- ts(normalized_data_numeric, start = c(2010, 1), frequency = 4)\n\nautoplot(normalized_data_ts, facets=TRUE) +\n  xlab(\"Year\") + ylab(\"\") +\n  ggtitle(\"ExxonMobil Stock Price, Inflation Rate and Unemployment Rate in USA 2010-2023\")\n\n\n\n\n\n\n\n\n\nCode\n# Convert your multivariate time series data to a matrix\nfinal_data_ts_multivariate <- as.matrix(normalized_data_ts)\n\n# Check for stationarity using Phillips-Perron test\nphillips_perron_test <- ur.pp(final_data_ts_multivariate)  \nsummary(phillips_perron_test)\n\n\n\n################################## \n# Phillips-Perron Unit Root Test # \n################################## \n\nTest regression with intercept \n\n\nCall:\nlm(formula = y ~ y.l1)\n\nResiduals:\n    Min      1Q  Median      3Q     Max \n-2.6785 -0.2163 -0.0613  0.1641  4.4054 \n\nCoefficients:\n            Estimate Std. Error t value Pr(>|t|)    \n(Intercept) 0.003019   0.049779   0.061    0.952    \ny.l1        0.780682   0.050284  15.526   <2e-16 ***\n---\nSignif. codes:  0 '***' 0.001 '**' 0.01 '*' 0.05 '.' 0.1 ' ' 1\n\nResidual standard error: 0.6197 on 153 degrees of freedom\nMultiple R-squared:  0.6117,    Adjusted R-squared:  0.6092 \nF-statistic:   241 on 1 and 153 DF,  p-value: < 2.2e-16\n\n\nValue of test-statistic, type: Z-alpha  is: -35.294 \n\n         aux. Z statistics\nZ-tau-mu            0.0608\n\n\n\n\n\nThe results of the Phillips-Perron unit root test indicate strong evidence against the null hypothesis of a unit root, as the p-value for the coefficient of the lagged variable is less than the significance level of 0.05. This suggests that the variable y, which is being tested for stationarity, is likely stationary. Furthermore, the test statistic Z-tau-mu is 0.0608, which is smaller than the critical value of Z-alpha (-35.294), providing further evidence of stationarity.\nTo determine whether the linear model requires an ARCH model, an ARCH test is conducted. The ACF and PACF plots are also used to identify suitable model values.\n\n\nModel Fitting\n\nPlotARCH TestACF PlotPACF Plot\n\n\n\n\nCode\nnormalized_numeric_df$XOM.Adjusted<-ts(normalized_numeric_df$XOM.Adjusted,star=decimal_date(as.Date(\"2010-01-01\",format = \"%Y-%m-%d\")),frequency = 4)\nnormalized_numeric_df$inflation<-ts(normalized_numeric_df$inflation,star=decimal_date(as.Date(\"2010-01-01\",format = \"%Y-%m-%d\")),frequency = 4)\nnormalized_numeric_df$unemployment<-ts(normalized_numeric_df$unemployment,star=decimal_date(as.Date(\"2010-01-01\",format = \"%Y-%m-%d\")),frequency = 4)\n\nfit <- lm(XOM.Adjusted ~ inflation+unemployment, data=normalized_numeric_df)\nfit.res<-ts(residuals(fit),star=decimal_date(as.Date(\"2010-01-01\",format = \"%Y-%m-%d\")),frequency = 4)\n############## Then look at the residuals ############\nreturns <- fit.res  %>% diff()\nautoplot(returns)+ggtitle(\"Linear Model Returns\")\n\n\n\n\n\n\n\n\n\nCode\nbyd.archTest <- ArchTest(fit.res, lags = 1, demean = TRUE)\nbyd.archTest\n\n\n\n    ARCH LM-test; Null hypothesis: no ARCH effects\n\ndata:  fit.res\nChi-squared = 6.3634, df = 1, p-value = 0.01165\n\n\n\n\n\n\nCode\nggAcf(returns) +ggtitle(\"ACF for returns\")\n\n\n\n\n\n\n\n\n\nCode\nggPacf(returns) +ggtitle(\"PACF for returns\")\n\n\n\n\n\n\n\n\nThe ARCH LM-test was conducted with the null hypothesis of no ARCH effects. The test resulted in a chi-squared value of 36.896 with one degree of freedom, and a very low p-value of 0.01165. This suggests strong evidence against the null hypothesis, indicating the presence of ARCH effects in the data.\nBased on the ACF and PACF plots, it appears that there is some significant autocorrelation and partial autocorrelation at multiple lags, which suggests that an ARIMA model may not be sufficient to capture the time series behavior. Additionally, the values for p and q appear to be relatively high, with p = 0 and q = 0 being suggested by the plots.\n\n\nARIMAX Model\n\nAuto Arima ModelAuto Arima Residuals\n\n\n\n\nCode\nxreg <- cbind(Inflation = normalized_data_ts[, \"inflation\"],\n              Unemployment = normalized_data_ts[, \"unemployment\"])\nfit.auto <- auto.arima(normalized_data_ts[, \"XOM.Adjusted\"], xreg = xreg)\nsummary(fit.auto)\n\n\nSeries: normalized_data_ts[, \"XOM.Adjusted\"] \nRegression with ARIMA(0,1,0) errors \n\nCoefficients:\n      Inflation  Unemployment\n         0.4785       -0.3455\ns.e.     0.2235        0.1014\n\nsigma^2 = 0.2642:  log likelihood = -37.4\nAIC=80.8   AICc=81.32   BIC=86.6\n\nTraining set error measures:\n                     ME      RMSE       MAE      MPE     MAPE      MASE\nTraining set 0.02751447 0.4989419 0.3970164 40.31885 217.5686 0.4290686\n                   ACF1\nTraining set -0.1780107\n\n\n\n\n\n\nCode\ncheckresiduals(fit.auto)\n\n\n\n\n\n\n    Ljung-Box test\n\ndata:  Residuals from Regression with ARIMA(0,1,0) errors\nQ* = 5.972, df = 8, p-value = 0.6504\n\nModel df: 0.   Total lags used: 8\n\n\n\n\n\nBased on the results of the auto.arima function, the suggested best model is ARIMA(0,1,0) which is the same as the manual choosen arima model. So, we can then proceed to choose the best GARCH model using ARIMA(0,1,0) as the base model.\nWe can then proceed to choose the best GARCH model using ARIMA(0,1,0) as the base model.\n\n\nSquared Residuals\n\nPlotACF PlotPACF PlotGRACH(1,1)\n\n\n\n\nCode\nfit <- lm(XOM.Adjusted ~ inflation+unemployment, data=normalized_numeric_df)\nfit.res<-ts(residuals(fit),star=decimal_date(as.Date(\"2010-01-01\",format = \"%Y-%m-%d\")),frequency = 4)\nfit <- Arima(fit.res,order=c(0,1,0))\nres=fit$res\nplot(res^2,main='Squared Residuals')\n\n\n\n\n\n\n\n\n\nCode\nacf(res^2,24, main = \"ACF Residuals Square\")\n\n\n\n\n\n\n\n\n\nCode\npacf(res^2,24, main = \"PACF Residuals Square\")\n\n\n\n\n\n\n\n\n\nCode\nsummary(garchFit(~garch(1,1),res, trace=F))\n\n\n\nTitle:\n GARCH Modelling \n\nCall:\n garchFit(formula = ~garch(1, 1), data = res, trace = F) \n\nMean and Variance Equation:\n data ~ garch(1, 1)\n<environment: 0x10aaacf20>\n [data = res]\n\nConditional Distribution:\n norm \n\nCoefficient(s):\n       mu      omega     alpha1      beta1  \n0.0090957  0.0010266  0.0948422  0.9289556  \n\nStd. Errors:\n based on Hessian \n\nError Analysis:\n        Estimate  Std. Error  t value Pr(>|t|)    \nmu      0.009096    0.066163    0.137    0.891    \nomega   0.001027    0.025018    0.041    0.967    \nalpha1  0.094842    0.089034    1.065    0.287    \nbeta1   0.928956    0.128327    7.239 4.52e-13 ***\n---\nSignif. codes:  0 '***' 0.001 '**' 0.01 '*' 0.05 '.' 0.1 ' ' 1\n\nLog Likelihood:\n -38.1069    normalized:  -0.7328249 \n\nDescription:\n Tue Jan  9 21:00:15 2024 by user:  \n\n\nStandardised Residuals Tests:\n                                Statistic p-Value  \n Jarque-Bera Test   R    Chi^2  1.133828  0.5672733\n Shapiro-Wilk Test  R    W      0.9863453 0.8107262\n Ljung-Box Test     R    Q(10)  6.442417  0.7768255\n Ljung-Box Test     R    Q(15)  10.96014  0.7554148\n Ljung-Box Test     R    Q(20)  13.75258  0.8428156\n Ljung-Box Test     R^2  Q(10)  11.8193   0.2973321\n Ljung-Box Test     R^2  Q(15)  13.76539  0.5433921\n Ljung-Box Test     R^2  Q(20)  16.25387  0.7007548\n LM Arch Test       R    TR^2   13.29826  0.3477401\n\nInformation Criterion Statistics:\n     AIC      BIC      SIC     HQIC \n1.619496 1.769592 1.608751 1.677039 \n\n\n\n\n\nFrom the squared residuals of the best ARIMA model, it can be observed that the ACF plot and PACF plot indicate that the residuals are not autocorrelated and are white noise, indicating a good fit of the model. Based on the squared residuals of the best ARIMA model, we can see that the ACF and PACF plots indicate that most of the values lie between the blue lines. Additionally, the p-value is 1 and q-value is 1. This suggests that the model has a good fit and that there is no significant autocorrelation or partial autocorrelation in the residuals.\nThe bst model is ARIMA(0,1,0) and GARCH(1,1). #### Best Model\n\nARIMA ModelGARCH ModelVolatility\n\n\n\n\nCode\n#fiting an ARIMA model to the Inflation variable\ninflation_fit<-auto.arima(normalized_numeric_df$inflation) \nfinflation<-forecast(inflation_fit)\n\n#fitting an ARIMA model to the Unemployment variable\nunemployment_fit<-auto.arima(normalized_numeric_df$unemployment) \nfunemployment<-forecast(unemployment_fit)\n\n# best model fit for forcasting\nxreg <- cbind(Inflation = normalized_data_ts[, \"inflation\"],\n              Unemployment = normalized_data_ts[, \"unemployment\"])\n\n\n\nsummary(arima.fit<-Arima(normalized_data_ts[, \"XOM.Adjusted\"],order=c(0,1,0),xreg=xreg),include.drift = TRUE)\n\n\nSeries: normalized_data_ts[, \"XOM.Adjusted\"] \nRegression with ARIMA(0,1,0) errors \n\nCoefficients:\n      Inflation  Unemployment\n         0.4785       -0.3455\ns.e.     0.2235        0.1014\n\nsigma^2 = 0.2642:  log likelihood = -37.4\nAIC=80.8   AICc=81.32   BIC=86.6\n\nTraining set error measures:\n                     ME      RMSE       MAE      MPE     MAPE      MASE\nTraining set 0.02751447 0.4989419 0.3970164 40.31885 217.5686 0.4290686\n                   ACF1\nTraining set -0.1780107\n\n\n\n\n\n\nCode\nsummary(final.fit <- garchFit(~garch(1,1), res,trace = F))\n\n\n\nTitle:\n GARCH Modelling \n\nCall:\n garchFit(formula = ~garch(1, 1), data = res, trace = F) \n\nMean and Variance Equation:\n data ~ garch(1, 1)\n<environment: 0x12ea4fb90>\n [data = res]\n\nConditional Distribution:\n norm \n\nCoefficient(s):\n       mu      omega     alpha1      beta1  \n0.0090957  0.0010266  0.0948422  0.9289556  \n\nStd. Errors:\n based on Hessian \n\nError Analysis:\n        Estimate  Std. Error  t value Pr(>|t|)    \nmu      0.009096    0.066163    0.137    0.891    \nomega   0.001027    0.025018    0.041    0.967    \nalpha1  0.094842    0.089034    1.065    0.287    \nbeta1   0.928956    0.128327    7.239 4.52e-13 ***\n---\nSignif. codes:  0 '***' 0.001 '**' 0.01 '*' 0.05 '.' 0.1 ' ' 1\n\nLog Likelihood:\n -38.1069    normalized:  -0.7328249 \n\nDescription:\n Tue Jan  9 21:00:15 2024 by user:  \n\n\nStandardised Residuals Tests:\n                                Statistic p-Value  \n Jarque-Bera Test   R    Chi^2  1.133828  0.5672733\n Shapiro-Wilk Test  R    W      0.9863453 0.8107262\n Ljung-Box Test     R    Q(10)  6.442417  0.7768255\n Ljung-Box Test     R    Q(15)  10.96014  0.7554148\n Ljung-Box Test     R    Q(20)  13.75258  0.8428156\n Ljung-Box Test     R^2  Q(10)  11.8193   0.2973321\n Ljung-Box Test     R^2  Q(15)  13.76539  0.5433921\n Ljung-Box Test     R^2  Q(20)  16.25387  0.7007548\n LM Arch Test       R    TR^2   13.29826  0.3477401\n\nInformation Criterion Statistics:\n     AIC      BIC      SIC     HQIC \n1.619496 1.769592 1.608751 1.677039 \n\n\n\n\n\n\nCode\nht <- final.fit@h.t #a numeric vector with the conditional variances (h.t = sigma.t^delta)\n\n#############################\ndata=data.frame(final)\ndata$Date<-as.Date(data$Date,\"%Y-%m-%d\")\n\n\ndata2= data.frame(ht,data$Date)\nggplot(data2, aes(y = ht, x = data.Date)) + geom_line(col = '#F9E79F') + ylab('Conditional Variance') + xlab('Date')\n\n\n\n\n\n\n\n\nFrom the ARIMA(0,1,0), we see that the training set error measures also suggest a good fit, with low mean absolute error, root mean squared error, and autocorrelation of the residuals. GATCH(1,1) model model is used to estimate the volatility of the standardized residuals of the previous regression model. The model includes a mean equation that estimates the mean of the residuals and a variance equation that models the conditional variance of the residuals. The coefficients of the mean equation suggest that the mean of the residuals is close to zero. The variance equation coefficients suggest that the conditional variance of the residuals is dependent on the past conditional variances and the past squared standardized residuals. The model’s the AIC, BIC, SIC, and HQIC values are all relatively low, indicating a good fit of the model. The standardized residuals tests indicate that the residuals are approximately normally distributed and that there is no significant autocorrelation in the residuals.\nThe volatility of the model seems high in 2020 but has decreased gradually in the past few months. This could indicate that the asset’s price was experiencing a lot of fluctuations in 2020, but the market has stabilized recently.\n\n\nModel Diagnostics\n\nResidualsQQ PlotBox Test\n\n\n\n\nCode\nfit2<-garch(res,order=c(1,1),trace=F)\ncheckresiduals(fit2) \n\n\nWarning in modeldf.default(object): Could not find appropriate degrees of\nfreedom for this model.\n\n\n\n\n\n\n\n\n\nCode\nqqnorm(fit2$residuals, pch = 1)\nqqline(fit2$residuals, col = \"blue\", lwd = 2)\n\n\n\n\n\n\n\n\n\nCode\nBox.test (fit2$residuals, type = \"Ljung\")\n\n\n\n    Box-Ljung test\n\ndata:  fit2$residuals\nX-squared = 0.88262, df = 1, p-value = 0.3475\n\n\nThe ACF plot of the residuals shows all the values between the blue lines, which indicates that the residuals are not significantly autocorrelated. The range of values for the residual plot between -2 and 2 is considered acceptable. Additionally, the QQ plot of the residuals shows a linear plot on the line, which is another good indication that the residuals are normally distributed. The QQ plot is a valuable tool to assess if the residuals follow a normal distribution, and in this case, the plot suggests that the residuals do indeed follow a normal distribution.\nThe Box-Ljung test, a p-value of 0.3475 indicates that the model’s residuals are not significantly autocorrelated, meaning that the model has captured most of the information in the data. This result is good because it suggests that the model is a good fit for the data and has accounted for most of the underlying patterns in the data. Therefore, we can rely on the model’s predictions and use them to make informed decisions.\n\n\n\n\n\nForecast\n\n\nCode\npredict(final.fit, n.ahead = 5, plot=TRUE)\n\n\n\n\n\n  meanForecast meanError standardDeviation lowerInterval upperInterval\n1  0.009095674 0.7486581         0.7486581     -1.458247      1.476439\n2  0.009095674 0.7581913         0.7581913     -1.476932      1.495123\n3  0.009095674 0.7678288         0.7678288     -1.495821      1.514012\n4  0.009095674 0.7775718         0.7775718     -1.514917      1.533108\n5  0.009095674 0.7874219         0.7874219     -1.534223      1.552414\n\n\nThe forecasted plot is based on the best model ARIMAX(0,1,0)+GARCH(1,1). This model takes into account the autoregressive and moving average components of the data, as well as the impact off exogenous variables on the time series. Additionally, the GARCH component of the model accounts for the volatility clustering in the data. Overall, this model is well-suited to make accurate predictions about future values of the time series.\n\n\nEquation of the Model\nThe equation of the ARIMAX(0,1,0) model is:\n\\(Y(t) = Y(t-1) + \\epsilon(t)\\)\nwhere \\(Y(t)\\) is the time series variable and \\(\\epsilon(t)\\) is the error term.\nThe equation of the GARCH(1,1) model is:\n\\(\\sigma^2(t) = \\alpha_0+\\alpha_1\\epsilon_t^2(t-1)+ \\beta_1\\sigma^2(t-1)\\)\nwhere \\(\\sigma^2_t\\) is the conditional variance at time \\(t\\), \\(\\alpha_0\\) is a constant, \\(\\alpha_1\\) and \\(\\beta_1\\) are the parameters, and \\(\\epsilon_t\\) is the error term.\nThe combined equation of the ARIMAX(0,1,0)+GARCH(1,1) model is:\n\\(Y(t) = Y(t-1) + \\epsilon(t)\\)\n\\(\\epsilon(t) = \\sigma(t) * \\epsilon~(t)\\)\n\\(\\sigma^2(t) = \\alpha_0+\\alpha_1\\epsilon_t^2(t-1)+ \\beta_1\\sigma^2(t-1)\\)"
  },
  {
    "objectID": "arch_Financial_Sector_Fund.html",
    "href": "arch_Financial_Sector_Fund.html",
    "title": "",
    "section": "",
    "text": "Impact of Macroeconomic Factors on JPMorgan Chase & Co. Stock Price\n\n\n\nJPMorgan Chase & Co. is a multinational investment bank and financial services company that provides a range of services such as commercial banking, investment banking, asset management, and private banking.\nTo analyze the stock price behavior of JPMorgan Chase & Co., an ARIMAX+ARCH/GARCH model can also be employed. This type of model can help to identify how macroeconomic factors such as interest rates, inflation, and GDP growth rate may affect the company’s performance.\n\n\nTime series Plot\n\nJPMorgan Chase & Co.Differentitaed Chart SeriesGDP GrowthInflationInterestUnemployment\n\n\n\n\nCode\n# get data\noptions(\"getSymbols.warning4.0\"=FALSE)\noptions(\"getSymbols.yahoo.warning\"=FALSE)\n\n\ndata.info = getSymbols(\"JPM\",src='yahoo', from = '2010-01-01',to = \"2023-03-01\",auto.assign = FALSE)\ndata = getSymbols(\"JPM\",src='yahoo', from = '2010-01-01',to = \"2023-03-01\")\ndf <- data.frame(Date=index(JPM),coredata(JPM))\n\n# create Bollinger Bands\nbbands <- BBands(JPM[,c(\"JPM.High\",\"JPM.Low\",\"JPM.Close\")])\n\n# join and subset data\ndf <- subset(cbind(df, data.frame(bbands[,1:3])), Date >= \"2010-01-01\")\n\n\n# colors column for increasing and decreasing\nfor (i in 1:length(df[,1])) {\n  if (df$JPM.Close[i] >= df$JPM.Open[i]) {\n      df$direction[i] = 'Increasing'\n  } else {\n      df$direction[i] = 'Decreasing'\n  }\n}\n\ni <- list(line = list(color = '#F1948A'))\nd <- list(line = list(color = '#7F7F7F'))\n\n# plot candlestick chart\n\nfig <- df %>% plot_ly(x = ~Date, type=\"candlestick\",\n          open = ~JPM.Open, close = ~JPM.Close,\n          high = ~JPM.High, low = ~JPM.Low, name = \"JPM\",\n          increasing = i, decreasing = d) \nfig <- fig %>% add_lines(x = ~Date, y = ~up , name = \"B Bands\",\n            line = list(color = '#ccc', width = 0.5),\n            legendgroup = \"Bollinger Bands\",\n            hoverinfo = \"none\", inherit = F) \nfig <- fig %>% add_lines(x = ~Date, y = ~dn, name = \"B Bands\",\n            line = list(color = '#ccc', width = 0.5),\n            legendgroup = \"Bollinger Bands\", inherit = F,\n            showlegend = FALSE, hoverinfo = \"none\") \nfig <- fig %>% add_lines(x = ~Date, y = ~mavg, name = \"Mv Avg\",\n            line = list(color = '#C052B3', width = 0.5),\n            hoverinfo = \"none\", inherit = F) \nfig <- fig %>% layout(yaxis = list(title = \"Price\"))\n\n# plot volume bar chart\nfig2 <- df \nfig2 <- fig2 %>% plot_ly(x=~Date, y=~JPM.Volume, type='bar', name = \"JPM Volume\",\n          color = ~direction, colors = c('#F1948A','#7F7F7F')) \nfig2 <- fig2 %>% layout(yaxis = list(title = \"Volume\"))\n\n# create rangeselector buttons\nrs <- list(visible = TRUE, x = 0.5, y = -0.055,\n           xanchor = 'center', yref = 'paper',\n           font = list(size = 9),\n           buttons = list(\n             list(count=1,\n                  label='RESET',\n                  step='all'),\n             list(count=3,\n                  label='3 YR',\n                  step='year',\n                  stepmode='backward'),\n             list(count=1,\n                  label='1 YR',\n                  step='year',\n                  stepmode='backward'),\n             list(count=1,\n                  label='1 MO',\n                  step='month',\n                  stepmode='backward')\n           ))\n\n# subplot with shared x axis\nfig <- subplot(fig, fig2, heights = c(0.7,0.2), nrows=2,\n             shareX = TRUE, titleY = TRUE)\nfig <- fig %>% layout(title = paste(\"JPMorgan Chase & Co.  Stock Price: January 2010 - March 2023\"),\n         xaxis = list(rangeselector = rs),\n         legend = list(orientation = 'h', x = 0.5, y = 1,\n                       xanchor = 'center', yref = 'paper',\n                       font = list(size = 10),\n                       bgcolor = 'transparent'))\n\nfig\n\n\n\n\n\n\n\n\n\n\nCode\nlog(data.info$`JPM.Adjusted`) %>% diff() %>% chartSeries(theme=chartTheme('white'),up.col='#F1948A')\n\n\n\n\n\n\n\n\n\nCode\n#import the data\ngdp <- read.csv(\"DATA/RAW DATA/gdp-growth.csv\")\n\n#change date format\ngdp$Date <- as.Date(gdp$DATE , \"%m/%d/%Y\")\n\n#drop DATE column\ngdp <- subset(gdp, select = -c(1))\n\n#export the cleaned data\ngdp_clean <- gdp\nwrite.csv(gdp_clean, \"DATA/CLEANED DATA/gdp_clean_data.csv\", row.names=FALSE)\n\n#plot gdp growth rate \nfig <- plot_ly(gdp, x = ~Date, y = ~value, type = 'scatter', mode = 'lines',line = list(color = 'rgb(240, 128, 128)'))\nfig <- fig %>% layout(title = \"U.S GPD Growth Rate: 2010 - 2022\",xaxis = list(title = \"Time\"),yaxis = list(title =\"GDP Growth Rate\"))\nfig\n\n\n\n\n\n\n\n\n\n\nCode\n#import the data\ninflation_rate <- read.csv(\"DATA/RAW DATA/inflation-rate.csv\")\n\n#cleaning the data\n#remove unwanted columns\ninflation_rate_clean <- subset(inflation_rate, select = -c(1,HALF1,HALF2))\n\n#convert the data to time series data\ninflation_data_ts <- ts(as.vector(t(as.matrix(inflation_rate_clean))), start=c(2010,1), end=c(2023,2), frequency=12)\n\n#export the data\nwrite.csv(inflation_rate_clean, \"DATA/CLEANED DATA/inflation_rate_clean_data.csv\", row.names=FALSE)\n\n\n#plot inflation rate \nfig <- autoplot(inflation_data_ts, ylab = \"Inflation Rate\", color=\"#FFA07A\")+ggtitle(\"U.S Inflation Rate: January 2010 - February 2023\")+theme_bw()\nggplotly(fig)\n\n\n\n\n\n\n\n\n\n\nCode\n#import the data\ninterest_data <- read.csv(\"DATA/RAW DATA/interest-rate.csv\")\n\n#change date format\ninterest_data$Date <- as.Date(interest_data$Date , \"%m/%d/%Y\")\n\n#export the cleaned data\ninterest_clean_data <- interest_data\nwrite.csv(interest_clean_data, \"DATA/CLEANED DATA/interest_rate_clean_data.csv\", row.names=FALSE)\n\n#plot interest rate \nfig <- plot_ly(interest_data, x = ~Date, y = ~value, type = 'scatter', mode = 'lines',line = list(color='rgb(219, 112, 147)'))\nfig <- fig %>% layout(title = \"U.S Interest Rate: January 2010 - March 2023\",xaxis = list(title = \"Time\"),yaxis = list(title =\"Interest Rate\"))\nfig\n\n\n\n\n\n\n\n\n\n\nCode\n#import the data\nunemployment_rate <- read.csv(\"DATA/RAW DATA/unemployment-rate.csv\")\n\n#change date format\nunemployment_rate$Date <- as.Date(unemployment_rate$Date , \"%m/%d/%Y\")\n\n# export the data\nwrite.csv(unemployment_rate, \"DATA/CLEANED DATA/unemployment_rate_clean_data.csv\", row.names=FALSE)\n\n#plot unemployment rate \nfig <- plot_ly(unemployment_rate, x = ~Date, y = ~Value, type = 'scatter', mode = 'lines',line = list(color = 'rgb(189, 183, 107)'))\nfig <- fig %>% layout(title = \"U.S Unemployment Rate: January 2010 - March 2023\",xaxis = list(title = \"Time\"),yaxis = list(title =\"Unemployment Rate\"))\nfig\n\n\n\n\n\n\n\n\n\nOver the years, JPMorgan Chase & Co. has weathered various economic challenges, including the global financial crisis of 2008. Since then, the company has implemented various measures to strengthen its balance sheet and improve its risk management practices. As a result, JPMorgan Chase & Co. has been able to maintain its position as one of the most financially sound banks in the industry.\nFrom 2010 to 2014, JPMorgan Chase & Co.’s stock price steadily increased, reflecting the company’s strong financial performance and consistent dividend payouts. However, from 2014 to 2016, the company’s stock price experienced a decline, which could be attributed to a combination of factors such as slowing sales growth and increased competition in the consumer goods industry.\nFrom 2016 to 2018, JPMorgan Chase & Co.’s stock price began to recover, likely due to the company’s efforts to streamline its operations and focus on core brands. This trend continued into 2019, with JPMorgan Chase & Co.’s stock price reaching an all-time high in mid-2019.\nThe outbreak of the COVID-19 pandemic in early 2020 caused a brief dip in JPMorgan Chase & Co.’s stock price, as investors were uncertain about the impact of the pandemic on the company’s operations and financial performance. However, the company’s strong position in the banking industry and its ability to adapt to changing market conditions helped it to quickly rebound and continue its growth trend throughout 2020 and into early 2021.\nSince early 2021, JPMorgan Chase & Co.’s stock price has experienced some volatility, likely due to a combination of factors such as global economic uncertainty and fluctuations in consumer demand for the company’s products and services.\nAs discussed before, the macroeconomic factors of GDP growth, inflation, interest rates, and unemployment rate are closely interrelated and play a crucial role in the overall health and stability of an economy. From 2010 to 2023, the global economy experienced a mix of ups and downs, with periods of strong GDP growth followed by slowdowns and recessions.\nThe second plot shows the first difference of the logarithm of the adjusted JPMorgan Chase & Co. stock price. Taking the first difference removes any long-term trends and transforms the time series into a stationary process. From the plot, we can observe that the first difference of the logarithm of the JPMorgan Chase & Co. stock price appears to be stationary, as the mean and variance are roughly constant over time.\n\n\nEnodogenous and Exogenous Variables\n\nPlotCorrelation HeatmapCCF GDPCCF InterestCCF InflationCCF Unemployment\n\n\n\n\nCode\nnumeric_data <- c(\"JPM.Adjusted\",\"gdp\", \"interest\", \"inflation\", \"unemployment\")\nnumeric_data <- final[, numeric_data]\nnormalized_data_numeric <- scale(numeric_data)\nnormalized_data <- ts(normalized_data_numeric, start = c(2010, 1), end = c(2021,10),frequency = 4)\nts_plot(normalized_data,\n        title = \"Normalized Time Series Data for JPM Stock and Macroeconomic Variables\",\n        Ytitle = \"Normalized Values\",\n        Xtitle = \"Year\")\n\n\n\n\n\n\n\n\n\n\nCode\n# Get upper triangle of the correlation matrix\nget_upper_tri <- function(cormat){\n    cormat[lower.tri(cormat)]<- NA\n    return(cormat)\n}\ncormat <- round(cor(normalized_data_numeric),2)\n\nupper_tri <- get_upper_tri(cormat)\n\nmelted_cormat <- melt(upper_tri, na.rm = TRUE)\n# Create a ggheatmap\nggheatmap <- ggplot(melted_cormat, aes(Var2, Var1, fill = value))+\n geom_tile(color = \"white\")+\n scale_fill_gradient2(low = \"blue\", high = \"red\", mid = \"white\", \n   midpoint = 0, limit = c(-1,1), space = \"Lab\", \n    name=\"Pearson\\nCorrelation\") +\n  theme_minimal()+ # minimal theme\n theme(axis.text.x = element_text(angle = 45, vjust = 1, \n    size = 12, hjust = 1))+\n coord_fixed()\n\nggheatmap + \ngeom_text(aes(Var2, Var1, label = value), color = \"black\", size = 4) +\ntheme(\n  axis.title.x = element_blank(),\n  axis.title.y = element_blank(),\n  panel.grid.major = element_blank(),\n  panel.border = element_blank(),\n  panel.background = element_blank(),\n  axis.ticks = element_blank(),\n  legend.justification = c(1, 0),\n  legend.position = c(0.6, 0.7),\n  legend.direction = \"horizontal\")+\n  guides(fill = guide_colorbar(barwidth = 7, barheight = 1,\n                title.position = \"top\", title.hjust = 0.5))\n\n\n\n\n\n\n\n\n\nCode\npar(mfrow=c(1,1))\nccf_result <- ccf(normalized_data[, c(\"JPM.Adjusted\")], normalized_data[, c(\"gdp\")], \n    lag.max = 300,\n    main = \"Cros-Correlation Plot for JPM Stock Price and GDP Growth Rate \",\n    ylab = \"CCF\")\n\n\n\n\n\nCode\ncat(\"The sum of cross correlation function is\", sum(abs(ccf_result$acf)))\n\n\nThe sum of cross correlation function is 5.286491\n\n\n\n\n\n\nCode\npar(mfrow=c(1,1))\nccf_result <- ccf(normalized_data[, c(\"JPM.Adjusted\")], normalized_data[, c(\"interest\")], \n    lag.max = 300,\n    main = \"Cros-Correlation Plot for JPM Stock Price and Interest Rate\",\n    ylab = \"CCF\")\n\n\n\n\n\nCode\ncat(\"The sum of cross correlation function is\", sum(abs(ccf_result$acf)))\n\n\nThe sum of cross correlation function is 10.89197\n\n\n\n\n\n\nCode\npar(mfrow=c(1,1))\nccf_result <- ccf(normalized_data[, c(\"JPM.Adjusted\")], normalized_data[, c(\"inflation\")], \n    lag.max = 300,\n    main = \"Cros-Correlation Plot for JPM Stock Price and Inflation Rate\",\n    ylab = \"CCF\")\n\n\n\n\n\nCode\ncat(\"The sum of cross correlation function is\", sum(abs(ccf_result$acf)))\n\n\nThe sum of cross correlation function is 17.12742\n\n\n\n\n\n\nCode\npar(mfrow=c(1,1))\nccf_result <- ccf(normalized_data[, c(\"JPM.Adjusted\")], normalized_data[, c(\"unemployment\")], \n    lag.max = 300,\n    main = \"Cros-Correlation Plot for JPM Stock Priceand Unemployment Rate\",\n    ylab = \"CCF\")\n\n\n\n\n\nCode\ncat(\"The sum of cross correlation function is\", sum(abs(ccf_result$acf)))\n\n\nThe sum of cross correlation function is 20.36533\n\n\n\n\n\nThe Normalized Time Series Data for Stock Price and Macroeconomic Variables plot shows the same variables as the first plot but has been normalized to a common range of 0 to 1 using the scale() function in R, which standardizes the variables to have a mean of 0 and a standard deviation of 1. The heatmap analysis of the normalized data reveals that inflation and unemployment rate exhibit strong positive correlations with the stock price indices, indicating that these variables may significantly influence stock price movements. On the other hand, weaker correlations were observed between the stock price indices and GDP and interest rates, suggesting that these variables may have less impact on stock price fluctuations. The cross-correlation feature plots confirm these findings, indicating that inflation and unemployment rate are more suitable feature variables for the ARIMAX model when predicting JPMorgan Chase & Co. movements.\nFinal Exogenous variables: Macroeconomic indicators: Inflation rate and unemployment rate.\n\n\nEnodogenous and Exogenous Variables Plot\n\nPlotCheck the stationarity\n\n\n\n\nCode\nfinal_data <- final %>%dplyr::select( Date,JPM.Adjusted, inflation,unemployment)\nnumeric_data <- c(\"JPM.Adjusted\", \"inflation\",\"unemployment\")\nnumeric_data <- final_data[, numeric_data]\nnormalized_data_numeric <- scale(numeric_data)\nnormalized_numeric_df <- data.frame(normalized_data_numeric)\nnormalized_data_ts <- ts(normalized_data_numeric, start = c(2010, 1), frequency = 4)\n\nautoplot(normalized_data_ts, facets=TRUE) +\n  xlab(\"Year\") + ylab(\"\") +\n  ggtitle(\"JPMorgan Chase & Co.  Stock Price, Inflation Rate and Unemployment Rate in USA 2010-2023\")\n\n\n\n\n\n\n\n\n\nCode\n# Convert your multivariate time series data to a matrix\nfinal_data_ts_multivariate <- as.matrix(normalized_data_ts)\n\n# Check for stationarity using Phillips-Perron test\nphillips_perron_test <- ur.pp(final_data_ts_multivariate)  \nsummary(phillips_perron_test)\n\n\n\n################################## \n# Phillips-Perron Unit Root Test # \n################################## \n\nTest regression with intercept \n\n\nCall:\nlm(formula = y ~ y.l1)\n\nResiduals:\n    Min      1Q  Median      3Q     Max \n-1.3694 -0.1559 -0.0528  0.0968  4.4945 \n\nCoefficients:\n             Estimate Std. Error t value Pr(>|t|)    \n(Intercept) 0.0007685  0.0411891   0.019    0.985    \ny.l1        0.8576417  0.0416064  20.613   <2e-16 ***\n---\nSignif. codes:  0 '***' 0.001 '**' 0.01 '*' 0.05 '.' 0.1 ' ' 1\n\nResidual standard error: 0.5128 on 153 degrees of freedom\nMultiple R-squared:  0.7353,    Adjusted R-squared:  0.7335 \nF-statistic: 424.9 on 1 and 153 DF,  p-value: < 2.2e-16\n\n\nValue of test-statistic, type: Z-alpha  is: -23.1811 \n\n         aux. Z statistics\nZ-tau-mu            0.0193\n\n\n\n\n\nThe results of the Phillips-Perron unit root test indicate strong evidence against the null hypothesis of a unit root, as the p-value for the coefficient of the lagged variable is less than the significance level of 0.05. This suggests that the variable y, which is being tested for stationarity, is likely stationary. Furthermore, the test statistic Z-tau-mu is 0.0193, which is smaller than the critical value of Z-alpha (-23.1811), providing further evidence of stationarity.\nTo determine whether the linear model requires an ARCH model, an ARCH test is conducted. The ACF and PACF plots are also used to identify suitable model values.\n\n\nModel Fitting\n\nPlotARCH TestACF PlotPACF Plot\n\n\n\n\nCode\nnormalized_numeric_df$JPM.Adjusted<-ts(normalized_numeric_df$JPM.Adjusted,star=decimal_date(as.Date(\"2010-01-01\",format = \"%Y-%m-%d\")),frequency = 4)\nnormalized_numeric_df$inflation<-ts(normalized_numeric_df$inflation,star=decimal_date(as.Date(\"2010-01-01\",format = \"%Y-%m-%d\")),frequency = 4)\nnormalized_numeric_df$unemployment<-ts(normalized_numeric_df$unemployment,star=decimal_date(as.Date(\"2010-01-01\",format = \"%Y-%m-%d\")),frequency = 4)\n\nfit <- lm(JPM.Adjusted ~ inflation+unemployment, data=normalized_numeric_df)\nfit.res<-ts(residuals(fit),star=decimal_date(as.Date(\"2010-01-01\",format = \"%Y-%m-%d\")),frequency = 4)\n############## Then look at the residuals ############\nreturns <- fit.res  %>% diff()\nautoplot(returns)+ggtitle(\"Linear Model Returns\")\n\n\n\n\n\n\n\n\n\nCode\nbyd.archTest <- ArchTest(fit.res, lags = 1, demean = TRUE)\nbyd.archTest\n\n\n\n    ARCH LM-test; Null hypothesis: no ARCH effects\n\ndata:  fit.res\nChi-squared = 25.657, df = 1, p-value = 4.077e-07\n\n\n\n\n\n\nCode\nggAcf(returns) +ggtitle(\"ACF for returns\")\n\n\n\n\n\n\n\n\n\nCode\nggPacf(returns) +ggtitle(\"PACF for returns\")\n\n\n\n\n\n\n\n\nThe ARCH LM-test was conducted with the null hypothesis of no ARCH effects. The test resulted in a chi-squared value of 25.657 with one degree of freedom, and a very low p-value of 4.077e-07. This suggests strong evidence against the null hypothesis, indicating the presence of ARCH effects in the data.\nBased on the ACF and PACF plots, it appears that there is some significant autocorrelation and partial autocorrelation at multiple lags, which suggests that an ARIMA model may not be sufficient to capture the time series behavior. Additionally, the values for p and q appear to be relatively high, with p = 3 and q = 3 being suggested by the plots.\n\n\nARIMAX Model\n\nAuto Arima ModelAuto Arima ResidualsARIMAX ModelARIMA (1,1,1) PlotARIMA (1,1,1) ModelCross Validation\n\n\n\n\nCode\nxreg <- cbind(Inflation = normalized_data_ts[, \"inflation\"],\n              Unemployment = normalized_data_ts[, \"unemployment\"])\nfit.auto <- auto.arima(normalized_data_ts[, \"JPM.Adjusted\"], xreg = xreg)\nsummary(fit.auto)\n\n\nSeries: normalized_data_ts[, \"JPM.Adjusted\"] \nRegression with ARIMA(0,1,0) errors \n\nCoefficients:\n      Inflation  Unemployment\n         0.1048       -0.2416\ns.e.     0.1070        0.0486\n\nsigma^2 = 0.06062:  log likelihood = 0.14\nAIC=5.73   AICc=6.24   BIC=11.52\n\nTraining set error measures:\n                     ME      RMSE       MAE      MPE     MAPE      MASE\nTraining set 0.01856899 0.2389951 0.1664957 7.224058 29.02617 0.4044533\n                  ACF1\nTraining set 0.2273719\n\n\n\n\n\n\nCode\ncheckresiduals(fit.auto)\n\n\n\n\n\n\n    Ljung-Box test\n\ndata:  Residuals from Regression with ARIMA(0,1,0) errors\nQ* = 7.2584, df = 8, p-value = 0.509\n\nModel df: 0.   Total lags used: 8\n\n\n\n\n\n\nCode\nARIMA.c=function(p1,p2,q1,q2,data){\ntemp=c()\nd=1\ni=1\ntemp= data.frame()\nls=matrix(rep(NA,6*30),nrow=30)\n\n\nfor (p in p1:p2)#\n{\n  for(q in q1:q2)#\n  {\n    for(d in 0:1)\n    {\n      \n      if(p+d+q<=6)\n      {\n        \n        model<- Arima(data,order=c(p,d,q))\n        ls[i,]= c(p,d,q,model$aic,model$bic,model$aicc)\n        i=i+1\n        #print(i)\n        \n      }\n      \n    }\n  }\n}\n\n\ntemp= as.data.frame(ls)\nnames(temp)= c(\"p\",\"d\",\"q\",\"AIC\",\"BIC\",\"AICc\")\n\ntemp\n}\n\noutput <- ARIMA.c(1,3,1,3,data=residuals(fit))\n\noutput[which.min(output$AIC),] \n\n\n  p d q      AIC      BIC     AICc\n2 1 1 1 20.18394 25.97942 20.69458\n\n\nCode\noutput[which.min(output$BIC),]\n\n\n  p d q      AIC      BIC     AICc\n2 1 1 1 20.18394 25.97942 20.69458\n\n\nCode\noutput[which.min(output$AICc),]\n\n\n  p d q      AIC      BIC     AICc\n2 1 1 1 20.18394 25.97942 20.69458\n\n\n\n\n\n\nCode\nset.seed(1234)\n\nmodel_output <- capture.output(sarima(fit.res, 1,1,1)) \n\n\n\n\n\n\n\n\n\nCode\ncat(model_output[39:69], model_output[length(model_output)], sep = \"\\n\")\n\n\n$fit\n\nCall:\narima(x = xdata, order = c(p, d, q), seasonal = list(order = c(P, D, Q), period = S), \n    xreg = constant, transform.pars = trans, fixed = fixed, optim.control = list(trace = trc, \n        REPORT = 1, reltol = tol))\n\nCoefficients:\n          ar1     ma1  constant\n      -0.5131  0.8051   -0.0107\ns.e.   0.3978  0.3186    0.0462\n\nsigma^2 estimated as 0.07681:  log likelihood = -7.07,  aic = 22.13\n\n$degrees_of_freedom\n[1] 48\n\n$ttable\n         Estimate     SE t.value p.value\nar1       -0.5131 0.3978 -1.2901  0.2032\nma1        0.8051 0.3186  2.5267  0.0149\nconstant  -0.0107 0.0462 -0.2321  0.8174\n\n$AIC\n[1] 0.4339237\n\n$AICc\n[1] 0.4439362\n\n$BIC\n[1] 0.5854395\n\n\n\n\n\n\nCode\nn=length(fit.res)\nk= 51\n \n \nrmse1 <- matrix(NA, (n-k),4)\nrmse2 <- matrix(NA, (n-k),4)\nrmse3 <- matrix(NA, (n-k),4)\n\n\nst <- tsp(fit.res)[1]+(k-5)/4 \n\nfor(i in 1:(n-k))\n{\n  xtrain <- window(fit.res, end=st + i/4)\n  xtest <- window(fit.res, start=st + (i+1)/4, end=st + (i+4)/4)\n  \n  #ARIMA(0,1,0) ARIMA(1,1,1)\n  \n  fit <- Arima(xtrain, order=c(0,1,0),\n                include.drift=TRUE, method=\"ML\")\n  fcast <- forecast(fit, h=4)\n  \n  fit2 <- Arima(xtrain, order=c(1,1,1),\n                include.drift=TRUE, method=\"ML\")\n  fcast2 <- forecast(fit2, h=4)\n\n\n  rmse1[i,1:length(xtest)]   <- sqrt((fcast$mean-xtest)^2)\n  rmse2[i,1:length(xtest)] <- sqrt((fcast2$mean-xtest)^2)\n}\n\nplot(1:4,colMeans(rmse1,na.rm=TRUE), type=\"l\",col=2, xlab=\"horizon\", ylab=\"RMSE\")\nlines(1:4, colMeans(rmse2,na.rm=TRUE), type=\"l\",col=3)\nlegend(\"topleft\",legend=c(\"fit1\",\"fit2\"),col=2:4,lty=1)\n\n\n\n\n\n\n\n\nBased on the results of the auto.arima function, the suggested best model is ARIMA(0,1,0). However, when we manually test different ARIMA models, we find that ARIMA(1,1,1) has the lowest values for AIC, BIC, and AICC. Additionally, both models have similar standardized residual plots, with means close to 0, indicating a good fit. The ACF plot of residuals also shows no significant lags, further indicating a well-fitted model.\nTo determine the best model, we conduct cross-validation and compare the RMSE values of both models. The results show that ARIMA(0,1,0) has lower RMSE values than ARIMA(1,1,1), indicating that it is the better model.\nWe can then proceed to choose the best GARCH model using ARIMA(0,1,0) as the base model.\n\n\nSquared Residuals\n\nPlotACF PlotPACF Plot\n\n\n\n\nCode\nfit <- lm(JPM.Adjusted ~ inflation+unemployment, data=normalized_numeric_df)\nfit.res<-ts(residuals(fit),star=decimal_date(as.Date(\"2010-01-01\",format = \"%Y-%m-%d\")),frequency = 4)\nfit <- Arima(fit.res,order=c(0,1,0))\nres=fit$res\nplot(res^2,main='Squared Residuals')\n\n\n\n\n\n\n\n\n\nCode\nacf(res^2,24, main = \"ACF Residuals Square\")\n\n\n\n\n\n\n\n\n\nCode\npacf(res^2,24, main = \"PACF Residuals Square\")\n\n\n\n\n\n\n\n\nFrom the squared residuals of the best ARIMA model, it can be observed that the ACF plot and PACF plot indicate that the residuals are not autocorrelated and are white noise, indicating a good fit of the model. Based on the squared residuals of the best ARIMA model, we can see that the ACF and PACF plots indicate that most of the values lie between the blue lines. Additionally, the p-value is 1 and q-value is 2. This suggests that the model has a good fit and that there is no significant autocorrelation or partial autocorrelation in the residuals. Now we can proceed by fitting GARCH Model for p and q values.\n\n\nGARCH Model\n\nModelGRACH(1,1)GRACH(1,2)\n\n\n\n\nCode\nmodel <- list() ## set counter\ncc <- 1\nfor (p in 1) {\n  for (q in 1:2) {\n  \nmodel[[cc]] <- garch(res,order=c(q,p),trace=F)\ncc <- cc + 1\n}\n} \n\n## get AIC values for model evaluation\nGARCH_AIC <- sapply(model, AIC) ## model with lowest AIC is the best\nwhich(GARCH_AIC == min(GARCH_AIC))\n\n\n[1] 1\n\n\nCode\nmodel[[which(GARCH_AIC == min(GARCH_AIC))]]\n\n\n\nCall:\ngarch(x = res, order = c(q, p), trace = F)\n\nCoefficient(s):\n      a0        a1        b1  \n0.001917  0.305848  0.736524  \n\n\n\n\n\n\nCode\nsummary(garchFit(~garch(1,1),res, trace=F))\n\n\n\nTitle:\n GARCH Modelling \n\nCall:\n garchFit(formula = ~garch(1, 1), data = res, trace = F) \n\nMean and Variance Equation:\n data ~ garch(1, 1)\n<environment: 0x1349a61f8>\n [data = res]\n\nConditional Distribution:\n norm \n\nCoefficient(s):\n       mu      omega     alpha1      beta1  \n0.0204567  0.0019945  0.3210359  0.7204177  \n\nStd. Errors:\n based on Hessian \n\nError Analysis:\n        Estimate  Std. Error  t value Pr(>|t|)    \nmu      0.020457    0.026942    0.759   0.4477    \nomega   0.001995    0.003465    0.576   0.5649    \nalpha1  0.321036    0.168345    1.907   0.0565 .  \nbeta1   0.720418    0.122860    5.864 4.53e-09 ***\n---\nSignif. codes:  0 '***' 0.001 '**' 0.01 '*' 0.05 '.' 0.1 ' ' 1\n\nLog Likelihood:\n -0.1441219    normalized:  -0.002771574 \n\nDescription:\n Tue Jan  9 21:01:30 2024 by user:  \n\n\nStandardised Residuals Tests:\n                                Statistic p-Value  \n Jarque-Bera Test   R    Chi^2  1.63773   0.4409319\n Shapiro-Wilk Test  R    W      0.9791929 0.4921099\n Ljung-Box Test     R    Q(10)  5.279386  0.8717505\n Ljung-Box Test     R    Q(15)  11.33345  0.7286181\n Ljung-Box Test     R    Q(20)  15.95037  0.7196983\n Ljung-Box Test     R^2  Q(10)  10.91973  0.3638056\n Ljung-Box Test     R^2  Q(15)  19.9126   0.1753112\n Ljung-Box Test     R^2  Q(20)  20.83641  0.406817 \n LM Arch Test       R    TR^2   15.33253  0.223753 \n\nInformation Criterion Statistics:\n      AIC       BIC       SIC      HQIC \n0.1593893 0.3094850 0.1486440 0.2169324 \n\n\n\n\n\n\nCode\nsummary(garchFit(~garch(1,2),res, trace=F))\n\n\n\nTitle:\n GARCH Modelling \n\nCall:\n garchFit(formula = ~garch(1, 2), data = res, trace = F) \n\nMean and Variance Equation:\n data ~ garch(1, 2)\n<environment: 0x155bb6000>\n [data = res]\n\nConditional Distribution:\n norm \n\nCoefficient(s):\n        mu       omega      alpha1       beta1       beta2  \n0.02043647  0.00220288  0.32580173  0.70965382  0.00000001  \n\nStd. Errors:\n based on Hessian \n\nError Analysis:\n        Estimate  Std. Error  t value Pr(>|t|)\nmu     2.044e-02   2.738e-02    0.746    0.455\nomega  2.203e-03   3.617e-03    0.609    0.542\nalpha1 3.258e-01   2.562e-01    1.272    0.203\nbeta1  7.097e-01   8.327e-01    0.852    0.394\nbeta2  1.000e-08   6.657e-01    0.000    1.000\n\nLog Likelihood:\n -0.2447927    normalized:  -0.004707552 \n\nDescription:\n Tue Jan  9 21:01:30 2024 by user:  \n\n\nStandardised Residuals Tests:\n                                Statistic p-Value  \n Jarque-Bera Test   R    Chi^2  1.745884  0.4177208\n Shapiro-Wilk Test  R    W      0.9787465 0.4740266\n Ljung-Box Test     R    Q(10)  5.342593  0.8671497\n Ljung-Box Test     R    Q(15)  11.45855  0.7194635\n Ljung-Box Test     R    Q(20)  15.86405  0.7250219\n Ljung-Box Test     R^2  Q(10)  11.02923  0.3552454\n Ljung-Box Test     R^2  Q(15)  20.21135  0.1639742\n Ljung-Box Test     R^2  Q(20)  21.07103  0.3929625\n LM Arch Test       R    TR^2   16.27217  0.1790856\n\nInformation Criterion Statistics:\n      AIC       BIC       SIC      HQIC \n0.2017228 0.3893424 0.1853058 0.2736517 \n\n\n\n\n\nBased on the analysis of the different GARCH models, it appears that GARCH(1,1) is the optimal choice. Although the AIC values of the different models are relatively similar, we can further evaluate their significance to make a final determination. Upon closer inspection, it appears that GARCH(1,1) has significantly better values than the other models, indicating that it is the most appropriate choice. Therefore, we can conclude that the GARCH(1,1) model is the best fit for the data.\n\n\nBest Model\n\nARIMA ModelGARCH ModelVolatility\n\n\n\n\nCode\n#fiting an ARIMA model to the Inflation variable\ninflation_fit<-auto.arima(normalized_numeric_df$inflation) \nfinflation<-forecast(inflation_fit)\n\n#fitting an ARIMA model to the Unemployment variable\nunemployment_fit<-auto.arima(normalized_numeric_df$unemployment) \nfunemployment<-forecast(unemployment_fit)\n\n# best model fit for forcasting\nxreg <- cbind(Inflation = normalized_data_ts[, \"inflation\"],\n              Unemployment = normalized_data_ts[, \"unemployment\"])\n\n\n\nsummary(arima.fit<-Arima(normalized_data_ts[, \"JPM.Adjusted\"],order=c(0,1,0),xreg=xreg),include.drift = TRUE)\n\n\nSeries: normalized_data_ts[, \"JPM.Adjusted\"] \nRegression with ARIMA(0,1,0) errors \n\nCoefficients:\n      Inflation  Unemployment\n         0.1048       -0.2416\ns.e.     0.1070        0.0486\n\nsigma^2 = 0.06062:  log likelihood = 0.14\nAIC=5.73   AICc=6.24   BIC=11.52\n\nTraining set error measures:\n                     ME      RMSE       MAE      MPE     MAPE      MASE\nTraining set 0.01856899 0.2389951 0.1664957 7.224058 29.02617 0.4044533\n                  ACF1\nTraining set 0.2273719\n\n\n\n\n\n\nCode\nsummary(final.fit <- garchFit(~garch(1,1), res,trace = F))\n\n\n\nTitle:\n GARCH Modelling \n\nCall:\n garchFit(formula = ~garch(1, 1), data = res, trace = F) \n\nMean and Variance Equation:\n data ~ garch(1, 1)\n<environment: 0x1559d8fd0>\n [data = res]\n\nConditional Distribution:\n norm \n\nCoefficient(s):\n       mu      omega     alpha1      beta1  \n0.0204567  0.0019945  0.3210359  0.7204177  \n\nStd. Errors:\n based on Hessian \n\nError Analysis:\n        Estimate  Std. Error  t value Pr(>|t|)    \nmu      0.020457    0.026942    0.759   0.4477    \nomega   0.001995    0.003465    0.576   0.5649    \nalpha1  0.321036    0.168345    1.907   0.0565 .  \nbeta1   0.720418    0.122860    5.864 4.53e-09 ***\n---\nSignif. codes:  0 '***' 0.001 '**' 0.01 '*' 0.05 '.' 0.1 ' ' 1\n\nLog Likelihood:\n -0.1441219    normalized:  -0.002771574 \n\nDescription:\n Tue Jan  9 21:01:31 2024 by user:  \n\n\nStandardised Residuals Tests:\n                                Statistic p-Value  \n Jarque-Bera Test   R    Chi^2  1.63773   0.4409319\n Shapiro-Wilk Test  R    W      0.9791929 0.4921099\n Ljung-Box Test     R    Q(10)  5.279386  0.8717505\n Ljung-Box Test     R    Q(15)  11.33345  0.7286181\n Ljung-Box Test     R    Q(20)  15.95037  0.7196983\n Ljung-Box Test     R^2  Q(10)  10.91973  0.3638056\n Ljung-Box Test     R^2  Q(15)  19.9126   0.1753112\n Ljung-Box Test     R^2  Q(20)  20.83641  0.406817 \n LM Arch Test       R    TR^2   15.33253  0.223753 \n\nInformation Criterion Statistics:\n      AIC       BIC       SIC      HQIC \n0.1593893 0.3094850 0.1486440 0.2169324 \n\n\n\n\n\n\nCode\nht <- final.fit@h.t #a numeric vector with the conditional variances (h.t = sigma.t^delta)\n\n#############################\ndata=data.frame(final)\ndata$Date<-as.Date(data$Date,\"%Y-%m-%d\")\n\n\ndata2= data.frame(ht,data$Date)\nggplot(data2, aes(y = ht, x = data.Date)) + geom_line(col = '#F1948A') + ylab('Conditional Variance') + xlab('Date')\n\n\n\n\n\n\n\n\nFrom the ARIMA(1,1,1), we see that the training set error measures also suggest a good fit, with low mean absolute error, root mean squared error, and autocorrelation of the residuals. GATCH(1,1) model model is used to estimate the volatility of the standardized residuals of the previous regression model. The model includes a mean equation that estimates the mean of the residuals and a variance equation that models the conditional variance of the residuals. The coefficients of the mean equation suggest that the mean of the residuals is close to zero. The variance equation coefficients suggest that the conditional variance of the residuals is dependent on the past conditional variances and the past squared standardized residuals. The model’s AIC, BIC, SIC, and HQIC values are all relatively low, indicating a good fit of the model. The standardized residuals tests indicate that the residuals are approximately normally distributed and that there is no significant autocorrelation in the residuals.\nThe volatility of the model seems high in 2020 but has decreased gradually in the past few months. This could indicate that the asset’s price was experiencing a lot of fluctuations in 2020, but the market has stabilized recently.\n\n\nModel Diagnostics\n\nResidualsQQ PlotBox Test\n\n\n\n\nCode\nfit2<-garch(res,order=c(1,1),trace=F)\ncheckresiduals(fit2) \n\n\n\n\n\n\n\n\n\nCode\nqqnorm(fit2$residuals, pch = 1)\nqqline(fit2$residuals, col = \"blue\", lwd = 2)\n\n\n\n\n\n\n\n\n\nCode\nBox.test (fit2$residuals, type = \"Ljung\")\n\n\n\n    Box-Ljung test\n\ndata:  fit2$residuals\nX-squared = 0.42692, df = 1, p-value = 0.5135\n\n\nThe ACF plot of the residuals shows all the values between the blue lines, which indicates that the residuals are not significantly autocorrelated. The range of values for the residual plot between -2 and 2 is considered acceptable. Additionally, the QQ plot of the residuals shows a linear plot on the line, which is another good indication that the residuals are normally distributed. The QQ plot is a valuable tool to assess if the residuals follow a normal distribution, and in this case, the plot suggests that the residuals do indeed follow a normal distribution.\nThe Box-Ljung test, a p-value of 0.5135 indicates that the model’s residuals are not significantly autocorrelated, meaning that the model has captured most of the information in the data. This result is good because it suggests that the model is a good fit for the data and has accounted for most of the underlying patterns in the data. Therefore, we can rely on the model’s predictions and use them to make informed decisions.\n\n\n\n\n\nForecast\n\n\nCode\npredict(final.fit, n.ahead = 5, plot=TRUE)\n\n\n\n\n\n  meanForecast meanError standardDeviation lowerInterval upperInterval\n1   0.02045667 0.5405365         0.5405365     -1.038975      1.079889\n2   0.02045667 0.5534312         0.5534312     -1.064249      1.105162\n3   0.02045667 0.5665486         0.5665486     -1.089958      1.130871\n4   0.02045667 0.5798944         0.5798944     -1.116115      1.157029\n5   0.02045667 0.5934745         0.5934745     -1.142732      1.183645\n\n\nThe forecasted plot is based on the best model ARIMAX(0,1,0)+GARCH(1,1). This model takes into account the autoregressive and moving average components of the data, as well as the impact of exogenous variables on the time series. Additionally, the GARCH component of the model accounts for the volatility clustering in the data. Overall, this model is well-suited to make accurate predictions about future values of the time series.\n\n\nEquation of the Model\nThe equation of the ARIMAX(0,1,0) model is:\n\\(Y(t) = Y(t-1) + \\epsilon(t)\\)\nwhere \\(Y(t)\\) is the time series variable and \\(\\epsilon(t)\\) is the error term.\nThe equation of the GARCH(1,1) model is:\n\\(\\sigma^2(t) = \\alpha_0+\\alpha_1\\epsilon_t^2(t-1)+ \\beta_1\\sigma^2(t-1)\\)\nwhere \\(\\sigma^2_t\\) is the conditional variance at time \\(t\\), \\(\\alpha_0\\) is a constant, \\(\\alpha_1\\) and \\(\\beta_1\\) are the parameters, and \\(\\epsilon_t\\) is the error term.\nThe combined equation of the ARIMAX(0,1,0)+GARCH(1,1) model is:\n\\(Y(t) = Y(t-1) + \\epsilon(t)\\)\n\\(\\epsilon(t) = \\sigma(t) * \\epsilon~(t)\\)\n\\(\\sigma^2(t) = \\alpha_0+\\alpha_1\\epsilon_t^2(t-1)+ \\beta_1\\sigma^2(t-1)\\)"
  },
  {
    "objectID": "arch_Health_Care_Sector_Fund.html",
    "href": "arch_Health_Care_Sector_Fund.html",
    "title": "",
    "section": "",
    "text": "Impact of Macroeconomic Factors on CVS Stock Price\n\n\n\nCVS Health Corp. is one of the top companies in the healthcare sector, providing a wide range of services including pharmacy, health insurance, and retail clinics. An ARIMAX+ARCH/GARCH model can be used to analyze the stock price behavior of CVS Health Corp.\nBy using an ARIMAX+ARCH/GARCH model to analyze the stock price behavior of CVS Health Corp., we can gain insights into how macroeconomic factors impact the company’s performance. For example, changes in healthcare policies, government regulations, and interest rates may affect the company’s financial performance and subsequently its stock price. Additionally, demographic trends such as an aging population and the increasing prevalence of chronic diseases may also impact the demand for healthcare services provided by CVS Health Corp. and in turn affect the company’s stock price.\n\n\nTime series Plot\n\nCVSDifferentitaed Chart SeriesGDP GrowthInflationInterestUnemployment\n\n\n\n\nCode\n# get data\noptions(\"getSymbols.warning4.0\"=FALSE)\noptions(\"getSymbols.yahoo.warning\"=FALSE)\n\n\ndata.info = getSymbols(\"CVS\",src='yahoo', from = '2010-01-01',to = \"2023-03-01\",auto.assign = FALSE)\ndata = getSymbols(\"CVS\",src='yahoo', from = '2010-01-01',to = \"2023-03-01\")\ndf <- data.frame(Date=index(CVS),coredata(CVS))\n\n# create Bollinger Bands\nbbands <- BBands(CVS[,c(\"CVS.High\",\"CVS.Low\",\"CVS.Close\")])\n\n# join and subset data\ndf <- subset(cbind(df, data.frame(bbands[,1:3])), Date >= \"2010-01-01\")\n\n\n# colors column for increasing and decreasing\nfor (i in 1:length(df[,1])) {\n  if (df$CVS.Close[i] >= df$CVS.Open[i]) {\n      df$direction[i] = 'Increasing'\n  } else {\n      df$direction[i] = 'Decreasing'\n  }\n}\n\ni <- list(line = list(color = '#AD1457'))\nd <- list(line = list(color = '#7F7F7F'))\n\n# plot candlestick chart\n\nfig <- df %>% plot_ly(x = ~Date, type=\"candlestick\",\n          open = ~CVS.Open, close = ~CVS.Close,\n          high = ~CVS.High, low = ~CVS.Low, name = \"CVS\",\n          increasing = i, decreasing = d) \nfig <- fig %>% add_lines(x = ~Date, y = ~up , name = \"B Bands\",\n            line = list(color = '#ccc', width = 0.5),\n            legendgroup = \"Bollinger Bands\",\n            hoverinfo = \"none\", inherit = F) \nfig <- fig %>% add_lines(x = ~Date, y = ~dn, name = \"B Bands\",\n            line = list(color = '#ccc', width = 0.5),\n            legendgroup = \"Bollinger Bands\", inherit = F,\n            showlegend = FALSE, hoverinfo = \"none\") \nfig <- fig %>% add_lines(x = ~Date, y = ~mavg, name = \"Mv Avg\",\n            line = list(color = '#C052B3', width = 0.5),\n            hoverinfo = \"none\", inherit = F) \nfig <- fig %>% layout(yaxis = list(title = \"Price\"))\n\n# plot volume bar chart\nfig2 <- df \nfig2 <- fig2 %>% plot_ly(x=~Date, y=~CVS.Volume, type='bar', name = \"CVS Volume\",\n          color = ~direction, colors = c('#AD1457','#7F7F7F')) \nfig2 <- fig2 %>% layout(yaxis = list(title = \"Volume\"))\n\n# create rangeselector buttons\nrs <- list(visible = TRUE, x = 0.5, y = -0.055,\n           xanchor = 'center', yref = 'paper',\n           font = list(size = 9),\n           buttons = list(\n             list(count=1,\n                  label='RESET',\n                  step='all'),\n             list(count=3,\n                  label='3 YR',\n                  step='year',\n                  stepmode='backward'),\n             list(count=1,\n                  label='1 YR',\n                  step='year',\n                  stepmode='backward'),\n             list(count=1,\n                  label='1 MO',\n                  step='month',\n                  stepmode='backward')\n           ))\n\n# subplot with shared x axis\nfig <- subplot(fig, fig2, heights = c(0.7,0.2), nrows=2,\n             shareX = TRUE, titleY = TRUE)\nfig <- fig %>% layout(title = paste(\"CVS Stock Price: January 2010 - March 2023\"),\n         xaxis = list(rangeselector = rs),\n         legend = list(orientation = 'h', x = 0.5, y = 1,\n                       xanchor = 'center', yref = 'paper',\n                       font = list(size = 10),\n                       bgcolor = 'transparent'))\n\nfig\n\n\n\n\n\n\n\n\n\n\nCode\nlog(data.info$`CVS.Adjusted`) %>% diff() %>% chartSeries(theme=chartTheme('white'),up.col='#AD1457')\n\n\n\n\n\n\n\n\n\nCode\n#import the data\ngdp <- read.csv(\"DATA/RAW DATA/gdp-growth.csv\")\n\n#change date format\ngdp$Date <- as.Date(gdp$DATE , \"%m/%d/%Y\")\n\n#drop DATE column\ngdp <- subset(gdp, select = -c(1))\n\n#export the cleaned data\ngdp_clean <- gdp\nwrite.csv(gdp_clean, \"DATA/CLEANED DATA/gdp_clean_data.csv\", row.names=FALSE)\n\n#plot gdp growth rate \nfig <- plot_ly(gdp, x = ~Date, y = ~value, type = 'scatter', mode = 'lines',line = list(color = 'rgb(240, 128, 128)'))\nfig <- fig %>% layout(title = \"U.S GPD Growth Rate: 2010 - 2022\",xaxis = list(title = \"Time\"),yaxis = list(title =\"GDP Growth Rate\"))\nfig\n\n\n\n\n\n\n\n\n\n\nCode\n#import the data\ninflation_rate <- read.csv(\"DATA/RAW DATA/inflation-rate.csv\")\n\n#cleaning the data\n#remove unwanted columns\ninflation_rate_clean <- subset(inflation_rate, select = -c(1,HALF1,HALF2))\n\n#convert the data to time series data\ninflation_data_ts <- ts(as.vector(t(as.matrix(inflation_rate_clean))), start=c(2010,1), end=c(2023,2), frequency=12)\n\n#export the data\nwrite.csv(inflation_rate_clean, \"DATA/CLEANED DATA/inflation_rate_clean_data.csv\", row.names=FALSE)\n\n\n#plot inflation rate \nfig <- autoplot(inflation_data_ts, ylab = \"Inflation Rate\", color=\"#FFA07A\")+ggtitle(\"U.S Inflation Rate: January 2010 - February 2023\")+theme_bw()\nggplotly(fig)\n\n\n\n\n\n\n\n\n\n\nCode\n#import the data\ninterest_data <- read.csv(\"DATA/RAW DATA/interest-rate.csv\")\n\n#change date format\ninterest_data$Date <- as.Date(interest_data$Date , \"%m/%d/%Y\")\n\n#export the cleaned data\ninterest_clean_data <- interest_data\nwrite.csv(interest_clean_data, \"DATA/CLEANED DATA/interest_rate_clean_data.csv\", row.names=FALSE)\n\n#plot interest rate \nfig <- plot_ly(interest_data, x = ~Date, y = ~value, type = 'scatter', mode = 'lines',line = list(color='rgb(219, 112, 147)'))\nfig <- fig %>% layout(title = \"U.S Interest Rate: January 2010 - March 2023\",xaxis = list(title = \"Time\"),yaxis = list(title =\"Interest Rate\"))\nfig\n\n\n\n\n\n\n\n\n\n\nCode\n#import the data\nunemployment_rate <- read.csv(\"DATA/RAW DATA/unemployment-rate.csv\")\n\n#change date format\nunemployment_rate$Date <- as.Date(unemployment_rate$Date , \"%m/%d/%Y\")\n\n# export the data\nwrite.csv(unemployment_rate, \"DATA/CLEANED DATA/unemployment_rate_clean_data.csv\", row.names=FALSE)\n\n#plot unemployment rate \nfig <- plot_ly(unemployment_rate, x = ~Date, y = ~Value, type = 'scatter', mode = 'lines',line = list(color = 'rgb(189, 183, 107)'))\nfig <- fig %>% layout(title = \"U.S Unemployment Rate: January 2010 - March 2023\",xaxis = list(title = \"Time\"),yaxis = list(title =\"Unemployment Rate\"))\nfig\n\n\n\n\n\n\n\n\n\nThe company’s stock price has exhibited a similar pattern to that of the broader healthcare sector, with fluctuations that are often driven by changes in healthcare policy, market competition, and macroeconomic conditions.\nFrom 2010 to 2015, CVS’s stock price experienced a steady rise, as the company expanded its retail pharmacy and healthcare services businesses. However, in 2015, CVS announced its plans to acquire health insurer Aetna, which led to some investor uncertainty and caused a temporary dip in the company’s stock price.\nFollowing the acquisition, CVS’s stock price remained relatively stable until early 2020, when the COVID-19 pandemic began to impact the healthcare industry. While the pandemic initially caused some volatility in CVS’s stock price, the company’s strong position in the healthcare sector and its focus on expanding its digital capabilities helped to stabilize its performance.\nSince mid-2020, CVS’s stock price has exhibited a generally upward trend, likely due to the company’s efforts to enhance its digital and e-commerce offerings, expand its healthcare services, and streamline its operations. In early 2021, CVS also announced plans to acquire health insurer Centene Corp’s Illinois unit, which could help to further bolster its position in the healthcare industry.\nSince early 2021, CVS’s stock price has experienced some volatility, likely due to a combination of factors such as global economic uncertainty and fluctuations in consumer demand for the company’s products.\nAs discussed before, the macroeconomic factors of GDP growth, inflation, interest rates, and unemployment rate are closely interrelated and play a crucial role in the overall health and stability of an economy. From 2010 to 2023, the global economy experienced a mix of ups and downs, with periods of strong GDP growth followed by slowdowns and recessions.\nThe second plot shows the first difference of the logarithm of the adjusted CVS stock price. Taking the first difference removes any long-term trends and transforms the time series into a stationary process. From the plot, we can observe that the first difference of the logarithm of the CVS stock price appears to be stationary, as the mean and variance are roughly constant over time.\n\n\nEnodogenous and Exogenous Variables\n\nPlotCorrelation HeatmapCCF GDPCCF InterestCCF InflationCCF Unemployment\n\n\n\n\nCode\nnumeric_data <- c(\"CVS.Adjusted\",\"gdp\", \"interest\", \"inflation\", \"unemployment\")\nnumeric_data <- final[, numeric_data]\nnormalized_data_numeric <- scale(numeric_data)\nnormalized_data <- ts(normalized_data_numeric, start = c(2010, 1), end = c(2021,10),frequency = 4)\nts_plot(normalized_data,\n        title = \"Normalized Time Series Data for CVS Stock and Macroeconomic Variables\",\n        Ytitle = \"Normalized Values\",\n        Xtitle = \"Year\")\n\n\n\n\n\n\n\n\n\n\nCode\n# Get upper triangle of the correlation matrix\nget_upper_tri <- function(cormat){\n    cormat[lower.tri(cormat)]<- NA\n    return(cormat)\n}\ncormat <- round(cor(normalized_data_numeric),2)\n\nupper_tri <- get_upper_tri(cormat)\n\nmelted_cormat <- melt(upper_tri, na.rm = TRUE)\n# Create a ggheatmap\nggheatmap <- ggplot(melted_cormat, aes(Var2, Var1, fill = value))+\n geom_tile(color = \"white\")+\n scale_fill_gradient2(low = \"blue\", high = \"red\", mid = \"white\", \n   midpoint = 0, limit = c(-1,1), space = \"Lab\", \n    name=\"Pearson\\nCorrelation\") +\n  theme_minimal()+ # minimal theme\n theme(axis.text.x = element_text(angle = 45, vjust = 1, \n    size = 12, hjust = 1))+\n coord_fixed()\n\nggheatmap + \ngeom_text(aes(Var2, Var1, label = value), color = \"black\", size = 4) +\ntheme(\n  axis.title.x = element_blank(),\n  axis.title.y = element_blank(),\n  panel.grid.major = element_blank(),\n  panel.border = element_blank(),\n  panel.background = element_blank(),\n  axis.ticks = element_blank(),\n  legend.justification = c(1, 0),\n  legend.position = c(0.6, 0.7),\n  legend.direction = \"horizontal\")+\n  guides(fill = guide_colorbar(barwidth = 7, barheight = 1,\n                title.position = \"top\", title.hjust = 0.5))\n\n\n\n\n\n\n\n\n\nCode\npar(mfrow=c(1,1))\nccf_result <- ccf(normalized_data[, c(\"CVS.Adjusted\")], normalized_data[, c(\"gdp\")], \n    lag.max = 300,\n    main = \"Cros-Correlation Plot for CVS Stock Price and GDP Growth Rate \",\n    ylab = \"CCF\")\n\n\n\n\n\nCode\ncat(\"The sum of cross correlation function is\", sum(abs(ccf_result$acf)))\n\n\nThe sum of cross correlation function is 6.667531\n\n\n\n\n\n\nCode\npar(mfrow=c(1,1))\nccf_result <- ccf(normalized_data[, c(\"CVS.Adjusted\")], normalized_data[, c(\"interest\")], \n    lag.max = 300,\n    main = \"Cros-Correlation Plot for CVS Stock Price and Interest Rate\",\n    ylab = \"CCF\")\n\n\n\n\n\nCode\ncat(\"The sum of cross correlation function is\", sum(abs(ccf_result$acf)))\n\n\nThe sum of cross correlation function is 14.04391\n\n\n\n\n\n\nCode\npar(mfrow=c(1,1))\nccf_result <- ccf(normalized_data[, c(\"CVS.Adjusted\")], normalized_data[, c(\"inflation\")], \n    lag.max = 300,\n    main = \"Cros-Correlation Plot for CVS Stock Price and Inflation Rate\",\n    ylab = \"CCF\")\n\n\n\n\n\nCode\ncat(\"The sum of cross correlation function is\", sum(abs(ccf_result$acf)))\n\n\nThe sum of cross correlation function is 15.34855\n\n\n\n\n\n\nCode\npar(mfrow=c(1,1))\nccf_result <- ccf(normalized_data[, c(\"CVS.Adjusted\")], normalized_data[, c(\"unemployment\")], \n    lag.max = 300,\n    main = \"Cros-Correlation Plot for CVS Stock Priceand Unemployment Rate\",\n    ylab = \"CCF\")\n\n\n\n\n\nCode\ncat(\"The sum of cross correlation function is\", sum(abs(ccf_result$acf)))\n\n\nThe sum of cross correlation function is 15.40656\n\n\n\n\n\nThe Normalized Time Series Data for Stock Price and Macroeconomic Variables plot shows the same variables as the first plot but has been normalized to a common range of 0 to 1 using the scale() function in R, which standardizes the variables to have a mean of 0 and a standard deviation of 1. The heatmap analysis of the normalized data reveals that inflation and unemployment rate exhibit strong positive correlations with the stock price indices, indicating that these variables may significantly influence stock price movements. On the other hand, weaker correlations were observed between the stock price indices and GDP and interest rates, suggesting that these variables may have less impact on stock price fluctuations. The cross-correlation feature plots confirm these findings, indicating that inflation and unemployment rate are more suitable feature variables for the ARIMAX model when predicting CVS movements.\nFinal Exogenous variables: Macroeconomic indicators: Inflation rate and unemployment rate.\n\n\nEnodogenous and Exogenous Variables Plot\n\nPlotCheck the stationarity\n\n\n\n\nCode\nfinal_data <- final %>%dplyr::select( Date,CVS.Adjusted, inflation,unemployment)\nnumeric_data <- c(\"CVS.Adjusted\", \"inflation\",\"unemployment\")\nnumeric_data <- final_data[, numeric_data]\nnormalized_data_numeric <- scale(numeric_data)\nnormalized_numeric_df <- data.frame(normalized_data_numeric)\nnormalized_data_ts <- ts(normalized_data_numeric, start = c(2010, 1), frequency = 4)\n\nautoplot(normalized_data_ts, facets=TRUE) +\n  xlab(\"Year\") + ylab(\"\") +\n  ggtitle(\"CVS Stock Price, Inflation Rate and Unemployment Rate in USA 2010-2023\")\n\n\n\n\n\n\n\n\n\nCode\n# Convert your multivariate time series data to a matrix\nfinal_data_ts_multivariate <- as.matrix(normalized_data_ts)\n\n# Check for stationarity using Phillips-Perron test\nphillips_perron_test <- ur.pp(final_data_ts_multivariate)  \nsummary(phillips_perron_test)\n\n\n\n################################## \n# Phillips-Perron Unit Root Test # \n################################## \n\nTest regression with intercept \n\n\nCall:\nlm(formula = y ~ y.l1)\n\nResiduals:\n    Min      1Q  Median      3Q     Max \n-1.9919 -0.1733 -0.0624  0.1315  4.4728 \n\nCoefficients:\n            Estimate Std. Error t value Pr(>|t|)    \n(Intercept) 0.004471   0.042538   0.105    0.916    \ny.l1        0.841642   0.042969  19.587   <2e-16 ***\n---\nSignif. codes:  0 '***' 0.001 '**' 0.01 '*' 0.05 '.' 0.1 ' ' 1\n\nResidual standard error: 0.5296 on 153 degrees of freedom\nMultiple R-squared:  0.7149,    Adjusted R-squared:  0.713 \nF-statistic: 383.7 on 1 and 153 DF,  p-value: < 2.2e-16\n\n\nValue of test-statistic, type: Z-alpha  is: -25.4391 \n\n         aux. Z statistics\nZ-tau-mu            0.1044\n\n\n\n\n\nThe results of the Phillips-Perron unit root test indicate strong evidence against the null hypothesis of a unit root, as the p-value for the coefficient of the lagged variable is less than the significance level of 0.05. This suggests that the variable y, which is being tested for stationarity, is likely stationary. Furthermore, the test statistic Z-tau-mu is 0.1044, which is smaller than the critical value of Z-alpha (-25.4391), providing further evidence of stationarity.\nTo determine whether the linear model requires an ARCH model, an ARCH test is conducted. The ACF and PACF plots are also used to identify suitable model values.\n\n\nModel Fitting\n\nPlotARCH TestACF PlotPACF Plot\n\n\n\n\nCode\nnormalized_numeric_df$CVS.Adjusted<-ts(normalized_numeric_df$CVS.Adjusted,star=decimal_date(as.Date(\"2010-01-01\",format = \"%Y-%m-%d\")),frequency = 4)\nnormalized_numeric_df$inflation<-ts(normalized_numeric_df$inflation,star=decimal_date(as.Date(\"2010-01-01\",format = \"%Y-%m-%d\")),frequency = 4)\nnormalized_numeric_df$unemployment<-ts(normalized_numeric_df$unemployment,star=decimal_date(as.Date(\"2010-01-01\",format = \"%Y-%m-%d\")),frequency = 4)\n\nfit <- lm(CVS.Adjusted ~ inflation+unemployment, data=normalized_numeric_df)\nfit.res<-ts(residuals(fit),star=decimal_date(as.Date(\"2010-01-01\",format = \"%Y-%m-%d\")),frequency = 4)\n############## Then look at the residuals ############\nreturns <- fit.res  %>% diff()\nautoplot(returns)+ggtitle(\"Linear Model Returns\")\n\n\n\n\n\n\n\n\n\nCode\nbyd.archTest <- ArchTest(fit.res, lags = 1, demean = TRUE)\nbyd.archTest\n\n\n\n    ARCH LM-test; Null hypothesis: no ARCH effects\n\ndata:  fit.res\nChi-squared = 6.232, df = 1, p-value = 0.01255\n\n\n\n\n\n\nCode\nggAcf(returns) +ggtitle(\"ACF for returns\")\n\n\n\n\n\n\n\n\n\nCode\nggPacf(returns) +ggtitle(\"PACF for returns\")\n\n\n\n\n\n\n\n\nThe ARCH LM-test was conducted with the null hypothesis of no ARCH effects. The test resulted in a chi-squared value of 6.232 with one degree of freedom, and a low p-value of 0.01255. This suggests strong evidence against the null hypothesis, indicating the presence of ARCH effects in the data.\nBased on the ACF and PACF plots, it appears that there is some significant autocorrelation and partial autocorrelation at multiple lags, which suggests that an ARIMA model may not be sufficient to capture the time series behavior. Additionally, the values for p and q appear to be relatively high, with p = 3 and q = 8 being suggested by the plots.\n\n\nARIMAX Model\n\nAuto Arima ModelAuto Arima ResidualsARIMAX ModelARIMA (2,0,1) PlotARIMA (2,0,1) ModelARIMA (1,1,1) PlotARIMA (1,1,1) ModelCross Validation\n\n\n\n\nCode\nxreg <- cbind(Inflation = normalized_data_ts[, \"inflation\"],\n              Unemployment = normalized_data_ts[, \"unemployment\"])\nfit.auto <- auto.arima(normalized_data_ts[, \"CVS.Adjusted\"], xreg = xreg)\nsummary(fit.auto)\n\n\nSeries: normalized_data_ts[, \"CVS.Adjusted\"] \nRegression with ARIMA(0,1,0)(0,0,1)[4] errors \n\nCoefficients:\n        sma1  Inflation  Unemployment\n      0.4196     0.2311       -0.0734\ns.e.  0.1281     0.1097        0.0511\n\nsigma^2 = 0.07929:  log likelihood = -6.57\nAIC=21.15   AICc=22.02   BIC=28.88\n\nTraining set error measures:\n                     ME      RMSE       MAE      MPE     MAPE      MASE\nTraining set 0.03193477 0.2705408 0.2096642 36.92304 74.53091 0.4005754\n                    ACF1\nTraining set -0.07923534\n\n\n\n\n\n\nCode\ncheckresiduals(fit.auto)\n\n\n\n\n\n\n    Ljung-Box test\n\ndata:  Residuals from Regression with ARIMA(0,1,0)(0,0,1)[4] errors\nQ* = 7.2258, df = 7, p-value = 0.4058\n\nModel df: 1.   Total lags used: 8\n\n\n\n\n\n\nCode\nARIMA.c=function(p1,p2,q1,q2,data){\ntemp=c()\nd=1\ni=1\ntemp= data.frame()\nls=matrix(rep(NA,6*30),nrow=30)\n\n\nfor (p in p1:p2)#\n{\n  for(q in q1:q2)#\n  {\n    for(d in 0:1)\n    {\n      \n      if(p+d+q<=6)\n      {\n        \n        model<- Arima(data,order=c(p,d,q))\n        ls[i,]= c(p,d,q,model$aic,model$bic,model$aicc)\n        i=i+1\n        #print(i)\n        \n      }\n      \n    }\n  }\n}\n\n\ntemp= as.data.frame(ls)\nnames(temp)= c(\"p\",\"d\",\"q\",\"AIC\",\"BIC\",\"AICc\")\n\ntemp\n}\n\noutput <- ARIMA.c(1,8,1,2,data=residuals(fit))\n\noutput[which.min(output$AIC),] \n\n\n  p d q      AIC      BIC     AICc\n5 2 0 1 59.21016 68.96638 60.51451\n\n\nCode\noutput[which.min(output$BIC),]\n\n\n  p d q      AIC      BIC     AICc\n2 1 1 1 60.59255 66.38802 61.10318\n\n\nCode\noutput[which.min(output$AICc),]\n\n\n  p d q      AIC      BIC     AICc\n5 2 0 1 59.21016 68.96638 60.51451\n\n\n\n\n\n\nCode\nset.seed(1234)\n\nmodel_output <- capture.output(sarima(fit.res, 2,0,1)) \n\n\n\n\n\n\n\n\n\nCode\ncat(model_output[38:69], model_output[length(model_output)], sep = \"\\n\")\n\n\n$fit\n\nCall:\narima(x = xdata, order = c(p, d, q), seasonal = list(order = c(P, D, Q), period = S), \n    xreg = xmean, include.mean = FALSE, transform.pars = trans, fixed = fixed, \n    optim.control = list(trace = trc, REPORT = 1, reltol = tol))\n\nCoefficients:\n          ar1     ar2     ma1    xmean\n      -0.0494  0.5842  1.0000  -0.0355\ns.e.   0.1158  0.1153  0.0813   0.2141\n\nsigma^2 estimated as 0.1426:  log likelihood = -24.61,  aic = 59.21\n\n$degrees_of_freedom\n[1] 48\n\n$ttable\n      Estimate     SE t.value p.value\nar1    -0.0494 0.1158 -0.4267  0.6715\nar2     0.5842 0.1153  5.0647  0.0000\nma1     1.0000 0.0813 12.2960  0.0000\nxmean  -0.0355 0.2141 -0.1659  0.8690\n\n$AIC\n[1] 1.138657\n\n$AICc\n[1] 1.155024\n\n$BIC\n[1] 1.326277\n\n\n\n\n\n\nCode\nset.seed(1234)\n\nmodel_output <- capture.output(sarima(fit.res, 1,1,1)) \n\n\n\n\n\n\n\n\n\nCode\ncat(model_output[40:70], model_output[length(model_output)], sep = \"\\n\")\n\n\n$fit\n\nCall:\narima(x = xdata, order = c(p, d, q), seasonal = list(order = c(P, D, Q), period = S), \n    xreg = constant, transform.pars = trans, fixed = fixed, optim.control = list(trace = trc, \n        REPORT = 1, reltol = tol))\n\nCoefficients:\n          ar1     ma1  constant\n      -0.8170  1.0000    0.0126\ns.e.   0.0886  0.0711    0.0625\n\nsigma^2 estimated as 0.1647:  log likelihood = -27.28,  aic = 62.55\n\n$degrees_of_freedom\n[1] 48\n\n$ttable\n         Estimate     SE t.value p.value\nar1       -0.8170 0.0886 -9.2196  0.0000\nma1        1.0000 0.0711 14.0601  0.0000\nconstant   0.0126 0.0625  0.2024  0.8404\n\n$AIC\n[1] 1.226501\n\n$AICc\n[1] 1.236514\n\n$BIC\n[1] 1.378017\n\n\n\n\n\n\nCode\nn=length(fit.res)\nk= 51\n \n \nrmse1 <- matrix(NA, (n-k),4)\nrmse2 <- matrix(NA, (n-k),4)\nrmse3 <- matrix(NA, (n-k),4)\n\n\nst <- tsp(fit.res)[1]+(k-5)/4 \n\nfor(i in 1:(n-k))\n{\n  xtrain <- window(fit.res, end=st + i/4)\n  xtest <- window(fit.res, start=st + (i+1)/4, end=st + (i+4)/4)\n  \n  #ARIMA(0,1,0) ARIMA(1,1,1)\n  \n  fit <- Arima(xtrain, order=c(2,0,1),\n                include.drift=TRUE, method=\"ML\")\n  fcast <- forecast(fit, h=4)\n  \n  fit2 <- Arima(xtrain, order=c(1,1,1),\n                include.drift=TRUE, method=\"ML\")\n  fcast2 <- forecast(fit2, h=4)\n\n  fit3 <- Arima(xtrain, order=c(0,1,0),seasonal=c(0,0,1),\n                include.drift=TRUE, method=\"ML\")\n  fcast3 <- forecast(fit2, h=4)\n  \n  rmse1[i,1:length(xtest)]   <- sqrt((fcast$mean-xtest)^2)\n  rmse2[i,1:length(xtest)] <- sqrt((fcast2$mean-xtest)^2)\n  rmse3[i,1:length(xtest)] <- sqrt((fcast3$mean-xtest)^2)\n}\n\nplot(1:4,colMeans(rmse1,na.rm=TRUE), type=\"l\",col=2, xlab=\"horizon\", ylab=\"RMSE\")\nlines(1:4, colMeans(rmse2,na.rm=TRUE), type=\"l\",col=3)\nlines(1:4, colMeans(rmse3,na.rm=TRUE), type=\"l\",col=3)\nlegend(\"topleft\",legend=c(\"fit1\",\"fit2\",\"fit3\"),col=2:4,lty=1)\n\n\n\n\n\n\n\n\nBased on the results of the auto.arima function, the suggested best model is ARIMA(0,1,0)(0,0,1)[4]. However, when we manually test different ARIMA models, we find that ARIMA(1,1,1) has the lowest values for AIC and AICC and (2,0,1) has the lowest BIC value. Additionally,when comparing the models, RMSE values are for ARIMA(1,1,1) and ARIMA(0,1,0)(0,0,1)[4], but the AIC and BIC value is way lesser in ARIMA(1,1,1) than ARIMA(0,1,0)(0,0,1)[4]. So, out best model id ARIMA(1,1,1)\nWe can then proceed to choose the best GARCH model using ARIMA(1,1,1) as the base model.\n\n\nSquared Residuals\n\nPlotACF PlotPACF PlotACF AbsoluteARCH Model\n\n\n\n\nCode\nfit <- lm(CVS.Adjusted ~ inflation+unemployment, data=normalized_numeric_df)\nfit.res<-ts(residuals(fit),star=decimal_date(as.Date(\"2010-01-01\",format = \"%Y-%m-%d\")),frequency = 4)\nfit <- Arima(fit.res,order=c(1,1,1))\nres=fit$res\nplot(res^2,main='Squared Residuals')\n\n\n\n\n\n\n\n\n\nCode\nacf(res^2,24, main = \"ACF Residuals Square\")\n\n\n\n\n\n\n\n\n\nCode\npacf(res^2,24, main = \"PACF Residuals Square\")\n\n\n\n\n\n\n\n\n\nCode\nacf(abs(res), main = \"ACF of Absolute Residuals\")\n\n\n\n\n\n\n\nCode\npacf(abs(res), main = \"PACF of Absolute Residuals\")\n\n\n\n\n\n\n\n\n\nCode\nsummary(garchFit(~garch(1,0),res, trace=F))\n\n\n\nTitle:\n GARCH Modelling \n\nCall:\n garchFit(formula = ~garch(1, 0), data = res, trace = F) \n\nMean and Variance Equation:\n data ~ garch(1, 0)\n<environment: 0x1625601e0>\n [data = res]\n\nConditional Distribution:\n norm \n\nCoefficient(s):\n        mu       omega      alpha1  \n-0.0046569   0.0745273   0.7725225  \n\nStd. Errors:\n based on Hessian \n\nError Analysis:\n        Estimate  Std. Error  t value Pr(>|t|)   \nmu     -0.004657    0.046134   -0.101  0.91960   \nomega   0.074527    0.022857    3.261  0.00111 **\nalpha1  0.772523    0.378440    2.041  0.04122 * \n---\nSignif. codes:  0 '***' 0.001 '**' 0.01 '*' 0.05 '.' 0.1 ' ' 1\n\nLog Likelihood:\n -21.43537    normalized:  -0.4122186 \n\nDescription:\n Tue Jan  9 21:02:12 2024 by user:  \n\n\nStandardised Residuals Tests:\n                                Statistic p-Value     \n Jarque-Bera Test   R    Chi^2  37.97502  5.673205e-09\n Shapiro-Wilk Test  R    W      0.9085435 0.0007248108\n Ljung-Box Test     R    Q(10)  7.703946  0.6577296   \n Ljung-Box Test     R    Q(15)  11.08964  0.7462146   \n Ljung-Box Test     R    Q(20)  20.53372  0.4250188   \n Ljung-Box Test     R^2  Q(10)  2.098708  0.9955261   \n Ljung-Box Test     R^2  Q(15)  3.245973  0.9993468   \n Ljung-Box Test     R^2  Q(20)  4.06065   0.9999474   \n LM Arch Test       R    TR^2   3.304597  0.9929847   \n\nInformation Criterion Statistics:\n      AIC       BIC       SIC      HQIC \n0.9398219 1.0523936 0.9336365 0.9829792 \n\n\n\n\n\nFrom the squared residuals of the best ARIMA model, it can be observed that the ACF plot and PACF plot indicate that the residuals are not autocorrelated and are white noise, indicating a good fit of the model. Based on the squared residuals of the best ARIMA model, we can see that the ACF and PACF plots indicate that most of the values lie between the blue lines. Additionally, the p-value is 1. This suggests that the model has a good fit and that there is no significant autocorrelation or partial autocorrelation in the residuals.\nThe best models are ARIMA(1,1,1) and ARCH(1)\n\n\nBest Model\n\nARIMA ModelGARCH ModelVolatility\n\n\n\n\nCode\n#fiting an ARIMA model to the Inflation variable\ninflation_fit<-auto.arima(normalized_numeric_df$inflation) \nfinflation<-forecast(inflation_fit)\n\n#fitting an ARIMA model to the Unemployment variable\nunemployment_fit<-auto.arima(normalized_numeric_df$unemployment) \nfunemployment<-forecast(unemployment_fit)\n\n# best model fit for forcasting\nxreg <- cbind(Inflation = normalized_data_ts[, \"inflation\"],\n              Unemployment = normalized_data_ts[, \"unemployment\"])\n\n\n\nsummary(arima.fit<-Arima(normalized_data_ts[, \"CVS.Adjusted\"],order=c(1,1,1),xreg=xreg),include.drift = TRUE)\n\n\nSeries: normalized_data_ts[, \"CVS.Adjusted\"] \nRegression with ARIMA(1,1,1) errors \n\nCoefficients:\n          ar1     ma1  Inflation  Unemployment\n      -0.3281  0.3754     0.2266       -0.1084\ns.e.   0.8664  0.8446     0.1377        0.0637\n\nsigma^2 = 0.09542:  log likelihood = -10.37\nAIC=30.75   AICc=32.08   BIC=40.4\n\nTraining set error measures:\n                     ME      RMSE       MAE      MPE     MAPE      MASE\nTraining set 0.04128572 0.2936735 0.2223189 35.05627 89.53572 0.4247529\n                    ACF1\nTraining set -0.05055494\n\n\n\n\n\n\nCode\nsummary(final.fit <- garchFit(~garch(1,0), res,trace = F))\n\n\n\nTitle:\n GARCH Modelling \n\nCall:\n garchFit(formula = ~garch(1, 0), data = res, trace = F) \n\nMean and Variance Equation:\n data ~ garch(1, 0)\n<environment: 0x13c887d98>\n [data = res]\n\nConditional Distribution:\n norm \n\nCoefficient(s):\n        mu       omega      alpha1  \n-0.0046569   0.0745273   0.7725225  \n\nStd. Errors:\n based on Hessian \n\nError Analysis:\n        Estimate  Std. Error  t value Pr(>|t|)   \nmu     -0.004657    0.046134   -0.101  0.91960   \nomega   0.074527    0.022857    3.261  0.00111 **\nalpha1  0.772523    0.378440    2.041  0.04122 * \n---\nSignif. codes:  0 '***' 0.001 '**' 0.01 '*' 0.05 '.' 0.1 ' ' 1\n\nLog Likelihood:\n -21.43537    normalized:  -0.4122186 \n\nDescription:\n Tue Jan  9 21:02:13 2024 by user:  \n\n\nStandardised Residuals Tests:\n                                Statistic p-Value     \n Jarque-Bera Test   R    Chi^2  37.97502  5.673205e-09\n Shapiro-Wilk Test  R    W      0.9085435 0.0007248108\n Ljung-Box Test     R    Q(10)  7.703946  0.6577296   \n Ljung-Box Test     R    Q(15)  11.08964  0.7462146   \n Ljung-Box Test     R    Q(20)  20.53372  0.4250188   \n Ljung-Box Test     R^2  Q(10)  2.098708  0.9955261   \n Ljung-Box Test     R^2  Q(15)  3.245973  0.9993468   \n Ljung-Box Test     R^2  Q(20)  4.06065   0.9999474   \n LM Arch Test       R    TR^2   3.304597  0.9929847   \n\nInformation Criterion Statistics:\n      AIC       BIC       SIC      HQIC \n0.9398219 1.0523936 0.9336365 0.9829792 \n\n\n\n\n\n\nCode\nht <- final.fit@h.t #a numeric vector with the conditional variances (h.t = sigma.t^delta)\n\n#############################\ndata=data.frame(final)\ndata$Date<-as.Date(data$Date,\"%Y-%m-%d\")\n\n\ndata2= data.frame(ht,data$Date)\nggplot(data2, aes(y = ht, x = data.Date)) + geom_line(col = '#AD1457') + ylab('Conditional Variance') + xlab('Date')\n\n\n\n\n\n\n\n\nFrom the ARIMA(1,1,1), we see that the training set error measures also suggest a good fit, with low mean absolute error, root mean squared error, and autocorrelation of the residuals. GATCH(1,0) model model is used to estimate the volatility of the standardized residuals of the previous regression model. The model includes a mean equation that estimates the mean of the residuals and a variance equation that models the conditional variance of the residuals. The coefficients of the mean equation suggest that the mean of the residuals is close to zero. The variance equation coefficients suggest that the conditional variance of the residuals is dependent on the past conditional variances and the past squared standardized residuals. The model’s the AIC, BIC, SIC, and HQIC values are all relatively low, indicating a good fit of the model. The standardized residuals tests indicate that the residuals are approximately normally distributed and that there is no significant autocorrelation in the residuals.\nThe volatility of the model seems high in 2020 but has decreased gradually in the past few months. This could indicate that the asset’s price was experiencing a lot of fluctuations in 2020, but the market has stabilized recently.\n\n\nModel Diagnostics\n\nResidualsQQ PlotBox Test\n\n\n\n\nCode\nfit2<-garch(res,order=c(1,0),trace=F)\ncheckresiduals(fit2) \n\n\n\n\n\n\n\n\n\nCode\nqqnorm(fit2$residuals, pch = 1)\nqqline(fit2$residuals, col = \"blue\", lwd = 2)\n\n\n\n\n\n\n\n\n\nCode\nBox.test (fit2$residuals, type = \"Ljung\")\n\n\n\n    Box-Ljung test\n\ndata:  fit2$residuals\nX-squared = 0.48065, df = 1, p-value = 0.4881\n\n\nThe ACF plot of the residuals shows all the values between the blue lines, which indicates that the residuals are not significantly autocorrelated. The range of values for the residual plot between -2 and 2 is considered acceptable. Additionally, the QQ plot of the residuals shows a linear plot on the line, which is another good indication that the residuals are normally distributed. The QQ plot is a valuable tool to assess if the residuals follow a normal distribution, and in this case, the plot suggests that the residuals do indeed follow a normal distribution.\nThe Box-Ljung test, a p-value of 0.8885 indicates that the model’s residuals are not significantly autocorrelated, meaning that the model has captured most of the information in the data. This result is good because it suggests that the model is a good fit for the data and has accounted for most of the underlying patterns in the data. Therefore, we can rely on the model’s predictions and use them to make informed decisions.\n\n\n\n\n\nForecast\n\n\nCode\npredict(final.fit, n.ahead = 5, plot=TRUE)\n\n\n\n\n\n  meanForecast meanError standardDeviation lowerInterval upperInterval\n1 -0.004656909 0.2997057         0.2997057    -0.5920692     0.5827554\n2 -0.004656909 0.3793652         0.3793652    -0.7481990     0.7388851\n3 -0.004656909 0.4309375         0.4309375    -0.8492788     0.8399650\n4 -0.004656909 0.4668942         0.4668942    -0.9197527     0.9104388\n5 -0.004656909 0.4928789         0.4928789    -0.9706817     0.9613679\n\n\nThe forecasted plot is based on the best model ARIMAX(1,1,1)+GARCH(1,1). This model takes into account the autoregressive and moving average components of the data, as well as the impact of exogenous variables on the time series. Additionally, the GARCH component of the model accounts for the volatility clustering in the data. Overall, this model is well-suited to make accurate predictions about future values of the time series.\n\n\nEquation of the Model\nThe equation of the ARIMAX(1,1,1) model is:\n\\(Y(t) = c + \\phi_1(Y{(t-1)} - X{(t-1)}) + \\theta_1\\epsilon{(t-1)} + \\epsilon(t)\\)\nwhere, \\(Y(t)\\) is the time series variable, \\(X(t-1)\\) is the exogenous variable, \\(c\\) is a constant, \\(\\phi_1\\) and \\(\\theta_1\\) are the parameters, and \\(\\epsilon(t)\\) is the error term.\nThe equation of the ARCH(1) model is:\n\\(\\sigma_{t}^{2}=\\alpha_{0}+\\alpha_{1}\\epsilon_{t-1}^{2}\\)\nwhere \\(\\sigma^2_t\\) is the conditional variance at time \\(t\\), \\(\\alpha_0\\) is a constant, \\(\\alpha_1\\) and \\(\\beta_1\\) are the parameters, and \\(\\epsilon_t\\) is the error term.\nThe combined equation of the ARIMAX(1,1,1)+GARCH(1,1) model is:\n\\(Y(t) = c + \\phi_1(Y(t-1) - X(t-1)) + \\theta_1\\epsilon(t-1) + \\epsilon(t)\\)\n\\(\\sigma_{t}^{2}=\\alpha_{0}+\\alpha_{1}\\epsilon_{t-1}^{2}\\)"
  },
  {
    "objectID": "arch_Industrial_Sector_Fund.html",
    "href": "arch_Industrial_Sector_Fund.html",
    "title": "",
    "section": "",
    "text": "Impact of Macroeconomic Factors on General Electric Stock Price\n\n\n\nOne of the top companies in the industrial sector is General Electric (GE), which is a multinational conglomerate that produces a wide range of products including aircraft engines, power generation equipment, renewable energy solutions, and medical imaging devices. To analyze the stock price behavior of General Electric, an ARIMAX+ARCH/GARCH model can be employed.\nBy using an ARIMAX+ARCH/GARCH model to analyze the stock price behavior of General Electric, we can gain insights into how macroeconomic factors impact the company’s performance. For example, an increase in government spending on infrastructure projects or a rise in demand for commercial aircraft may lead to increased revenue and a rise in General Electric’s stock price. On the other hand, global economic uncertainty or changes in trade policies may lead to a decline in General Electric’s stock price.\n\n\nTime series Plot\n\nGeneral ElectricDifferentitaed Chart SeriesGDP GrowthInflationInterestUnemployment\n\n\n\n\nCode\n# get data\noptions(\"getSymbols.warning4.0\"=FALSE)\noptions(\"getSymbols.yahoo.warning\"=FALSE)\n\n\ndata.info = getSymbols(\"GE\",src='yahoo', from = '2010-01-01',to = \"2023-03-01\",auto.assign = FALSE)\ndata = getSymbols(\"GE\",src='yahoo', from = '2010-01-01',to = \"2023-03-01\")\ndf <- data.frame(Date=index(GE),coredata(GE))\n\n# create Bollinger Bands\nbbands <- BBands(GE[,c(\"GE.High\",\"GE.Low\",\"GE.Close\")])\n\n# join and subset data\ndf <- subset(cbind(df, data.frame(bbands[,1:3])), Date >= \"2010-01-01\")\n\n\n# colors column for increasing and decreasing\nfor (i in 1:length(df[,1])) {\n  if (df$GE.Close[i] >= df$GE.Open[i]) {\n      df$direction[i] = 'Increasing'\n  } else {\n      df$direction[i] = 'Decreasing'\n  }\n}\n\ni <- list(line = list(color = '#FDD835'))\nd <- list(line = list(color = '#7F7F7F'))\n\n# plot candlestick chart\n\nfig <- df %>% plot_ly(x = ~Date, type=\"candlestick\",\n          open = ~GE.Open, close = ~GE.Close,\n          high = ~GE.High, low = ~GE.Low, name = \"GE\",\n          increasing = i, decreasing = d) \nfig <- fig %>% add_lines(x = ~Date, y = ~up , name = \"B Bands\",\n            line = list(color = '#ccc', width = 0.5),\n            legendgroup = \"Bollinger Bands\",\n            hoverinfo = \"none\", inherit = F) \nfig <- fig %>% add_lines(x = ~Date, y = ~dn, name = \"B Bands\",\n            line = list(color = '#ccc', width = 0.5),\n            legendgroup = \"Bollinger Bands\", inherit = F,\n            showlegend = FALSE, hoverinfo = \"none\") \nfig <- fig %>% add_lines(x = ~Date, y = ~mavg, name = \"Mv Avg\",\n            line = list(color = '#C052B3', width = 0.5),\n            hoverinfo = \"none\", inherit = F) \nfig <- fig %>% layout(yaxis = list(title = \"Price\"))\n\n# plot volume bar chart\nfig2 <- df \nfig2 <- fig2 %>% plot_ly(x=~Date, y=~GE.Volume, type='bar', name = \"GE Volume\",\n          color = ~direction, colors = c('#FDD835','#7F7F7F')) \nfig2 <- fig2 %>% layout(yaxis = list(title = \"Volume\"))\n\n# create rangeselector buttons\nrs <- list(visible = TRUE, x = 0.5, y = -0.055,\n           xanchor = 'center', yref = 'paper',\n           font = list(size = 9),\n           buttons = list(\n             list(count=1,\n                  label='RESET',\n                  step='all'),\n             list(count=3,\n                  label='3 YR',\n                  step='year',\n                  stepmode='backward'),\n             list(count=1,\n                  label='1 YR',\n                  step='year',\n                  stepmode='backward'),\n             list(count=1,\n                  label='1 MO',\n                  step='month',\n                  stepmode='backward')\n           ))\n\n# subplot with shared x axis\nfig <- subplot(fig, fig2, heights = c(0.7,0.2), nrows=2,\n             shareX = TRUE, titleY = TRUE)\nfig <- fig %>% layout(title = paste(\"General Electric  Stock Price: January 2010 - March 2023\"),\n         xaxis = list(rangeselector = rs),\n         legend = list(orientation = 'h', x = 0.5, y = 1,\n                       xanchor = 'center', yref = 'paper',\n                       font = list(size = 10),\n                       bgcolor = 'transparent'))\n\nfig\n\n\n\n\n\n\n\n\n\n\nCode\nlog(data.info$`GE.Adjusted`) %>% diff() %>% chartSeries(theme=chartTheme('white'),up.col='#FDD835')\n\n\n\n\n\n\n\n\n\nCode\n#import the data\ngdp <- read.csv(\"DATA/RAW DATA/gdp-growth.csv\")\n\n#change date format\ngdp$Date <- as.Date(gdp$DATE , \"%m/%d/%Y\")\n\n#drop DATE column\ngdp <- subset(gdp, select = -c(1))\n\n#export the cleaned data\ngdp_clean <- gdp\nwrite.csv(gdp_clean, \"DATA/CLEANED DATA/gdp_clean_data.csv\", row.names=FALSE)\n\n#plot gdp growth rate \nfig <- plot_ly(gdp, x = ~Date, y = ~value, type = 'scatter', mode = 'lines',line = list(color = 'rgb(240, 128, 128)'))\nfig <- fig %>% layout(title = \"U.S GPD Growth Rate: 2010 - 2022\",xaxis = list(title = \"Time\"),yaxis = list(title =\"GDP Growth Rate\"))\nfig\n\n\n\n\n\n\n\n\n\n\nCode\n#import the data\ninflation_rate <- read.csv(\"DATA/RAW DATA/inflation-rate.csv\")\n\n#cleaning the data\n#remove unwanted columns\ninflation_rate_clean <- subset(inflation_rate, select = -c(1,HALF1,HALF2))\n\n#convert the data to time series data\ninflation_data_ts <- ts(as.vector(t(as.matrix(inflation_rate_clean))), start=c(2010,1), end=c(2023,2), frequency=12)\n\n#export the data\nwrite.csv(inflation_rate_clean, \"DATA/CLEANED DATA/inflation_rate_clean_data.csv\", row.names=FALSE)\n\n\n#plot inflation rate \nfig <- autoplot(inflation_data_ts, ylab = \"Inflation Rate\", color=\"#FFA07A\")+ggtitle(\"U.S Inflation Rate: January 2010 - February 2023\")+theme_bw()\nggplotly(fig)\n\n\n\n\n\n\n\n\n\n\nCode\n#import the data\ninterest_data <- read.csv(\"DATA/RAW DATA/interest-rate.csv\")\n\n#change date format\ninterest_data$Date <- as.Date(interest_data$Date , \"%m/%d/%Y\")\n\n#export the cleaned data\ninterest_clean_data <- interest_data\nwrite.csv(interest_clean_data, \"DATA/CLEANED DATA/interest_rate_clean_data.csv\", row.names=FALSE)\n\n#plot interest rate \nfig <- plot_ly(interest_data, x = ~Date, y = ~value, type = 'scatter', mode = 'lines',line = list(color='rgb(219, 112, 147)'))\nfig <- fig %>% layout(title = \"U.S Interest Rate: January 2010 - March 2023\",xaxis = list(title = \"Time\"),yaxis = list(title =\"Interest Rate\"))\nfig\n\n\n\n\n\n\n\n\n\n\nCode\n#import the data\nunemployment_rate <- read.csv(\"DATA/RAW DATA/unemployment-rate.csv\")\n\n#change date format\nunemployment_rate$Date <- as.Date(unemployment_rate$Date , \"%m/%d/%Y\")\n\n# export the data\nwrite.csv(unemployment_rate, \"DATA/CLEANED DATA/unemployment_rate_clean_data.csv\", row.names=FALSE)\n\n#plot unemployment rate \nfig <- plot_ly(unemployment_rate, x = ~Date, y = ~Value, type = 'scatter', mode = 'lines',line = list(color = 'rgb(189, 183, 107)'))\nfig <- fig %>% layout(title = \"U.S Unemployment Rate: January 2010 - March 2023\",xaxis = list(title = \"Time\"),yaxis = list(title =\"Unemployment Rate\"))\nfig\n\n\n\n\n\n\n\n\n\nFrom 2010 to 2014, GE experienced a steady increase in its business operations due to strong financial performance and consistent dividend payouts. However, from 2014 to 2016, the company faced challenges such as slowing sales growth and increased competition in the consumer goods industry, resulting in a decline in its business operations.\nIn response, GE began to streamline its operations and focus on core brands, which helped the company to recover from 2016 to 2018. This trend continued into 2019, with GE reaching an all-time high in mid-2019. However, the COVID-19 pandemic caused a brief dip in the company’s business operations in 2020, and GE’s stock price experienced some volatility in early 2021 due to global economic uncertainty and fluctuations in consumer demand for the company’s products.\nSince early 2021, General Electric ’s stock price has experienced some volatility, likely due to a combination of factors such as global economic uncertainty and fluctuations in consumer demand for the company’s products.\nAs discussed before, the macroeconomic factors of GDP growth, inflation, interest rates, and unemployment rate are closely interrelated and play a crucial role in the overall health and stability of an economy. From 2010 to 2023, the global economy experienced a mix of GE and downs, with periods of strong GDP growth followed by slowdowns and recessions.\nThe second plot shows the first difference of the logarithm of the adjusted General Electric stock price. Taking the first difference removes any long-term trends and transforms the time series into a stationary process. From the plot, we can observe that the first difference of the logarithm of the General Electric stock price appears to be stationary, as the mean and variance are roughly constant over time.\n\n\nEnodogenous and Exogenous Variables\n\nPlotCorrelation HeatmapCCF GDPCCF InterestCCF InflationCCF Unemployment\n\n\n\n\nCode\nnumeric_data <- c(\"GE.Adjusted\",\"gdp\", \"interest\", \"inflation\", \"unemployment\")\nnumeric_data <- final[, numeric_data]\nnormalized_data_numeric <- scale(numeric_data)\nnormalized_data <- ts(normalized_data_numeric, start = c(2010, 1), end = c(2021,10),frequency = 4)\nts_plot(normalized_data,\n        title = \"Normalized Time Series Data for GE Stock and Macroeconomic Variables\",\n        Ytitle = \"Normalized Values\",\n        Xtitle = \"Year\")\n\n\n\n\n\n\n\n\n\n\nCode\n# Get upper triangle of the correlation matrix\nget_upper_tri <- function(cormat){\n    cormat[lower.tri(cormat)]<- NA\n    return(cormat)\n}\ncormat <- round(cor(normalized_data_numeric),2)\n\nupper_tri <- get_upper_tri(cormat)\n\nmelted_cormat <- melt(upper_tri, na.rm = TRUE)\n# Create a ggheatmap\nggheatmap <- ggplot(melted_cormat, aes(Var2, Var1, fill = value))+\n geom_tile(color = \"white\")+\n scale_fill_gradient2(low = \"blue\", high = \"red\", mid = \"white\", \n   midpoint = 0, limit = c(-1,1), space = \"Lab\", \n    name=\"Pearson\\nCorrelation\") +\n  theme_minimal()+ # minimal theme\n theme(axis.text.x = element_text(angle = 45, vjust = 1, \n    size = 12, hjust = 1))+\n coord_fixed()\n\nggheatmap + \ngeom_text(aes(Var2, Var1, label = value), color = \"black\", size = 4) +\ntheme(\n  axis.title.x = element_blank(),\n  axis.title.y = element_blank(),\n  panel.grid.major = element_blank(),\n  panel.border = element_blank(),\n  panel.background = element_blank(),\n  axis.ticks = element_blank(),\n  legend.justification = c(1, 0),\n  legend.position = c(0.6, 0.7),\n  legend.direction = \"horizontal\")+\n  guides(fill = guide_colorbar(barwidth = 7, barheight = 1,\n                title.position = \"top\", title.hjust = 0.5))\n\n\n\n\n\n\n\n\n\nCode\npar(mfrow=c(1,1))\nccf_result <- ccf(normalized_data[, c(\"GE.Adjusted\")], normalized_data[, c(\"gdp\")], \n    lag.max = 300,\n    main = \"Cros-Correlation Plot for GE Stock Price and GDP Growth Rate \",\n    ylab = \"CCF\")\n\n\n\n\n\nCode\ncat(\"The sum of cross correlation function is\", sum(abs(ccf_result$acf)))\n\n\nThe sum of cross correlation function is 5.53155\n\n\n\n\n\n\nCode\npar(mfrow=c(1,1))\nccf_result <- ccf(normalized_data[, c(\"GE.Adjusted\")], normalized_data[, c(\"interest\")], \n    lag.max = 300,\n    main = \"Cros-Correlation Plot for GE Stock Price and Interest Rate\",\n    ylab = \"CCF\")\n\n\n\n\n\nCode\ncat(\"The sum of cross correlation function is\", sum(abs(ccf_result$acf)))\n\n\nThe sum of cross correlation function is 12.52537\n\n\n\n\n\n\nCode\npar(mfrow=c(1,1))\nccf_result <- ccf(normalized_data[, c(\"GE.Adjusted\")], normalized_data[, c(\"inflation\")], \n    lag.max = 300,\n    main = \"Cros-Correlation Plot for GE Stock Price and Inflation Rate\",\n    ylab = \"CCF\")\n\n\n\n\n\nCode\ncat(\"The sum of cross correlation function is\", sum(abs(ccf_result$acf)))\n\n\nThe sum of cross correlation function is 16.47864\n\n\n\n\n\n\nCode\npar(mfrow=c(1,1))\nccf_result <- ccf(normalized_data[, c(\"GE.Adjusted\")], normalized_data[, c(\"unemployment\")], \n    lag.max = 300,\n    main = \"Cros-Correlation Plot for GE Stock Priceand Unemployment Rate\",\n    ylab = \"CCF\")\n\n\n\n\n\nCode\ncat(\"The sum of cross correlation function is\", sum(abs(ccf_result$acf)))\n\n\nThe sum of cross correlation function is 19.28415\n\n\n\n\n\nThe Normalized Time Series Data for Stock Price and Macroeconomic Variables plot shows the same variables as the first plot but has been normalized to a common range of 0 to 1 using the scale() function in R, which standardizes the variables to have a mean of 0 and a standard deviation of 1. The heatmap analysis of the normalized data reveals that inflation and unemployment rate exhibit strong positive correlations with the stock price indices, indicating that these variables may significantly influence stock price movements. On the other hand, weaker correlations were observed between the stock price indices and GDP and interest rates, suggesting that these variables may have less impact on stock price fluctuations. The cross-correlation feature plots confirm these findings, indicating that inflation and unemployment rate are more suitable feature variables for the ARIMAX model when predicting General Electric movements.\nFinal Exogenous variables: Macroeconomic indicators: Inflation rate and unemployment rate.\n\n\nEnodogenous and Exogenous Variables Plot\n\nPlotCheck the stationarity\n\n\n\n\nCode\nfinal_data <- final %>%dplyr::select( Date,GE.Adjusted, inflation,unemployment)\nnumeric_data <- c(\"GE.Adjusted\", \"inflation\",\"unemployment\")\nnumeric_data <- final_data[, numeric_data]\nnormalized_data_numeric <- scale(numeric_data)\nnormalized_numeric_df <- data.frame(normalized_data_numeric)\nnormalized_data_ts <- ts(normalized_data_numeric, start = c(2010, 1), frequency = 4)\n\nautoplot(normalized_data_ts, facets=TRUE) +\n  xlab(\"Year\") + ylab(\"\") +\n  ggtitle(\"General Electric  Stock Price, Inflation Rate and Unemployment Rate in USA 2010-2023\")\n\n\n\n\n\n\n\n\n\nCode\n# Convert your multivariate time series data to a matrix\nfinal_data_ts_multivariate <- as.matrix(normalized_data_ts)\n\n# Check for stationarity using Phillips-Perron test\nphillips_perron_test <- ur.pp(final_data_ts_multivariate)  \nsummary(phillips_perron_test)\n\n\n\n################################## \n# Phillips-Perron Unit Root Test # \n################################## \n\nTest regression with intercept \n\n\nCall:\nlm(formula = y ~ y.l1)\n\nResiduals:\n    Min      1Q  Median      3Q     Max \n-1.3707 -0.1831 -0.0486  0.1363  4.4969 \n\nCoefficients:\n              Estimate Std. Error t value Pr(>|t|)    \n(Intercept) -0.0006864  0.0412911  -0.017    0.987    \ny.l1         0.8584440  0.0417094  20.582   <2e-16 ***\n---\nSignif. codes:  0 '***' 0.001 '**' 0.01 '*' 0.05 '.' 0.1 ' ' 1\n\nResidual standard error: 0.5141 on 153 degrees of freedom\nMultiple R-squared:  0.7347,    Adjusted R-squared:  0.7329 \nF-statistic: 423.6 on 1 and 153 DF,  p-value: < 2.2e-16\n\n\nValue of test-statistic, type: Z-alpha  is: -22.1435 \n\n         aux. Z statistics\nZ-tau-mu           -0.0164\n\n\n\n\n\nThe results of the Phillips-Perron unit root test indicate strong evidence against the null hypothesis of a unit root, as the p-value for the coefficient of the lagged variable is less than the significance level of 0.05. This suggests that the variable y, which is being tested for stationarity, is likely stationary. Furthermore, the test statistic Z-tau-mu is 0.0164, which is smaller than the critical value of Z-alpha (-22.1435), providing further evidence of stationarity.\nTo determine whether the linear model requires an ARCH model, an ARCH test is conducted. The ACF and PACF plots are also used to identify suitable model values.\n\n\nModel Fitting\n\nPlotARCH TestACF PlotPACF Plot\n\n\n\n\nCode\nnormalized_numeric_df$GE.Adjusted<-ts(normalized_numeric_df$GE.Adjusted,star=decimal_date(as.Date(\"2010-01-01\",format = \"%Y-%m-%d\")),frequency = 4)\nnormalized_numeric_df$inflation<-ts(normalized_numeric_df$inflation,star=decimal_date(as.Date(\"2010-01-01\",format = \"%Y-%m-%d\")),frequency = 4)\nnormalized_numeric_df$unemployment<-ts(normalized_numeric_df$unemployment,star=decimal_date(as.Date(\"2010-01-01\",format = \"%Y-%m-%d\")),frequency = 4)\n\nfit <- lm(GE.Adjusted ~ inflation+unemployment, data=normalized_numeric_df)\nfit.res<-ts(residuals(fit),star=decimal_date(as.Date(\"2010-01-01\",format = \"%Y-%m-%d\")),frequency = 4)\n############## Then look at the residuals ############\nreturns <- fit.res  %>% diff()\nautoplot(returns)+ggtitle(\"Linear Model Returns\")\n\n\n\n\n\n\n\n\n\nCode\nbyd.archTest <- ArchTest(fit.res, lags = 1, demean = TRUE)\nbyd.archTest\n\n\n\n    ARCH LM-test; Null hypothesis: no ARCH effects\n\ndata:  fit.res\nChi-squared = 25.401, df = 1, p-value = 4.657e-07\n\n\n\n\n\n\nCode\nggAcf(returns) +ggtitle(\"ACF for returns\")\n\n\n\n\n\n\n\n\n\nCode\nggPacf(returns) +ggtitle(\"PACF for returns\")\n\n\n\n\n\n\n\n\nThe ARCH LM-test was conducted with the null hypothesis of no ARCH effects. The test resulted in a chi-squared value of 25.401 with one degree of freedom, and a very low p-value of 4.657e-07. This suggests strong evidence against the null hypothesis, indicating the presence of ARCH effects in the data.\nBased on the ACF and PACF plots, it appears that there is some significant autocorrelation and partial autocorrelation at multiple lags, which suggests that an ARIMA model may not be sufficient to capture the time series behavior. Additionally, the values for p and q appear to be relatively high, with p = 0 and q = 9 being suggested by the plots.\n\n\nARIMAX Model\n\nAuto Arima ModelAuto Arima ResidualsARIMA (0,1,9) PlotARIMA (0,1,9) ModelCross Validation\n\n\n\n\nCode\nxreg <- cbind(Inflation = normalized_data_ts[, \"inflation\"],\n              Unemployment = normalized_data_ts[, \"unemployment\"])\nfit.auto <- auto.arima(normalized_data_ts[, \"GE.Adjusted\"], xreg = xreg)\nsummary(fit.auto)\n\n\nSeries: normalized_data_ts[, \"GE.Adjusted\"] \nRegression with ARIMA(0,1,0) errors \n\nCoefficients:\n      Inflation  Unemployment\n         0.0262       -0.1293\ns.e.     0.1538        0.0698\n\nsigma^2 = 0.1252:  log likelihood = -18.35\nAIC=42.71   AICc=43.22   BIC=48.5\n\nTraining set error measures:\n                      ME      RMSE       MAE       MPE     MAPE      MASE\nTraining set -0.01726622 0.3434329 0.2723991 -30.29902 65.14664 0.4320894\n                   ACF1\nTraining set 0.09578325\n\n\n\n\n\n\nCode\ncheckresiduals(fit.auto)\n\n\n\n\n\n\n    Ljung-Box test\n\ndata:  Residuals from Regression with ARIMA(0,1,0) errors\nQ* = 4.2215, df = 8, p-value = 0.8366\n\nModel df: 0.   Total lags used: 8\n\n\n\n\n\n\nCode\nset.seed(1234)\n\nmodel_output <- capture.output(sarima(fit.res,0,1,9)) \n\n\n\n\n\n\n\n\n\nCode\ncat(model_output[124:160], model_output[length(model_output)], sep = \"\\n\")\n\n\n\nCall:\narima(x = xdata, order = c(p, d, q), seasonal = list(order = c(P, D, Q), period = S), \n    xreg = constant, transform.pars = trans, fixed = fixed, optim.control = list(trace = trc, \n        REPORT = 1, reltol = tol))\n\nCoefficients:\n         ma1      ma2     ma3     ma4      ma5      ma6     ma7      ma8\n      0.0971  -0.0973  0.0659  0.2270  -0.2299  -0.1028  0.2258  -0.3203\ns.e.  0.1525   0.1693  0.1704  0.1934   0.1897   0.1696  0.1777   0.1800\n          ma9  constant\n      -0.8656   -0.0174\ns.e.   0.1923    0.0208\n\nsigma^2 estimated as 0.08399:  log likelihood = -16.28,  aic = 54.56\n\n$degrees_of_freedom\n[1] 41\n\n$ttable\n         Estimate     SE t.value p.value\nma1        0.0971 0.1525  0.6369  0.5277\nma2       -0.0973 0.1693 -0.5748  0.5686\nma3        0.0659 0.1704  0.3868  0.7009\nma4        0.2270 0.1934  1.1737  0.2473\nma5       -0.2299 0.1897 -1.2118  0.2325\nma6       -0.1028 0.1696 -0.6058  0.5480\nma7        0.2258 0.1777  1.2709  0.2109\nma8       -0.3203 0.1800 -1.7802  0.0825\nma9       -0.8656 0.1923 -4.5006  0.0001\nconstant  -0.0174 0.0208 -0.8327  0.4098\n\n$AIC\n[1] 1.069809\n\n$AICc\n[1] 1.177653\n\n\n\n\n\n\nCode\nn=length(fit.res)\nk= 51\n \n \nrmse1 <- matrix(NA, (n-k),4)\nrmse2 <- matrix(NA, (n-k),4)\nrmse3 <- matrix(NA, (n-k),4)\n\n\nst <- tsp(fit.res)[1]+(k-5)/4 \n\nfor(i in 1:(n-k))\n{\n  xtrain <- window(fit.res, end=st + i/4)\n  xtest <- window(fit.res, start=st + (i+1)/4, end=st + (i+4)/4)\n  \n  #ARIMA(0,1,0) ARIMA(1,1,1)\n  \n  fit <- Arima(xtrain, order=c(0,1,0),\n                include.drift=TRUE, method=\"ML\")\n  fcast <- forecast(fit, h=4)\n  \n  fit2 <- Arima(xtrain, order=c(0,1,9),\n                include.drift=TRUE, method=\"ML\")\n  fcast2 <- forecast(fit2, h=4)\n\n\n  rmse1[i,1:length(xtest)]   <- sqrt((fcast$mean-xtest)^2)\n  rmse2[i,1:length(xtest)] <- sqrt((fcast2$mean-xtest)^2)\n}\n\nplot(1:4,colMeans(rmse1,na.rm=TRUE), type=\"l\",col=2, xlab=\"horizon\", ylab=\"RMSE\")\nlines(1:4, colMeans(rmse2,na.rm=TRUE), type=\"l\",col=3)\nlegend(\"topleft\",legend=c(\"fit1\",\"fit2\"),col=2:4,lty=1)\n\n\n\n\n\n\n\n\nThe auto.arima function suggests the ARIMA(0,1,0) model, but the acf and pacf plots suggest a simpler ARIMA(0,1,9) model. Upon comparison, the AIC, BIC values and the RMSE values of the ARIMA(0,1,0) model are much lower than the other mode, indicating that it is the better choice.\nWe can then proceed to choose the best GARCH model using ARIMA(0,1,0) as the base model.\n\n\nSquared Residuals\n\nPlotACF PlotPACF Plot\n\n\n\n\nCode\nfit <- lm(GE.Adjusted ~ inflation+unemployment, data=normalized_numeric_df)\nfit.res<-ts(residuals(fit),star=decimal_date(as.Date(\"2010-01-01\",format = \"%Y-%m-%d\")),frequency = 4)\nfit <- Arima(fit.res,order=c(0,1,0))\nres=fit$res\nplot(res^2,main='Squared Residuals')\n\n\n\n\n\n\n\n\n\nCode\nacf(res^2,24, main = \"ACF Residuals Square\")\n\n\n\n\n\n\n\n\n\nCode\npacf(res^2,24, main = \"PACF Residuals Square\")\n\n\n\n\n\n\n\n\nFrom the squared residuals of the best ARIMA model, it can be observed that the ACF plot and PACF plot indicate that the residuals are not autocorrelated and are white noise, indicating a good fit of the model. Based on the squared residuals of the best ARIMA model, we can see that the ACF and PACF plots indicate that most of the values lie between the blue lines. Additionally, the p-value is 2 and q-value is 2. This suggests that the model has a good fit and that there is no significant autocorrelation or partial autocorrelation in the residuals. Now we can proceed by fitting GARCH Model for p and q values.\n\n\nGARCH Model\n\nModelGRACH(1,1)GRACH(2,1)GRACH(1,2)\n\n\n\n\nCode\nmodel <- list() ## set counter\ncc <- 1\nfor (p in 1:2) {\n  for (q in 1:2) {\n  \nmodel[[cc]] <- garch(res,order=c(q,p),trace=F)\ncc <- cc + 1\n}\n} \n\n## get AIC values for model evaluation\nGARCH_AIC <- sapply(model, AIC) ## model with lowest AIC is the best\nwhich(GARCH_AIC == min(GARCH_AIC))\n\n\n[1] 1\n\n\nCode\nmodel[[which(GARCH_AIC == min(GARCH_AIC))]]\n\n\n\nCall:\ngarch(x = res, order = c(q, p), trace = F)\n\nCoefficient(s):\n       a0         a1         b1  \n1.080e-01  4.104e-01  4.362e-15  \n\n\n\n\n\n\nCode\nsummary(garchFit(~garch(1,1),res, trace=F))\n\n\n\nTitle:\n GARCH Modelling \n\nCall:\n garchFit(formula = ~garch(1, 1), data = res, trace = F) \n\nMean and Variance Equation:\n data ~ garch(1, 1)\n<environment: 0x143df7858>\n [data = res]\n\nConditional Distribution:\n norm \n\nCoefficient(s):\n       mu      omega     alpha1      beta1  \n0.0026982  0.0219390  0.1528911  0.7287892  \n\nStd. Errors:\n based on Hessian \n\nError Analysis:\n        Estimate  Std. Error  t value Pr(>|t|)  \nmu      0.002698    0.056443    0.048   0.9619  \nomega   0.021939    0.033878    0.648   0.5173  \nalpha1  0.152891    0.161894    0.944   0.3450  \nbeta1   0.728789    0.314481    2.317   0.0205 *\n---\nSignif. codes:  0 '***' 0.001 '**' 0.01 '*' 0.05 '.' 0.1 ' ' 1\n\nLog Likelihood:\n -26.01022    normalized:  -0.5001964 \n\nDescription:\n Tue Jan  9 21:02:45 2024 by user:  \n\n\nStandardised Residuals Tests:\n                                Statistic p-Value  \n Jarque-Bera Test   R    Chi^2  0.4666785 0.7918849\n Shapiro-Wilk Test  R    W      0.9819011 0.6096062\n Ljung-Box Test     R    Q(10)  12.73465  0.2388888\n Ljung-Box Test     R    Q(15)  15.02448  0.449655 \n Ljung-Box Test     R    Q(20)  18.2661   0.5698841\n Ljung-Box Test     R^2  Q(10)  15.00178  0.1319969\n Ljung-Box Test     R^2  Q(15)  18.03366  0.2608999\n Ljung-Box Test     R^2  Q(20)  23.85812  0.2486445\n LM Arch Test       R    TR^2   10.46569  0.5751764\n\nInformation Criterion Statistics:\n     AIC      BIC      SIC     HQIC \n1.154239 1.304335 1.143494 1.211782 \n\n\n\n\n\n\nCode\nsummary(garchFit(~garch(2,1),res, trace=F))\n\n\n\nTitle:\n GARCH Modelling \n\nCall:\n garchFit(formula = ~garch(2, 1), data = res, trace = F) \n\nMean and Variance Equation:\n data ~ garch(2, 1)\n<environment: 0x1406f4f20>\n [data = res]\n\nConditional Distribution:\n norm \n\nCoefficient(s):\n        mu       omega      alpha1      alpha2       beta1  \n0.00269818  0.02535762  0.16597569  0.00000001  0.69408701  \n\nStd. Errors:\n based on Hessian \n\nError Analysis:\n        Estimate  Std. Error  t value Pr(>|t|)  \nmu     2.698e-03   5.762e-02    0.047   0.9627  \nomega  2.536e-02   4.245e-02    0.597   0.5503  \nalpha1 1.660e-01   2.491e-01    0.666   0.5052  \nalpha2 1.000e-08   2.846e-01    0.000   1.0000  \nbeta1  6.941e-01   3.998e-01    1.736   0.0825 .\n---\nSignif. codes:  0 '***' 0.001 '**' 0.01 '*' 0.05 '.' 0.1 ' ' 1\n\nLog Likelihood:\n -26.07054    normalized:  -0.5013566 \n\nDescription:\n Tue Jan  9 21:02:45 2024 by user:  \n\n\nStandardised Residuals Tests:\n                                Statistic p-Value  \n Jarque-Bera Test   R    Chi^2  0.4948635 0.7808035\n Shapiro-Wilk Test  R    W      0.9819356 0.6111687\n Ljung-Box Test     R    Q(10)  13.05934  0.2203731\n Ljung-Box Test     R    Q(15)  15.32623  0.4281832\n Ljung-Box Test     R    Q(20)  18.53298  0.552342 \n Ljung-Box Test     R^2  Q(10)  15.58897  0.1120186\n Ljung-Box Test     R^2  Q(15)  18.56017  0.2343609\n Ljung-Box Test     R^2  Q(20)  24.51469  0.2206318\n LM Arch Test       R    TR^2   10.70848  0.5540532\n\nInformation Criterion Statistics:\n     AIC      BIC      SIC     HQIC \n1.195021 1.382640 1.178604 1.266950 \n\n\n\n\n\n\nCode\nsummary(garchFit(~garch(1,2),res, trace=F))\n\n\n\nTitle:\n GARCH Modelling \n\nCall:\n garchFit(formula = ~garch(1, 2), data = res, trace = F) \n\nMean and Variance Equation:\n data ~ garch(1, 2)\n<environment: 0x144b393f8>\n [data = res]\n\nConditional Distribution:\n norm \n\nCoefficient(s):\n        mu       omega      alpha1       beta1       beta2  \n0.00269818  0.04213902  0.30074805  0.00000001  0.46607725  \n\nStd. Errors:\n based on Hessian \n\nError Analysis:\n        Estimate  Std. Error  t value Pr(>|t|)    \nmu     2.698e-03   3.484e-02    0.077 0.938266    \nomega  4.214e-02   5.345e-02    0.788 0.430431    \nalpha1 3.007e-01   8.079e-02    3.723 0.000197 ***\nbeta1  1.000e-08         NaN      NaN      NaN    \nbeta2  4.661e-01   3.142e-01    1.484 0.137930    \n---\nSignif. codes:  0 '***' 0.001 '**' 0.01 '*' 0.05 '.' 0.1 ' ' 1\n\nLog Likelihood:\n -25.62943    normalized:  -0.4928737 \n\nDescription:\n Tue Jan  9 21:02:45 2024 by user:  \n\n\nStandardised Residuals Tests:\n                                Statistic p-Value  \n Jarque-Bera Test   R    Chi^2  0.6040764 0.7393098\n Shapiro-Wilk Test  R    W      0.984106  0.7108869\n Ljung-Box Test     R    Q(10)  13.05998  0.2203374\n Ljung-Box Test     R    Q(15)  15.24152  0.4341642\n Ljung-Box Test     R    Q(20)  18.46494  0.556809 \n Ljung-Box Test     R^2  Q(10)  14.60177  0.1472692\n Ljung-Box Test     R^2  Q(15)  17.7075   0.2783552\n Ljung-Box Test     R^2  Q(20)  22.47554  0.3152738\n LM Arch Test       R    TR^2   10.36303  0.5841446\n\nInformation Criterion Statistics:\n     AIC      BIC      SIC     HQIC \n1.178055 1.365675 1.161638 1.249984 \n\n\n\n\n\nBased on the analysis of the different GARCH models, it appears that GARCH(1,1) is the optimal choice. Although the AIC values of the different models are relatively similar, we can further evaluate their significance to make a final determination. Upon closer inspection, it appears that GARCH(1,1) has significantly better values than the other models, indicating that it is the most appropriate choice. Therefore, we can conclude that the GARCH(1,1) model is the best fit for the data.\n\n\nBest Model\n\nARIMA ModelGARCH ModelVolatility\n\n\n\n\nCode\n#fiting an ARIMA model to the Inflation variable\ninflation_fit<-auto.arima(normalized_numeric_df$inflation) \nfinflation<-forecast(inflation_fit)\n\n#fitting an ARIMA model to the Unemployment variable\nunemployment_fit<-auto.arima(normalized_numeric_df$unemployment) \nfunemployment<-forecast(unemployment_fit)\n\n# best model fit for forcasting\nxreg <- cbind(Inflation = normalized_data_ts[, \"inflation\"],\n              Unemployment = normalized_data_ts[, \"unemployment\"])\n\n\n\nsummary(arima.fit<-Arima(normalized_data_ts[, \"GE.Adjusted\"],order=c(0,1,0),xreg=xreg),include.drift = TRUE)\n\n\nSeries: normalized_data_ts[, \"GE.Adjusted\"] \nRegression with ARIMA(0,1,0) errors \n\nCoefficients:\n      Inflation  Unemployment\n         0.0262       -0.1293\ns.e.     0.1538        0.0698\n\nsigma^2 = 0.1252:  log likelihood = -18.35\nAIC=42.71   AICc=43.22   BIC=48.5\n\nTraining set error measures:\n                      ME      RMSE       MAE       MPE     MAPE      MASE\nTraining set -0.01726622 0.3434329 0.2723991 -30.29902 65.14664 0.4320894\n                   ACF1\nTraining set 0.09578325\n\n\n\n\n\n\nCode\nsummary(final.fit <- garchFit(~garch(1,1), res,trace = F))\n\n\n\nTitle:\n GARCH Modelling \n\nCall:\n garchFit(formula = ~garch(1, 1), data = res, trace = F) \n\nMean and Variance Equation:\n data ~ garch(1, 1)\n<environment: 0x137654000>\n [data = res]\n\nConditional Distribution:\n norm \n\nCoefficient(s):\n       mu      omega     alpha1      beta1  \n0.0026982  0.0219390  0.1528911  0.7287892  \n\nStd. Errors:\n based on Hessian \n\nError Analysis:\n        Estimate  Std. Error  t value Pr(>|t|)  \nmu      0.002698    0.056443    0.048   0.9619  \nomega   0.021939    0.033878    0.648   0.5173  \nalpha1  0.152891    0.161894    0.944   0.3450  \nbeta1   0.728789    0.314481    2.317   0.0205 *\n---\nSignif. codes:  0 '***' 0.001 '**' 0.01 '*' 0.05 '.' 0.1 ' ' 1\n\nLog Likelihood:\n -26.01022    normalized:  -0.5001964 \n\nDescription:\n Tue Jan  9 21:02:46 2024 by user:  \n\n\nStandardised Residuals Tests:\n                                Statistic p-Value  \n Jarque-Bera Test   R    Chi^2  0.4666785 0.7918849\n Shapiro-Wilk Test  R    W      0.9819011 0.6096062\n Ljung-Box Test     R    Q(10)  12.73465  0.2388888\n Ljung-Box Test     R    Q(15)  15.02448  0.449655 \n Ljung-Box Test     R    Q(20)  18.2661   0.5698841\n Ljung-Box Test     R^2  Q(10)  15.00178  0.1319969\n Ljung-Box Test     R^2  Q(15)  18.03366  0.2608999\n Ljung-Box Test     R^2  Q(20)  23.85812  0.2486445\n LM Arch Test       R    TR^2   10.46569  0.5751764\n\nInformation Criterion Statistics:\n     AIC      BIC      SIC     HQIC \n1.154239 1.304335 1.143494 1.211782 \n\n\n\n\n\n\nCode\nht <- final.fit@h.t #a numeric vector with the conditional variances (h.t = sigma.t^delta)\n\n#############################\ndata=data.frame(final)\ndata$Date<-as.Date(data$Date,\"%Y-%m-%d\")\n\n\ndata2= data.frame(ht,data$Date)\nggplot(data2, aes(y = ht, x = data.Date)) + geom_line(col = '#FDD835') + ylab('Conditional Variance') + xlab('Date')\n\n\n\n\n\n\n\n\nFrom the ARIMA(0,1,0), we see that the training set error measures also suggest a good fit, with low mean absolute error, root mean squared error, and autocorrelation of the residuals. GATCH(1,1) model model is used to estimate the volatility of the standardized residuals of the previous regression model. The model includes a mean equation that estimates the mean of the residuals and a variance equation that models the conditional variance of the residuals. The coefficients of the mean equation suggest that the mean of the residuals is close to zero. The variance equation coefficients suggest that the conditional variance of the residuals is dependent on the past conditional variances and the past squared standardized residuals. The model’s AIC, BIC, SIC, and HQIC values are all relatively low, indicating a good fit of the model. The standardized residuals tests indicate that the residuals are approximately normally distributed and that there is no significant autocorrelation in the residuals.\nThe volatility of the model seems high in 2020 but has decreased gradually in the past few months. This could indicate that the asset’s price was experiencing a lot of fluctuations in 2020, but the market has stabilized recently.\n\n\nModel Diagnostics\n\nResidualsQQ PlotBox Test\n\n\n\n\nCode\nfit2<-garch(res,order=c(1,1),trace=F)\ncheckresiduals(fit2) \n\n\n\n\n\n\n\n\n\nCode\nqqnorm(fit2$residuals, pch = 1)\nqqline(fit2$residuals, col = \"blue\", lwd = 2)\n\n\n\n\n\n\n\n\n\nCode\nBox.test (fit2$residuals, type = \"Ljung\")\n\n\n\n    Box-Ljung test\n\ndata:  fit2$residuals\nX-squared = 0.27111, df = 1, p-value = 0.6026\n\n\nThe ACF plot of the residuals shows all the values between the blue lines, which indicates that the residuals are not significantly autocorrelated. The range of values for the residual plot between -2 and 2 is considered acceptable. Additionally, the QQ plot of the residuals shows a linear plot on the line, which is another good indication that the residuals are normally distributed. The QQ plot is a valuable tool to assess if the residuals follow a normal distribution, and in this case, the plot suggests that the residuals do indeed follow a normal distribution.\nThe Box-Ljung test, a p-value of 0.6026 indicates that the model’s residuals are not significantly autocorrelated, meaning that the model has captured most of the information in the data. This result is good because it suggests that the model is a good fit for the data and has accounted for most of the underlying patterns in the data. Therefore, we can rely on the model’s predictions and use them to make informed decisions.\n\n\n\n\n\nForecast\n\n\nCode\npredict(final.fit, n.ahead = 5, plot=TRUE)\n\n\n\n\n\n  meanForecast meanError standardDeviation lowerInterval upperInterval\n1  0.002698185 0.4377442         0.4377442    -0.8552647     0.8606611\n2  0.002698185 0.4369057         0.4369057    -0.8536212     0.8590175\n3  0.002698185 0.4361650         0.4361650    -0.8521695     0.8575659\n4  0.002698185 0.4355109         0.4355109    -0.8508875     0.8562839\n5  0.002698185 0.4349334         0.4349334    -0.8497556     0.8551520\n\n\nThe forecasted plot is based on the best model ARIMAX(0,1,0)+GARCH(1,1). This model takes into account the autoregressive and moving average components of the data, as well as the impact of exogenous variables on the time series. Additionally, the GARCH component of the model accounts for the volatility clustering in the data. Overall, this model is well-suited to make accurate predictions about future values of the time series.\n\n\nEquation of the Model\nThe equation of the ARIMAX(0,1,0) model is:\n\\(Y(t) = Y(t-1) + \\epsilon(t)\\)\nwhere \\(Y(t)\\) is the time series variable and \\(\\epsilon(t)\\) is the error term.\nThe equation of the GARCH(1,1) model is:\n\\(\\sigma^2(t) = \\alpha_0+\\alpha_1\\epsilon_t^2(t-1)+ \\beta_1\\sigma^2(t-1)\\)\nwhere \\(\\sigma^2_t\\) is the conditional variance at time \\(t\\), \\(\\alpha_0\\) is a constant, \\(\\alpha_1\\) and \\(\\beta_1\\) are the parameters, and \\(\\epsilon_t\\) is the error term.\nThe combined equation of the ARIMAX(1,1,1)+GARCH(1,1) model is:\n\\(Y(t) = Y(t-1) + \\epsilon(t)\\)\n\\(\\epsilon(t) = \\sigma(t) * \\epsilon~(t)\\)\n\\(\\sigma^2(t) = \\alpha_0+\\alpha_1\\epsilon_t^2(t-1)+ \\beta_1\\sigma^2(t-1)\\)"
  },
  {
    "objectID": "arch_Materials_Sector_Fund.html",
    "href": "arch_Materials_Sector_Fund.html",
    "title": "",
    "section": "",
    "text": "Impact of Macroeconomic Factors on Dow Inc. Stock Price\n\n\n\nOne of the best companies in the material sector is Dow Inc. (DOW), a multinational chemical corporation that produces a wide range of products including plastics, chemicals, and agricultural products. As of 2021, Dow Inc. is ranked as one of the largest chemical producers in the world, with operations in over 160 countries.\nBy using an ARIMAX+ARCH/GARCH model to analyze the stock price behavior of Dow, we can gain insights into how macroeconomic factors impact the company’s performance. For example, an increase in GDP growth rate or a decrease in unemployment rate may lead to increased consumer spending and a rise in Dow’s stock price. Conversely, an increase in inflation or interest rates may lead to a decrease in consumer spending and a decline in Dow’s stock price.\n\n\nTime series Plot\n\nDow Inc.Differentitaed Chart SeriesGDP GrowthInflationInterestUnemployment\n\n\n\n\nCode\n# get data\noptions(\"getSymbols.warning4.0\"=FALSE)\noptions(\"getSymbols.yahoo.warning\"=FALSE)\n\n\ndata.info = getSymbols(\"DOW\",src='yahoo', from = '2020-01-01',to = \"2023-03-01\",auto.assign = FALSE)\ndata = getSymbols(\"DOW\",src='yahoo', from = '2020-01-01',to = \"2023-03-01\")\ndf <- data.frame(Date=index(DOW),coredata(DOW))\n\n# create Bollinger Bands\nbbands <- BBands(DOW[,c(\"DOW.High\",\"DOW.Low\",\"DOW.Close\")])\n\n# join and subset data\ndf <- subset(cbind(df, data.frame(bbands[,1:3])), Date >= \"2020-01-01\")\n\n\n# colors column for increasing and decreasing\nfor (i in 1:length(df[,1])) {\n  if (df$DOW.Close[i] >= df$DOW.Open[i]) {\n      df$direction[i] = 'Increasing'\n  } else {\n      df$direction[i] = 'Decreasing'\n  }\n}\n\ni <- list(line = list(color = '#DC7633'))\nd <- list(line = list(color = '#7F7F7F'))\n\n# plot candlestick chart\n\nfig <- df %>% plot_ly(x = ~Date, type=\"candlestick\",\n          open = ~DOW.Open, close = ~DOW.Close,\n          high = ~DOW.High, low = ~DOW.Low, name = \"DOW\",\n          increasing = i, decreasing = d) \nfig <- fig %>% add_lines(x = ~Date, y = ~up , name = \"B Bands\",\n            line = list(color = '#ccc', width = 0.5),\n            legendgroup = \"Bollinger Bands\",\n            hoverinfo = \"none\", inherit = F) \nfig <- fig %>% add_lines(x = ~Date, y = ~dn, name = \"B Bands\",\n            line = list(color = '#ccc', width = 0.5),\n            legendgroup = \"Bollinger Bands\", inherit = F,\n            showlegend = FALSE, hoverinfo = \"none\") \nfig <- fig %>% add_lines(x = ~Date, y = ~mavg, name = \"Mv Avg\",\n            line = list(color = '#C052B3', width = 0.5),\n            hoverinfo = \"none\", inherit = F) \nfig <- fig %>% layout(yaxis = list(title = \"Price\"))\n\n# plot volume bar chart\nfig2 <- df \nfig2 <- fig2 %>% plot_ly(x=~Date, y=~DOW.Volume, type='bar', name = \"DOW Volume\",\n          color = ~direction, colors = c('#DC7633','#7F7F7F')) \nfig2 <- fig2 %>% layout(yaxis = list(title = \"Volume\"))\n\n# create rangeselector buttons\nrs <- list(visible = TRUE, x = 0.5, y = -0.055,\n           xanchor = 'center', yref = 'paper',\n           font = list(size = 9),\n           buttons = list(\n             list(count=1,\n                  label='RESET',\n                  step='all'),\n             list(count=3,\n                  label='3 YR',\n                  step='year',\n                  stepmode='backward'),\n             list(count=1,\n                  label='1 YR',\n                  step='year',\n                  stepmode='backward'),\n             list(count=1,\n                  label='1 MO',\n                  step='month',\n                  stepmode='backward')\n           ))\n\n# subplot with shared x axis\nfig <- subplot(fig, fig2, heights = c(0.7,0.2), nrows=2,\n             shareX = TRUE, titleY = TRUE)\nfig <- fig %>% layout(title = paste(\"Dow Inc. Stock Price: January 2020 - March 2023\"),\n         xaxis = list(rangeselector = rs),\n         legend = list(orientation = 'h', x = 0.5, y = 1,\n                       xanchor = 'center', yref = 'paper',\n                       font = list(size = 10),\n                       bgcolor = 'transparent'))\n\nfig\n\n\n\n\n\n\n\n\n\n\nCode\nlog(data.info$`DOW.Adjusted`) %>% diff() %>% chartSeries(theme=chartTheme('white'),up.col='#DC7633')\n\n\n\n\n\n\n\n\n\nCode\n#import the data\ngdp <- read.csv(\"DATA/RAW DATA/gdp-growth.csv\")\n\n#change date format\ngdp$Date <- as.Date(gdp$DATE , \"%m/%d/%Y\")\n\n#drop DATE column\ngdp <- subset(gdp, select = -c(1))\n\n#export the cleaned data\ngdp_clean <- gdp\nwrite.csv(gdp_clean, \"DATA/CLEANED DATA/gdp_clean_data.csv\", row.names=FALSE)\n\n#plot gdp growth rate \nfig <- plot_ly(gdp, x = ~Date, y = ~value, type = 'scatter', mode = 'lines',line = list(color = 'rgb(240, 128, 128)'))\nfig <- fig %>% layout(title = \"U.S GPD Growth Rate: 2010 - 2022\",xaxis = list(title = \"Time\"),yaxis = list(title =\"GDP Growth Rate\"))\nfig\n\n\n\n\n\n\n\n\n\n\nCode\n#import the data\ninflation_rate <- read.csv(\"DATA/RAW DATA/inflation-rate.csv\")\n\n#cleaning the data\n#remove unwanted columns\ninflation_rate_clean <- subset(inflation_rate, select = -c(1,HALF1,HALF2))\n\n#convert the data to time series data\ninflation_data_ts <- ts(as.vector(t(as.matrix(inflation_rate_clean))), start=c(2010,1), end=c(2023,2), frequency=12)\n\n#export the data\nwrite.csv(inflation_rate_clean, \"DATA/CLEANED DATA/inflation_rate_clean_data.csv\", row.names=FALSE)\n\n\n#plot inflation rate \nfig <- autoplot(inflation_data_ts, ylab = \"Inflation Rate\", color=\"#FFA07A\")+ggtitle(\"U.S Inflation Rate: January 2020 - February 2023\")+theme_bw()\nggplotly(fig)\n\n\n\n\n\n\n\n\n\n\nCode\n#import the data\ninterest_data <- read.csv(\"DATA/RAW DATA/interest-rate.csv\")\n\n#change date format\ninterest_data$Date <- as.Date(interest_data$Date , \"%m/%d/%Y\")\n\n#export the cleaned data\ninterest_clean_data <- interest_data\nwrite.csv(interest_clean_data, \"DATA/CLEANED DATA/interest_rate_clean_data.csv\", row.names=FALSE)\n\n#plot interest rate \nfig <- plot_ly(interest_data, x = ~Date, y = ~value, type = 'scatter', mode = 'lines',line = list(color='rgb(219, 112, 147)'))\nfig <- fig %>% layout(title = \"U.S Interest Rate: January 2010 - March 2023\",xaxis = list(title = \"Time\"),yaxis = list(title =\"Interest Rate\"))\nfig\n\n\n\n\n\n\n\n\n\n\nCode\n#import the data\nunemployment_rate <- read.csv(\"DATA/RAW DATA/unemployment-rate.csv\")\n\n#change date format\nunemployment_rate$Date <- as.Date(unemployment_rate$Date , \"%m/%d/%Y\")\n\n# export the data\nwrite.csv(unemployment_rate, \"DATA/CLEANED DATA/unemployment_rate_clean_data.csv\", row.names=FALSE)\n\n#plot unemployment rate \nfig <- plot_ly(unemployment_rate, x = ~Date, y = ~Value, type = 'scatter', mode = 'lines',line = list(color = 'rgb(189, 183, 107)'))\nfig <- fig %>% layout(title = \"U.S Unemployment Rate: January 2010 - March 2023\",xaxis = list(title = \"Time\"),yaxis = list(title =\"Unemployment Rate\"))\nfig\n\n\n\n\n\n\n\n\n\nIn 2020, the COVID-19 pandemic had a significant impact on Dow Inc.’s stock price, as the company’s operations were affected by global supply chain disruptions and a decline in demand for its products. The company’s stock price reached a low point in March 2020 but began to recover gradually in the following months as markets responded positively to government stimulus measures and signs of economic recovery.\nIn 2021, Dow Inc.’s stock price continued to trend upwards, reflecting the company’s strong financial performance and positive outlook for its core businesses. The company’s focus on sustainability and innovation, as well as its strategic partnerships and acquisitions, have helped to position it for long-term growth and success.\nOverall, the stock market of Dow Inc. from 2020 to 2023 has been characterized by volatility and uncertainty, but the company’s resilience and ability to adapt to changing market conditions have allowed it to emerge as a leader in the materials science industry.\nAs discussed before, the macroeconomic factors of GDP growth, inflation, interest rates, and unemployment rate are closely interrelated and play a crucial role in the overall health and stability of an economy. From 2020 to 2023, the global economy experienced a mix of ups and downs, with periods of strong GDP growth followed by slowdowns and recessions.\nThe second plot shows the first difference of the logarithm of the adjusted Dow Inc. stock price. Taking the first difference removes any long-term trends and transforms the time series into a stationary process. From the plot, we can observe that the first difference of the logarithm of the Dow Inc. stock price appears to be stationary, as the mean and variance are roughly constant over time.\n\n\nEnodogenous and Exogenous Variables\n\nPlotCorrelation HeatmapCCF GDPCCF InterestCCF InflationCCF Unemployment\n\n\n\n\nCode\nnumeric_data <- c(\"DOW.Adjusted\",\"gdp\", \"interest\", \"inflation\", \"unemployment\")\nnumeric_data <- final[, numeric_data]\nnormalized_data_numeric <- scale(numeric_data)\nnormalized_data <- ts(normalized_data_numeric, start = c(2020, 1), end = c(2021,10),frequency = 4)\nts_plot(normalized_data,\n        title = \"Normalized Time Series Data for DOW Stock and Macroeconomic Variables\",\n        Ytitle = \"Normalized Values\",\n        Xtitle = \"Year\")\n\n\n\n\n\n\n\n\n\n\nCode\n# Get upper triangle of the correlation matrix\nget_upper_tri <- function(cormat){\n    cormat[lower.tri(cormat)]<- NA\n    return(cormat)\n}\ncormat <- round(cor(normalized_data_numeric),2)\n\nupper_tri <- get_upper_tri(cormat)\n\nmelted_cormat <- melt(upper_tri, na.rm = TRUE)\n# Create a ggheatmap\nggheatmap <- ggplot(melted_cormat, aes(Var2, Var1, fill = value))+\n geom_tile(color = \"white\")+\n scale_fill_gradient2(low = \"blue\", high = \"red\", mid = \"white\", \n   midpoint = 0, limit = c(-1,1), space = \"Lab\", \n    name=\"Pearson\\nCorrelation\") +\n  theme_minimal()+ # minimal theme\n theme(axis.text.x = element_text(angle = 45, vjust = 1, \n    size = 12, hjust = 1))+\n coord_fixed()\n\nggheatmap + \ngeom_text(aes(Var2, Var1, label = value), color = \"black\", size = 4) +\ntheme(\n  axis.title.x = element_blank(),\n  axis.title.y = element_blank(),\n  panel.grid.major = element_blank(),\n  panel.border = element_blank(),\n  panel.background = element_blank(),\n  axis.ticks = element_blank(),\n  legend.justification = c(1, 0),\n  legend.position = c(0.6, 0.7),\n  legend.direction = \"horizontal\")+\n  guides(fill = guide_colorbar(barwidth = 7, barheight = 1,\n                title.position = \"top\", title.hjust = 0.5))\n\n\n\n\n\n\n\n\n\nCode\npar(mfrow=c(1,1))\nccf_result <- ccf(normalized_data[, c(\"DOW.Adjusted\")], normalized_data[, c(\"gdp\")], \n    lag.max = 300,\n    main = \"Cros-Correlation Plot for DOW Stock Price and GDP Growth Rate \",\n    ylab = \"CCF\")\n\n\n\n\n\nCode\ncat(\"The sum of cross correlation function is\", sum(abs(ccf_result$acf)))\n\n\nThe sum of cross correlation function is 6.093041\n\n\n\n\n\n\nCode\npar(mfrow=c(1,1))\nccf_result <- ccf(normalized_data[, c(\"DOW.Adjusted\")], normalized_data[, c(\"interest\")], \n    lag.max = 300,\n    main = \"Cros-Correlation Plot for DOW Stock Price and Interest Rate\",\n    ylab = \"CCF\")\n\n\n\n\n\nCode\ncat(\"The sum of cross correlation function is\", sum(abs(ccf_result$acf)))\n\n\nThe sum of cross correlation function is 6.720415\n\n\n\n\n\n\nCode\npar(mfrow=c(1,1))\nccf_result <- ccf(normalized_data[, c(\"DOW.Adjusted\")], normalized_data[, c(\"inflation\")], \n    lag.max = 300,\n    main = \"Cros-Correlation Plot for DOW Stock Price and Inflation Rate\",\n    ylab = \"CCF\")\n\n\n\n\n\nCode\ncat(\"The sum of cross correlation function is\", sum(abs(ccf_result$acf)))\n\n\nThe sum of cross correlation function is 7.439476\n\n\n\n\n\n\nCode\npar(mfrow=c(1,1))\nccf_result <- ccf(normalized_data[, c(\"DOW.Adjusted\")], normalized_data[, c(\"unemployment\")], \n    lag.max = 300,\n    main = \"Cros-Correlation Plot for DOW Stock Priceand Unemployment Rate\",\n    ylab = \"CCF\")\n\n\n\n\n\nCode\ncat(\"The sum of cross correlation function is\", sum(abs(ccf_result$acf)))\n\n\nThe sum of cross correlation function is 5.23597\n\n\n\n\n\nThe Normalized Time Series Data for Stock Price and Macroeconomic Variables plot shows the same variables as the first plot but has been normalized to a common range of 0 to 1 using the scale() function in R, which standardizes the variables to have a mean of 0 and a standard deviation of 1. The heatmap analysis of the normalized data reveals that inflation rate exhibit strong positive correlations with the stock price indices, indicating that these variables may significantly influence stock price movements. On the other hand, weaker correlations were observed between the stock price indices and GDP and interest rates, suggesting that these variables may have less impact on stock price fluctuations. The cross-correlation feature plots confirm these findings, indicating that inflation rate are more suitable feature variables for the ARIMAX model when predicting Dow Inc. movements.\nFinal Exogenous variables: Macroeconomic indicator: Inflation rate\n\n\nEnodogenous and Exogenous Variables Plot\n\nPlotCheck the stationarity\n\n\n\n\nCode\nfinal_data <- final %>%dplyr::select( Date,DOW.Adjusted, inflation)\nnumeric_data <- c(\"DOW.Adjusted\", \"inflation\")\nnumeric_data <- final_data[, numeric_data]\nnormalized_data_numeric <- scale(numeric_data)\nnormalized_numeric_df <- data.frame(normalized_data_numeric)\nnormalized_data_ts <- ts(normalized_data_numeric, start = c(2020, 1), frequency = 4)\n\nautoplot(normalized_data_ts, facets=TRUE) +\n  xlab(\"Year\") + ylab(\"\") +\n  ggtitle(\"Dow Inc. Stock Price, Inflation Rate in USA 2020-2023\")\n\n\n\n\n\n\n\n\n\nCode\n# Convert your multivariate time series data to a matrix\nfinal_data_ts_multivariate <- as.matrix(normalized_data_ts)\n\n# Check for stationarity using Phillips-Perron test\nphillips_perron_test <- ur.pp(final_data_ts_multivariate)  \nsummary(phillips_perron_test)\n\n\n\n################################## \n# Phillips-Perron Unit Root Test # \n################################## \n\nTest regression with intercept \n\n\nCall:\nlm(formula = y ~ y.l1)\n\nResiduals:\n    Min      1Q  Median      3Q     Max \n-2.1410 -0.3354  0.1386  0.4878  0.8967 \n\nCoefficients:\n            Estimate Std. Error t value Pr(>|t|)    \n(Intercept)  0.05182    0.14270   0.363     0.72    \ny.l1         0.77281    0.15149   5.101 4.72e-05 ***\n---\nSignif. codes:  0 '***' 0.001 '**' 0.01 '*' 0.05 '.' 0.1 ' ' 1\n\nResidual standard error: 0.6832 on 21 degrees of freedom\nMultiple R-squared:  0.5534,    Adjusted R-squared:  0.5322 \nF-statistic: 26.02 on 1 and 21 DF,  p-value: 4.72e-05\n\n\nValue of test-statistic, type: Z-alpha  is: -6.0229 \n\n         aux. Z statistics\nZ-tau-mu            0.3401\n\n\n\n\n\nThe results of the Phillips-Perron unit root test indicate strong evidence against the null hypothesis of a unit root, as the p-value for the coefficient of the lagged variable is less than the significance level of 0.05. This suggests that the variable y, which is being tested for stationarity, is likely stationary. Furthermore, the test statistic Z-tau-mu is 0.3401, which is smaller than the critical value of Z-alpha (-6.0229), providing further evidence of stationarity.\nTo determine whether the linear model requires an ARCH model, an ARCH test is conducted. The ACF and PACF plots are also used to identify suitable model values.\n\n\nModel Fitting\n\nPlotARCH Test\n\n\n\n\nCode\nnormalized_numeric_df$DOW.Adjusted<-ts(normalized_numeric_df$DOW.Adjusted,star=decimal_date(as.Date(\"2020-01-01\",format = \"%Y-%m-%d\")),frequency = 4)\nnormalized_numeric_df$inflation<-ts(normalized_numeric_df$inflation,star=decimal_date(as.Date(\"2020-01-01\",format = \"%Y-%m-%d\")),frequency = 4)\n\n\nfit <- lm(DOW.Adjusted ~ inflation, data=normalized_numeric_df)\nfit.res<-ts(residuals(fit),star=decimal_date(as.Date(\"2020-01-01\",format = \"%Y-%m-%d\")),frequency = 4)\n############## Then look at the residuals ############\nreturns <- fit.res  %>% diff()\nautoplot(returns)+ggtitle(\"Linear Model Returns\")\n\n\n\n\n\n\n\n\n\nCode\nbyd.archTest <- ArchTest(fit.res, lags = 1, demean = TRUE)\nbyd.archTest\n\n\n\n    ARCH LM-test; Null hypothesis: no ARCH effects\n\ndata:  fit.res\nChi-squared = 0.37793, df = 1, p-value = 0.5387\n\n\n\n\n\nThe ARCH LM-test was conducted with the null hypothesis of no ARCH effects. The test resulted in a chi-squared value of 0.37793 with one degree of freedom, and a very low p-value of 0.5387. This suggests strong evidence for the null hypothesis, indicating the no presence of ARCH effects in the data."
  },
  {
    "objectID": "arch_Real_Estate_Sector_Fund.html",
    "href": "arch_Real_Estate_Sector_Fund.html",
    "title": "",
    "section": "",
    "text": "Impact of Macroeconomic Factors on Simon Property Group Inc Stock Price\n\n\n\nOne of the best companies in the real estate sector is Simon Property Group Inc. (SPG). Simon Property Group is a real estate investment trust (REIT) that owns and operates shopping malls, premium outlets, and mixed-use properties in the United States, Europe, and Asia. The company has a strong track record of generating steady rental income from its properties and has consistently paid out dividends to its shareholders. Additionally, Simon Property Group has a diversified portfolio of properties, which helps to mitigate risk and provide stable returns.\nBy using an ARIMAX+ARCH/GARCH model to analyze the stock price behavior ofSimon Property Group Inc., we can gain insights into how macroeconomic factors impact the company’s performance. For example, an increase in GDP growth rate or a decrease in unemployment rate may lead to increased consumer spending and a rise in Simon Property Group’s stock price. Conversely, an increase in inflation or interest rates may lead to a decrease in consumer spending and a decline in Simon Property Group’s stock price.\n\n\nTime series Plot\n\nSimon Property Group IncDifferentitaed Chart SeriesGDP GrowthInflationInterestUnemployment\n\n\n\n\nCode\n# get data\noptions(\"getSymbols.warning4.0\"=FALSE)\noptions(\"getSymbols.yahoo.warning\"=FALSE)\n\n\ndata.info = getSymbols(\"SPG\",src='yahoo', from = '2010-01-01',to = \"2023-03-01\",auto.assign = FALSE)\ndata = getSymbols(\"SPG\",src='yahoo', from = '2010-01-01',to = \"2023-03-01\")\ndf <- data.frame(Date=index(SPG),coredata(SPG))\n\n# create Bollinger Bands\nbbands <- BBands(SPG[,c(\"SPG.High\",\"SPG.Low\",\"SPG.Close\")])\n\n# join and subset data\ndf <- subset(cbind(df, data.frame(bbands[,1:3])), Date >= \"2010-01-01\")\n\n\n# colors column for increasing and decreasing\nfor (i in 1:length(df[,1])) {\n  if (df$SPG.Close[i] >= df$SPG.Open[i]) {\n      df$direction[i] = 'Increasing'\n  } else {\n      df$direction[i] = 'Decreasing'\n  }\n}\n\ni <- list(line = list(color = '#C39BD3'))\nd <- list(line = list(color = '#7F7F7F'))\n\n# plot candlestick chart\n\nfig <- df %>% plot_ly(x = ~Date, type=\"candlestick\",\n          open = ~SPG.Open, close = ~SPG.Close,\n          high = ~SPG.High, low = ~SPG.Low, name = \"SPG\",\n          increasing = i, decreasing = d) \nfig <- fig %>% add_lines(x = ~Date, y = ~up , name = \"B Bands\",\n            line = list(color = '#ccc', width = 0.5),\n            legendgroup = \"Bollinger Bands\",\n            hoverinfo = \"none\", inherit = F) \nfig <- fig %>% add_lines(x = ~Date, y = ~dn, name = \"B Bands\",\n            line = list(color = '#ccc', width = 0.5),\n            legendgroup = \"Bollinger Bands\", inherit = F,\n            showlegend = FALSE, hoverinfo = \"none\") \nfig <- fig %>% add_lines(x = ~Date, y = ~mavg, name = \"Mv Avg\",\n            line = list(color = '#C052B3', width = 0.5),\n            hoverinfo = \"none\", inherit = F) \nfig <- fig %>% layout(yaxis = list(title = \"Price\"))\n\n# plot volume bar chart\nfig2 <- df \nfig2 <- fig2 %>% plot_ly(x=~Date, y=~SPG.Volume, type='bar', name = \"SPG Volume\",\n          color = ~direction, colors = c('#C39BD3','#7F7F7F')) \nfig2 <- fig2 %>% layout(yaxis = list(title = \"Volume\"))\n\n# create rangeselector buttons\nrs <- list(visible = TRUE, x = 0.5, y = -0.055,\n           xanchor = 'center', yref = 'paper',\n           font = list(size = 9),\n           buttons = list(\n             list(count=1,\n                  label='RESET',\n                  step='all'),\n             list(count=3,\n                  label='3 YR',\n                  step='year',\n                  stepmode='backward'),\n             list(count=1,\n                  label='1 YR',\n                  step='year',\n                  stepmode='backward'),\n             list(count=1,\n                  label='1 MO',\n                  step='month',\n                  stepmode='backward')\n           ))\n\n# subplot with shared x axis\nfig <- subplot(fig, fig2, heights = c(0.7,0.2), nrows=2,\n             shareX = TRUE, titleY = TRUE)\nfig <- fig %>% layout(title = paste(\"Simon Property Group Inc Stock Price: January 2010 - March 2023\"),\n         xaxis = list(rangeselector = rs),\n         legend = list(orientation = 'h', x = 0.5, y = 1,\n                       xanchor = 'center', yref = 'paper',\n                       font = list(size = 10),\n                       bgcolor = 'transparent'))\n\nfig\n\n\n\n\n\n\n\n\n\n\nCode\nlog(data.info$`SPG.Adjusted`) %>% diff() %>% chartSeries(theme=chartTheme('white'),up.col='#C39BD3')\n\n\n\n\n\n\n\n\n\nCode\n#import the data\ngdp <- read.csv(\"DATA/RAW DATA/gdp-growth.csv\")\n\n#change date format\ngdp$Date <- as.Date(gdp$DATE , \"%m/%d/%Y\")\n\n#drop DATE column\ngdp <- subset(gdp, select = -c(1))\n\n#export the cleaned data\ngdp_clean <- gdp\nwrite.csv(gdp_clean, \"DATA/CLEANED DATA/gdp_clean_data.csv\", row.names=FALSE)\n\n#plot gdp growth rate \nfig <- plot_ly(gdp, x = ~Date, y = ~value, type = 'scatter', mode = 'lines',line = list(color = 'rgb(240, 128, 128)'))\nfig <- fig %>% layout(title = \"U.S GPD Growth Rate: 2010 - 2022\",xaxis = list(title = \"Time\"),yaxis = list(title =\"GDP Growth Rate\"))\nfig\n\n\n\n\n\n\n\n\n\n\nCode\n#import the data\ninflation_rate <- read.csv(\"DATA/RAW DATA/inflation-rate.csv\")\n\n#cleaning the data\n#remove unwanted columns\ninflation_rate_clean <- subset(inflation_rate, select = -c(1,HALF1,HALF2))\n\n#convert the data to time series data\ninflation_data_ts <- ts(as.vector(t(as.matrix(inflation_rate_clean))), start=c(2010,1), end=c(2023,2), frequency=12)\n\n#export the data\nwrite.csv(inflation_rate_clean, \"DATA/CLEANED DATA/inflation_rate_clean_data.csv\", row.names=FALSE)\n\n\n#plot inflation rate \nfig <- autoplot(inflation_data_ts, ylab = \"Inflation Rate\", color=\"#FFA07A\")+ggtitle(\"U.S Inflation Rate: January 2010 - February 2023\")+theme_bw()\nggplotly(fig)\n\n\n\n\n\n\n\n\n\n\nCode\n#import the data\ninterest_data <- read.csv(\"DATA/RAW DATA/interest-rate.csv\")\n\n#change date format\ninterest_data$Date <- as.Date(interest_data$Date , \"%m/%d/%Y\")\n\n#export the cleaned data\ninterest_clean_data <- interest_data\nwrite.csv(interest_clean_data, \"DATA/CLEANED DATA/interest_rate_clean_data.csv\", row.names=FALSE)\n\n#plot interest rate \nfig <- plot_ly(interest_data, x = ~Date, y = ~value, type = 'scatter', mode = 'lines',line = list(color='rgb(219, 112, 147)'))\nfig <- fig %>% layout(title = \"U.S Interest Rate: January 2010 - March 2023\",xaxis = list(title = \"Time\"),yaxis = list(title =\"Interest Rate\"))\nfig\n\n\n\n\n\n\n\n\n\n\nCode\n#import the data\nunemployment_rate <- read.csv(\"DATA/RAW DATA/unemployment-rate.csv\")\n\n#change date format\nunemployment_rate$Date <- as.Date(unemployment_rate$Date , \"%m/%d/%Y\")\n\n# export the data\nwrite.csv(unemployment_rate, \"DATA/CLEANED DATA/unemployment_rate_clean_data.csv\", row.names=FALSE)\n\n#plot unemployment rate \nfig <- plot_ly(unemployment_rate, x = ~Date, y = ~Value, type = 'scatter', mode = 'lines',line = list(color = 'rgb(189, 183, 107)'))\nfig <- fig %>% layout(title = \"U.S Unemployment Rate: January 2010 - March 2023\",xaxis = list(title = \"Time\"),yaxis = list(title =\"Unemployment Rate\"))\nfig\n\n\n\n\n\n\n\n\n\nFrom 2010 to early 2020, SPG’s stock price exhibited a generally upward trend, with occasional dips and plateaus along the way.\nIn particular, SPG’s stock price experienced a strong rebound following the economic downturn of 2008-2009, reaching pre-recession levels by early 2010. From there, the stock price continued to climb, reaching an all-time high in mid-2016. However, in the years that followed, SPG’s stock price experienced more volatility and fluctuation.\nIn early 2020, the outbreak of the COVID-19 pandemic led to a sharp decline in SPG’s stock price, as investors became concerned about the impact of the pandemic on the retail industry and consumer spending. However, since mid-2020, SPG’s stock price has shown signs of recovery, likely due to the gradual reopening of retail properties and improving economic conditions.\nOverall, SPG’s stock price has been influenced by a range of factors over the years, including economic conditions, consumer spending, competition in the retail industry, and changes in the real estate market.\nSince early 2020, Simon Property Group Inc’s stock price has experienced some volatility, likely due to a combination of factors such as global economic uncertainty and fluctuations in consumer demand for the company’s products.\nAs discussed before, the macroeconomic factors of GDP growth, inflation, interest rates, and unemployment rate are closely interrelated and play a crucial role in the overall health and stability of an economy. From 2010 to 2023, the global economy experienced a mix of ups and downs, with periods of strong GDP growth followed by slowdowns and recessions.\nThe second plot shows the first difference of the logarithm of the adjusted Simon Property Group Inc stock price. Taking the first difference removes any long-term trends and transforms the time series into a stationary process. From the plot, we can observe that the first difference of the logarithm of the Simon Property Group Inc stock price appears to be stationary, as the mean and variance are roughly constant over time.\n\n\nEnodogenous and Exogenous Variables\n\nPlotCorrelation HeatmapCCF GDPCCF InterestCCF InflationCCF Unemployment\n\n\n\n\nCode\nnumeric_data <- c(\"SPG.Adjusted\",\"gdp\", \"interest\", \"inflation\", \"unemployment\")\nnumeric_data <- final[, numeric_data]\nnormalized_data_numeric <- scale(numeric_data)\nnormalized_data <- ts(normalized_data_numeric, start = c(2010, 1), end = c(2021,10),frequency = 4)\nts_plot(normalized_data,\n        title = \"Normalized Time Series Data for SPG Stock and Macroeconomic Variables\",\n        Ytitle = \"Normalized Values\",\n        Xtitle = \"Year\")\n\n\n\n\n\n\n\n\n\n\nCode\n# Get upper triangle of the correlation matrix\nget_upper_tri <- function(cormat){\n    cormat[lower.tri(cormat)]<- NA\n    return(cormat)\n}\ncormat <- round(cor(normalized_data_numeric),2)\n\nupper_tri <- get_upper_tri(cormat)\n\nmelted_cormat <- melt(upper_tri, na.rm = TRUE)\n# Create a ggheatmap\nggheatmap <- ggplot(melted_cormat, aes(Var2, Var1, fill = value))+\n geom_tile(color = \"white\")+\n scale_fill_gradient2(low = \"blue\", high = \"red\", mid = \"white\", \n   midpoint = 0, limit = c(-1,1), space = \"Lab\", \n    name=\"Pearson\\nCorrelation\") +\n  theme_minimal()+ # minimal theme\n theme(axis.text.x = element_text(angle = 45, vjust = 1, \n    size = 12, hjust = 1))+\n coord_fixed()\n\nggheatmap + \ngeom_text(aes(Var2, Var1, label = value), color = \"black\", size = 4) +\ntheme(\n  axis.title.x = element_blank(),\n  axis.title.y = element_blank(),\n  panel.grid.major = element_blank(),\n  panel.border = element_blank(),\n  panel.background = element_blank(),\n  axis.ticks = element_blank(),\n  legend.justification = c(1, 0),\n  legend.position = c(0.6, 0.7),\n  legend.direction = \"horizontal\")+\n  guides(fill = guide_colorbar(barwidth = 7, barheight = 1,\n                title.position = \"top\", title.hjust = 0.5))\n\n\n\n\n\n\n\n\n\nCode\npar(mfrow=c(1,1))\nccf_result <- ccf(normalized_data[, c(\"SPG.Adjusted\")], normalized_data[, c(\"gdp\")], \n    lag.max = 300,\n    main = \"Cros-Correlation Plot for SPG Stock Price and GDP Growth Rate \",\n    ylab = \"CCF\")\n\n\n\n\n\nCode\ncat(\"The sum of cross correlation function is\", sum(abs(ccf_result$acf)))\n\n\nThe sum of cross correlation function is 8.137909\n\n\n\n\n\n\nCode\npar(mfrow=c(1,1))\nccf_result <- ccf(normalized_data[, c(\"SPG.Adjusted\")], normalized_data[, c(\"interest\")], \n    lag.max = 300,\n    main = \"Cros-Correlation Plot for SPG Stock Price and Interest Rate\",\n    ylab = \"CCF\")\n\n\n\n\n\nCode\ncat(\"The sum of cross correlation function is\", sum(abs(ccf_result$acf)))\n\n\nThe sum of cross correlation function is 9.09151\n\n\n\n\n\n\nCode\npar(mfrow=c(1,1))\nccf_result <- ccf(normalized_data[, c(\"SPG.Adjusted\")], normalized_data[, c(\"inflation\")], \n    lag.max = 300,\n    main = \"Cros-Correlation Plot for SPG Stock Price and Inflation Rate\",\n    ylab = \"CCF\")\n\n\n\n\n\nCode\ncat(\"The sum of cross correlation function is\", sum(abs(ccf_result$acf)))\n\n\nThe sum of cross correlation function is 15.82909\n\n\n\n\n\n\nCode\npar(mfrow=c(1,1))\nccf_result <- ccf(normalized_data[, c(\"SPG.Adjusted\")], normalized_data[, c(\"unemployment\")], \n    lag.max = 300,\n    main = \"Cros-Correlation Plot for SPG Stock Priceand Unemployment Rate\",\n    ylab = \"CCF\")\n\n\n\n\n\nCode\ncat(\"The sum of cross correlation function is\", sum(abs(ccf_result$acf)))\n\n\nThe sum of cross correlation function is 17.5948\n\n\n\n\n\nThe Normalized Time Series Data for Stock Price and Macroeconomic Variables plot shows the same variables as the first plot but has been normalized to a common range of 0 to 1 using the scale() function in R, which standardizes the variables to have a mean of 0 and a standard deviation of 1. The heatmap analysis of the normalized data reveals that inflation and unemployment rate exhibit strong positive correlations with the stock price indices, indicating that these variables may significantly influence stock price movements. On the other hand, weaker correlations were observed between the stock price indices and GDP and interest rates, suggesting that these variables may have less impact on stock price fluctuations. The cross-correlation feature plots confirm these findings, indicating that inflation and unemployment rate are more suitable feature variables for the ARIMAX model when predicting Simon Property Group Inc movements.\nFinal Exogenous variables: Macroeconomic indicators: Inflation rate and unemployment rate.\n\n\nEnodogenous and Exogenous Variables Plot\n\nPlotCheck the stationarity\n\n\n\n\nCode\nfinal_data <- final %>%dplyr::select( Date,SPG.Adjusted, inflation,unemployment)\nnumeric_data <- c(\"SPG.Adjusted\", \"inflation\",\"unemployment\")\nnumeric_data <- final_data[, numeric_data]\nnormalized_data_numeric <- scale(numeric_data)\nnormalized_numeric_df <- data.frame(normalized_data_numeric)\nnormalized_data_ts <- ts(normalized_data_numeric, start = c(2010, 1), frequency = 4)\n\nautoplot(normalized_data_ts, facets=TRUE) +\n  xlab(\"Year\") + ylab(\"\") +\n  ggtitle(\"Simon Property Group Inc Stock Price, Inflation Rate and Unemployment Rate in USA 2010-2023\")\n\n\n\n\n\n\n\n\n\nCode\n# Convert your multivariate time series data to a matrix\nfinal_data_ts_multivariate <- as.matrix(normalized_data_ts)\n\n# Check for stationarity using Phillips-Perron test\nphillips_perron_test <- ur.pp(final_data_ts_multivariate)  \nsummary(phillips_perron_test)\n\n\n\n################################## \n# Phillips-Perron Unit Root Test # \n################################## \n\nTest regression with intercept \n\n\nCall:\nlm(formula = y ~ y.l1)\n\nResiduals:\n    Min      1Q  Median      3Q     Max \n-2.4356 -0.1842 -0.0432  0.1307  4.4556 \n\nCoefficients:\n            Estimate Std. Error t value Pr(>|t|)    \n(Intercept) 0.006439   0.043712   0.147    0.883    \ny.l1        0.828205   0.044155  18.757   <2e-16 ***\n---\nSignif. codes:  0 '***' 0.001 '**' 0.01 '*' 0.05 '.' 0.1 ' ' 1\n\nResidual standard error: 0.5442 on 153 degrees of freedom\nMultiple R-squared:  0.6969,    Adjusted R-squared:  0.6949 \nF-statistic: 351.8 on 1 and 153 DF,  p-value: < 2.2e-16\n\n\nValue of test-statistic, type: Z-alpha  is: -27.5627 \n\n         aux. Z statistics\nZ-tau-mu            0.1461\n\n\n\n\n\nThe results of the Phillips-Perron unit root test indicate strong evidence against the null hypothesis of a unit root, as the p-value for the coefficient of the lagged variable is less than the significance level of 0.05. This suggests that the variable y, which is being tested for stationarity, is likely stationary. Furthermore, the test statistic Z-tau-mu is 0.1461, which is smaller than the critical value of Z-alpha (-27.5627), providing further evidence of stationarity.\nTo determine whether the linear model requires an ARCH model, an ARCH test is conducted. The ACF and PACF plots are also used to identify suitable model values.\n\n\nModel Fitting\n\nPlotARCH TestACF PlotPACF Plot\n\n\n\n\nCode\nnormalized_numeric_df$SPG.Adjusted<-ts(normalized_numeric_df$SPG.Adjusted,star=decimal_date(as.Date(\"2010-01-01\",format = \"%Y-%m-%d\")),frequency = 4)\nnormalized_numeric_df$inflation<-ts(normalized_numeric_df$inflation,star=decimal_date(as.Date(\"2010-01-01\",format = \"%Y-%m-%d\")),frequency = 4)\nnormalized_numeric_df$unemployment<-ts(normalized_numeric_df$unemployment,star=decimal_date(as.Date(\"2010-01-01\",format = \"%Y-%m-%d\")),frequency = 4)\n\nfit <- lm(SPG.Adjusted ~ inflation+unemployment, data=normalized_numeric_df)\nfit.res<-ts(residuals(fit),star=decimal_date(as.Date(\"2010-01-01\",format = \"%Y-%m-%d\")),frequency = 4)\n############## Then look at the residuals ############\nreturns <- fit.res  %>% diff()\nautoplot(returns)+ggtitle(\"Linear Model Returns\")\n\n\n\n\n\n\n\n\n\nCode\nbyd.archTest <- ArchTest(fit.res, lags = 1, demean = TRUE)\nbyd.archTest\n\n\n\n    ARCH LM-test; Null hypothesis: no ARCH effects\n\ndata:  fit.res\nChi-squared = 4.086, df = 1, p-value = 0.04324\n\n\n\n\n\n\nCode\nggAcf(returns) +ggtitle(\"ACF for returns\")\n\n\n\n\n\n\n\n\n\nCode\nggPacf(returns) +ggtitle(\"PACF for returns\")\n\n\n\n\n\n\n\n\nThe ARCH LM-test was conducted with the null hypothesis of no ARCH effects. The test resulted in a chi-squared value of 4.086 with one degree of freedom, and a very low p-value of 0.04324. This suggests strong evidence against the null hypothesis, indicating the presence of ARCH effects in the data.\nBased on the ACF and PACF plots, it appears that there is some significant autocorrelation and partial autocorrelation at multiple lags, which suggests that an ARIMA model may not be sufficient to capture the time series behavior. Additionally, the values for p and q appear to be relatively high, with p = 2 and q = 2 being suggested by the plots.\n\n\nARIMAX Model\n\nAuto Arima ModelAuto Arima ResidualsARIMAX ModelARIMA (1,0,1) PlotARIMA (1,1,1) ModelCross Validation\n\n\n\n\nCode\nxreg <- cbind(Inflation = normalized_data_ts[, \"inflation\"],\n              Unemployment = normalized_data_ts[, \"unemployment\"])\nfit.auto <- auto.arima(normalized_data_ts[, \"SPG.Adjusted\"], xreg = xreg)\nsummary(fit.auto)\n\n\nSeries: normalized_data_ts[, \"SPG.Adjusted\"] \nRegression with ARIMA(1,0,0)(1,0,0)[4] errors \n\nCoefficients:\n         ar1     sar1  Inflation  Unemployment\n      0.9534  -0.3300     0.2778       -0.3783\ns.e.  0.0495   0.1831     0.1652        0.0677\n\nsigma^2 = 0.1182:  log likelihood = -17.37\nAIC=44.75   AICc=46.05   BIC=54.5\n\nTraining set error measures:\n                      ME      RMSE       MAE       MPE     MAPE     MASE\nTraining set -0.01361406 0.3303232 0.2707079 -56.23445 97.35066 0.343688\n                   ACF1\nTraining set 0.05506913\n\n\n\n\n\n\nCode\ncheckresiduals(fit.auto)\n\n\n\n\n\n\n    Ljung-Box test\n\ndata:  Residuals from Regression with ARIMA(1,0,0)(1,0,0)[4] errors\nQ* = 3.0527, df = 6, p-value = 0.8022\n\nModel df: 2.   Total lags used: 8\n\n\n\n\n\n\nCode\nARIMA.c=function(p1,p2,q1,q2,data){\ntemp=c()\nd=1\ni=1\ntemp= data.frame()\nls=matrix(rep(NA,6*30),nrow=30)\n\n\nfor (p in p1:p2)#\n{\n  for(q in q1:q2)#\n  {\n    for(d in 0:1)\n    {\n      \n      if(p+d+q<=6)\n      {\n        \n        model<- Arima(data,order=c(p,d,q))\n        ls[i,]= c(p,d,q,model$aic,model$bic,model$aicc)\n        i=i+1\n        #print(i)\n        \n      }\n      \n    }\n  }\n}\n\n\ntemp= as.data.frame(ls)\nnames(temp)= c(\"p\",\"d\",\"q\",\"AIC\",\"BIC\",\"AICc\")\n\ntemp\n}\n\noutput <- ARIMA.c(1,2,1,2,data=residuals(fit))\n\noutput[which.min(output$AIC),] \n\n\n  p d q      AIC      BIC     AICc\n1 1 0 1 65.50307 73.30805 66.35414\n\n\nCode\noutput[which.min(output$BIC),]\n\n\n  p d q      AIC      BIC     AICc\n1 1 0 1 65.50307 73.30805 66.35414\n\n\nCode\noutput[which.min(output$AICc),]\n\n\n  p d q      AIC      BIC     AICc\n1 1 0 1 65.50307 73.30805 66.35414\n\n\n\n\n\n\nCode\nset.seed(1234)\n\nmodel_output <- capture.output(sarima(fit.res, 1,0,1)) \n\n\n\n\n\n\n\n\n\nCode\ncat(model_output[25:56], model_output[length(model_output)], sep = \"\\n\")\n\n\n$fit\n\nCall:\narima(x = xdata, order = c(p, d, q), seasonal = list(order = c(P, D, Q), period = S), \n    xreg = xmean, include.mean = FALSE, transform.pars = trans, fixed = fixed, \n    optim.control = list(trace = trc, REPORT = 1, reltol = tol))\n\nCoefficients:\n         ar1     ma1    xmean\n      0.2774  0.4238  -0.0205\ns.e.  0.2080  0.1863   0.1133\n\nsigma^2 estimated as 0.1752:  log likelihood = -28.75,  aic = 65.5\n\n$degrees_of_freedom\n[1] 49\n\n$ttable\n      Estimate     SE t.value p.value\nar1     0.2774 0.2080  1.3340  0.1884\nma1     0.4238 0.1863  2.2748  0.0273\nxmean  -0.0205 0.1133 -0.1813  0.8569\n\n$AIC\n[1] 1.259674\n\n$AICc\n[1] 1.26929\n\n$BIC\n[1] 1.40977\n\n\n\n\n\n\nCode\nn=length(fit.res)\nk= 51\n \n \nrmse1 <- matrix(NA, (n-k),4)\nrmse2 <- matrix(NA, (n-k),4)\nrmse3 <- matrix(NA, (n-k),4)\n\n\nst <- tsp(fit.res)[1]+(k-5)/4 \n\nfor(i in 1:(n-k))\n{\n  xtrain <- window(fit.res, end=st + i/4)\n  xtest <- window(fit.res, start=st + (i+1)/4, end=st + (i+4)/4)\n  \n  #ARIMA(0,1,0) ARIMA(1,1,1)\n  \n  fit <- Arima(xtrain, order=c(0,1,0),\n                include.drift=TRUE, method=\"ML\")\n  fcast <- forecast(fit, h=4)\n  \n  fit2 <- Arima(xtrain, order=c(1,0,0), seasonal = c(1,0,0),\n                include.drift=TRUE, method=\"ML\")\n  fcast2 <- forecast(fit2, h=4)\n\n\n  rmse1[i,1:length(xtest)]   <- sqrt((fcast$mean-xtest)^2)\n  rmse2[i,1:length(xtest)] <- sqrt((fcast2$mean-xtest)^2)\n}\n\nplot(1:4,colMeans(rmse1,na.rm=TRUE), type=\"l\",col=2, xlab=\"horizon\", ylab=\"RMSE\")\nlines(1:4, colMeans(rmse2,na.rm=TRUE), type=\"l\",col=3)\nlegend(\"topleft\",legend=c(\"fit1\",\"fit2\"),col=2:4,lty=1)\n\n\n\n\n\n\n\n\nBased on the results of the auto.arima function, the suggested best model is ARIMA(1,0,0)(1,0,0)[4]. However, when we manually test different ARIMA models, we find that ARIMA(0,1,0) has the lowest values for AIC, BIC, and AICC. Additionally, both models have similar standardized residual plots, with means close to 0, indicating a good fit. The ACF plot of residuals also shows no significant lags, further indicating a well-fitted model.\nTo determine the best model, we conduct cross-validation and compare the RMSE values of both models. The results show that ARIMA(0,1,0) has lower RMSE values than the other model, indicating that it is the better model.\nWe can then proceed to choose the best GARCH model using ARIMA(0,1,0) as the base model.\n\n\nSquared Residuals\n\nPlotACF PlotPACF PlotGRACH(1,2)\n\n\n\n\nCode\nfit <- lm(SPG.Adjusted ~ inflation+unemployment, data=normalized_numeric_df)\nfit.res<-ts(residuals(fit),star=decimal_date(as.Date(\"2010-01-01\",format = \"%Y-%m-%d\")),frequency = 4)\nfit <- Arima(fit.res,order=c(0,1,0))\nres=fit$res\nplot(res^2,main='Squared Residuals')\n\n\n\n\n\n\n\n\n\nCode\nacf(res^2,24, main = \"ACF Residuals Square\")\n\n\n\n\n\n\n\n\n\nCode\npacf(res^2,24, main = \"PACF Residuals Square\")\n\n\n\n\n\n\n\n\n\nCode\nsummary(garchFit(~garch(2,1),res, trace=F))\n\n\n\nTitle:\n GARCH Modelling \n\nCall:\n garchFit(formula = ~garch(2, 1), data = res, trace = F) \n\nMean and Variance Equation:\n data ~ garch(2, 1)\n<environment: 0x13a5684a8>\n [data = res]\n\nConditional Distribution:\n norm \n\nCoefficient(s):\n        mu       omega      alpha1      alpha2       beta1  \n0.04528839  0.00059517  0.14604719  0.38866129  0.66486298  \n\nStd. Errors:\n based on Hessian \n\nError Analysis:\n        Estimate  Std. Error  t value Pr(>|t|)    \nmu     0.0452884   0.0388984    1.164 0.244312    \nomega  0.0005952   0.0190009    0.031 0.975012    \nalpha1 0.1460472   0.1482030    0.985 0.324401    \nalpha2 0.3886613   0.2958813    1.314 0.188990    \nbeta1  0.6648630   0.1933162    3.439 0.000583 ***\n---\nSignif. codes:  0 '***' 0.001 '**' 0.01 '*' 0.05 '.' 0.1 ' ' 1\n\nLog Likelihood:\n -28.51055    normalized:  -0.5482799 \n\nDescription:\n Tue Jan  9 21:03:46 2024 by user:  \n\n\nStandardised Residuals Tests:\n                                Statistic p-Value    \n Jarque-Bera Test   R    Chi^2  22.15694  1.54412e-05\n Shapiro-Wilk Test  R    W      0.926558  0.003307071\n Ljung-Box Test     R    Q(10)  7.382848  0.6888745  \n Ljung-Box Test     R    Q(15)  11.82492  0.6922298  \n Ljung-Box Test     R    Q(20)  18.67737  0.5428777  \n Ljung-Box Test     R^2  Q(10)  3.102515  0.9789075  \n Ljung-Box Test     R^2  Q(15)  3.785774  0.9983585  \n Ljung-Box Test     R^2  Q(20)  5.344295  0.9995367  \n LM Arch Test       R    TR^2   3.42088   0.9917726  \n\nInformation Criterion Statistics:\n     AIC      BIC      SIC     HQIC \n1.288867 1.476487 1.272450 1.360796 \n\n\n\n\n\nFrom the squared residuals of the best ARIMA model, it can be observed that the ACF plot and PACF plot indicate that the residuals are not autocorrelated and are white noise, indicating a good fit of the model. Based on the squared residuals of the best ARIMA model, we can see that the ACF and PACF plots indicate that most of the values lie between the blue lines. Additionally, the p-value is 2 and q-value is 1.\nSo, the best model is ARIMA(0,1,0) and GARCH(2,1).\n\n\nBest Model\n\nARIMA ModelGARCH ModelVolatility\n\n\n\n\nCode\n#fiting an ARIMA model to the Inflation variable\ninflation_fit<-auto.arima(normalized_numeric_df$inflation) \nfinflation<-forecast(inflation_fit)\n\n#fitting an ARIMA model to the Unemployment variable\nunemployment_fit<-auto.arima(normalized_numeric_df$unemployment) \nfunemployment<-forecast(unemployment_fit)\n\n# best model fit for forcasting\nxreg <- cbind(Inflation = normalized_data_ts[, \"inflation\"],\n              Unemployment = normalized_data_ts[, \"unemployment\"])\n\n\n\nsummary(arima.fit<-Arima(normalized_data_ts[, \"SPG.Adjusted\"],order=c(0,1,0),xreg=xreg),include.drift = TRUE)\n\n\nSeries: normalized_data_ts[, \"SPG.Adjusted\"] \nRegression with ARIMA(0,1,0) errors \n\nCoefficients:\n      Inflation  Unemployment\n         0.4599       -0.4115\ns.e.     0.1513        0.0687\n\nsigma^2 = 0.1211:  log likelihood = -17.51\nAIC=41.03   AICc=41.54   BIC=46.82\n\nTraining set error measures:\n                      ME     RMSE       MAE       MPE     MAPE      MASE\nTraining set -0.02507978 0.337813 0.2655551 -55.51624 91.68249 0.3371461\n                  ACF1\nTraining set 0.0474485\n\n\n\n\n\n\nCode\nsummary(final.fit <- garchFit(~garch(2,1), res,trace = F))\n\n\n\nTitle:\n GARCH Modelling \n\nCall:\n garchFit(formula = ~garch(2, 1), data = res, trace = F) \n\nMean and Variance Equation:\n data ~ garch(2, 1)\n<environment: 0x13916c0a8>\n [data = res]\n\nConditional Distribution:\n norm \n\nCoefficient(s):\n        mu       omega      alpha1      alpha2       beta1  \n0.04528839  0.00059517  0.14604719  0.38866129  0.66486298  \n\nStd. Errors:\n based on Hessian \n\nError Analysis:\n        Estimate  Std. Error  t value Pr(>|t|)    \nmu     0.0452884   0.0388984    1.164 0.244312    \nomega  0.0005952   0.0190009    0.031 0.975012    \nalpha1 0.1460472   0.1482030    0.985 0.324401    \nalpha2 0.3886613   0.2958813    1.314 0.188990    \nbeta1  0.6648630   0.1933162    3.439 0.000583 ***\n---\nSignif. codes:  0 '***' 0.001 '**' 0.01 '*' 0.05 '.' 0.1 ' ' 1\n\nLog Likelihood:\n -28.51055    normalized:  -0.5482799 \n\nDescription:\n Tue Jan  9 21:03:46 2024 by user:  \n\n\nStandardised Residuals Tests:\n                                Statistic p-Value    \n Jarque-Bera Test   R    Chi^2  22.15694  1.54412e-05\n Shapiro-Wilk Test  R    W      0.926558  0.003307071\n Ljung-Box Test     R    Q(10)  7.382848  0.6888745  \n Ljung-Box Test     R    Q(15)  11.82492  0.6922298  \n Ljung-Box Test     R    Q(20)  18.67737  0.5428777  \n Ljung-Box Test     R^2  Q(10)  3.102515  0.9789075  \n Ljung-Box Test     R^2  Q(15)  3.785774  0.9983585  \n Ljung-Box Test     R^2  Q(20)  5.344295  0.9995367  \n LM Arch Test       R    TR^2   3.42088   0.9917726  \n\nInformation Criterion Statistics:\n     AIC      BIC      SIC     HQIC \n1.288867 1.476487 1.272450 1.360796 \n\n\n\n\n\n\nCode\nht <- final.fit@h.t #a numeric vector with the conditional variances (h.t = sigma.t^delta)\n\n#############################\ndata=data.frame(final)\ndata$Date<-as.Date(data$Date,\"%Y-%m-%d\")\n\n\ndata2= data.frame(ht,data$Date)\nggplot(data2, aes(y = ht, x = data.Date)) + geom_line(col = '#C39BD3') + ylab('Conditional Variance') + xlab('Date')\n\n\n\n\n\n\n\n\nFrom the ARIMA(0,1,0), we see that the training set error measures also suggest a good fit, with low mean absolute error, root mean squared error, and autocorrelation of the residuals. GATCH(2,1) model model is used to estimate the volatility of the standardized residuals of the previous regression model. The model includes a mean equation that estimates the mean of the residuals and a variance equation that models the conditional variance of the residuals. The coefficients of the mean equation suggest that the mean of the residuals is close to zero. The variance equation coefficients suggest that the conditional variance of the residuals is dependent on the past conditional variances and the past squared standardized residuals. The model’s AIC, BIC, SIC, and HQIC values are all relatively low, indicating a good fit of the model. The standardized residuals tests indicate that the residuals are approximately normally distributed and that there is no significant autocorrelation in the residuals.\nThe volatility of the model seems high in 2020 but has decreased gradually in the past few months. This could indicate that the asset’s price was experiencing a lot of fluctuations in 2020, but the market has stabilized recently.\n\n\nModel Diagnostics\n\nResidualsQQ PlotBox Test\n\n\n\n\nCode\nfit2<-garch(res,order=c(2,1),trace=F)\ncheckresiduals(fit2) \n\n\n\n\n\n\n\n\n\nCode\nqqnorm(fit2$residuals, pch = 1)\nqqline(fit2$residuals, col = \"blue\", lwd = 2)\n\n\n\n\n\n\n\n\n\nCode\nBox.test (fit2$residuals, type = \"Ljung\")\n\n\n\n    Box-Ljung test\n\ndata:  fit2$residuals\nX-squared = 0.41569, df = 1, p-value = 0.5191\n\n\nThe ACF plot of the residuals shows all the values between the blue lines, which indicates that the residuals are not significantly autocorrelated. The range of values for the residual plot between -2 and 4 is considered acceptable. Additionally, the QQ plot of the residuals shows a linear plot on the line, which is another good indication that the residuals are normally distributed. The QQ plot is a valuable tool to assess if the residuals follow a normal distribution, and in this case, the plot suggests that the residuals do indeed follow a normal distribution.\nThe Box-Ljung test, a p-value of 0.5186 indicates that the model’s residuals are not significantly autocorrelated, meaning that the model has captured most of the information in the data. This result is good because it suggests that the model is a good fit for the data and has accounted for most of the underlying patterns in the data. Therefore, we can rely on the model’s predictions and use them to make informed decisions.\n\n\n\n\n\nForecast\n\n\nCode\npredict(final.fit, n.ahead = 5, plot=TRUE)\n\n\n\n\n\n  meanForecast meanError standardDeviation lowerInterval upperInterval\n1   0.04528839 1.1060591         1.1060591     -2.122548      2.213124\n2   0.04528839 0.9964814         0.9964814     -1.907779      1.998356\n3   0.04528839 1.1319382         1.1319382     -2.173270      2.263847\n4   0.04528839 1.1939566         1.1939566     -2.294824      2.385400\n5   0.04528839 1.2862968         1.2862968     -2.475807      2.566384\n\n\nThe forecasted plot is based on the best model ARIMAX(0,1,0)+GARCH(2,1). This model takes into account the autoregressive and moving average components of the data, as well as the impact of exogenous variables on the time series. Additionally, the GARCH component of the model accounts for the volatility clustering in the data. Overall, this model is well-suited to make accurate predictions about future values of the time series.\n\n\nEquation of the Model\nThe equation of the ARIMAX(0,1,0) model is:\n\\(Y(t) = Y(t-1) + \\epsilon(t)\\)\nwhere \\(Y(t)\\) is the time series variable and \\(\\epsilon(t)\\) is the error term. The equation of the GARCH(2,1) model is:\n\\(\\sigma_t^2 = \\omega + \\alpha_1\\varepsilon^2_(t-1) + \\alpha_2\\varepsilon^2_(t-2) + \\beta_1\\sigma^2(t-1)\\)\nwhere \\(\\sigma_t^2\\) represents the conditional variance of the time series at time \\(t\\), \\(\\varepsilon_t\\) represents the error term or innovation at time \\(t\\), \\(\\omega\\) is a constant representing the long-run average variance, and \\(\\alpha_1\\), \\(\\alpha_2\\), and \\(\\beta_1\\) are the coefficients to be estimated.\nThe combined equation of the ARIMAX(1,1,1)+GARCH(1,1) model is:\n\\(Y(t) = Y(t-1) + \\epsilon(t)\\)\n\\(\\epsilon(t) = \\sigma(t) * \\epsilon~(t)\\)\n\\(\\sigma_t^2 = \\omega + \\alpha_1\\varepsilon^2_(t-1) + \\alpha_2\\varepsilon^2_(t-2) + \\beta_1\\sigma^2(t-1)\\)"
  },
  {
    "objectID": "arch_Technology_Sector_Fund.html",
    "href": "arch_Technology_Sector_Fund.html",
    "title": "",
    "section": "",
    "text": "Impact of Macroeconomic Factors on Apple Stock Price\n\n\n\nThere are many great technology sector companies to choose from, but one of the best is undoubtedly Apple Inc. (AAPL). Apple has consistently ranked among the world’s most valuable companies, with a market capitalization of over $2 trillion as of 2021.\nApple’s success can be attributed to its innovative products, such as the iPhone, iPad, and MacBook, as well as its ecosystem of software and services, including the App Store, iCloud, and Apple Music. The company also has a strong brand reputation and a loyal customer base.\nApple’s financial performance has been impressive, with steady revenue growth and strong profits. The company has consistently returned value to its shareholders through share buybacks and dividend payments.\nBy using an ARIMAX+ARCH/GARCH model to analyze the stock price behavior of Apple, we can gain insights into how macroeconomic factors impact the company’s performance. For example, an increase in GDP growth rate or a decrease in unemployment rate may lead to increased consumer spending and a rise in Apple’s stock price. Conversely, an increase in inflation or interest rates may lead to a decrease in consumer spending and a decline in Apple’s stock price.\n\n\nTime series Plot\n\nAppleDifferentitaed Chart SeriesGDP GrowthInflationInterestUnemployment\n\n\n\n\nCode\n# get data\noptions(\"getSymbols.warning4.0\"=FALSE)\noptions(\"getSymbols.yahoo.warning\"=FALSE)\n\n\ndata.info = getSymbols(\"AAPL\",src='yahoo', from = '2010-01-01',to = \"2023-03-01\",auto.assign = FALSE)\ndata = getSymbols(\"AAPL\",src='yahoo', from = '2010-01-01',to = \"2023-03-01\")\ndf <- data.frame(Date=index(AAPL),coredata(AAPL))\n\n# create Bollinger Bands\nbbands <- BBands(AAPL[,c(\"AAPL.High\",\"AAPL.Low\",\"AAPL.Close\")])\n\n# join and subset data\ndf <- subset(cbind(df, data.frame(bbands[,1:3])), Date >= \"2010-01-01\")\n\n\n# colors column for increasing and decreasing\nfor (i in 1:length(df[,1])) {\n  if (df$AAPL.Close[i] >= df$AAPL.Open[i]) {\n      df$direction[i] = 'Increasing'\n  } else {\n      df$direction[i] = 'Decreasing'\n  }\n}\n\ni <- list(line = list(color = '#922B21'))\nd <- list(line = list(color = '#7F7F7F'))\n\n# plot candlestick chart\n\nfig <- df %>% plot_ly(x = ~Date, type=\"candlestick\",\n          open = ~AAPL.Open, close = ~AAPL.Close,\n          high = ~AAPL.High, low = ~AAPL.Low, name = \"AAPL\",\n          increasing = i, decreasing = d) \nfig <- fig %>% add_lines(x = ~Date, y = ~up , name = \"B Bands\",\n            line = list(color = '#ccc', width = 0.5),\n            legendgroup = \"Bollinger Bands\",\n            hoverinfo = \"none\", inherit = F) \nfig <- fig %>% add_lines(x = ~Date, y = ~dn, name = \"B Bands\",\n            line = list(color = '#ccc', width = 0.5),\n            legendgroup = \"Bollinger Bands\", inherit = F,\n            showlegend = FALSE, hoverinfo = \"none\") \nfig <- fig %>% add_lines(x = ~Date, y = ~mavg, name = \"Mv Avg\",\n            line = list(color = '#C052B3', width = 0.5),\n            hoverinfo = \"none\", inherit = F) \nfig <- fig %>% layout(yaxis = list(title = \"Price\"))\n\n# plot volume bar chart\nfig2 <- df \nfig2 <- fig2 %>% plot_ly(x=~Date, y=~AAPL.Volume, type='bar', name = \"AAPL Volume\",\n          color = ~direction, colors = c('#922B21','#7F7F7F')) \nfig2 <- fig2 %>% layout(yaxis = list(title = \"Volume\"))\n\n# create rangeselector buttons\nrs <- list(visible = TRUE, x = 0.5, y = -0.055,\n           xanchor = 'center', yref = 'paper',\n           font = list(size = 9),\n           buttons = list(\n             list(count=1,\n                  label='RESET',\n                  step='all'),\n             list(count=3,\n                  label='3 YR',\n                  step='year',\n                  stepmode='backward'),\n             list(count=1,\n                  label='1 YR',\n                  step='year',\n                  stepmode='backward'),\n             list(count=1,\n                  label='1 MO',\n                  step='month',\n                  stepmode='backward')\n           ))\n\n# subplot with shared x axis\nfig <- subplot(fig, fig2, heights = c(0.7,0.2), nrows=2,\n             shareX = TRUE, titleY = TRUE)\nfig <- fig %>% layout(title = paste(\"Apple Stock Price: January 2010 - March 2023\"),\n         xaxis = list(rangeselector = rs),\n         legend = list(orientation = 'h', x = 0.5, y = 1,\n                       xanchor = 'center', yref = 'paper',\n                       font = list(size = 10),\n                       bgcolor = 'transparent'))\n\nfig\n\n\n\n\n\n\n\n\n\n\nCode\nlog(data.info$`AAPL.Adjusted`) %>% diff() %>% chartSeries(theme=chartTheme('white'),up.col='#922B21')\n\n\n\n\n\n\n\n\n\nCode\n#import the data\ngdp <- read.csv(\"DATA/RAW DATA/gdp-growth.csv\")\n\n#change date format\ngdp$Date <- as.Date(gdp$DATE , \"%m/%d/%Y\")\n\n#drop DATE column\ngdp <- subset(gdp, select = -c(1))\n\n#export the cleaned data\ngdp_clean <- gdp\nwrite.csv(gdp_clean, \"DATA/CLEANED DATA/gdp_clean_data.csv\", row.names=FALSE)\n\n#plot gdp growth rate \nfig <- plot_ly(gdp, x = ~Date, y = ~value, type = 'scatter', mode = 'lines',line = list(color = 'rgb(240, 128, 128)'))\nfig <- fig %>% layout(title = \"U.S GPD Growth Rate: 2010 - 2022\",xaxis = list(title = \"Time\"),yaxis = list(title =\"GDP Growth Rate\"))\nfig\n\n\n\n\n\n\n\n\n\n\nCode\n#import the data\ninflation_rate <- read.csv(\"DATA/RAW DATA/inflation-rate.csv\")\n\n#cleaning the data\n#remove unwanted columns\ninflation_rate_clean <- subset(inflation_rate, select = -c(1,HALF1,HALF2))\n\n#convert the data to time series data\ninflation_data_ts <- ts(as.vector(t(as.matrix(inflation_rate_clean))), start=c(2010,1), end=c(2023,2), frequency=12)\n\n#export the data\nwrite.csv(inflation_rate_clean, \"DATA/CLEANED DATA/inflation_rate_clean_data.csv\", row.names=FALSE)\n\n\n#plot inflation rate \nfig <- autoplot(inflation_data_ts, ylab = \"Inflation Rate\", color=\"#FFA07A\")+ggtitle(\"U.S Inflation Rate: January 2010 - February 2023\")+theme_bw()\nggplotly(fig)\n\n\n\n\n\n\n\n\n\n\nCode\n#import the data\ninterest_data <- read.csv(\"DATA/RAW DATA/interest-rate.csv\")\n\n#change date format\ninterest_data$Date <- as.Date(interest_data$Date , \"%m/%d/%Y\")\n\n#export the cleaned data\ninterest_clean_data <- interest_data\nwrite.csv(interest_clean_data, \"DATA/CLEANED DATA/interest_rate_clean_data.csv\", row.names=FALSE)\n\n#plot interest rate \nfig <- plot_ly(interest_data, x = ~Date, y = ~value, type = 'scatter', mode = 'lines',line = list(color='rgb(219, 112, 147)'))\nfig <- fig %>% layout(title = \"U.S Interest Rate: January 2010 - March 2023\",xaxis = list(title = \"Time\"),yaxis = list(title =\"Interest Rate\"))\nfig\n\n\n\n\n\n\n\n\n\n\nCode\n#import the data\nunemployment_rate <- read.csv(\"DATA/RAW DATA/unemployment-rate.csv\")\n\n#change date format\nunemployment_rate$Date <- as.Date(unemployment_rate$Date , \"%m/%d/%Y\")\n\n# export the data\nwrite.csv(unemployment_rate, \"DATA/CLEANED DATA/unemployment_rate_clean_data.csv\", row.names=FALSE)\n\n#plot unemployment rate \nfig <- plot_ly(unemployment_rate, x = ~Date, y = ~Value, type = 'scatter', mode = 'lines',line = list(color = 'rgb(189, 183, 107)'))\nfig <- fig %>% layout(title = \"U.S Unemployment Rate: January 2010 - March 2023\",xaxis = list(title = \"Time\"),yaxis = list(title =\"Unemployment Rate\"))\nfig\n\n\n\n\n\n\n\n\n\nApple Inc.’s stock price has had a tumultuous journey from 2010 to 2023. In 2010, the company was just beginning to recover from the impact of the 2008 financial crisis, and its stock price was around $30 per share. However, the introduction of new products such as the iPad and iPhone in the following years propelled the company’s stock price to new heights, reaching over $700 per share in 2012.\nFrom 2013 to 2016, Apple’s stock price continued to fluctuate, experiencing highs and lows due to factors such as changes in consumer demand and increased competition in the technology sector. The company’s stock price began to recover in 2017, as investors became more optimistic about the potential of Apple’s new products such as the iPhone X and the Apple Watch.\nHowever, the outbreak of the COVID-19 pandemic in 2020 caused a brief dip in Apple’s stock price, as investors were uncertain about the impact of the pandemic on the company’s operations and financial performance. Nevertheless, Apple’s strong position in the technology sector and its ability to adapt to changing market conditions helped it to quickly rebound and continue its growth trend throughout 2020 and into early 2021.\nSince mid 2020, Apple’s stock price has experienced some volatility, likely due to a combination of factors such as global economic uncertainty and fluctuations in consumer demand for the company’s products.\nAs discussed before, the macroeconomic factors of GDP growth, inflation, interest rates, and unemployment rate are closely interrelated and play a crucial role in the overall health and stability of an economy. From 2010 to 2023, the global economy experienced a mix of ups and downs, with periods of strong GDP growth followed by slowdowns and recessions.\nThe second plot shows the first difference of the logarithm of the adjusted Apple stock price. Taking the first difference removes any long-term trends and transforms the time series into a stationary process. From the plot, we can observe that the first difference of the logarithm of the Apple stock price appears to be stationary, as the mean and variance are roughly constant over time.\n\n\nEnodogenous and Exogenous Variables\n\nPlotCorrelation HeatmapCCF GDPCCF InterestCCF InflationCCF Unemployment\n\n\n\n\nCode\nnumeric_data <- c(\"AAPL.Adjusted\",\"gdp\", \"interest\", \"inflation\", \"unemployment\")\nnumeric_data <- final[, numeric_data]\nnormalized_data_numeric <- scale(numeric_data)\nnormalized_data <- ts(normalized_data_numeric, start = c(2010, 1), end = c(2021,10),frequency = 4)\nts_plot(normalized_data,\n        title = \"Normalized Time Series Data for AAPL Stock and Macroeconomic Variables\",\n        Ytitle = \"Normalized Values\",\n        Xtitle = \"Year\")\n\n\n\n\n\n\n\n\n\n\nCode\n# Get upper triangle of the correlation matrix\nget_upper_tri <- function(cormat){\n    cormat[lower.tri(cormat)]<- NA\n    return(cormat)\n}\ncormat <- round(cor(normalized_data_numeric),2)\n\nupper_tri <- get_upper_tri(cormat)\n\nmelted_cormat <- melt(upper_tri, na.rm = TRUE)\n# Create a ggheatmap\nggheatmap <- ggplot(melted_cormat, aes(Var2, Var1, fill = value))+\n geom_tile(color = \"white\")+\n scale_fill_gradient2(low = \"blue\", high = \"red\", mid = \"white\", \n   midpoint = 0, limit = c(-1,1), space = \"Lab\", \n    name=\"Pearson\\nCorrelation\") +\n  theme_minimal()+ # minimal theme\n theme(axis.text.x = element_text(angle = 45, vjust = 1, \n    size = 12, hjust = 1))+\n coord_fixed()\n\nggheatmap + \ngeom_text(aes(Var2, Var1, label = value), color = \"black\", size = 4) +\ntheme(\n  axis.title.x = element_blank(),\n  axis.title.y = element_blank(),\n  panel.grid.major = element_blank(),\n  panel.border = element_blank(),\n  panel.background = element_blank(),\n  axis.ticks = element_blank(),\n  legend.justification = c(1, 0),\n  legend.position = c(0.6, 0.7),\n  legend.direction = \"horizontal\")+\n  guides(fill = guide_colorbar(barwidth = 7, barheight = 1,\n                title.position = \"top\", title.hjust = 0.5))\n\n\n\n\n\n\n\n\n\nCode\npar(mfrow=c(1,1))\nccf_result <- ccf(normalized_data[, c(\"AAPL.Adjusted\")], normalized_data[, c(\"gdp\")], \n    lag.max = 300,\n    main = \"Cros-Correlation Plot for AAPL Stock Price and GDP Growth Rate \",\n    ylab = \"CCF\")\n\n\n\n\n\nCode\ncat(\"The sum of cross correlation function is\", sum(abs(ccf_result$acf)))\n\n\nThe sum of cross correlation function is 5.344796\n\n\n\n\n\n\nCode\npar(mfrow=c(1,1))\nccf_result <- ccf(normalized_data[, c(\"AAPL.Adjusted\")], normalized_data[, c(\"interest\")], \n    lag.max = 300,\n    main = \"Cros-Correlation Plot for AAPL Stock Price and Interest Rate\",\n    ylab = \"CCF\")\n\n\n\n\n\nCode\ncat(\"The sum of cross correlation function is\", sum(abs(ccf_result$acf)))\n\n\nThe sum of cross correlation function is 12.1218\n\n\n\n\n\n\nCode\npar(mfrow=c(1,1))\nccf_result <- ccf(normalized_data[, c(\"AAPL.Adjusted\")], normalized_data[, c(\"inflation\")], \n    lag.max = 300,\n    main = \"Cros-Correlation Plot for AAPL Stock Price and Inflation Rate\",\n    ylab = \"CCF\")\n\n\n\n\n\nCode\ncat(\"The sum of cross correlation function is\", sum(abs(ccf_result$acf)))\n\n\nThe sum of cross correlation function is 14.80688\n\n\n\n\n\n\nCode\npar(mfrow=c(1,1))\nccf_result <- ccf(normalized_data[, c(\"AAPL.Adjusted\")], normalized_data[, c(\"unemployment\")], \n    lag.max = 300,\n    main = \"Cros-Correlation Plot for AAPL Stock Priceand Unemployment Rate\",\n    ylab = \"CCF\")\n\n\n\n\n\nCode\ncat(\"The sum of cross correlation function is\", sum(abs(ccf_result$acf)))\n\n\nThe sum of cross correlation function is 18.29428\n\n\n\n\n\nThe Normalized Time Series Data for Stock Price and Macroeconomic Variables plot shows the same variables as the first plot but has been normalized to a common range of 0 to 1 using the scale() function in R, which standardizes the variables to have a mean of 0 and a standard deviation of 1. The heatmap analysis of the normalized data reveals that inflation and unemployment rate exhibit strong positive correlations with the stock price indices, indicating that these variables may significantly influence stock price movements. On the other hand, weaker correlations were observed between the stock price indices and GDP and interest rates, suggesting that these variables may have less impact on stock price fluctuations. The cross-correlation feature plots confirm these findings, indicating that inflation and unemployment rate are more suitable feature variables for the ARIMAX model when predicting Apple movements.\nFinal Exogenous variables: Macroeconomic indicators: Inflation rate and unemployment rate.\n\n\nEnodogenous and Exogenous Variables Plot\n\nPlotCheck the stationarity\n\n\n\n\nCode\nfinal_data <- final %>%dplyr::select( Date,AAPL.Adjusted, inflation,unemployment)\nnumeric_data <- c(\"AAPL.Adjusted\", \"inflation\",\"unemployment\")\nnumeric_data <- final_data[, numeric_data]\nnormalized_data_numeric <- scale(numeric_data)\nnormalized_numeric_df <- data.frame(normalized_data_numeric)\nnormalized_data_ts <- ts(normalized_data_numeric, start = c(2010, 1), frequency = 4)\n\nautoplot(normalized_data_ts, facets=TRUE) +\n  xlab(\"Year\") + ylab(\"\") +\n  ggtitle(\"Apple Stock Price, Inflation Rate and Unemployment Rate in USA 2010-2023\")\n\n\n\n\n\n\n\n\n\nCode\n# Convert your multivariate time series data to a matrix\nfinal_data_ts_multivariate <- as.matrix(normalized_data_ts)\n\n# Check for stationarity using Phillips-Perron test\nphillips_perron_test <- ur.pp(final_data_ts_multivariate)  \nsummary(phillips_perron_test)\n\n\n\n################################## \n# Phillips-Perron Unit Root Test # \n################################## \n\nTest regression with intercept \n\n\nCall:\nlm(formula = y ~ y.l1)\n\nResiduals:\n    Min      1Q  Median      3Q     Max \n-2.2155 -0.1522 -0.0540  0.0799  4.4890 \n\nCoefficients:\n             Estimate Std. Error t value Pr(>|t|)    \n(Intercept) -0.000148   0.042080  -0.004    0.997    \ny.l1         0.851949   0.042506  20.043   <2e-16 ***\n---\nSignif. codes:  0 '***' 0.001 '**' 0.01 '*' 0.05 '.' 0.1 ' ' 1\n\nResidual standard error: 0.5239 on 153 degrees of freedom\nMultiple R-squared:  0.7242,    Adjusted R-squared:  0.7224 \nF-statistic: 401.7 on 1 and 153 DF,  p-value: < 2.2e-16\n\n\nValue of test-statistic, type: Z-alpha  is: -24.89 \n\n         aux. Z statistics\nZ-tau-mu           -0.0018\n\n\n\n\n\nThe results of the Phillips-Perron unit root test indicate strong evidence against the null hypothesis of a unit root, as the p-value for the coefficient of the lagged variable is less than the significance level of 0.05. This suggests that the variable y, which is being tested for stationarity, is likely stationary. Furthermore, the test statistic Z-tau-mu is -0.0018, which is smaller than the critical value of Z-alpha (-24.89), providing further evidence of stationarity.\nTo determine whether the linear model requires an ARCH model, an ARCH test is conducted. The ACF and PACF plots are also used to identify suitable model values.\n\n\nModel Fitting\n\nPlotARCH TestACF PlotPACF Plot\n\n\n\n\nCode\nnormalized_numeric_df$AAPL.Adjusted<-ts(normalized_numeric_df$AAPL.Adjusted,star=decimal_date(as.Date(\"2010-01-01\",format = \"%Y-%m-%d\")),frequency = 4)\nnormalized_numeric_df$inflation<-ts(normalized_numeric_df$inflation,star=decimal_date(as.Date(\"2010-01-01\",format = \"%Y-%m-%d\")),frequency = 4)\nnormalized_numeric_df$unemployment<-ts(normalized_numeric_df$unemployment,star=decimal_date(as.Date(\"2010-01-01\",format = \"%Y-%m-%d\")),frequency = 4)\n\nfit <- lm(AAPL.Adjusted ~ inflation+unemployment, data=normalized_numeric_df)\nfit.res<-ts(residuals(fit),star=decimal_date(as.Date(\"2010-01-01\",format = \"%Y-%m-%d\")),frequency = 4)\n############## Then look at the residuals ############\nreturns <- fit.res  %>% diff()\nautoplot(returns)+ggtitle(\"Linear Model Returns\")\n\n\n\n\n\n\n\n\n\nCode\nbyd.archTest <- ArchTest(fit.res, lags = 1, demean = TRUE)\nbyd.archTest\n\n\n\n    ARCH LM-test; Null hypothesis: no ARCH effects\n\ndata:  fit.res\nChi-squared = 24.051, df = 1, p-value = 9.383e-07\n\n\n\n\n\n\nCode\nggAcf(returns) +ggtitle(\"ACF for returns\")\n\n\n\n\n\n\n\n\n\nCode\nggPacf(returns) +ggtitle(\"PACF for returns\")\n\n\n\n\n\n\n\n\nThe ARCH LM-test was conducted with the null hypothesis of no ARCH effects. The test resulted in a chi-squared value of 24.051 with one degree of freedom, and a very low p-value of 9.383e-07. This suggests strong evidence against the null hypothesis, indicating the presence of ARCH effects in the data.\nBased on the ACF and PACF plots, it appears that there is some significant autocorrelation and partial autocorrelation at multiple lags, which suggests that an ARIMA model may not be sufficient to capture the time series behavior. Additionally, the values for p and q appear to be relatively high, with p = 0 and q = 0 being suggested by the plots.\n\n\nARIMAX Model\n\nAuto Arima ModelAuto Arima Residuals\n\n\n\n\nCode\nxreg <- cbind(Inflation = normalized_data_ts[, \"inflation\"],\n              Unemployment = normalized_data_ts[, \"unemployment\"])\nfit.auto <- auto.arima(normalized_data_ts[, \"AAPL.Adjusted\"], xreg = xreg)\nsummary(fit.auto)\n\n\nSeries: normalized_data_ts[, \"AAPL.Adjusted\"] \nRegression with ARIMA(0,1,0) errors \n\nCoefficients:\n      Inflation  Unemployment\n         0.1639       -0.1104\ns.e.     0.0896        0.0407\n\nsigma^2 = 0.04248:  log likelihood = 9.2\nAIC=-12.4   AICc=-11.89   BIC=-6.61\n\nTraining set error measures:\n                    ME      RMSE       MAE       MPE     MAPE      MASE\nTraining set 0.0381237 0.2000733 0.1274114 0.7801298 40.00078 0.4385178\n                 ACF1\nTraining set 0.122828\n\n\n\n\n\n\nCode\ncheckresiduals(fit.auto)\n\n\n\n\n\n\n    Ljung-Box test\n\ndata:  Residuals from Regression with ARIMA(0,1,0) errors\nQ* = 4.5496, df = 8, p-value = 0.8044\n\nModel df: 0.   Total lags used: 8\n\n\n\n\n\nBased on the results of the auto.arima function, the suggested best model is ARIMA(0,1,0) which is the same as the manual choosen arima model. So, we can then proceed to choose the best GARCH model using ARIMA(0,1,0) as the base model.\nWe can then proceed to choose the best GARCH model using ARIMA(0,1,0) as the base model.\n\n\nSquared Residuals\n\nPlotACF PlotPACF PlotACF AbsoluteARCH Model\n\n\n\n\nCode\nfit <- lm(AAPL.Adjusted ~ inflation+unemployment, data=normalized_numeric_df)\nfit.res<-ts(residuals(fit),star=decimal_date(as.Date(\"2010-01-01\",format = \"%Y-%m-%d\")),frequency = 4)\nfit <- Arima(fit.res,order=c(0,1,0))\nres=fit$res\nplot(res^2,main='Squared Residuals')\n\n\n\n\n\n\n\n\n\nCode\nacf(res^2,24, main = \"ACF Residuals Square\")\n\n\n\n\n\n\n\n\n\nCode\npacf(res^2,24, main = \"PACF Residuals Square\")\n\n\n\n\n\n\n\n\n\nCode\nacf(abs(res), main = \"ACF of Absolute Residuals\")\n\n\n\n\n\n\n\nCode\npacf(abs(res), main = \"PACF of Absolute Residuals\")\n\n\n\n\n\n\n\n\n\nCode\nsummary(garchFit(~garch(1,0),res, trace=F))\n\n\n\nTitle:\n GARCH Modelling \n\nCall:\n garchFit(formula = ~garch(1, 0), data = res, trace = F) \n\nMean and Variance Equation:\n data ~ garch(1, 0)\n<environment: 0x10f56cdb0>\n [data = res]\n\nConditional Distribution:\n norm \n\nCoefficient(s):\n       mu      omega     alpha1  \n0.0090524  0.0375268  0.6903871  \n\nStd. Errors:\n based on Hessian \n\nError Analysis:\n        Estimate  Std. Error  t value Pr(>|t|)   \nmu      0.009052    0.033063    0.274  0.78424   \nomega   0.037527    0.012149    3.089  0.00201 **\nalpha1  0.690387    0.365974    1.886  0.05924 . \n---\nSignif. codes:  0 '***' 0.001 '**' 0.01 '*' 0.05 '.' 0.1 ' ' 1\n\nLog Likelihood:\n -3.60568    normalized:  -0.06933999 \n\nDescription:\n Tue Jan  9 21:04:13 2024 by user:  \n\n\nStandardised Residuals Tests:\n                                Statistic p-Value  \n Jarque-Bera Test   R    Chi^2  2.85827   0.239516 \n Shapiro-Wilk Test  R    W      0.9672877 0.1618376\n Ljung-Box Test     R    Q(10)  7.912167  0.6374161\n Ljung-Box Test     R    Q(15)  8.892122  0.8830901\n Ljung-Box Test     R    Q(20)  11.4269   0.9343917\n Ljung-Box Test     R^2  Q(10)  12.0452   0.2820428\n Ljung-Box Test     R^2  Q(15)  12.59754  0.6333525\n Ljung-Box Test     R^2  Q(20)  14.78189  0.788749 \n LM Arch Test       R    TR^2   16.50617  0.1691365\n\nInformation Criterion Statistics:\n      AIC       BIC       SIC      HQIC \n0.2540646 0.3666364 0.2478793 0.2972220 \n\n\n\n\n\nFrom the squared residuals of the best ARIMA model, it can be observed that the ACF plot and PACF plot indicate that the residuals are not autocorrelated and are white noise, indicating a good fit of the model. Based on the squared residuals of the best ARIMA model, we can see that the ACF and PACF plots indicate that most of the values lie between the blue lines. Additionally, the p-value is 1 and q-value is 0. This suggests that the model has a good fit and that there is no significant autocorrelation or partial autocorrelation in the residuals.\nThe best models are ARIMA(0,1,0) and ARCH(1)\n\n\nBest Model\n\nARIMA ModelGARCH ModelVolatility\n\n\n\n\nCode\n#fiting an ARIMA model to the Inflation variable\ninflation_fit<-auto.arima(normalized_numeric_df$inflation) \nfinflation<-forecast(inflation_fit)\n\n#fitting an ARIMA model to the Unemployment variable\nunemployment_fit<-auto.arima(normalized_numeric_df$unemployment) \nfunemployment<-forecast(unemployment_fit)\n\n# best model fit for forcasting\nxreg <- cbind(Inflation = normalized_data_ts[, \"inflation\"],\n              Unemployment = normalized_data_ts[, \"unemployment\"])\n\n\n\nsummary(arima.fit<-Arima(normalized_data_ts[, \"AAPL.Adjusted\"],order=c(0,1,0),xreg=xreg),include.drift = TRUE)\n\n\nSeries: normalized_data_ts[, \"AAPL.Adjusted\"] \nRegression with ARIMA(0,1,0) errors \n\nCoefficients:\n      Inflation  Unemployment\n         0.1639       -0.1104\ns.e.     0.0896        0.0407\n\nsigma^2 = 0.04248:  log likelihood = 9.2\nAIC=-12.4   AICc=-11.89   BIC=-6.61\n\nTraining set error measures:\n                    ME      RMSE       MAE       MPE     MAPE      MASE\nTraining set 0.0381237 0.2000733 0.1274114 0.7801298 40.00078 0.4385178\n                 ACF1\nTraining set 0.122828\n\n\n\n\n\n\nCode\nsummary(final.fit <- garchFit(~garch(1,0), res,trace = F))\n\n\n\nTitle:\n GARCH Modelling \n\nCall:\n garchFit(formula = ~garch(1, 0), data = res, trace = F) \n\nMean and Variance Equation:\n data ~ garch(1, 0)\n<environment: 0x12b0df890>\n [data = res]\n\nConditional Distribution:\n norm \n\nCoefficient(s):\n       mu      omega     alpha1  \n0.0090524  0.0375268  0.6903871  \n\nStd. Errors:\n based on Hessian \n\nError Analysis:\n        Estimate  Std. Error  t value Pr(>|t|)   \nmu      0.009052    0.033063    0.274  0.78424   \nomega   0.037527    0.012149    3.089  0.00201 **\nalpha1  0.690387    0.365974    1.886  0.05924 . \n---\nSignif. codes:  0 '***' 0.001 '**' 0.01 '*' 0.05 '.' 0.1 ' ' 1\n\nLog Likelihood:\n -3.60568    normalized:  -0.06933999 \n\nDescription:\n Tue Jan  9 21:04:14 2024 by user:  \n\n\nStandardised Residuals Tests:\n                                Statistic p-Value  \n Jarque-Bera Test   R    Chi^2  2.85827   0.239516 \n Shapiro-Wilk Test  R    W      0.9672877 0.1618376\n Ljung-Box Test     R    Q(10)  7.912167  0.6374161\n Ljung-Box Test     R    Q(15)  8.892122  0.8830901\n Ljung-Box Test     R    Q(20)  11.4269   0.9343917\n Ljung-Box Test     R^2  Q(10)  12.0452   0.2820428\n Ljung-Box Test     R^2  Q(15)  12.59754  0.6333525\n Ljung-Box Test     R^2  Q(20)  14.78189  0.788749 \n LM Arch Test       R    TR^2   16.50617  0.1691365\n\nInformation Criterion Statistics:\n      AIC       BIC       SIC      HQIC \n0.2540646 0.3666364 0.2478793 0.2972220 \n\n\n\n\n\n\nCode\nht <- final.fit@h.t #a numeric vector with the conditional variances (h.t = sigma.t^delta)\n\n#############################\ndata=data.frame(final)\ndata$Date<-as.Date(data$Date,\"%Y-%m-%d\")\n\n\ndata2= data.frame(ht,data$Date)\nggplot(data2, aes(y = ht, x = data.Date)) + geom_line(col = '#922B21') + ylab('Conditional Variance') + xlab('Date')\n\n\n\n\n\n\n\n\nFrom the ARIMA(0,1,0), we see that the training set error measures also suggest a good fit, with low mean absolute error, root mean squared error, and autocorrelation of the residuals. GATCH(1,0) model model is used to estimate the volatility of the standardized residuals of the previous regression model. The model includes a mean equation that estimates the mean of the residuals and a variance equation that models the conditional variance of the residuals. The coefficients of the mean equation suggest that the mean of the residuals is close to zero. The variance equation coefficients suggest that the conditional variance of the residuals is dependent on the past conditional variances and the past squared standardized residuals. The model’s AIC, BIC, SIC, and HQIC values are all relatively low, indicating a good fit of the model. The standardized residuals tests indicate that the residuals are approximately normally distributed and that there is no significant autocorrelation in the residuals.\nThe volatility of the model seems high in 2020 but has decreased gradually in the past few months. This could indicate that the asset’s price was experiencing a lot of fluctuations in 2020, but the market has stabilized recently.\n\n\nModel Diagnostics\n\nResidualsQQ PlotBox Test\n\n\n\n\nCode\nfit2<-garch(res,order=c(1,0),trace=F)\ncheckresiduals(fit2) \n\n\nWarning in modeldf.default(object): Could not find appropriate degrees of\nfreedom for this model.\n\n\n\n\n\n\n\n\n\nCode\nqqnorm(fit2$residuals, pch = 1)\nqqline(fit2$residuals, col = \"blue\", lwd = 2)\n\n\n\n\n\n\n\n\n\nCode\nBox.test (fit2$residuals, type = \"Ljung\")\n\n\n\n    Box-Ljung test\n\ndata:  fit2$residuals\nX-squared = 2.6613, df = 1, p-value = 0.1028\n\n\nThe ACF plot of the residuals shows all the values between the blue lines, which indicates that the residuals are not significantly autocorrelated. The range of values for the residual plot between -2 and 2 is considered acceptable. Additionally, the QQ plot of the residuals shows a linear plot on the line, which is another good indication that the residuals are normally distributed. The QQ plot is a valuable tool to assess if the residuals follow a normal distribution, and in this case, the plot suggests that the residuals do indeed follow a normal distribution.\nThe Box-Ljung test, a p-value of 0.1028 indicates that the model’s residuals are not significantly autocorrelated, meaning that the model has captured most of the information in the data. This result is good because it suggests that the model is a good fit for the data and has accounted for most of the underlying patterns in the data. Therefore, we can rely on the model’s predictions and use them to make informed decisions.\n\n\n\n\n\nForecast\n\n\nCode\npredict(final.fit, n.ahead = 5, plot=TRUE)\n\n\n\n\n\n  meanForecast meanError standardDeviation lowerInterval upperInterval\n1  0.009052438 0.2478172         0.2478172    -0.4766603     0.4947652\n2  0.009052438 0.2827115         0.2827115    -0.5450520     0.5631569\n3  0.009052438 0.3044776         0.3044776    -0.5877126     0.6058175\n4  0.009052438 0.3186381         0.3186381    -0.6154668     0.6335717\n5  0.009052438 0.3280579         0.3280579    -0.6339293     0.6520342\n\n\nThe forecasted plot is based on the best model ARIMAX(0,1,0)+GARCH(1,0). This model takes into account the autoregressive and moving average components of the data, as well as the impact of exogenous variables on the time series. Additionally, the GARCH component of the model accounts for the volatility clustering in the data. Overall, this model is well-suited to make accurate predictions about future values of the time series.\n\n\nEquation of the Model\nThe equation of the ARIMAX(0,1,0) model is:\n\\(Y(t) = Y(t-1) + \\epsilon(t)\\)\nwhere \\(Y(t)\\) is the time series variable and \\(\\epsilon(t)\\) is the error term.\nThe equation of the ARCH(1) model is:\n\\(\\sigma_{t}^{2}=\\alpha_{0}+\\alpha_{1}\\epsilon_{t-1}^{2}\\)\nwhere \\(\\sigma^2_t\\) is the conditional variance at time \\(t\\), \\(\\alpha_0\\) is a constant, \\(\\alpha_1\\) and \\(\\beta_1\\) are the parameters, and \\(\\epsilon_t\\) is the error term.\nThe combined equation of the ARIMAX(1,1,1)+GARCH(1,1) model is:\n\\(Y(t) = Y(t-1) + \\epsilon(t)\\)\n\\(\\sigma_{t}^{2}=\\alpha_{0}+\\alpha_{1}\\epsilon_{t-1}^{2}\\)"
  },
  {
    "objectID": "arch_Utilities_Sector_Fund.html",
    "href": "arch_Utilities_Sector_Fund.html",
    "title": "",
    "section": "",
    "text": "Impact of Macroeconomic Factors on NextEra Energy Stock Price\n\n\n\nOne of the top companies in the utility sector is NextEra Energy, Inc. (NEE), which operates as a clean energy company. To analyze the stock price behavior of NextEra Energy, an ARIMAX+ARCH/GARCH model can be employed.\nBy using an ARIMAX+ARCH/GARCH model to analyze the stock price behavior of NextEra Energy, we can gain insights into how macroeconomic factors impact the company’s performance. For example, an increase in renewable energy adoption, government policies supporting clean energy, or a decrease in natural gas prices may lead to increased demand for NextEra Energy’s clean energy services and a rise in its stock price. Conversely, extreme weather conditions or natural disasters that impact NextEra Energy’s operations may lead to a decrease in its stock price. Additionally, changes in interest rates or inflation may impact NextEra Energy’s cost of capital, which may affect the company’s profitability and stock price.\n\n\nTime series Plot\n\nNEEDifferentitaed Chart SeriesGDP GrowthInflationInterestUnemployment\n\n\n\n\nCode\n# get data\noptions(\"getSymbols.warning4.0\"=FALSE)\noptions(\"getSymbols.yahoo.warning\"=FALSE)\n\n\ndata.info = getSymbols(\"NEE\",src='yahoo', from = '2010-01-01',to = \"2023-03-01\",auto.assign = FALSE)\ndata = getSymbols(\"NEE\",src='yahoo', from = '2010-01-01',to = \"2023-03-01\")\ndf <- data.frame(Date=index(NEE),coredata(NEE))\n\n# create Bollinger Bands\nbbands <- BBands(NEE[,c(\"NEE.High\",\"NEE.Low\",\"NEE.Close\")])\n\n# join and subset data\ndf <- subset(cbind(df, data.frame(bbands[,1:3])), Date >= \"2010-01-01\")\n\n\n# colors column for increasing and decreasing\nfor (i in 1:length(df[,1])) {\n  if (df$NEE.Close[i] >= df$NEE.Open[i]) {\n      df$direction[i] = 'Increasing'\n  } else {\n      df$direction[i] = 'Decreasing'\n  }\n}\n\ni <- list(line = list(color = '#7DCEA0'))\nd <- list(line = list(color = '#7F7F7F'))\n\n# plot candlestick chart\n\nfig <- df %>% plot_ly(x = ~Date, type=\"candlestick\",\n          open = ~NEE.Open, close = ~NEE.Close,\n          high = ~NEE.High, low = ~NEE.Low, name = \"NEE\",\n          increasing = i, decreasing = d) \nfig <- fig %>% add_lines(x = ~Date, y = ~up , name = \"B Bands\",\n            line = list(color = '#ccc', width = 0.5),\n            legendgroup = \"Bollinger Bands\",\n            hoverinfo = \"none\", inherit = F) \nfig <- fig %>% add_lines(x = ~Date, y = ~dn, name = \"B Bands\",\n            line = list(color = '#ccc', width = 0.5),\n            legendgroup = \"Bollinger Bands\", inherit = F,\n            showlegend = FALSE, hoverinfo = \"none\") \nfig <- fig %>% add_lines(x = ~Date, y = ~mavg, name = \"Mv Avg\",\n            line = list(color = '#C052B3', width = 0.5),\n            hoverinfo = \"none\", inherit = F) \nfig <- fig %>% layout(yaxis = list(title = \"Price\"))\n\n# plot volume bar chart\nfig2 <- df \nfig2 <- fig2 %>% plot_ly(x=~Date, y=~NEE.Volume, type='bar', name = \"NEE Volume\",\n          color = ~direction, colors = c('#7DCEA0','#7F7F7F')) \nfig2 <- fig2 %>% layout(yaxis = list(title = \"Volume\"))\n\n# create rangeselector buttons\nrs <- list(visible = TRUE, x = 0.5, y = -0.055,\n           xanchor = 'center', yref = 'paper',\n           font = list(size = 9),\n           buttons = list(\n             list(count=1,\n                  label='RESET',\n                  step='all'),\n             list(count=3,\n                  label='3 YR',\n                  step='year',\n                  stepmode='backward'),\n             list(count=1,\n                  label='1 YR',\n                  step='year',\n                  stepmode='backward'),\n             list(count=1,\n                  label='1 MO',\n                  step='month',\n                  stepmode='backward')\n           ))\n\n# subplot with shared x axis\nfig <- subplot(fig, fig2, heights = c(0.7,0.2), nrows=2,\n             shareX = TRUE, titleY = TRUE)\nfig <- fig %>% layout(title = paste(\"NEE Stock Price: January 2010 - March 2023\"),\n         xaxis = list(rangeselector = rs),\n         legend = list(orientation = 'h', x = 0.5, y = 1,\n                       xanchor = 'center', yref = 'paper',\n                       font = list(size = 10),\n                       bgcolor = 'transparent'))\n\nfig\n\n\n\n\n\n\n\n\n\n\nCode\nlog(data.info$`NEE.Adjusted`) %>% diff() %>% chartSeries(theme=chartTheme('white'),up.col='#7DCEA0')\n\n\n\n\n\n\n\n\n\nCode\n#import the data\ngdp <- read.csv(\"DATA/RAW DATA/gdp-growth.csv\")\n\n#change date format\ngdp$Date <- as.Date(gdp$DATE , \"%m/%d/%Y\")\n\n#drop DATE column\ngdp <- subset(gdp, select = -c(1))\n\n#export the cleaned data\ngdp_clean <- gdp\nwrite.csv(gdp_clean, \"DATA/CLEANED DATA/gdp_clean_data.csv\", row.names=FALSE)\n\n#plot gdp growth rate \nfig <- plot_ly(gdp, x = ~Date, y = ~value, type = 'scatter', mode = 'lines',line = list(color = 'rgb(240, 128, 128)'))\nfig <- fig %>% layout(title = \"U.S GPD Growth Rate: 2010 - 2022\",xaxis = list(title = \"Time\"),yaxis = list(title =\"GDP Growth Rate\"))\nfig\n\n\n\n\n\n\n\n\n\n\nCode\n#import the data\ninflation_rate <- read.csv(\"DATA/RAW DATA/inflation-rate.csv\")\n\n#cleaning the data\n#remove unwanted columns\ninflation_rate_clean <- subset(inflation_rate, select = -c(1,HALF1,HALF2))\n\n#convert the data to time series data\ninflation_data_ts <- ts(as.vector(t(as.matrix(inflation_rate_clean))), start=c(2010,1), end=c(2023,2), frequency=12)\n\n#export the data\nwrite.csv(inflation_rate_clean, \"DATA/CLEANED DATA/inflation_rate_clean_data.csv\", row.names=FALSE)\n\n\n#plot inflation rate \nfig <- autoplot(inflation_data_ts, ylab = \"Inflation Rate\", color=\"#FFA07A\")+ggtitle(\"U.S Inflation Rate: January 2010 - February 2023\")+theme_bw()\nggplotly(fig)\n\n\n\n\n\n\n\n\n\n\nCode\n#import the data\ninterest_data <- read.csv(\"DATA/RAW DATA/interest-rate.csv\")\n\n#change date format\ninterest_data$Date <- as.Date(interest_data$Date , \"%m/%d/%Y\")\n\n#export the cleaned data\ninterest_clean_data <- interest_data\nwrite.csv(interest_clean_data, \"DATA/CLEANED DATA/interest_rate_clean_data.csv\", row.names=FALSE)\n\n#plot interest rate \nfig <- plot_ly(interest_data, x = ~Date, y = ~value, type = 'scatter', mode = 'lines',line = list(color='rgb(219, 112, 147)'))\nfig <- fig %>% layout(title = \"U.S Interest Rate: January 2010 - March 2023\",xaxis = list(title = \"Time\"),yaxis = list(title =\"Interest Rate\"))\nfig\n\n\n\n\n\n\n\n\n\n\nCode\n#import the data\nunemployment_rate <- read.csv(\"DATA/RAW DATA/unemployment-rate.csv\")\n\n#change date format\nunemployment_rate$Date <- as.Date(unemployment_rate$Date , \"%m/%d/%Y\")\n\n# export the data\nwrite.csv(unemployment_rate, \"DATA/CLEANED DATA/unemployment_rate_clean_data.csv\", row.names=FALSE)\n\n#plot unemployment rate \nfig <- plot_ly(unemployment_rate, x = ~Date, y = ~Value, type = 'scatter', mode = 'lines',line = list(color = 'rgb(189, 183, 107)'))\nfig <- fig %>% layout(title = \"U.S Unemployment Rate: January 2010 - March 2023\",xaxis = list(title = \"Time\"),yaxis = list(title =\"Unemployment Rate\"))\nfig\n\n\n\n\n\n\n\n\n\nthe stock prices and volume of NextEra Energy (NEE) from January 2010 to March 2023. Overall, NEE has experienced a steady increase in stock prices over the period, with some fluctuations along the way.\nThe Bollinger Bands, which are represented by the upper and lower bands in the candlestick chart, show the standard deviation of the stock prices over a certain period of time. When the prices approach the upper band, it is an indication that the stock is overbought and may be due for a correction. Conversely, when the prices approach the lower band, it is an indication that the stock is oversold and may be due for a rebound.\nThe volume chart shows the trading volume of NEE during the same period. It is evident that the highest trading volumes occurred during periods of significant price changes, indicating high investor interest and activity.\nBased on the plot, it appears that NEE is a relatively stable and reliable investment option, with steady growth over the years. The stock has shown volatility for the year during 2021, that is because of COVID19.\nAs discussed before, the macroeconomic factors of GDP growth, inflation, interest rates, and unemployment rate are closely interrelated and play a crucial role in the overall health and stability of an economy. From 2010 to 2023, the global economy experienced a mix of ups and downs, with periods of strong GDP growth followed by slowdowns and recessions.\nThe second plot shows the first difference of the logarithm of the adjusted NEE stock price. Taking the first difference removes any long-term trends and transforms the time series into a stationary process. From the plot, we can observe that the first difference of the logarithm of the NEE stock price appears to be stationary, as the mean and variance are roughly constant over time.\n\n\nEnodogenous and Exogenous Variables\n\nPlotCorrelation HeatmapCCF GDPCCF InterestCCF InflationCCF Unemployment\n\n\n\n\nCode\nnumeric_data <- c(\"NEE.Adjusted\",\"gdp\", \"interest\", \"inflation\", \"unemployment\")\nnumeric_data <- final[, numeric_data]\nnormalized_data_numeric <- scale(numeric_data)\nnormalized_data <- ts(normalized_data_numeric, start = c(2010, 1), end = c(2021,10),frequency = 4)\nts_plot(normalized_data,\n        title = \"Normalized Time Series Data for NEE Stock and Macroeconomic Variables\",\n        Ytitle = \"Normalized Values\",\n        Xtitle = \"Year\")\n\n\n\n\n\n\n\n\n\n\nCode\n# Get upper triangle of the correlation matrix\nget_upper_tri <- function(cormat){\n    cormat[lower.tri(cormat)]<- NA\n    return(cormat)\n}\ncormat <- round(cor(normalized_data_numeric),2)\n\nupper_tri <- get_upper_tri(cormat)\n\nmelted_cormat <- melt(upper_tri, na.rm = TRUE)\n# Create a ggheatmap\nggheatmap <- ggplot(melted_cormat, aes(Var2, Var1, fill = value))+\n geom_tile(color = \"white\")+\n scale_fill_gradient2(low = \"blue\", high = \"red\", mid = \"white\", \n   midpoint = 0, limit = c(-1,1), space = \"Lab\", \n    name=\"Pearson\\nCorrelation\") +\n  theme_minimal()+ # minimal theme\n theme(axis.text.x = element_text(angle = 45, vjust = 1, \n    size = 12, hjust = 1))+\n coord_fixed()\n\nggheatmap + \ngeom_text(aes(Var2, Var1, label = value), color = \"black\", size = 4) +\ntheme(\n  axis.title.x = element_blank(),\n  axis.title.y = element_blank(),\n  panel.grid.major = element_blank(),\n  panel.border = element_blank(),\n  panel.background = element_blank(),\n  axis.ticks = element_blank(),\n  legend.justification = c(1, 0),\n  legend.position = c(0.6, 0.7),\n  legend.direction = \"horizontal\")+\n  guides(fill = guide_colorbar(barwidth = 7, barheight = 1,\n                title.position = \"top\", title.hjust = 0.5))\n\n\n\n\n\n\n\n\n\nCode\npar(mfrow=c(1,1))\nccf_result <- ccf(normalized_data[, c(\"NEE.Adjusted\")], normalized_data[, c(\"gdp\")], \n    lag.max = 300,\n    main = \"Cros-Correlation Plot for NEE Stock Price and GDP Growth Rate \",\n    ylab = \"CCF\")\n\n\n\n\n\nCode\ncat(\"The sum of cross correlation function is\", sum(abs(ccf_result$acf)))\n\n\nThe sum of cross correlation function is 4.619392\n\n\n\n\n\n\nCode\npar(mfrow=c(1,1))\nccf_result <- ccf(normalized_data[, c(\"NEE.Adjusted\")], normalized_data[, c(\"interest\")], \n    lag.max = 300,\n    main = \"Cros-Correlation Plot for NEE Stock Price and Interest Rate\",\n    ylab = \"CCF\")\n\n\n\n\n\nCode\ncat(\"The sum of cross correlation function is\", sum(abs(ccf_result$acf)))\n\n\nThe sum of cross correlation function is 11.24368\n\n\n\n\n\n\nCode\npar(mfrow=c(1,1))\nccf_result <- ccf(normalized_data[, c(\"NEE.Adjusted\")], normalized_data[, c(\"inflation\")], \n    lag.max = 300,\n    main = \"Cros-Correlation Plot for NEE Stock Price and Inflation Rate\",\n    ylab = \"CCF\")\n\n\n\n\n\nCode\ncat(\"The sum of cross correlation function is\", sum(abs(ccf_result$acf)))\n\n\nThe sum of cross correlation function is 16.46885\n\n\n\n\n\n\nCode\npar(mfrow=c(1,1))\nccf_result <- ccf(normalized_data[, c(\"NEE.Adjusted\")], normalized_data[, c(\"unemployment\")], \n    lag.max = 300,\n    main = \"Cros-Correlation Plot for NEE Stock Priceand Unemployment Rate\",\n    ylab = \"CCF\")\n\n\n\n\n\nCode\ncat(\"The sum of cross correlation function is\", sum(abs(ccf_result$acf)))\n\n\nThe sum of cross correlation function is 19.96215\n\n\n\n\n\nThe Normalized Time Series Data for Stock Price and Macroeconomic Variables plot shows the same variables as the first plot but has been normalized to a common range of 0 to 1 using the scale() function in R, which standardizes the variables to have a mean of 0 and a standard deviation of 1. The heatmap analysis of the normalized data reveals that inflation and unemployment rate exhibit strong positive correlations with the stock price indices, indicating that these variables may significantly influence stock price movements. On the other hand, weaker correlations were observed between the stock price indices and GDP and interest rates, suggesting that these variables may have less impact on stock price fluctuations. The cross-correlation feature plots confirm these findings, indicating that inflation and unemployment rate are more suitable feature variables for the ARIMAX model when predicting NEE movements.\nFinal Exogenous variables: Macroeconomic indicators: Inflation rate and unemployment rate.\n\n\nEnodogenous and Exogenous Variables Plot\n\nPlotCheck the stationarity\n\n\n\n\nCode\nfinal_data <- final %>%dplyr::select( Date,NEE.Adjusted, inflation,unemployment)\nnumeric_data <- c(\"NEE.Adjusted\", \"inflation\",\"unemployment\")\nnumeric_data <- final_data[, numeric_data]\nnormalized_data_numeric <- scale(numeric_data)\nnormalized_numeric_df <- data.frame(normalized_data_numeric)\nnormalized_data_ts <- ts(normalized_data_numeric, start = c(2010, 1), frequency = 4)\n\nautoplot(normalized_data_ts, facets=TRUE) +\n  xlab(\"Year\") + ylab(\"\") +\n  ggtitle(\"NEE Stock Price, Inflation Rate and Unemployment Rate in USA 2010-2023\")\n\n\n\n\n\n\n\n\n\nCode\n# Convert your multivariate time series data to a matrix\nfinal_data_ts_multivariate <- as.matrix(normalized_data_ts)\n\n# Check for stationarity using Phillips-Perron test\nphillips_perron_test <- ur.pp(final_data_ts_multivariate)  \nsummary(phillips_perron_test)\n\n\n\n################################## \n# Phillips-Perron Unit Root Test # \n################################## \n\nTest regression with intercept \n\n\nCall:\nlm(formula = y ~ y.l1)\n\nResiduals:\n    Min      1Q  Median      3Q     Max \n-2.1915 -0.1453 -0.0565  0.0819  4.4942 \n\nCoefficients:\n             Estimate Std. Error t value Pr(>|t|)    \n(Intercept) 0.0008174  0.0412102    0.02    0.984    \ny.l1        0.8574256  0.0416277   20.60   <2e-16 ***\n---\nSignif. codes:  0 '***' 0.001 '**' 0.01 '*' 0.05 '.' 0.1 ' ' 1\n\nResidual standard error: 0.5131 on 153 degrees of freedom\nMultiple R-squared:  0.735, Adjusted R-squared:  0.7332 \nF-statistic: 424.3 on 1 and 153 DF,  p-value: < 2.2e-16\n\n\nValue of test-statistic, type: Z-alpha  is: -23.1887 \n\n         aux. Z statistics\nZ-tau-mu            0.0204\n\n\n\n\n\nThe results of the Phillips-Perron unit root test indicate strong evidence against the null hypothesis of a unit root, as the p-value for the coefficient of the lagged variable is less than the significance level of 0.05. This suggests that the variable y, which is being tested for stationarity, is likely stationary. Furthermore, the test statistic Z-tau-mu is 0.0204, which is smaller than the critical value of Z-alpha (-23.1887), providing further evidence of stationarity.\nTo determine whether the linear model requires an ARCH model, an ARCH test is conducted. The ACF and PACF plots are also used to identify suitable model values.\n\n\nModel Fitting\n\nPlotARCH TestACF PlotPACF Plot\n\n\n\n\nCode\nnormalized_numeric_df$NEE.Adjusted<-ts(normalized_numeric_df$NEE.Adjusted,star=decimal_date(as.Date(\"2010-01-01\",format = \"%Y-%m-%d\")),frequency = 4)\nnormalized_numeric_df$inflation<-ts(normalized_numeric_df$inflation,star=decimal_date(as.Date(\"2010-01-01\",format = \"%Y-%m-%d\")),frequency = 4)\nnormalized_numeric_df$unemployment<-ts(normalized_numeric_df$unemployment,star=decimal_date(as.Date(\"2010-01-01\",format = \"%Y-%m-%d\")),frequency = 4)\n\nfit <- lm(NEE.Adjusted ~ inflation+unemployment, data=normalized_numeric_df)\nfit.res<-ts(residuals(fit),star=decimal_date(as.Date(\"2010-01-01\",format = \"%Y-%m-%d\")),frequency = 4)\n############## Then look at the residuals ############\nreturns <- fit.res  %>% diff()\nautoplot(returns)+ggtitle(\"Linear Model Returns\")\n\n\n\n\n\n\n\n\n\nCode\nbyd.archTest <- ArchTest(fit.res, lags = 1, demean = TRUE)\nbyd.archTest\n\n\n\n    ARCH LM-test; Null hypothesis: no ARCH effects\n\ndata:  fit.res\nChi-squared = 33.062, df = 1, p-value = 8.926e-09\n\n\n\n\n\n\nCode\nggAcf(returns) +ggtitle(\"ACF for returns\")\n\n\n\n\n\n\n\n\n\nCode\nggPacf(returns) +ggtitle(\"PACF for returns\")\n\n\n\n\n\n\n\n\nThe ARCH LM-test was conducted with the null hypothesis of no ARCH effects. The test resulted in a chi-squared value of 33.062 with one degree of freedom, and a very low p-value of 8.926e-09. This suggests strong evidence against the null hypothesis, indicating the presence of ARCH effects in the data.\nBased on the ACF and PACF plots, it appears that there is some significant autocorrelation and partial autocorrelation at multiple lags, which suggests that an ARIMA model may not be sufficient to capture the time series behavior. Additionally, the values for p and q appear to be relatively high, with p = 0 and q = 0 being suggested by the plots.\n\n\nARIMAX Model\n\nAuto Arima ModelAuto Arima ResidualsARIMA (0,1,0) PlotARIMA (0,1,0) Model\n\n\n\n\nCode\nxreg <- cbind(Inflation = normalized_data_ts[, \"inflation\"],\n              Unemployment = normalized_data_ts[, \"unemployment\"])\nfit.auto <- auto.arima(normalized_data_ts[, \"NEE.Adjusted\"], xreg = xreg)\nsummary(fit.auto)\n\n\nSeries: normalized_data_ts[, \"NEE.Adjusted\"] \nRegression with ARIMA(0,1,0)(0,0,1)[4] errors \n\nCoefficients:\n        sma1   drift  Inflation  Unemployment\n      0.5305  0.0468     0.1021       -0.0654\ns.e.  0.1278  0.0208     0.0400        0.0175\n\nsigma^2 = 0.01067:  log likelihood = 44.82\nAIC=-79.65   AICc=-78.32   BIC=-69.99\n\nTraining set error measures:\n                       ME       RMSE        MAE      MPE     MAPE      MASE\nTraining set 0.0002636708 0.09822202 0.07276088 1.973295 16.47722 0.2817092\n                  ACF1\nTraining set 0.1378288\n\n\n\n\n\n\nCode\ncheckresiduals(fit.auto)\n\n\n\n\n\n\n    Ljung-Box test\n\ndata:  Residuals from Regression with ARIMA(0,1,0)(0,0,1)[4] errors\nQ* = 5.6574, df = 7, p-value = 0.5803\n\nModel df: 1.   Total lags used: 8\n\n\n\n\n\n\nCode\nset.seed(1234)\n\nmodel_output <- capture.output(sarima(fit.res, 0,1,0)) \n\n\n\n\n\n\n\n\n\nCode\ncat(model_output[9:38], model_output[length(model_output)], sep = \"\\n\")\n\n\n$fit\n\nCall:\narima(x = xdata, order = c(p, d, q), seasonal = list(order = c(P, D, Q), period = S), \n    xreg = constant, transform.pars = trans, fixed = fixed, optim.control = list(trace = trc, \n        REPORT = 1, reltol = tol))\n\nCoefficients:\n      constant\n        0.0036\ns.e.    0.0341\n\nsigma^2 estimated as 0.05931:  log likelihood = -0.33,  aic = 4.66\n\n$degrees_of_freedom\n[1] 50\n\n$ttable\n         Estimate     SE t.value p.value\nconstant   0.0036 0.0341  0.1056  0.9163\n\n$AIC\n[1] 0.0912776\n\n$AICc\n[1] 0.09287824\n\n$BIC\n[1] 0.1670355\n\n\n\n\n\nThe auto.arima function suggests the ARIMA(0,1,0)(0,0,1)[4] model, but the acf and pacf plots suggest a simpler ARIMA(0,1,0) model. Upon comparison, the AIC and BIC values of the ARIMA(0,1,0) model are much lower than the other model, indicating that it is the better choice.\nWe can then proceed to choose the best GARCH model using ARIMA(0,1,0) as the base model.\n\n\nSquared Residuals\n\nPlotACF PlotPACF PlotGRACH(1,1)\n\n\n\n\nCode\nfit <- lm(NEE.Adjusted ~ inflation+unemployment, data=normalized_numeric_df)\nfit.res<-ts(residuals(fit),star=decimal_date(as.Date(\"2010-01-01\",format = \"%Y-%m-%d\")),frequency = 4)\nfit <- Arima(fit.res,order=c(0,1,0))\nres=fit$res\nplot(res^2,main='Squared Residuals')\n\n\n\n\n\n\n\n\n\nCode\nacf(res^2,24, main = \"ACF Residuals Square\")\n\n\n\n\n\n\n\n\n\nCode\npacf(res^2,24, main = \"PACF Residuals Square\")\n\n\n\n\n\n\n\n\n\nCode\nsummary(garchFit(~garch(1,1),res, trace=F))\n\n\n\nTitle:\n GARCH Modelling \n\nCall:\n garchFit(formula = ~garch(1, 1), data = res, trace = F) \n\nMean and Variance Equation:\n data ~ garch(1, 1)\n<environment: 0x128d514e0>\n [data = res]\n\nConditional Distribution:\n norm \n\nCoefficient(s):\n       mu      omega     alpha1      beta1  \n0.0140516  0.0051686  0.2653368  0.6986085  \n\nStd. Errors:\n based on Hessian \n\nError Analysis:\n        Estimate  Std. Error  t value Pr(>|t|)    \nmu      0.014052    0.024565    0.572    0.567    \nomega   0.005169    0.004689    1.102    0.270    \nalpha1  0.265337    0.189149    1.403    0.161    \nbeta1   0.698608    0.152846    4.571 4.86e-06 ***\n---\nSignif. codes:  0 '***' 0.001 '**' 0.01 '*' 0.05 '.' 0.1 ' ' 1\n\nLog Likelihood:\n 5.38499    normalized:  0.1035575 \n\nDescription:\n Tue Jan  9 21:04:31 2024 by user:  \n\n\nStandardised Residuals Tests:\n                                Statistic p-Value     \n Jarque-Bera Test   R    Chi^2  46.08944  9.813084e-11\n Shapiro-Wilk Test  R    W      0.9212264 0.002081904 \n Ljung-Box Test     R    Q(10)  11.23045  0.3398482   \n Ljung-Box Test     R    Q(15)  14.68594  0.4742673   \n Ljung-Box Test     R    Q(20)  18.72226  0.5399399   \n Ljung-Box Test     R^2  Q(10)  8.744659  0.5564958   \n Ljung-Box Test     R^2  Q(15)  9.787824  0.8328698   \n Ljung-Box Test     R^2  Q(20)  10.5537   0.956986    \n LM Arch Test       R    TR^2   16.96835  0.1507919   \n\nInformation Criterion Statistics:\n         AIC          BIC          SIC         HQIC \n-0.053268834  0.096826836 -0.064014144  0.004274303 \n\n\n\n\n\nFrom the squared residuals of the best ARIMA model, it can be observed that the ACF plot and PACF plot indicate that the residuals are not autocorrelated and are white noise, indicating a good fit of the model. Based on the squared residuals of the best ARIMA model, we can see that the ACF and PACF plots indicate that most of the values lie between the blue lines. Additionally, the p-value is 1 and q-value is 1. This suggests that the model has a good fit and that there is no significant autocorrelation or partial autocorrelation in the residuals.\nBased on the analysis of the different GARCH models, it appears that GARCH(1,1) is the best model.\n\n\nBest Model\n\nARIMA ModelGARCH ModelVolatility\n\n\n\n\nCode\n#fiting an ARIMA model to the Inflation variable\ninflation_fit<-auto.arima(normalized_numeric_df$inflation) \nfinflation<-forecast(inflation_fit)\n\n#fitting an ARIMA model to the Unemployment variable\nunemployment_fit<-auto.arima(normalized_numeric_df$unemployment) \nfunemployment<-forecast(unemployment_fit)\n\n# best model fit for forcasting\nxreg <- cbind(Inflation = normalized_data_ts[, \"inflation\"],\n              Unemployment = normalized_data_ts[, \"unemployment\"])\n\n\n\nsummary(arima.fit<-Arima(normalized_data_ts[, \"NEE.Adjusted\"],order=c(0,1,0),xreg=xreg),include.drift = TRUE)\n\n\nSeries: normalized_data_ts[, \"NEE.Adjusted\"] \nRegression with ARIMA(0,1,0) errors \n\nCoefficients:\n      Inflation  Unemployment\n         0.1026       -0.0633\ns.e.     0.0539        0.0245\n\nsigma^2 = 0.01536:  log likelihood = 35.15\nAIC=-64.29   AICc=-63.78   BIC=-58.5\n\nTraining set error measures:\n                   ME      RMSE        MAE      MPE     MAPE      MASE\nTraining set 0.046881 0.1202993 0.08733903 1.985176 20.98353 0.3381516\n                  ACF1\nTraining set 0.2016607\n\n\n\n\n\n\nCode\nsummary(final.fit <- garchFit(~garch(1,1), res,trace = F))\n\n\n\nTitle:\n GARCH Modelling \n\nCall:\n garchFit(formula = ~garch(1, 1), data = res, trace = F) \n\nMean and Variance Equation:\n data ~ garch(1, 1)\n<environment: 0x12d5f9080>\n [data = res]\n\nConditional Distribution:\n norm \n\nCoefficient(s):\n       mu      omega     alpha1      beta1  \n0.0140516  0.0051686  0.2653368  0.6986085  \n\nStd. Errors:\n based on Hessian \n\nError Analysis:\n        Estimate  Std. Error  t value Pr(>|t|)    \nmu      0.014052    0.024565    0.572    0.567    \nomega   0.005169    0.004689    1.102    0.270    \nalpha1  0.265337    0.189149    1.403    0.161    \nbeta1   0.698608    0.152846    4.571 4.86e-06 ***\n---\nSignif. codes:  0 '***' 0.001 '**' 0.01 '*' 0.05 '.' 0.1 ' ' 1\n\nLog Likelihood:\n 5.38499    normalized:  0.1035575 \n\nDescription:\n Tue Jan  9 21:04:31 2024 by user:  \n\n\nStandardised Residuals Tests:\n                                Statistic p-Value     \n Jarque-Bera Test   R    Chi^2  46.08944  9.813084e-11\n Shapiro-Wilk Test  R    W      0.9212264 0.002081904 \n Ljung-Box Test     R    Q(10)  11.23045  0.3398482   \n Ljung-Box Test     R    Q(15)  14.68594  0.4742673   \n Ljung-Box Test     R    Q(20)  18.72226  0.5399399   \n Ljung-Box Test     R^2  Q(10)  8.744659  0.5564958   \n Ljung-Box Test     R^2  Q(15)  9.787824  0.8328698   \n Ljung-Box Test     R^2  Q(20)  10.5537   0.956986    \n LM Arch Test       R    TR^2   16.96835  0.1507919   \n\nInformation Criterion Statistics:\n         AIC          BIC          SIC         HQIC \n-0.053268834  0.096826836 -0.064014144  0.004274303 \n\n\n\n\n\n\nCode\nht <- final.fit@h.t #a numeric vector with the conditional variances (h.t = sigma.t^delta)\n\n#############################\ndata=data.frame(final)\ndata$Date<-as.Date(data$Date,\"%Y-%m-%d\")\n\n\ndata2= data.frame(ht,data$Date)\nggplot(data2, aes(y = ht, x = data.Date)) + geom_line(col = '#7DCEA0') + ylab('Conditional Variance') + xlab('Date')\n\n\n\n\n\n\n\n\nFrom the ARIMA(0,1,0), we see that the training set error measures also suggest a good fit, with low mean absolute error, root mean squared error, and autocorrelation of the residuals. GATCH(1,1) model model is used to estimate the volatility of the standardized residuals of the previous regression model. The model includes a mean equation that estimates the mean of the residuals and a variance equation that models the conditional variance of the residuals. The coefficients of the mean equation suggest that the mean of the residuals is close to zero. The variance equation coefficients suggest that the conditional variance of the residuals is dependent on the past conditional variances and the past squared standardized residuals. The model’s AIC, BIC, SIC, and HQIC values are all relatively low, indicating a good fit of the model. The standardized residuals tests indicate that the residuals are approximately normally distributed and that there is no significant autocorrelation in the residuals.\nThe volatility of the model seems high in 2020 but has decreased gradually in the past few months. This could indicate that the asset’s price was experiencing a lot of fluctuations in 2020, but the market has stabilized recently.\n\n\nModel Diagnostics\n\nResidualsQQ PlotBox Test\n\n\n\n\nCode\nfit2<-garch(res,order=c(1,1),trace=F)\ncheckresiduals(fit2) #relatively doing a good job\n\n\n\n\n\n\n\n\n\nCode\nqqnorm(fit2$residuals, pch = 1)\nqqline(fit2$residuals, col = \"blue\", lwd = 2)\n\n\n\n\n\n\n\n\n\nCode\nBox.test (fit2$residuals, type = \"Ljung\")\n\n\n\n    Box-Ljung test\n\ndata:  fit2$residuals\nX-squared = 0.785, df = 1, p-value = 0.3756\n\n\nThe ACF plot of the residuals shows all the values between the blue lines, which indicates that the residuals are not significantly autocorrelated. The range of values for the residual plot between -2 and 2 is considered acceptable. Additionally, the QQ plot of the residuals shows a linear plot on the line, which is another good indication that the residuals are normally distributed. The QQ plot is a valuable tool to assess if the residuals follow a normal distribution, and in this case, the plot suggests that the residuals do indeed follow a normal distribution.\nThe Box-Ljung test, a p-value of 0.3754 indicates that the model’s residuals are not significantly autocorrelated, meaning that the model has captured most of the information in the data. This result is good because it suggests that the model is a good fit for the data and has accounted for most of the underlying patterns in the data. Therefore, we can rely on the model’s predictions and use them to make informed decisions.\n\n\n\n\n\nForecast\n\n\nCode\npredict(final.fit, n.ahead = 5, plot=TRUE)\n\n\n\n\n\n  meanForecast meanError standardDeviation lowerInterval upperInterval\n1    0.0140516 0.3214469         0.3214469    -0.6159727     0.6440759\n2    0.0140516 0.3236838         0.3236838    -0.6203569     0.6484601\n3    0.0140516 0.3258255         0.3258255    -0.6245547     0.6526579\n4    0.0140516 0.3278768         0.3278768    -0.6285751     0.6566783\n5    0.0140516 0.3298420         0.3298420    -0.6324269     0.6605301\n\n\nThe forecasted plot is based on the best model ARIMAX(0,1,0)+GARCH(1,1). This model takes into account the autoregressive and moving average components of the data, as well as the impact of exogenous variables on the time series. Additionally, the GARCH component of the model accounts for the volatility clustering in the data. Overall, this model is well-suited to make accurate predictions about future values of the time series.\n\n\nEquation of the Model\nThe equation of the ARIMAX(0,1,0) model is:\n\\(Y(t) = Y(t-1) + \\epsilon(t)\\)\nwhere \\(Y(t)\\) is the time series variable and \\(\\epsilon(t)\\) is the error term.\nThe equation of the GARCH(1,1) model is:\n\\(\\sigma^2(t) = \\alpha_0+\\alpha_1\\epsilon_t^2(t-1)+ \\beta_1\\sigma^2(t-1)\\) where \\(\\sigma^2_t\\) is the conditional variance at time \\(t\\), \\(\\alpha_0\\) is a constant, \\(\\alpha_1\\) and \\(\\beta_1\\) are the parameters, and \\(\\epsilon_t\\) is the error term.\nThe combined equation of the ARIMAX(0,1,0)+GARCH(1,1) model is:\n\\(Y(t) = Y(t-1) + \\epsilon(t)\\)\n\\(\\epsilon(t) = \\sigma(t) * \\epsilon~(t)\\)\n\\(\\sigma^2(t) = \\alpha_0+\\alpha_1\\epsilon_t^2(t-1)+ \\beta_1\\sigma^2(t-1)\\)"
  },
  {
    "objectID": "arima_index_dow_jones.html",
    "href": "arima_index_dow_jones.html",
    "title": "ARIMA Model for Dow Jones Index",
    "section": "",
    "text": "During exploratory data analysis (EDA) of Dow Jones Index, it was observed that the raw data is often non-stationary, exhibiting trends, seasonality, and other complex patterns that can make modeling and forecasting difficult. To address this issue, the data is often differentiated to achieve stationarity, which is a key assumption of the ARIMA model.\nDifferencing involves calculating the difference between each data point and the previous data point, which can help to remove trends and seasonality from the data. The goal is to create a new series of data that is stationary, meaning that the statistical properties of the data do not change over time.\n\nDifferentiated Time Series Plot\n\nPlotACFPACFADF Test\n\n\n\n\nCode\n#import the data\ndf <- read.csv(\"DATA/CLEANED DATA/dji_raw_data.csv\")\n#convert to time series data\nmyts<-ts(df$DJI.Adjusted,frequency=252,start=c(2010,1,4), end = c(2023,3,1)) \n#First order differentiation\ndf1 <- diff(myts)\n# Plot \nmyts  %>% diff() %>% ggtsdisplay(main = \"First order differentiation\") \n\n\n\n\n\n\n\n\n\nCode\n#ACF plot \nggAcf(df1,60,main=\"ACF Plot: First order differentiation\") \n\n\nWarning in ggplot2::geom_segment(lineend = \"butt\", ...): Ignoring unknown\nparameters: `main`\n\n\n\n\n\n\n\n\n\nCode\nggPacf(df1,60,main=\"PACF Plot: First order differentiation\") \n\n\n\n\n\n\n\n\n\nCode\ntseries::adf.test(df1)\n\n\n\n    Augmented Dickey-Fuller Test\n\ndata:  df1\nDickey-Fuller = -15.31, Lag order = 14, p-value = 0.01\nalternative hypothesis: stationary\n\n\n\n\n\nAfter analyzing the ACF and PACF plots, it can be observed that most of the bar lines lie between the blue lines, indicating that the time series is stationary. This has been further confirmed through an Augmented Dickey-Fuller test as the p-value is less than 0.05. Once the data is stationary, the next step is to model it using ARIMA. The combination of ACF and PACF plots can help determine the appropriate values for p, d, and q in the ARIMA model. The order of differencing required to achieve stationarity gives the d value. The p value is determined by examining significant spikes at various lags in the PACF plot, while the q value is determined through significant spikes in the ACF plot. It is important to consider other methods such as grid search and information criteria to ensure the most appropriate model is selected.\nHere the parameters are d = 1 p = 0,7 (PACF Plot) q = 0,7 (ACF Plot)\n\n\nModel Selection\n\nARIMA ResultAuto Arima Model\n\n\n\n\nCode\n######################## Check for different combinations ########\nd=1\ni=1\ntemp = data.frame()\nls=matrix(rep(NA,6*8),nrow=8) #nrow = 8x1x1\n\n\nfor (p in 1:8)# p=01,2,3,4,5,6,7 :7\n{\n  for(q in 1)# q=0 :1\n  {\n    for(d in 1)# d=1 :1\n    {\n      \n      if(p-1+d+q-1<=8)\n      {\n        \n        model<- Arima(myts,order=c(p-1,d,q-1),include.drift=TRUE) \n        ls[i,]= c(p-1,d,q-1,model$aic,model$bic,model$aicc)\n        i=i+1\n        \n      }\n      \n    }\n  }\n}\n\nmodel = as.data.frame(ls)\nnames(model)= c(\"p\",\"d\",\"q\",\"AIC\",\"BIC\",\"AICc\")\n\n#temp\nknitr::kable(model)\n\n\n\n\n\np\nd\nq\nAIC\nBIC\nAICc\n\n\n\n\n0\n1\n0\n45358.07\n45370.26\n45358.07\n\n\n1\n1\n0\n45316.63\n45334.91\n45316.64\n\n\n2\n1\n0\n45299.67\n45324.05\n45299.68\n\n\n3\n1\n0\n45301.65\n45332.13\n45301.67\n\n\n4\n1\n0\n45293.77\n45330.34\n45293.80\n\n\n5\n1\n0\n45292.51\n45335.17\n45292.54\n\n\n6\n1\n0\n45268.12\n45316.88\n45268.17\n\n\n7\n1\n0\n45227.46\n45282.32\n45227.52\n\n\n\n\n\n\n\n\n\nCode\nauto.arima(myts)\n\n\nSeries: myts \nARIMA(2,1,0) with drift \n\nCoefficients:\n          ar1     ar2   drift\n      -0.1060  0.0759  7.1363\ns.e.   0.0174  0.0174  4.1056\n\nsigma^2 = 58682:  log likelihood = -22645.83\nAIC=45299.67   AICc=45299.68   BIC=45324.05\n\n\n\n\n\nThe given table provides different ARIMA models with their corresponding AIC and BIC values. By sorting the table according to each criterion, we can identify the models with the lowest values. The smallest AIC value corresponds to an ARIMA (2,1,0) model, while the ARIMA (0,1,0) model has the lowest BIC value. Additionally, the auto.arima function in R suggests an ARIMA (0,1,0) model as the best fit for the data. Since there are different models to choose from, it is important to perform model diagnostics to determine the best model for the data. These diagnostics can include checking for residual normality, checking for autocorrelation and partial autocorrelation in the residuals, and comparing the forecasted values to the actual values. By conducting these tests, we can determine which model provides the best fit for the data and is therefore the most appropriate to use for forecasting future values.\n\n\nModel Diagnostic\n\nModel 1 PlotModel 1Model 2 PlotModel 2\n\n\n\n\nCode\nmodel_output <- capture.output(sarima(myts,2,1,0))\n\n\n\n\n\n\n\n\n\nCode\ncat(model_output[12:42], model_output[length(model_output)], sep = \"\\n\") \n\n\nconverged\n$fit\n\nCall:\narima(x = xdata, order = c(p, d, q), seasonal = list(order = c(P, D, Q), period = S), \n    xreg = constant, transform.pars = trans, fixed = fixed, optim.control = list(trace = trc, \n        REPORT = 1, reltol = tol))\n\nCoefficients:\n          ar1     ar2  constant\n      -0.1060  0.0759    7.1363\ns.e.   0.0174  0.0174    4.1056\n\nsigma^2 estimated as 58628:  log likelihood = -22645.83,  aic = 45299.67\n\n$degrees_of_freedom\n[1] 3275\n\n$ttable\n         Estimate     SE t.value p.value\nar1       -0.1060 0.0174 -6.0862  0.0000\nar2        0.0759 0.0174  4.3613  0.0000\nconstant   7.1363 4.1056  1.7382  0.0823\n\n$AIC\n[1] 13.8193\n\n$AICc\n[1] 13.8193\n\n$BIC\n\n\n\n\n\n\nCode\nmodel_output <- capture.output(sarima(myts,0,1,0))\n\n\n\n\n\n\n\n\n\nCode\ncat(model_output[9:38], model_output[length(model_output)], sep = \"\\n\") \n\n\n$fit\n\nCall:\narima(x = xdata, order = c(p, d, q), seasonal = list(order = c(P, D, Q), period = S), \n    xreg = constant, transform.pars = trans, fixed = fixed, optim.control = list(trace = trc, \n        REPORT = 1, reltol = tol))\n\nCoefficients:\n      constant\n        7.1352\ns.e.    4.2696\n\nsigma^2 estimated as 59756:  log likelihood = -22677.04,  aic = 45358.07\n\n$degrees_of_freedom\n[1] 3277\n\n$ttable\n         Estimate     SE t.value p.value\nconstant   7.1352 4.2696  1.6712  0.0948\n\n$AIC\n[1] 13.83712\n\n$AICc\n[1] 13.83712\n\n$BIC\n[1] 13.84084\n\n\n\n\n\nTo determine which model diagnosis is the best, we need to consider some key factors such as the coefficient estimates and standard errors, AIC, BIC, p-values, and the log-likelihood. Comparing the two models, model 2 is the best because it has the lowest BIC value when compared and the AIC value are almost the same, which indicate better goodness of fit. Also, all the coefficient estimates in the model are significant, as evidenced by the p-values are better than model 1, which implies that there is a strong correlation between the variables. The t-values are high, indicating that the estimates are highly reliable.\nTherefore, based on the AIC and BIC values, the significance of the coefficient estimates, and the reliability of the estimates, we can conclude that model 2 is the best model diagnosis. Therefore, (0,1,0) is the best model for this time series.\nThe best model identified is ARIMA(0,1,0). By analyzing the standardized residuals plot, it can be observed that the mean is close to 0 and the variance is slightly higher than 1. Deviations from these values could indicate poor model fit. However, in this case, the model seems to be well-fitted. The ACF plot of residuals shows very few significant lags, which is a positive sign for the model. The qq-plot also suggests normality in the residuals. Additionally, the p-values for the Ljung-Box test are greater than 0.05, indicating that the residuals are independent, which is favorable for the model’s accuracy.\nThe equation for this model: \\[(1-\\phi_1B-\\phi_2B^2)(1-B)(Y_t-\\mu) = (1+\\theta_1B+\\theta_2B^2)\\epsilon_t\\]\n\n\nForcast\n\nShort Term ForcastLong term Forcast\n\n\n\n\nCode\nmyts %>%\n  Arima(order=c(0,1,0),include.drift = TRUE) %>%\n  forecast %>%\n  autoplot(main = \"Dow Jones Index Stock Prices Prediction\") +\n  ylab(\"stock prices\") + xlab(\"Year\")\n\n\n\n\n\n\n\n\n\nCode\nsarima.for(myts,182, 0,1,0, main='Dow Jones Index Stock Prices Prediction')\n\n\n\n\n\n$pred\nTime Series:\nStart = c(2023, 4) \nEnd = c(2023, 185) \nFrequency = 252 \n  [1] 33980.15 33987.28 33994.42 34001.55 34008.69 34015.82 34022.96 34030.09\n  [9] 34037.23 34044.36 34051.50 34058.63 34065.77 34072.90 34080.04 34087.17\n [17] 34094.31 34101.44 34108.58 34115.71 34122.85 34129.99 34137.12 34144.26\n [25] 34151.39 34158.53 34165.66 34172.80 34179.93 34187.07 34194.20 34201.34\n [33] 34208.47 34215.61 34222.74 34229.88 34237.01 34244.15 34251.28 34258.42\n [41] 34265.55 34272.69 34279.82 34286.96 34294.09 34301.23 34308.36 34315.50\n [49] 34322.63 34329.77 34336.90 34344.04 34351.18 34358.31 34365.45 34372.58\n [57] 34379.72 34386.85 34393.99 34401.12 34408.26 34415.39 34422.53 34429.66\n [65] 34436.80 34443.93 34451.07 34458.20 34465.34 34472.47 34479.61 34486.74\n [73] 34493.88 34501.01 34508.15 34515.28 34522.42 34529.55 34536.69 34543.82\n [81] 34550.96 34558.09 34565.23 34572.37 34579.50 34586.64 34593.77 34600.91\n [89] 34608.04 34615.18 34622.31 34629.45 34636.58 34643.72 34650.85 34657.99\n [97] 34665.12 34672.26 34679.39 34686.53 34693.66 34700.80 34707.93 34715.07\n[105] 34722.20 34729.34 34736.47 34743.61 34750.74 34757.88 34765.01 34772.15\n[113] 34779.28 34786.42 34793.56 34800.69 34807.83 34814.96 34822.10 34829.23\n[121] 34836.37 34843.50 34850.64 34857.77 34864.91 34872.04 34879.18 34886.31\n[129] 34893.45 34900.58 34907.72 34914.85 34921.99 34929.12 34936.26 34943.39\n[137] 34950.53 34957.66 34964.80 34971.93 34979.07 34986.20 34993.34 35000.47\n[145] 35007.61 35014.74 35021.88 35029.02 35036.15 35043.29 35050.42 35057.56\n[153] 35064.69 35071.83 35078.96 35086.10 35093.23 35100.37 35107.50 35114.64\n[161] 35121.77 35128.91 35136.04 35143.18 35150.31 35157.45 35164.58 35171.72\n[169] 35178.85 35185.99 35193.12 35200.26 35207.39 35214.53 35221.66 35228.80\n[177] 35235.93 35243.07 35250.21 35257.34 35264.48 35271.61\n\n$se\nTime Series:\nStart = c(2023, 4) \nEnd = c(2023, 185) \nFrequency = 252 \n  [1]  244.4498  345.7043  423.3995  488.8996  546.6064  598.7773  646.7534\n  [8]  691.4085  733.3495  773.0182  810.7483  846.7990  881.3764  914.6475\n [15]  946.7501  977.7993 1007.8924 1037.1128 1065.5321 1093.2128 1120.2098\n [22] 1146.5713 1172.3402 1197.5547 1222.2491 1246.4544 1270.1985 1293.5069\n [29] 1316.4026 1338.9068 1361.0390 1382.8170 1404.2573 1425.3752 1446.1847\n [36] 1466.6989 1486.9302 1506.8899 1526.5887 1546.0364 1565.2426 1584.2159\n [43] 1602.9647 1621.4967 1639.8193 1657.9394 1675.8636 1693.5981 1711.1488\n [50] 1728.5213 1745.7209 1762.7527 1779.6216 1796.3320 1812.8884 1829.2950\n [57] 1845.5557 1861.6744 1877.6547 1893.5002 1909.2142 1924.7998 1940.2603\n [64] 1955.5986 1970.8175 1985.9198 2000.9080 2015.7849 2030.5527 2045.2140\n [71] 2059.7708 2074.2255 2088.5802 2102.8369 2116.9976 2131.0642 2145.0385\n [78] 2158.9224 2172.7176 2186.4257 2200.0484 2213.5873 2227.0439 2240.4196\n [85] 2253.7160 2266.9344 2280.0762 2293.1426 2306.1350 2319.0546 2331.9027\n [92] 2344.6803 2357.3887 2370.0290 2382.6022 2395.1093 2407.5515 2419.9298\n [99] 2432.2450 2444.4982 2456.6903 2468.8222 2480.8948 2492.9088 2504.8653\n[106] 2516.7650 2528.6086 2540.3971 2552.1311 2563.8114 2575.4387 2587.0138\n[113] 2598.5373 2610.0099 2621.4323 2632.8052 2644.1291 2655.4048 2666.6327\n[120] 2677.8136 2688.9481 2700.0365 2711.0797 2722.0780 2733.0321 2743.9425\n[127] 2754.8096 2765.6340 2776.4163 2787.1568 2797.8561 2808.5146 2819.1329\n[134] 2829.7113 2840.2503 2850.7503 2861.2118 2871.6352 2882.0209 2892.3693\n[141] 2902.6808 2912.9558 2923.1947 2933.3979 2943.5657 2953.6985 2963.7966\n[148] 2973.8605 2983.8904 2993.8867 3003.8497 3013.7798 3023.6773 3033.5425\n[155] 3043.3758 3053.1773 3062.9475 3072.6866 3082.3950 3092.0729 3101.7205\n[162] 3111.3383 3120.9264 3130.4852 3140.0148 3149.5157 3158.9879 3168.4318\n[169] 3177.8477 3187.2358 3196.5962 3205.9294 3215.2354 3224.5146 3233.7672\n[176] 3242.9934 3252.1934 3261.3674 3270.5157 3279.6385 3288.7360 3297.8084\n\n\n\n\n\nThe two figures above show an idea of the short term and the long-term forecasted stock price. The forecasted values is be based on the estimated parameters of the model, which were obtained by fitting it to the historical data. One way to assess the accuracy of the forecasts is to use cross-validation, which involves splitting the data into training and testing sets and comparing the predicted values to the actual values in the test set. This can help to identify any potential problems with the model and to fine-tune the model parameters.\n\n12 Step ahead Cross Validation1 Step ahead Cross Validation\n\n\n\n\nCode\n#a seasonal cross validation using 12 steps ahead forecasts\nfarima1 <- function(x, h){forecast(Arima(x, order=c(0,1,0)), h=h)}\n\n# Compute cross-validated errors for up to 12 steps ahead\ne <- tsCV(myts, forecastfunction = farima1, h = 12)\n\n# Compute the MSE values and remove missing values\nmse <- colMeans(e^2, na.rm = TRUE)\n\n# Plot the MSE values against the forecast horizon\ndata.frame(h = 1:12, MSE = mse) %>%\n  ggplot(aes(x = h, y = MSE)) + geom_point()+geom_line()+ggtitle(\"12 Step Ahead Cross Validation\")\n\n\n\n\n\n\n\n\n\nCode\n#a seasonal cross validation using 1 steps ahead forecasts\nfarima1 <- function(x, h){forecast(Arima(x, order=c(0,1,0)),h=h)}\n\n# Compute cross-validated errors for up to 1 steps ahead\ne <- tsCV(myts, forecastfunction = farima1, h = 1)\nmse <-abs(mean(e,na.rm=TRUE))\n\n# Plot the MSE values against the forecast horizon\ndata.frame(h = 1:12, MSE = mse) %>%\n  ggplot(aes(x = h, y = MSE)) + geom_point()+geom_line()+ggtitle(\"1 Step Ahead Cross Validation\")\n\n\n\n\n\n\n\n\nThe one-step-ahead forecasting plot shows that the MSE seems to be stable. The MSE is tend to rise by 12 steps at each step of cross-validation. Lastly, the above-mentioned ARIMA model should be compared to benchmark methods before being used for a long time.\n\n\nModel Comparison\n\nPlotModel Error Table\n\n\n\n\nCode\nfit <- Arima(myts, order=c(0,1,0))\nautoplot(myts) +\n  autolayer(meanf(myts, h=182),\n            series=\"Mean\", PI=FALSE) +\n  autolayer(naive(myts, h=182),\n            series=\"Naïve\", PI=FALSE) +\n  autolayer(snaive(myts, h=182),\n            series=\"SNaïve\", PI=FALSE)+\n  autolayer(rwf(myts, h=182, drift=TRUE),\n            series=\"Drift\", PI=FALSE)+\n  autolayer(forecast(fit,182), \n            series=\"fit\",PI=FALSE) +\n  guides(colour=guide_legend(title=\"Forecast\"))\n\n\n\n\n\n\n\n\n\nCode\nsummary <- summary(fit)\nsnaive <- snaive(myts, h=182)\naccuracy(snaive)\n\n\n                   ME     RMSE      MAE      MPE     MAPE MASE      ACF1\nTraining set 1844.923 3197.208 2427.248 8.453577 10.71957    1 0.9912204\n\n\nCode\nsummary\n\n\nSeries: myts \nARIMA(0,1,0) \n\nsigma^2 = 59807:  log likelihood = -22678.43\nAIC=45358.86   AICc=45358.86   BIC=45364.96\n\nTraining set error measures:\n                   ME     RMSE      MAE        MPE      MAPE       MASE\nTraining set 7.136211 244.5167 148.0226 0.02962254 0.7027494 0.06098373\n                   ACF1\nTraining set -0.1147373\n\n\n\n\n\nError\nModel\nSnavie\n\n\n\n\nME\n1.285039\n515.1907\n\n\nRMSE\n385.9824\n7308.222\n\n\nMAE\n134.981\n4427.736\n\n\nMPE\n-0.01604944\n-5.254495\n\n\nMAPE\n0.7299033\n29.22506\n\n\nMASE\n0.03048532\n1.0000000\n\n\nACF1\n-0.02473038\n0.9969796\n\n\n\n\n\n\nThe ARIMA fitted forecast and snaive tracks the actual data points very closely when compared to other model. The other forecast methods are less accurate when compared. From the table, Model error measurements of fit are much lower than snaive method.We can conclude that the fitted model is good."
  },
  {
    "objectID": "arima_index_nasdaq.html",
    "href": "arima_index_nasdaq.html",
    "title": "ARIMA Model for NASDAQ Composite index",
    "section": "",
    "text": "During exploratory data analysis (EDA) of NADSAQ Composite Index, it was observed that the raw data is often non-stationary, exhibiting trends, seasonality, and other complex patterns that can make modeling and forecasting difficult. To address this issue, the data is often differentiated to achieve stationarity, which is a key assumption of the ARIMA model.\nDifferencing involves calculating the difference between each data point and the previous data point, which can help to remove trends and seasonality from the data. The goal is to create a new series of data that is stationary, meaning that the statistical properties of the data do not change over time.\n\nDifferentiated Time Series Plot\n\nPlotACFPACFADF Test\n\n\n\n\nCode\n#import the data\ndf <- read.csv(\"DATA/CLEANED DATA/nasdaq_raw_data.csv\")\n#convert to time series data\nmyts<-ts(df$IXIC.Adjusted,frequency=252,start=c(2010,1,4), end = c(2023,3,1)) \n#First order differentiation\ndf1 <- diff(myts)\n# Plot \nmyts  %>% diff() %>% ggtsdisplay(main = \"First order differentiation\") \n\n\n\n\n\n\n\n\n\nCode\n#ACF plot \nggAcf(df1,60,main=\"ACF Plot: First order differentiation\") \n\n\n\n\n\n\n\n\n\nCode\nggPacf(df1,60,main=\"PACF Plot: First order differentiation\") \n\n\n\n\n\n\n\n\n\nCode\ntseries::adf.test(df1)\n\n\n\n    Augmented Dickey-Fuller Test\n\ndata:  df1\nDickey-Fuller = -15.135, Lag order = 14, p-value = 0.01\nalternative hypothesis: stationary\n\n\n\n\n\nAfter analyzing the ACF and PACF plots, it can be observed that most of the bar lines lie between the blue lines, indicating that the time series is stationary. This has been further confirmed through an Augmented Dickey-Fuller test as the p-value is less than 0.05. Once the data is stationary, the next step is to model it using ARIMA. The combination of ACF and PACF plots can help determine the appropriate values for p, d, and q in the ARIMA model. The order of differencing required to achieve stationarity gives the d value. The p value is determined by examining significant spikes at various lags in the PACF plot, while the q value is determined through significant spikes in the ACF plot. It is important to consider other methods such as grid search and information criteria to ensure the most appropriate model is selected.\nHere the parameters are d = 1 p = 0,7 (PACF Plot) q = 0,7 (ACF Plot)\n\n\nModel Selection\n\nARIMA ResultAuto Arima Model\n\n\n\n\nCode\n######################## Check for different combinations ########\nd=1\ni=1\ntemp = data.frame()\nls=matrix(rep(NA,6*8),nrow=8) #nrow = 7x1x1\n\n\nfor (p in 1:8)# p=0,1,2,3,4,5,6,7 :8\n{\n  for(q in 1)# q=0 :1\n  {\n    for(d in 1)# d=1 :1\n    {\n      \n      if(p-1+d+q-1<=8)\n      {\n        \n        model<- Arima(myts,order=c(p-1,d,q-1),include.drift=TRUE) \n        ls[i,]= c(p-1,d,q-1,model$aic,model$bic,model$aicc)\n        i=i+1\n        \n      }\n      \n    }\n  }\n}\n\nmodel = as.data.frame(ls)\nnames(model)= c(\"p\",\"d\",\"q\",\"AIC\",\"BIC\",\"AICc\")\n\n#temp\nknitr::kable(model)\n\n\n\n\n\np\nd\nq\nAIC\nBIC\nAICc\n\n\n\n\n0\n1\n0\n40013.93\n40026.12\n40013.93\n\n\n1\n1\n0\n39988.60\n40006.89\n39988.61\n\n\n2\n1\n0\n39986.51\n40010.89\n39986.52\n\n\n3\n1\n0\n39986.37\n40016.85\n39986.39\n\n\n4\n1\n0\n39984.84\n40021.41\n39984.86\n\n\n5\n1\n0\n39985.77\n40028.44\n39985.81\n\n\n6\n1\n0\n39977.33\n40026.08\n39977.37\n\n\n7\n1\n0\n39963.60\n40018.45\n39963.65\n\n\n\n\n\n\n\n\n\nCode\nauto.arima(myts)\n\n\nSeries: myts \nARIMA(2,1,0) with drift \n\nCoefficients:\n          ar1     ar2   drift\n      -0.0879  0.0354  2.6309\ns.e.   0.0175  0.0175  1.7867\n\nsigma^2 = 11603:  log likelihood = -19989.25\nAIC=39986.51   AICc=39986.52   BIC=40010.89\n\n\n\n\n\nThe given table provides different ARIMA models with their corresponding AIC and BIC values. By sorting the table according to each criterion, we can identify the models with the lowest values. The smallest AIC value corresponds to an ARIMA (1,1,0) model, while the ARIMA (0,1,0) model has the lowest BIC value. Additionally, the auto.arima function in R suggests an ARIMA (0,1,0) model as the best fit for the data. Since there are different models to choose from, it is important to perform model diagnostics to determine the best model for the data. These diagnostics can include checking for residual normality, checking for autocorrelation and partial autocorrelation in the residuals, and comparing the forecasted values to the actual values. By conducting these tests, we can determine which model provides the best fit for the data and is therefore the most appropriate to use for forecasting future values.\n\n\nModel Diagnostic\n\nModel 1 PlotModel 1Model 2 PlotModel 2\n\n\n\n\nCode\nmodel_output <- capture.output(sarima(myts,1,1,0))\n\n\n\n\n\n\n\n\n\nCode\ncat(model_output[11:41], model_output[length(model_output)], sep = \"\\n\") \n\n\n$fit\n\nCall:\narima(x = xdata, order = c(p, d, q), seasonal = list(order = c(P, D, Q), period = S), \n    xreg = constant, transform.pars = trans, fixed = fixed, optim.control = list(trace = trc, \n        REPORT = 1, reltol = tol))\n\nCoefficients:\n          ar1  constant\n      -0.0911    2.6303\ns.e.   0.0174    1.7246\n\nsigma^2 estimated as 11607:  log likelihood = -19991.3,  aic = 39988.6\n\n$degrees_of_freedom\n[1] 3276\n\n$ttable\n         Estimate     SE t.value p.value\nar1       -0.0911 0.0174 -5.2380  0.0000\nconstant   2.6303 1.7246  1.5252  0.1273\n\n$AIC\n[1] 12.19909\n\n$AICc\n[1] 12.19909\n\n$BIC\n[1] 12.20466\n\n\n\n\n\n\nCode\nmodel_output <- capture.output(sarima(myts,0,1,0))\n\n\n\n\n\n\n\n\n\nCode\ncat(model_output[9:38], model_output[length(model_output)], sep = \"\\n\") \n\n\n$fit\n\nCall:\narima(x = xdata, order = c(p, d, q), seasonal = list(order = c(P, D, Q), period = S), \n    xreg = constant, transform.pars = trans, fixed = fixed, optim.control = list(trace = trc, \n        REPORT = 1, reltol = tol))\n\nCoefficients:\n      constant\n        2.6306\ns.e.    1.8896\n\nsigma^2 estimated as 11704:  log likelihood = -20004.97,  aic = 40013.93\n\n$degrees_of_freedom\n[1] 3277\n\n$ttable\n         Estimate     SE t.value p.value\nconstant   2.6306 1.8896  1.3922   0.164\n\n$AIC\n[1] 12.20681\n\n$AICc\n[1] 12.20681\n\n$BIC\n[1] 12.21053\n\n\n\n\n\nTo determine which model diagnosis is the best, we need to consider some key factors such as the coefficient estimates and standard errors, AIC, BIC, p-values, and the log-likelihood. Comparing the two models, model 2 is the best because it has the lowest BIC value when compared and the AIC value are almost the same, which indicate better goodness of fit. Also, all the coefficient estimates in the model are significant, as evidenced by the p-values are better than model 1, which implies that there is a strong correlation between the variables. The t-values are high, indicating that the estimates are highly reliable.\nTherefore, based on the AIC and BIC values, the significance of the coefficient estimates, and the reliability of the estimates, we can conclude that model 2 is the best model diagnosis. Therefore, (0,1,0) is the best model for this time series.\nThe best model identified is ARIMA(0,1,0). By analyzing the standardized residuals plot, it can be observed that the mean is close to 0 and the variance is slightly higher than 1. Deviations from these values could indicate poor model fit. However, in this case, the model seems to be well-fitted. The ACF plot of residuals shows very few significant lags, which is a positive sign for the model. The qq-plot also suggests normality in the residuals. Additionally, the p-values for the Ljung-Box test are greater than 0.05, indicating that the residuals are independent, which is favorable for the model’s accuracy.\nThe equation for this model: \\[(1-\\phi_1B-\\phi_2B^2)(1-B)(Y_t-\\mu) = (1+\\theta_1B+\\theta_2B^2)\\epsilon_t\\]\n\n\nForcast\n\nShort Term ForcastLong term Forcast\n\n\n\n\nCode\nmyts %>%\n  Arima(order=c(0,1,0),include.drift = TRUE) %>%\n  forecast %>%\n  autoplot(main = \"NASDAQ Composite Index Stock Prices Prediction\") +\n  ylab(\"stock prices\") + xlab(\"Year\")\n\n\n\n\n\n\n\n\n\nCode\nsarima.for(myts,182, 0,1,0, main='NASDAQ Composite Index Stock Prices Prediction')\n\n\n\n\n\n$pred\nTime Series:\nStart = c(2023, 4) \nEnd = c(2023, 185) \nFrequency = 252 \n  [1] 10934.30 10936.93 10939.56 10942.19 10944.82 10947.45 10950.08 10952.72\n  [9] 10955.35 10957.98 10960.61 10963.24 10965.87 10968.50 10971.13 10973.76\n [17] 10976.39 10979.02 10981.65 10984.28 10986.91 10989.54 10992.17 10994.81\n [25] 10997.44 11000.07 11002.70 11005.33 11007.96 11010.59 11013.22 11015.85\n [33] 11018.48 11021.11 11023.74 11026.37 11029.00 11031.63 11034.27 11036.90\n [41] 11039.53 11042.16 11044.79 11047.42 11050.05 11052.68 11055.31 11057.94\n [49] 11060.57 11063.20 11065.83 11068.46 11071.09 11073.72 11076.36 11078.99\n [57] 11081.62 11084.25 11086.88 11089.51 11092.14 11094.77 11097.40 11100.03\n [65] 11102.66 11105.29 11107.92 11110.55 11113.18 11115.81 11118.45 11121.08\n [73] 11123.71 11126.34 11128.97 11131.60 11134.23 11136.86 11139.49 11142.12\n [81] 11144.75 11147.38 11150.01 11152.64 11155.27 11157.91 11160.54 11163.17\n [89] 11165.80 11168.43 11171.06 11173.69 11176.32 11178.95 11181.58 11184.21\n [97] 11186.84 11189.47 11192.10 11194.73 11197.36 11200.00 11202.63 11205.26\n[105] 11207.89 11210.52 11213.15 11215.78 11218.41 11221.04 11223.67 11226.30\n[113] 11228.93 11231.56 11234.19 11236.82 11239.46 11242.09 11244.72 11247.35\n[121] 11249.98 11252.61 11255.24 11257.87 11260.50 11263.13 11265.76 11268.39\n[129] 11271.02 11273.65 11276.28 11278.91 11281.55 11284.18 11286.81 11289.44\n[137] 11292.07 11294.70 11297.33 11299.96 11302.59 11305.22 11307.85 11310.48\n[145] 11313.11 11315.74 11318.37 11321.01 11323.64 11326.27 11328.90 11331.53\n[153] 11334.16 11336.79 11339.42 11342.05 11344.68 11347.31 11349.94 11352.57\n[161] 11355.20 11357.83 11360.46 11363.10 11365.73 11368.36 11370.99 11373.62\n[169] 11376.25 11378.88 11381.51 11384.14 11386.77 11389.40 11392.03 11394.66\n[177] 11397.29 11399.92 11402.56 11405.19 11407.82 11410.45\n\n$se\nTime Series:\nStart = c(2023, 4) \nEnd = c(2023, 185) \nFrequency = 252 \n  [1]  108.1866  152.9989  187.3847  216.3732  241.9126  265.0019  286.2348\n  [8]  305.9979  324.5598  342.1160  358.8143  374.7693  390.0723  404.7972\n [15]  419.0049  432.7464  446.0647  458.9968  471.5744  483.8251  495.7732\n [22]  507.4401  518.8447  530.0039  540.9330  551.6455  562.1540  572.4696\n [29]  582.6026  592.5624  602.3574  611.9958  621.4847  630.8308  640.0405\n [36]  649.1195  658.0733  666.9069  675.6250  684.2321  692.7322  701.1292\n [43]  709.4269  717.6287  725.7377  733.7572  741.6899  749.5387  757.3061\n [50]  764.9947  772.6068  780.1446  787.6103  795.0058  802.3332  809.5943\n [57]  816.7909  823.9245  830.9970  838.0097  844.9643  851.8621  858.7044\n [64]  865.4927  872.2282  878.9120  885.5454  892.1295  898.6653  905.1540\n [71]  911.5964  917.9937  924.3466  930.6562  936.9234  943.1488  949.3335\n [78]  955.4781  961.5835  967.6503  973.6793  979.6713  985.6268  991.5465\n [85]  997.4311 1003.2812 1009.0973 1014.8802 1020.6303 1026.3481 1032.0343\n [92] 1037.6893 1043.3137 1048.9079 1054.4725 1060.0078 1065.5144 1070.9926\n [99] 1076.4430 1081.8659 1087.2618 1092.6310 1097.9740 1103.2911 1108.5827\n[106] 1113.8491 1119.0908 1124.3080 1129.5012 1134.6705 1139.8165 1144.9393\n[113] 1150.0392 1155.1167 1160.1719 1165.2053 1170.2169 1175.2072 1180.1764\n[120] 1185.1247 1190.0525 1194.9600 1199.8473 1204.7149 1209.5629 1214.3915\n[127] 1219.2010 1223.9916 1228.7635 1233.5169 1238.2521 1242.9693 1247.6686\n[134] 1252.3503 1257.0146 1261.6616 1266.2916 1270.9047 1275.5011 1280.0810\n[141] 1284.6446 1289.1920 1293.7235 1298.2391 1302.7391 1307.2236 1311.6927\n[148] 1316.1467 1320.5856 1325.0097 1329.4191 1333.8139 1338.1942 1342.5603\n[155] 1346.9122 1351.2501 1355.5741 1359.8844 1364.1810 1368.4642 1372.7340\n[162] 1376.9905 1381.2339 1385.4644 1389.6819 1393.8867 1398.0789 1402.2585\n[169] 1406.4257 1410.5806 1414.7233 1418.8538 1422.9724 1427.0792 1431.1741\n[176] 1435.2573 1439.3290 1443.3892 1447.4379 1451.4754 1455.5017 1459.5169\n\n\n\n\n\nThe two figures above show an idea of the short term and the long-term forecasted stock price. The forecasted values is be based on the estimated parameters of the model, which were obtained by fitting it to the historical data. One way to assess the accuracy of the forecasts is to use cross-validation, which involves splitting the data into training and testing sets and comparing the predicted values to the actual values in the test set. This can help to identify any potential problems with the model and to fine-tune the model parameters.\n\n12 Step ahead Cross Validation1 Step ahead Cross Validation\n\n\n\n\nCode\n#a seasonal cross validation using 12 steps ahead forecasts\nfarima1 <- function(x, h){forecast(Arima(x, order=c(0,1,0)), h=h)}\n\n# Compute cross-validated errors for up to 12 steps ahead\ne <- tsCV(myts, forecastfunction = farima1, h = 12)\n\n# Compute the MSE values and remove missing values\nmse <- colMeans(e^2, na.rm = TRUE)\n\n# Plot the MSE values against the forecast horizon\ndata.frame(h = 1:12, MSE = mse) %>%\n  ggplot(aes(x = h, y = MSE)) + geom_point()+geom_line()+ggtitle(\"12 Step Ahead Cross Validation\")\n\n\n\n\n\n\n\n\n\nCode\n#a seasonal cross validation using 1 steps ahead forecasts\nfarima1 <- function(x, h){forecast(Arima(x, order=c(0,1,0)),h=h)}\n\n# Compute cross-validated errors for up to 1 steps ahead\ne <- tsCV(myts, forecastfunction = farima1, h = 1)\nmse <-abs(mean(e,na.rm=TRUE))\n\n# Plot the MSE values against the forecast horizon\ndata.frame(h = 1:12, MSE = mse) %>%\n  ggplot(aes(x = h, y = MSE)) + geom_point()+geom_line()+ggtitle(\"1 Step Ahead Cross Validation\")\n\n\n\n\n\n\n\n\nThe one-step-ahead forecasting plot shows that the MSE seems to be stable. The MSE is tend to rise by 12 steps at each step of cross-validation. Lastly, the above-mentioned ARIMA model should be compared to benchmark methods before being used for a long time.\n\n\nModel Comparison\n\nPlotModel Error Table\n\n\n\n\nCode\nfit <- Arima(myts, order=c(0,1,0))\nautoplot(myts) +\n  autolayer(meanf(myts, h=182),\n            series=\"Mean\", PI=FALSE) +\n  autolayer(naive(myts, h=182),\n            series=\"Naïve\", PI=FALSE) +\n  autolayer(snaive(myts, h=182),\n            series=\"SNaïve\", PI=FALSE)+\n  autolayer(rwf(myts, h=182, drift=TRUE),\n            series=\"Drift\", PI=FALSE)+\n  autolayer(forecast(fit,182), \n            series=\"fit\",PI=FALSE) +\n  guides(colour=guide_legend(title=\"Forecast\"))\n\n\n\n\n\n\n\n\n\nCode\nsummary <- summary(fit)\nsnaive <- snaive(myts, h=182)\naccuracy(snaive)\n\n\n                   ME     RMSE      MAE      MPE     MAPE MASE      ACF1\nTraining set 812.8214 1820.728 1250.681 11.65299 15.87608    1 0.9945471\n\n\nCode\nsummary\n\n\nSeries: myts \nARIMA(0,1,0) \n\nsigma^2 = 11711:  log likelihood = -20005.93\nAIC=40013.87   AICc=40013.87   BIC=40019.96\n\nTraining set error measures:\n                   ME     RMSE    MAE        MPE      MAPE       MASE\nTraining set 2.630545 108.2021 62.269 0.03895396 0.8925344 0.04978808\n                    ACF1\nTraining set -0.09108687\n\n\n\n\n\nError\nModel\nSnavie\n\n\n\n\nME\n0.5450376\n183.6083\n\n\nRMSE\n162.141\n3564.003\n\n\nMAE\n53.77502\n2124.839\n\n\nMPE\n-0.04171079\n-19.15607\n\n\nMAPE\n0.9503091\n53.5699\n\n\nMASE\n0.02530781\n1.0000000\n\n\nACF1\n-0.02640682\n0.9977458\n\n\n\n\n\n\nThe ARIMA forecast tracks the actual data points very closely when compared to other model. The other forecast methods are less accurate when compared. From the table, Model error measurements of fit are much lower than snaive method.We can conclude that the fitted model is good."
  },
  {
    "objectID": "arima_index_sp500.html",
    "href": "arima_index_sp500.html",
    "title": "ARIMA Model for S&P 500 Index",
    "section": "",
    "text": "During exploratory data analysis (EDA) of S&P 500 Index, it was observed that the raw data is often non-stationary, exhibiting trends, seasonality, and other complex patterns that can make modeling and forecasting difficult. To address this issue, the data is often differentiated to achieve stationarity, which is a key assumption of the ARIMA model.\nDifferencing involves calculating the difference between each data point and the previous data point, which can help to remove trends and seasonality from the data. The goal is to create a new series of data that is stationary, meaning that the statistical properties of the data do not change over time.\n\nDifferentiated Time Series Plot\n\nPlotACFPACFADF Test\n\n\n\n\nCode\n#import the data\ndf <- read.csv(\"DATA/CLEANED DATA/sp500_raw_data.csv\")\n#convert to time series data\nmyts<-ts(df$GSPC.Adjusted,frequency=252,start=c(2010,1,4), end = c(2023,3,1)) \n#First order differentiation\ndf1 <- diff(myts)\n# Plot \nmyts  %>% diff() %>% ggtsdisplay(main = \"First order differentiation\") \n\n\n\n\n\n\n\n\n\nCode\n#ACF plot \nggAcf(df1,60,main=\"ACF Plot: First order differentiation\") \n\n\nWarning in ggplot2::geom_segment(lineend = \"butt\", ...): Ignoring unknown\nparameters: `main`\n\n\n\n\n\n\n\n\n\nCode\nggPacf(df1,60,main=\"PACF Plot: First order differentiation\") \n\n\n\n\n\n\n\n\n\nCode\ntseries::adf.test(df1)\n\n\n\n    Augmented Dickey-Fuller Test\n\ndata:  df1\nDickey-Fuller = -15.353, Lag order = 14, p-value = 0.01\nalternative hypothesis: stationary\n\n\n\n\n\nAfter analyzing the ACF and PACF plots, it can be observed that most of the bar lines lie between the blue lines, indicating that the time series is stationary. This has been further confirmed through an Augmented Dickey-Fuller test as the p-value is less than 0.05. Once the data is stationary, the next step is to model it using ARIMA. The combination of ACF and PACF plots can help determine the appropriate values for p, d, and q in the ARIMA model. The order of differencing required to achieve stationarity gives the d value. The p value is determined by examining significant spikes at various lags in the PACF plot, while the q value is determined through significant spikes in the ACF plot. It is important to consider other methods such as grid search and information criteria to ensure the most appropriate model is selected.\nHere the parameters are d = 1 p = 0,7 (PACF Plot) q = 0,7 (ACF Plot)\n\n\nModel Selection\n\nARIMA ResultAuto Arima Model\n\n\n\n\nCode\n######################## Check for different combinations ########\nd=1\ni=1\ntemp = data.frame()\nls=matrix(rep(NA,6*8),nrow=8) #nrow = 8x1x1\n\n\nfor (p in 1:8)# p=01,2,3,4,5,6,7 :7\n{\n  for(q in 1)# q=0 :1\n  {\n    for(d in 1)# d=1 :1\n    {\n      \n      if(p-1+d+q-1<=8)\n      {\n        \n        model<- Arima(myts,order=c(p-1,d,q-1),include.drift=TRUE) \n        ls[i,]= c(p-1,d,q-1,model$aic,model$bic,model$aicc)\n        i=i+1\n        \n      }\n      \n    }\n  }\n}\n\nmodel = as.data.frame(ls)\nnames(model)= c(\"p\",\"d\",\"q\",\"AIC\",\"BIC\",\"AICc\")\n\n#temp\nknitr::kable(model)\n\n\n\n\n\np\nd\nq\nAIC\nBIC\nAICc\n\n\n\n\n0\n1\n0\n31635.53\n31647.72\n31635.53\n\n\n1\n1\n0\n31599.12\n31617.40\n31599.13\n\n\n2\n1\n0\n31592.41\n31616.79\n31592.42\n\n\n3\n1\n0\n31594.22\n31624.70\n31594.24\n\n\n4\n1\n0\n31587.46\n31624.03\n31587.48\n\n\n5\n1\n0\n31587.80\n31630.47\n31587.83\n\n\n6\n1\n0\n31570.03\n31618.79\n31570.07\n\n\n7\n1\n0\n31543.39\n31598.25\n31543.45\n\n\n\n\n\n\n\n\n\nCode\nauto.arima(myts)\n\n\nSeries: myts \nARIMA(0,1,0) with drift \n\nCoefficients:\n       drift\n      0.8654\ns.e.  0.5264\n\nsigma^2 = 908.8:  log likelihood = -15815.76\nAIC=31635.53   AICc=31635.53   BIC=31647.72\n\n\n\n\n\nThe given table provides different ARIMA models with their corresponding AIC and BIC values. By sorting the table according to each criterion, we can identify the models with the lowest values. The smallest AIC value corresponds to an ARIMA (1,1,0) model, while the ARIMA (0,1,0) model has the lowest BIC value. Additionally, the auto.arima function in R suggests an ARIMA (0,1,0) model as the best fit for the data. Since there are different models to choose from, it is important to perform model diagnostics to determine the best model for the data. These diagnostics can include checking for residual normality, checking for autocorrelation and partial autocorrelation in the residuals, and comparing the forecasted values to the actual values. By conducting these tests, we can determine which model provides the best fit for the data and is therefore the most appropriate to use for forecasting future values.\n\n\nModel Diagnostic\n\nModel 1 PlotModel 1Model 2 PlotModel 2\n\n\n\n\nCode\nmodel_output <- capture.output(sarima(myts,1,1,0))\n\n\n\n\n\n\n\n\n\nCode\ncat(model_output[11:41], model_output[length(model_output)], sep = \"\\n\") \n\n\n$fit\n\nCall:\narima(x = xdata, order = c(p, d, q), seasonal = list(order = c(P, D, Q), period = S), \n    xreg = constant, transform.pars = trans, fixed = fixed, optim.control = list(trace = trc, \n        REPORT = 1, reltol = tol))\n\nCoefficients:\n          ar1  constant\n      -0.1080    0.8652\ns.e.   0.0174    0.4724\n\nsigma^2 estimated as 897.9:  log likelihood = -15796.56,  aic = 31599.12\n\n$degrees_of_freedom\n[1] 3276\n\n$ttable\n         Estimate     SE t.value p.value\nar1       -0.1080 0.0174 -6.2151  0.0000\nconstant   0.8652 0.4724  1.8315  0.0671\n\n$AIC\n[1] 9.639756\n\n$AICc\n[1] 9.639757\n\n$BIC\n[1] 9.645334\n\n\n\n\n\n\nCode\nmodel_output <- capture.output(sarima(myts,0,1,0))\n\n\n\n\n\n\n\n\n\nCode\ncat(model_output[9:38], model_output[length(model_output)], sep = \"\\n\") \n\n\n$fit\n\nCall:\narima(x = xdata, order = c(p, d, q), seasonal = list(order = c(P, D, Q), period = S), \n    xreg = constant, transform.pars = trans, fixed = fixed, optim.control = list(trace = trc, \n        REPORT = 1, reltol = tol))\n\nCoefficients:\n      constant\n        0.8654\ns.e.    0.5264\n\nsigma^2 estimated as 908.5:  log likelihood = -15815.76,  aic = 31635.53\n\n$degrees_of_freedom\n[1] 3277\n\n$ttable\n         Estimate     SE t.value p.value\nconstant   0.8654 0.5264  1.6438  0.1003\n\n$AIC\n[1] 9.650863\n\n$AICc\n[1] 9.650863\n\n$BIC\n[1] 9.654582\n\n\n\n\n\nTo determine which model diagnosis is the best, we need to consider some key factors such as the coefficient estimates and standard errors, AIC, BIC, p-values, and the log-likelihood. Comparing the two models, model 2 is the best because it has the lowest BIC value when compared and the AIC value are almost the same, which indicate better goodness of fit. Also, all the coefficient estimates in the model are significant, as evidenced by the p-values are better than model 1, which implies that there is a strong correlation between the variables. The t-values are high, indicating that the estimates are highly reliable.\nTherefore, based on the AIC and BIC values, the significance of the coefficient estimates, and the reliability of the estimates, we can conclude that model 2 is the best model diagnosis. Therefore, (0,1,0) is the best model for this time series.\nThe best model identified is ARIMA(0,1,0). By analyzing the standardized residuals plot, it can be observed that the mean is close to 0 and the variance is slightly higher than 1. Deviations from these values could indicate poor model fit. However, in this case, the model seems to be well-fitted. The ACF plot of residuals shows very few significant lags, which is a positive sign for the model. The qq-plot also suggests normality in the residuals. Additionally, the p-values for the Ljung-Box test are greater than 0.05, indicating that the residuals are independent, which is favorable for the model’s accuracy.\nThe equation for this model: \\[(1-\\phi_1B-\\phi_2B^2)(1-B)(Y_t-\\mu) = (1+\\theta_1B+\\theta_2B^2)\\epsilon_t\\]\n\n\nForcast\n\nShort Term ForcastLong term Forcast\n\n\n\n\nCode\nmyts %>%\n  Arima(order=c(0,1,0),include.drift = TRUE) %>%\n  forecast %>%\n  autoplot(main = \"S&P 500 Index Stock Prices Prediction\") +\n  ylab(\"stock prices\") + xlab(\"Year\")\n\n\n\n\n\n\n\n\n\nCode\nsarima.for(myts,182, 0,1,0, main='S&P 500 Index Stock Prices Prediction')\n\n\n\n\n\n$pred\nTime Series:\nStart = c(2023, 4) \nEnd = c(2023, 185) \nFrequency = 252 \n  [1] 3970.475 3971.341 3972.206 3973.072 3973.937 3974.802 3975.668 3976.533\n  [9] 3977.398 3978.264 3979.129 3979.994 3980.860 3981.725 3982.590 3983.456\n [17] 3984.321 3985.186 3986.052 3986.917 3987.782 3988.648 3989.513 3990.379\n [25] 3991.244 3992.109 3992.975 3993.840 3994.705 3995.571 3996.436 3997.301\n [33] 3998.167 3999.032 3999.897 4000.763 4001.628 4002.493 4003.359 4004.224\n [41] 4005.089 4005.955 4006.820 4007.686 4008.551 4009.416 4010.282 4011.147\n [49] 4012.012 4012.878 4013.743 4014.608 4015.474 4016.339 4017.204 4018.070\n [57] 4018.935 4019.800 4020.666 4021.531 4022.397 4023.262 4024.127 4024.993\n [65] 4025.858 4026.723 4027.589 4028.454 4029.319 4030.185 4031.050 4031.915\n [73] 4032.781 4033.646 4034.511 4035.377 4036.242 4037.107 4037.973 4038.838\n [81] 4039.704 4040.569 4041.434 4042.300 4043.165 4044.030 4044.896 4045.761\n [89] 4046.626 4047.492 4048.357 4049.222 4050.088 4050.953 4051.818 4052.684\n [97] 4053.549 4054.414 4055.280 4056.145 4057.011 4057.876 4058.741 4059.607\n[105] 4060.472 4061.337 4062.203 4063.068 4063.933 4064.799 4065.664 4066.529\n[113] 4067.395 4068.260 4069.125 4069.991 4070.856 4071.722 4072.587 4073.452\n[121] 4074.318 4075.183 4076.048 4076.914 4077.779 4078.644 4079.510 4080.375\n[129] 4081.240 4082.106 4082.971 4083.836 4084.702 4085.567 4086.432 4087.298\n[137] 4088.163 4089.029 4089.894 4090.759 4091.625 4092.490 4093.355 4094.221\n[145] 4095.086 4095.951 4096.817 4097.682 4098.547 4099.413 4100.278 4101.143\n[153] 4102.009 4102.874 4103.739 4104.605 4105.470 4106.336 4107.201 4108.066\n[161] 4108.932 4109.797 4110.662 4111.528 4112.393 4113.258 4114.124 4114.989\n[169] 4115.854 4116.720 4117.585 4118.450 4119.316 4120.181 4121.047 4121.912\n[177] 4122.777 4123.643 4124.508 4125.373 4126.239 4127.104\n\n$se\nTime Series:\nStart = c(2023, 4) \nEnd = c(2023, 185) \nFrequency = 252 \n  [1]  30.14089  42.62566  52.20555  60.28178  67.39708  73.82980  79.74530\n  [8]  85.25131  90.42267  95.31387  99.96603 104.41111 108.67453 112.77689\n [15] 116.73517 120.56356 124.27408 127.87697 131.38110 134.79416 138.12291\n [22] 141.37331 144.55063 147.65961 150.70445 153.68899 156.61666 159.49060\n [29] 162.31366 165.08846 167.81738 170.50263 173.14624 175.75008 178.31591\n [36] 180.84534 183.33988 185.80093 188.22980 190.62773 192.99587 195.33530\n [43] 197.64704 199.93205 202.19124 204.42547 206.63554 208.82222 210.98624\n [50] 213.12828 215.24901 217.34905 219.42900 221.48941 223.53083 225.55377\n [57] 227.55874 229.54619 231.51658 233.47034 235.40788 237.32961 239.23590\n [64] 241.12713 243.00363 244.86575 246.71382 248.54815 250.36904 252.17679\n [71] 253.97166 255.75394 257.52388 259.28175 261.02777 262.76219 264.48524\n [78] 266.19714 267.89810 269.58832 271.26802 272.93737 274.59658 276.24583\n [85] 277.88528 279.51512 281.13551 282.74662 284.34860 285.94160 287.52577\n [92] 289.10127 290.66822 292.22678 293.77706 295.31921 296.85335 298.37960\n [99] 299.89808 301.40891 302.91220 304.40808 305.89663 307.37798 308.85222\n[106] 310.31946 311.77980 313.23333 314.68014 316.12033 317.55399 318.98121\n[113] 320.40206 321.81665 323.22504 324.62733 326.02358 327.41388 328.79830\n[120] 330.17692 331.54980 332.91702 334.27865 335.63476 336.98540 338.33066\n[127] 339.67059 341.00525 342.33471 343.65903 344.97826 346.29247 347.60171\n[134] 348.90604 350.20550 351.50017 352.79008 354.07530 355.35586 356.63183\n[141] 357.90325 359.17017 360.43263 361.69069 362.94439 364.19377 365.43888\n[148] 366.67976 367.91646 369.14901 370.37747 371.60186 372.82223 374.03862\n[155] 375.25106 376.45961 377.66428 378.86512 380.06217 381.25546 382.44503\n[162] 383.63091 384.81313 385.99174 387.16675 388.33821 389.50615 390.67060\n[169] 391.83158 392.98914 394.14329 395.29408 396.44152 397.58566 398.72651\n[176] 399.86410 400.99847 402.12964 403.25764 404.38248 405.50421 406.62285\n\n\n\n\n\nThe two figures above show an idea of the short term and the long-term forecasted stock price. The forecasted values is be based on the estimated parameters of the model, which were obtained by fitting it to the historical data. One way to assess the accuracy of the forecasts is to use cross-validation, which involves splitting the data into training and testing sets and comparing the predicted values to the actual values in the test set. This can help to identify any potential problems with the model and to fine-tune the model parameters.\n\n12 Step ahead Cross Validation1 Step ahead Cross Validation\n\n\n\n\nCode\n#a seasonal cross validation using 12 steps ahead forecasts\nfarima1 <- function(x, h){forecast(Arima(x, order=c(0,1,0)), h=h)}\n\n# Compute cross-validated errors for up to 12 steps ahead\ne <- tsCV(myts, forecastfunction = farima1, h = 12)\n\n# Compute the MSE values and remove missing values\nmse <- colMeans(e^2, na.rm = TRUE)\n\n# Plot the MSE values against the forecast horizon\ndata.frame(h = 1:12, MSE = mse) %>%\n  ggplot(aes(x = h, y = MSE)) + geom_point()+geom_line()+ggtitle(\"12 Step Ahead Cross Validation\")\n\n\n\n\n\n\n\n\n\nCode\n#a seasonal cross validation using 1 steps ahead forecasts\nfarima1 <- function(x, h){forecast(Arima(x, order=c(0,1,0)),h=h)}\n\n# Compute cross-validated errors for up to 1 steps ahead\ne <- tsCV(myts, forecastfunction = farima1, h = 1)\nmse <-abs(mean(e,na.rm=TRUE))\n\n# Plot the MSE values against the forecast horizon\ndata.frame(h = 1:12, MSE = mse) %>%\n  ggplot(aes(x = h, y = MSE)) + geom_point()+geom_line()+ggtitle(\"1 Step Ahead Cross Validation\")\n\n\n\n\n\n\n\n\nThe one-step-ahead forecasting plot shows that the MSE seems to be stable. The MSE is tend to rise by 12 steps at each step of cross-validation. Lastly, the above-mentioned ARIMA model should be compared to benchmark methods before being used for a long time.\n\n\nModel Comparison\n\nPlotModel Error Table\n\n\n\n\nCode\nfit <- Arima(myts, order=c(0,1,0))\nautoplot(myts) +\n  autolayer(meanf(myts, h=182),\n            series=\"Mean\", PI=FALSE) +\n  autolayer(naive(myts, h=182),\n            series=\"Naïve\", PI=FALSE) +\n  autolayer(snaive(myts, h=182),\n            series=\"SNaïve\", PI=FALSE)+\n  autolayer(rwf(myts, h=182, drift=TRUE),\n            series=\"Drift\", PI=FALSE)+\n  autolayer(forecast(fit,182), \n            series=\"fit\",PI=FALSE) +\n  guides(colour=guide_legend(title=\"Forecast\"))\n\n\n\n\n\n\n\n\n\nCode\nsummary <- summary(fit)\nsnaive <- snaive(myts, h=182)\naccuracy(snaive)\n\n\n                 ME     RMSE      MAE      MPE     MAPE MASE      ACF1\nTraining set 244.56 433.3665 321.6811 9.518344 11.83997    1 0.9922785\n\n\nCode\nsummary\n\n\nSeries: myts \nARIMA(0,1,0) \n\nsigma^2 = 909.2:  log likelihood = -15817.11\nAIC=31636.23   AICc=31636.23   BIC=31642.32\n\nTraining set error measures:\n                    ME     RMSE      MAE        MPE      MAPE       MASE\nTraining set 0.8654325 30.14872 18.13196 0.03191457 0.7380538 0.05636626\n                   ACF1\nTraining set -0.1079017\n\n\n\n\n\nError\nModel\nSnavie\n\n\n\n\nME\n0.1807799\n68.54409\n\n\nRMSE\n48.95665\n988.3694\n\n\nMAE\n16.46753\n593.4131\n\n\nMPE\n-0.0203944\n-7.952293\n\n\nMAPE\n0.7793433\n35.18675\n\n\nMASE\n0.02775052\n1.0000000\n\n\nACF1\n-0.02620846\n0.9973481\n\n\n\n\n\n\nThe ARIMA forecast tracks the actual data points very closely when compared to other model. The other forecast methods are less accurate when compared. From the table, Model error measurements of fit are much lower than snaive method.We can conclude that the fitted model is good."
  },
  {
    "objectID": "arima_macroeconomic_factors.html",
    "href": "arima_macroeconomic_factors.html",
    "title": "ARIMA/SARIMA Model for Macroeconomic Factors",
    "section": "",
    "text": "Macroeconomic factors, such as inflation rates, gross domestic product (GDP),interest rate and unemployment rates, play a critical role in the performance of the global economy. Accurately forecasting these factors is essential for policymakers, investors, and financial institutions seeking to make informed decisions.\nThe ARIMA and SARIMA models are commonly used time series analysis techniques for modeling and forecasting macroeconomic factors. The ARIMA model is a flexible and widely applicable model that can handle a wide range of data patterns, while the SARIMA model is a variant of the ARIMA model that incorporates seasonality into the analysis.\nThe choice between ARIMA and SARIMA models depends on the presence or absence of seasonality in the data. If the data exhibits seasonality, the SARIMA model may be more appropriate, as it allows for the modeling of both seasonal and non-seasonal patterns in the data. However, if the data does not exhibit seasonality, the ARIMA model may be sufficient for modeling and forecasting the macroeconomic factor of interest.\nOverall, the ARIMA and SARIMA models are powerful tools for modeling and forecasting macroeconomic factors, allowing analysts to identify unique patterns and trends and make more accurate predictions about future performance. However, as with any forecasting model, it is important to carefully consider the underlying assumptions and limitations of the model and incorporate relevant exogenous variables where appropriate.\nClick to view SARIMA Page for GDP Growth Rate\nClick to view SARIMA Page for Interest Rate\nClick to view SARIMA Page for Inflation Rate\nClick to view ARIMA Page for Unemployment Rate"
  },
  {
    "objectID": "arima_macroeconomic_gdp.html",
    "href": "arima_macroeconomic_gdp.html",
    "title": "SARIMA Model for GDP Growth Rate",
    "section": "",
    "text": "From the exploratory data analysis (EDA), it was observed that the raw data for GDP Growth Rate is already stationary, without the need for differencing. Therefore, an ARIMA or SARIMA model can be fitted directly to the raw data without the need for differencing.\n\nStationary Time Series\n\nPlotACFPACFADF TestSeasonal ACF PlotSeasonal PACF Ploy\n\n\n\n\nCode\n#import the data\ndf <- read.csv(\"DATA/CLEANED DATA/gdp_clean_data.csv\")\n#convert to time series data\nmyts<-ts(df$value,frequency=4,start=c(2010/1/1))\n# Plot \nmyts  %>% ggtsdisplay(main = \"GDP Groth Rate\") \n\n\n\n\n\n\n\n\n\nCode\n#ACF plot \nggAcf(myts,main=\"ACF Plot\") \n\n\nWarning in ggplot2::geom_segment(lineend = \"butt\", ...): Ignoring unknown\nparameters: `main`\n\n\n\n\n\n\n\n\n\nCode\nggPacf(myts,main=\"PACF Plot\") \n\n\n\n\n\n\n\n\n\nCode\ntseries::adf.test(myts)\n\n\n\n    Augmented Dickey-Fuller Test\n\ndata:  myts\nDickey-Fuller = -5.768, Lag order = 3, p-value = 0.01\nalternative hypothesis: stationary\n\n\n\n\n\n\nCode\n# Seasonal differencing (period = 4)\nggAcf(myts,lag = 4,main=\"ACF Plot: Seasonality\")\n\n\n\n\n\n\n\n\n\nCode\n# Seasonal differencing (period = 4)\nggPacf(myts,lag = 4,main=\"PACF Plot: Seasonality\")\n\n\n\n\n\n\n\n\nUpon analyzing the ACF and PACF plots, it was observed that most of the bar lines lie between the blue lines, indicating stationarity of the time series. This has been further confirmed by the Augmented Dickey-Fuller test, as evidenced by a p-value of less than 0.05. Given that the data is already stationary but there is presence of seasonality, an SARIMA model is preferred over ARIMA. The combination of ACF and PACF plots can help determine the appropriate values for p and q in the ARMA model. The p value is determined by examining significant spikes at various lags in the PACF plot, while the q value is determined through significant spikes in the ACF plot. P and Q are determined similarly, from seasonal plot.\nHere the parameters are d = 0 D = 0 p = 0,1 (PACF Plot) q = 0,1 (ACF Plot) P = 0,1 (PACF Seasonality Plot) Q = 0,1 (ACF Seasonality Plot)\n\n\nModel Selection\n\nSARIMA ResultAuto Arima Model\n\n\n\n\nCode\n######################## Check for different combinations ########\n\n\n#write a funtion\nSARIMA.c=function(p1,p2,q1,q2,P1,P2,Q1,Q2,data){\n  \n  #K=(p2+1)*(q2+1)*(P2+1)*(Q2+1)\n  \n  temp=c()\n  d=0\n  D=0\n  s=4\n  \n  i=1\n  temp= data.frame()\n  ls=matrix(rep(NA,9*16),nrow=16)\n  \n  \n  for (p in p1:p2)\n  {\n    for(q in q1:q2)\n    {\n      for(P in P1:P2)\n      {\n        for(Q in Q1:Q2)\n        {\n          if(p+d+q+P+D+Q<=9)\n          {\n            \n            model<- Arima(data,order=c(p-1,d,q-1),seasonal=c(P-1,D,Q-1))\n            ls[i,]= c(p-1,d,q-1,P-1,D,Q-1,model$aic,model$bic,model$aicc)\n            i=i+1\n            #print(i)\n            \n          }\n          \n        }\n      }\n    }\n    \n  }\n\n  \n  temp= as.data.frame(ls)\n  names(temp)= c(\"p\",\"d\",\"q\",\"P\",\"D\",\"Q\",\"AIC\",\"BIC\",\"AICc\")\n  \n  temp\n  \n}\n# q=0,1,; Q=0,1 and PACF plot: p=0,1; P=0,1, D=0 and d=O\noutput=SARIMA.c(p1=1,p2=2,q1=1,q2=2,P1=1,P2=2,Q1=1,Q2=2,data=myts)\n#output\n\nknitr::kable(output)\n\n\n\n\n\np\nd\nq\nP\nD\nQ\nAIC\nBIC\nAICc\n\n\n\n\n0\n0\n0\n0\n0\n0\n241.5679\n245.4703\n241.8128\n\n\n0\n0\n0\n0\n0\n1\n207.2709\n213.1247\n207.7709\n\n\n0\n0\n0\n1\n0\n0\n228.4538\n234.3075\n228.9538\n\n\n0\n0\n0\n1\n0\n1\n207.6683\n215.4733\n208.5194\n\n\n0\n0\n1\n0\n0\n0\n235.1650\n241.0187\n235.6650\n\n\n0\n0\n1\n0\n0\n1\n201.5710\n209.3760\n202.4221\n\n\n0\n0\n1\n1\n0\n0\n221.6803\n229.4853\n222.5314\n\n\n0\n0\n1\n1\n0\n1\n202.2159\n211.9721\n203.5202\n\n\n1\n0\n0\n0\n0\n0\n234.0196\n239.8734\n234.5196\n\n\n1\n0\n0\n0\n0\n1\n199.8027\n207.6077\n200.6538\n\n\n1\n0\n0\n1\n0\n0\n219.0909\n226.8959\n219.9419\n\n\n1\n0\n0\n1\n0\n1\n200.3070\n210.0632\n201.6113\n\n\n1\n0\n1\n0\n0\n0\n236.0128\n243.8178\n236.8639\n\n\n1\n0\n1\n0\n0\n1\n201.7720\n211.5282\n203.0763\n\n\n1\n0\n1\n1\n0\n0\n221.0125\n230.7687\n222.3169\n\n\n1\n0\n1\n1\n0\n1\n202.2245\n213.9320\n204.0912\n\n\n\n\n\n\n\n\n\nCode\nauto.arima(myts)\n\n\nSeries: myts \nARIMA(1,0,0)(2,0,1)[4] with non-zero mean \n\nCoefficients:\n         ar1     sar1     sar2     sma1    mean\n      0.4006  -0.3997  -0.3431  -0.7034  2.0673\ns.e.  0.1292   0.2515   0.2512   0.2626  0.0741\n\nsigma^2 = 2.105:  log likelihood = -94.11\nAIC=200.22   AICc=202.09   BIC=211.93\n\n\n\n\n\nSorting the table reveals that the AIC and BIC with the parameters (1,0,0) and seasonal parameters are the least (0,0,1). There is also a function that enables the automatic selection of an ARIMA model. According to R’s auto.arima technique, the model’s parameters should be (1,0,0) and seasonal parameters (2,0,1).\nSince there are different models to choose from, it is important to perform model diagnostics to determine the best model for the data. These diagnostics can include checking for residual normality, checking for autocorrelation and partial autocorrelation in the residuals, and comparing the forecasted values to the actual values. By conducting these tests, we can determine which model provides the best fit for the data and is therefore the most appropriate to use for forecasting future values.\n\n\nModel Diagnostic\n\nModel 1 PlotModel 1Model 2 PlotModel 2\n\n\n\n\nCode\nmodel_output <- capture.output(sarima(myts, 1,1,0,0,0,1,4))\n\n\n\n\n\n\n\n\n\nCode\ncat(model_output[26:57], model_output[length(model_output)], sep = \"\\n\") \n\n\n$fit\n\nCall:\narima(x = xdata, order = c(p, d, q), seasonal = list(order = c(P, D, Q), period = S), \n    xreg = constant, transform.pars = trans, fixed = fixed, optim.control = list(trace = trc, \n        REPORT = 1, reltol = tol))\n\nCoefficients:\n          ar1     sma1  constant\n      -0.3051  -1.0000    0.0012\ns.e.   0.1329   0.1524    0.0417\n\nsigma^2 estimated as 2.492:  log likelihood = -100.93,  aic = 209.87\n\n$degrees_of_freedom\n[1] 48\n\n$ttable\n         Estimate     SE t.value p.value\nar1       -0.3051 0.1329 -2.2954  0.0261\nsma1      -1.0000 0.1524 -6.5608  0.0000\nconstant   0.0012 0.0417  0.0290  0.9770\n\n$AIC\n[1] 4.115092\n\n$AICc\n[1] 4.125105\n\n$BIC\n[1] 4.266608\n\n\n\n\n\n\nCode\nmodel_output <- capture.output(sarima(myts, 1,0,0,2,0,1,4))\n\n\n\n\n\n\n\n\n\nCode\ncat(model_output[38:70], model_output[length(model_output)], sep = \"\\n\") \n\n\n$fit\n\nCall:\narima(x = xdata, order = c(p, d, q), seasonal = list(order = c(P, D, Q), period = S), \n    xreg = xmean, include.mean = FALSE, transform.pars = trans, fixed = fixed, \n    optim.control = list(trace = trc, REPORT = 1, reltol = tol))\n\nCoefficients:\n         ar1     sar1     sar2     sma1   xmean\n      0.4006  -0.3997  -0.3431  -0.7034  2.0673\ns.e.  0.1292   0.2515   0.2512   0.2626  0.0741\n\nsigma^2 estimated as 1.903:  log likelihood = -94.11,  aic = 200.22\n\n$degrees_of_freedom\n[1] 47\n\n$ttable\n      Estimate     SE t.value p.value\nar1     0.4006 0.1292  3.1000  0.0033\nsar1   -0.3997 0.2515 -1.5893  0.1187\nsar2   -0.3431 0.2512 -1.3658  0.1785\nsma1   -0.7034 0.2626 -2.6781  0.0102\nxmean   2.0673 0.0741 27.9061  0.0000\n\n$AIC\n[1] 3.850466\n\n$AICc\n[1] 3.87555\n\n$BIC\n[1] 4.07561\n\n\n\n\n\nIn the first model, the p-values for both ar1 and sma1 are less than 0.05, indicating that both coefficients are statistically significant. In the second model, the p-value for ar1 is less than 0.05, indicating that it is statistically significant, while the p-values for the seasonal coefficients (sar1 and sar2) are greater than 0.05, indicating that they are not statistically significant. The p-value for sma1 in the second model is less than 0.05, indicating that it is statistically significant. Based on the p-values, it appears that the first model (SARIMA(1,1,0)(0,1,1)[4]) has a statistically significant AR and MA component, while the second model (SARIMA(1,1,1)(0,0,2)[4]) has a statistically significant.\nBy analyzing the standardized residuals plot for the model (SARIMA(1,1,0)(0,1,1)[4]), it can be observed that the mean is close to 0 and the variance is slightly higher than 1. Deviations from these values could indicate poor model fit. However, in this case, the model seems to be well-fitted. The ACF plot of residuals shows no significant lags, which is a high sign for the model. The qq-plot also suggests high normality in the residuals. Additionally, the p-values for the Ljung-Box test are near than 0.05, indicating that the residuals are independent, which is favorable for the model’s accuracy.\nThe equation for this model: \\[ (1-\\phi_1B)(1-B^4)(1-\\Phi_1B^4)y_t = \\epsilon_t \\]\n\n\nForcast\n\nShort Term ForcastLong term Forcast\n\n\n\n\nCode\nmyts %>%\n  Arima(order=c(1,1,0),seasonal = c(0,1,1), include.drift = TRUE) %>%\n  forecast %>%\n  autoplot(main = \"GDP Growth Rate Prediction\") +\n  ylab(\"GDP growth\") + xlab(\"Year\")\n\n\n\n\n\n\n\n\n\nCode\nsarima.for(myts,12, 1,1,0,0,1,1,4, main='GDP Growth Rate Prediction')\n\n\n\n\n\n$pred\n          Qtr1      Qtr2      Qtr3      Qtr4\n2023 1.0795445 1.0756510 1.0580210 1.0348134\n2024 0.9436877 1.0147972 0.9763838 0.9589353\n2025 0.8662137 0.9377654 0.8992295 0.8818150\n\n$se\n         Qtr1     Qtr2     Qtr3     Qtr4\n2023 2.699600 3.327957 3.963720 4.483195\n2024 5.048923 5.523866 5.967367 6.377804\n2025 6.838954 7.240686 7.626250 7.991102\n\n\n\n\n\nThe two figures above show an idea of the short term and the long-term forecasted GDP Growth Rate. The forecasted values is be based on the estimated parameters of the model, which were obtained by fitting it to the historical data. One way to assess the accuracy of the forecasts is to use cross-validation, which involves splitting the data into training and testing sets and comparing the predicted values to the actual values in the test set. This can help to identify any potential problems with the model and to fine-tune the model parameters.\n\n12 Step ahead Cross Validation1 Step ahead Cross Validation\n\n\n\n\nCode\n#a seasonal cross validation using 12 steps ahead forecasts\nfarima1 <- function(x, h){forecast(Arima(x, order=c(1,1,0),seasonal = c(0,1,1)), h=h)}\n\n# Compute cross-validated errors for up to 12 steps ahead\ne <- tsCV(myts, forecastfunction = farima1, h = 12)\n\n# Compute the MSE values and remove missing values\nmse <- colMeans(e^2, na.rm = TRUE)\n\n# Plot the MSE values against the forecast horizon\ndata.frame(h = 1:12, MSE = mse) %>%\n  ggplot(aes(x = h, y = MSE)) + geom_point()+geom_line()+ggtitle(\"12 Step Ahead Cross Validation\")\n\n\n\n\n\n\n\n\n\nCode\n#a seasonal cross validation using 1 steps ahead forecasts\nfarima1 <- function(x, h){forecast(Arima(x, order=c(1,1,0),seasonal = c(0,1,1)),h=h)}\n\n# Compute cross-validated errors for up to 1 steps ahead\ne <- tsCV(myts, forecastfunction = farima1, h = 1)\nmse <-abs(mean(e,na.rm=TRUE))\n\n# Plot the MSE values against the forecast horizon\ndata.frame(h = 1:12, MSE = mse) %>%\n  ggplot(aes(x = h, y = MSE)) + geom_point()+geom_line()+ggtitle(\"1 Step Ahead Cross Validation\")\n\n\n\n\n\n\n\n\nThe one-step-ahead forecasting plot shows that the MSE seems to be stable. The MSE is tend to rise tell 10 step but there is sudden drop at 10th step of cross-validation. Lastly, the above-mentioned ARIMA model should be compared to benchmark methods before being used for a long time.\n\n\nModel Comparison\n\nPlotModel Error Table\n\n\n\n\nCode\nfit <- Arima(myts, order=c(1,1,0),seasonal = c(0,1,1), lambda = 4)\nautoplot(myts) +\n  autolayer(meanf(myts, h=12),\n            series=\"Mean\", PI=FALSE) +\n  autolayer(naive(myts, h=12),\n            series=\"Naïve\", PI=FALSE) +\n  autolayer(snaive(myts, h=12),\n            series=\"SNaïve\", PI=FALSE)+\n  autolayer(rwf(myts, h=12, drift=TRUE),\n            series=\"Drift\", PI=FALSE)+\n  autolayer(forecast(fit,12), \n            series=\"fit\",PI=FALSE) +\n  guides(colour=guide_legend(title=\"Forecast\"))\n\n\n\n\n\n\n\n\n\nCode\nsummary <- summary(fit)\nsnaive <- snaive(myts, h=12)\n#accuracy(snaive)\n\n\n\n\n\nError\nModel\nSnavie\n\n\n\n\nME\n-0.03237449\n-0.05208333\n\n\nRMSE\n2.466369\n4.285368\n\n\nMAE\n1.191784\n2.285417\n\n\nMPE\n-13.70472\n-18.72979\n\n\nMAPE\n45.10017\n89.98744\n\n\nMASE\n0.5214732\n1.0000000\n\n\nACF1\n-0.02438127\n0.3889634\n\n\n\n\n\n\nThe ARIMA forecast doesn’t track the actual data points very closely. The other forecast methods are less accurate when compared to other model.The trend is visible in the fitted model, which indicates it is a good fit. From the table, Model error measurements of fit are much lower than snaive method.We can conclude that the fitted model is good.\n\n\nStationary Time Series\n\nACFPACFADF TestSeasonal ACF PlotSeasonal PACF Ploy\n\n\n\n\nCode\n#import the data\ndf <- read.csv(\"DATA/CLEANED DATA/gdp_clean_data.csv\")\n#convert to time series data\nmyts<-ts(df$value,frequency=4,start=c(2010/1/1))\n#ACF plot \nggAcf(myts,main=\"ACF Plot\") \n\n\n\n\n\n\n\n\n\nCode\nggPacf(myts,main=\"PACF Plot\") \n\n\n\n\n\n\n\n\n\nCode\ntseries::adf.test(myts)\n\n\n\n    Augmented Dickey-Fuller Test\n\ndata:  myts\nDickey-Fuller = -5.768, Lag order = 3, p-value = 0.01\nalternative hypothesis: stationary\n\n\n\n\n\n\nCode\n# Seasonal differencing (period = 4)\nggAcf(myts,lag = 4,main=\"ACF Plot: Seasonality\")\n\n\n\n\n\n\n\n\n\nCode\n# Seasonal differencing (period = 4)\nggPacf(myts,lag = 4,main=\"PACF Plot: Seasonality\")\n\n\n\n\n\n\n\n\nUpon analyzing the ACF and PACF plots, it was observed that most of the bar lines lie between the blue lines, indicating stationarity of the time series. This has been further confirmed by the Augmented Dickey-Fuller test, as evidenced by a p-value of less than 0.05. Given that the data is already stationary but there is presence of seasonality, an SARIMA model is preferred over ARIMA. The combination of ACF and PACF plots can help determine the appropriate values for p and q in the ARMA model. The p value is determined by examining significant spikes at various lags in the PACF plot, while the q value is determined through significant spikes in the ACF plot. P and Q are determined similarly, from seasonal plot.\nHere the parameters are d = 0 D = 0 p = 0,1 (PACF Plot) q = 0,1 (ACF Plot) P = 0,1 (PACF Seasonality Plot) Q = 0,1 (ACF Seasonality Plot)\n\n\nModel Selection\n\nSARIMA ResultAuto Arima Model\n\n\n\n\nCode\n######################## Check for different combinations ########\n\n\n#write a funtion\nSARIMA.c=function(p1,p2,q1,q2,P1,P2,Q1,Q2,data){\n  \n  #K=(p2+1)*(q2+1)*(P2+1)*(Q2+1)\n  \n  temp=c()\n  d=0\n  D=0\n  s=4\n  \n  i=1\n  temp= data.frame()\n  ls=matrix(rep(NA,9*16),nrow=16)\n  \n  \n  for (p in p1:p2)\n  {\n    for(q in q1:q2)\n    {\n      for(P in P1:P2)\n      {\n        for(Q in Q1:Q2)\n        {\n          if(p+d+q+P+D+Q<=9)\n          {\n            \n            model<- Arima(data,order=c(p-1,d,q-1),seasonal=c(P-1,D,Q-1))\n            ls[i,]= c(p-1,d,q-1,P-1,D,Q-1,model$aic,model$bic,model$aicc)\n            i=i+1\n            #print(i)\n            \n          }\n          \n        }\n      }\n    }\n    \n  }\n\n  \n  temp= as.data.frame(ls)\n  names(temp)= c(\"p\",\"d\",\"q\",\"P\",\"D\",\"Q\",\"AIC\",\"BIC\",\"AICc\")\n  \n  temp\n  \n}\n# q=0,1,; Q=0,1 and PACF plot: p=0,1; P=0,1, D=0 and d=O\noutput=SARIMA.c(p1=1,p2=2,q1=1,q2=2,P1=1,P2=2,Q1=1,Q2=2,data=myts)\n#output\n\nknitr::kable(output)\n\n\n\n\n\np\nd\nq\nP\nD\nQ\nAIC\nBIC\nAICc\n\n\n\n\n0\n0\n0\n0\n0\n0\n241.5679\n245.4703\n241.8128\n\n\n0\n0\n0\n0\n0\n1\n207.2709\n213.1247\n207.7709\n\n\n0\n0\n0\n1\n0\n0\n228.4538\n234.3075\n228.9538\n\n\n0\n0\n0\n1\n0\n1\n207.6683\n215.4733\n208.5194\n\n\n0\n0\n1\n0\n0\n0\n235.1650\n241.0187\n235.6650\n\n\n0\n0\n1\n0\n0\n1\n201.5710\n209.3760\n202.4221\n\n\n0\n0\n1\n1\n0\n0\n221.6803\n229.4853\n222.5314\n\n\n0\n0\n1\n1\n0\n1\n202.2159\n211.9721\n203.5202\n\n\n1\n0\n0\n0\n0\n0\n234.0196\n239.8734\n234.5196\n\n\n1\n0\n0\n0\n0\n1\n199.8027\n207.6077\n200.6538\n\n\n1\n0\n0\n1\n0\n0\n219.0909\n226.8959\n219.9419\n\n\n1\n0\n0\n1\n0\n1\n200.3070\n210.0632\n201.6113\n\n\n1\n0\n1\n0\n0\n0\n236.0128\n243.8178\n236.8639\n\n\n1\n0\n1\n0\n0\n1\n201.7720\n211.5282\n203.0763\n\n\n1\n0\n1\n1\n0\n0\n221.0125\n230.7687\n222.3169\n\n\n1\n0\n1\n1\n0\n1\n202.2245\n213.9320\n204.0912\n\n\n\n\n\n\n\n\n\nCode\nauto.arima(myts)\n\n\nSeries: myts \nARIMA(1,0,0)(2,0,1)[4] with non-zero mean \n\nCoefficients:\n         ar1     sar1     sar2     sma1    mean\n      0.4006  -0.3997  -0.3431  -0.7034  2.0673\ns.e.  0.1292   0.2515   0.2512   0.2626  0.0741\n\nsigma^2 = 2.105:  log likelihood = -94.11\nAIC=200.22   AICc=202.09   BIC=211.93\n\n\n\n\n\nSorting the table reveals that the AIC and BIC with the parameters (1,0,0) and seasonal parameters are the least (0,0,1). There is also a function that enables the automatic selection of an ARIMA model. According to R’s auto.arima technique, the model’s parameters should be (1,0,0) and seasonal parameters (2,0,1).\nSince there are different models to choose from, it is important to perform model diagnostics to determine the best model for the data. These diagnostics can include checking for residual normality, checking for autocorrelation and partial autocorrelation in the residuals, and comparing the forecasted values to the actual values. By conducting these tests, we can determine which model provides the best fit for the data and is therefore the most appropriate to use for forecasting future values.\n\n\nModel Diagnostic\n\nModel 1 PlotModel 1Model 2 PlotModel 2\n\n\n\n\nCode\nmodel_output <- capture.output(sarima(myts, 1,1,0,0,0,1,4))\n\n\n\n\n\n\n\n\n\nCode\ncat(model_output[26:57], model_output[length(model_output)], sep = \"\\n\") \n\n\n$fit\n\nCall:\narima(x = xdata, order = c(p, d, q), seasonal = list(order = c(P, D, Q), period = S), \n    xreg = constant, transform.pars = trans, fixed = fixed, optim.control = list(trace = trc, \n        REPORT = 1, reltol = tol))\n\nCoefficients:\n          ar1     sma1  constant\n      -0.3051  -1.0000    0.0012\ns.e.   0.1329   0.1524    0.0417\n\nsigma^2 estimated as 2.492:  log likelihood = -100.93,  aic = 209.87\n\n$degrees_of_freedom\n[1] 48\n\n$ttable\n         Estimate     SE t.value p.value\nar1       -0.3051 0.1329 -2.2954  0.0261\nsma1      -1.0000 0.1524 -6.5608  0.0000\nconstant   0.0012 0.0417  0.0290  0.9770\n\n$AIC\n[1] 4.115092\n\n$AICc\n[1] 4.125105\n\n$BIC\n[1] 4.266608\n\n\n\n\n\n\nCode\nmodel_output <- capture.output(sarima(myts, 1,0,0,2,0,1,4))\n\n\n\n\n\n\n\n\n\nCode\ncat(model_output[38:70], model_output[length(model_output)], sep = \"\\n\") \n\n\n$fit\n\nCall:\narima(x = xdata, order = c(p, d, q), seasonal = list(order = c(P, D, Q), period = S), \n    xreg = xmean, include.mean = FALSE, transform.pars = trans, fixed = fixed, \n    optim.control = list(trace = trc, REPORT = 1, reltol = tol))\n\nCoefficients:\n         ar1     sar1     sar2     sma1   xmean\n      0.4006  -0.3997  -0.3431  -0.7034  2.0673\ns.e.  0.1292   0.2515   0.2512   0.2626  0.0741\n\nsigma^2 estimated as 1.903:  log likelihood = -94.11,  aic = 200.22\n\n$degrees_of_freedom\n[1] 47\n\n$ttable\n      Estimate     SE t.value p.value\nar1     0.4006 0.1292  3.1000  0.0033\nsar1   -0.3997 0.2515 -1.5893  0.1187\nsar2   -0.3431 0.2512 -1.3658  0.1785\nsma1   -0.7034 0.2626 -2.6781  0.0102\nxmean   2.0673 0.0741 27.9061  0.0000\n\n$AIC\n[1] 3.850466\n\n$AICc\n[1] 3.87555\n\n$BIC\n[1] 4.07561\n\n\n\n\n\nIn the first model, the p-values for both ar1 and sma1 are less than 0.05, indicating that both coefficients are statistically significant. In the second model, the p-value for ar1 is less than 0.05, indicating that it is statistically significant, while the p-values for the seasonal coefficients (sar1 and sar2) are greater than 0.05, indicating that they are not statistically significant. The p-value for sma1 in the second model is less than 0.05, indicating that it is statistically significant. Based on the p-values, it appears that the first model (SARIMA(1,1,0)(0,1,1)[4]) has a statistically significant AR and MA component, while the second model (SARIMA(1,1,1)(0,0,2)[4]) has a statistically significant.\nBy analyzing the standardized residuals plot for the model (SARIMA(1,1,0)(0,1,1)[4]), it can be observed that the mean is close to 0 and the variance is slightly higher than 1. Deviations from these values could indicate poor model fit. However, in this case, the model seems to be well-fitted. The ACF plot of residuals shows no significant lags, which is a high sign for the model. The qq-plot also suggests high normality in the residuals. Additionally, the p-values for the Ljung-Box test are near than 0.05, indicating that the residuals are independent, which is favorable for the model’s accuracy.\nThe equation for this model: \\[ (1-\\phi_1B)(1-B^4)(1-\\Phi_1B^4)y_t = \\epsilon_t \\]\n\n\nForcast\n\nShort Term ForcastLong term Forcast\n\n\n\n\nCode\nmyts %>%\n  Arima(order=c(1,1,0),seasonal = c(0,1,1), include.drift = TRUE) %>%\n  forecast %>%\n  autoplot(main = \"GDP Growth Rate Prediction\") +\n  ylab(\"GDP growth\") + xlab(\"Year\")\n\n\n\n\n\n\n\n\n\nCode\nsarima.for(myts,12, 1,1,0,0,1,1,4, main='GDP Growth Rate Prediction')\n\n\n\n\n\n$pred\n          Qtr1      Qtr2      Qtr3      Qtr4\n2023 1.0795445 1.0756510 1.0580210 1.0348134\n2024 0.9436877 1.0147972 0.9763838 0.9589353\n2025 0.8662137 0.9377654 0.8992295 0.8818150\n\n$se\n         Qtr1     Qtr2     Qtr3     Qtr4\n2023 2.699600 3.327957 3.963720 4.483195\n2024 5.048923 5.523866 5.967367 6.377804\n2025 6.838954 7.240686 7.626250 7.991102\n\n\n\n\n\nThe two figures above show an idea of the short term and the long-term forecasted GDP Growth Rate. The forecasted values is be based on the estimated parameters of the model, which were obtained by fitting it to the historical data. One way to assess the accuracy of the forecasts is to use cross-validation, which involves splitting the data into training and testing sets and comparing the predicted values to the actual values in the test set. This can help to identify any potential problems with the model and to fine-tune the model parameters.\n\n12 Step ahead Cross Validation1 Step ahead Cross Validation\n\n\n\n\nCode\n#a seasonal cross validation using 12 steps ahead forecasts\nfarima1 <- function(x, h){forecast(Arima(x, order=c(1,1,0),seasonal = c(0,1,1)), h=h)}\n\n# Compute cross-validated errors for up to 12 steps ahead\ne <- tsCV(myts, forecastfunction = farima1, h = 12)\n\n# Compute the MSE values and remove missing values\nmse <- colMeans(e^2, na.rm = TRUE)\n\n# Plot the MSE values against the forecast horizon\ndata.frame(h = 1:12, MSE = mse) %>%\n  ggplot(aes(x = h, y = MSE)) + geom_point()+geom_line()+ggtitle(\"12 Step Ahead Cross Validation\")\n\n\n\n\n\n\n\n\n\nCode\n#a seasonal cross validation using 1 steps ahead forecasts\nfarima1 <- function(x, h){forecast(Arima(x, order=c(1,1,0),seasonal = c(0,1,1)),h=h)}\n\n# Compute cross-validated errors for up to 1 steps ahead\ne <- tsCV(myts, forecastfunction = farima1, h = 1)\nmse <-abs(mean(e,na.rm=TRUE))\n\n# Plot the MSE values against the forecast horizon\ndata.frame(h = 1:12, MSE = mse) %>%\n  ggplot(aes(x = h, y = MSE)) + geom_point()+geom_line()+ggtitle(\"1 Step Ahead Cross Validation\")\n\n\n\n\n\n\n\n\nThe one-step-ahead forecasting plot shows that the MSE seems to be stable. The MSE is tend to rise tell 10 step but there is sudden drop at 10th step of cross-validation. Lastly, the above-mentioned ARIMA model should be compared to benchmark methods before being used for a long time.\n\n\nModel Comparison\n\nPlotModel Error Table\n\n\n\n\nCode\nfit <- Arima(myts, order=c(1,1,0),seasonal = c(0,1,1), lambda = 4)\nautoplot(myts) +\n  autolayer(meanf(myts, h=12),\n            series=\"Mean\", PI=FALSE) +\n  autolayer(naive(myts, h=12),\n            series=\"Naïve\", PI=FALSE) +\n  autolayer(snaive(myts, h=12),\n            series=\"SNaïve\", PI=FALSE)+\n  autolayer(rwf(myts, h=12, drift=TRUE),\n            series=\"Drift\", PI=FALSE)+\n  autolayer(forecast(fit,12), \n            series=\"fit\",PI=FALSE) +\n  guides(colour=guide_legend(title=\"Forecast\"))\n\n\n\n\n\n\n\n\n\nCode\nsummary <- summary(fit)\nsnaive <- snaive(myts, h=12)\n#accuracy(snaive)\n\n\n\n\n\nError\nModel\nSnavie\n\n\n\n\nME\n-0.03237449\n-0.05208333\n\n\nRMSE\n2.466369\n4.285368\n\n\nMAE\n1.191784\n2.285417\n\n\nMPE\n-13.70472\n-18.72979\n\n\nMAPE\n45.10017\n89.98744\n\n\nMASE\n0.5214732\n1.0000000\n\n\nACF1\n-0.02438127\n0.3889634\n\n\n\n\n\n\nThe ARIMA forecast doesn’t track the actual data points very closely. The other forecast methods are less accurate when compared to other model.The trend is visible in the fitted model, which indicates it is a good fit. From the table, Model error measurements of fit are much lower than snaive method.We can conclude that the fitted model is good."
  },
  {
    "objectID": "arima_macroeconomic_inflation.html",
    "href": "arima_macroeconomic_inflation.html",
    "title": "SARIMA Model for Inflation Rate",
    "section": "",
    "text": "From the exploratory data analysis (EDA), it may be observed that the raw data for certain macroeconomic factors, such as interest rates or stock prices, is non-stationary and requires differencing to achieve stationarity. By differencing the data, we remove the trend and other non-stationary components, making it easier to model and forecast accurately.\n\nDifferentiated Time Series Plot\n\nPlotACFPACFADF TestSeasonal ACF PlotSeasonal PACF Ploy\n\n\n\n\nCode\n#import the data\ndf <- read.csv(\"DATA/CLEANED DATA/inflation_rate_clean_data.csv\")\n#convert the data to time series data\nmyts <- ts(as.vector(t(as.matrix(df))), start=c(2010,1), end=c(2023,2), frequency=12)\n#First order differentiation\ndf1 <- diff(myts)\n# Plot \nmyts  %>% diff() %>% ggtsdisplay(main = \"First order differentiation\") \n\n\n\n\n\n\n\n\n\nCode\n#ACF plot \nggAcf(df1,36,main=\"ACF Plot: First order differentiation\") \n\n\n\n\n\n\n\n\n\nCode\nggPacf(df1,36,main=\"PACF Plot: First order differentiation\") \n\n\n\n\n\n\n\n\n\nCode\ntseries::adf.test(df1)\n\n\n\n    Augmented Dickey-Fuller Test\n\ndata:  df1\nDickey-Fuller = -3.8576, Lag order = 5, p-value = 0.01802\nalternative hypothesis: stationary\n\n\n\n\n\n\nCode\n# Seasonal differencing (period = 12)\ndf1_seasonal = myts %>%diff(differences = 1, lag = 12)\nggAcf(df1_seasonal,main=\"ACF Plot: Seasonality\")\n\n\n\n\n\n\n\n\n\nCode\n# Seasonal differencing (period = 12)\nggPacf(df1_seasonal,main=\"PACF Plot: Seasonality\")\n\n\n\n\n\n\n\n\nAfter analyzing the ACF and PACF plots, it can be observed that most of the bar lines lie between the blue lines, indicating that the time series is stationary. This has been further confirmed through an Augmented Dickey-Fuller test as the p-value is less than 0.05. Once the data is stationary and there is presence of seasonality, the next step is to model it using SARIMA. The combination of ACF and PACF plots can help determine the appropriate values for p, d, and q in the ARIMA model. The order of differencing required to achieve stationarity gives the d value. The p value is determined by examining significant spikes at various lags in the PACF plot, while the q value is determined through significant spikes in the ACF plot. P and Q are determined similarly, from seasonal plot.\nHere the parameters are d = 1, p = 0,1 (PACF Plot), q = 0,1 (ACF Plot), P = 1,2 (PACF Seasonality Plot), Q = 1,2,3,4 (ACF Seasonality Plot) D = 1\n\n\nModel Selection\n\nSARIMA ResultAuto Arima Model\n\n\n\n\nCode\n######################## Check for different combinations ########\n\n\n#write a funtion\nSARIMA.c=function(p1,q1,P1,P2,Q1,Q2,data){\n  \n  #K=(p2+1)*(q2+1)*(P2+1)*(Q2+1)\n  \n  temp=c()\n  d=1\n  D=1\n  s=12\n  \n  i=1\n  temp= data.frame()\n  ls=matrix(rep(NA,9*3),nrow=3)\n  \n  \n  for (p in p1)\n  {\n    for(q in q1)\n    {\n      for(P in P1:P2)\n      {\n        for(Q in Q1:Q2)\n        {\n          if(p+d+q+P+D+Q<=9)\n          {\n            \n            model<- Arima(data,order=c(p-1,d,q-1),seasonal=c(P-1,D,Q-1))\n            ls[i,]= c(p-1,d,q-1,P-1,D,Q-1,model$aic,model$bic,model$aicc)\n            i=i+1\n            #print(i)\n            \n          }\n          \n        }\n      }\n    }\n    \n  }\n\n  \n  temp= as.data.frame(ls)\n  names(temp)= c(\"p\",\"d\",\"q\",\"P\",\"D\",\"Q\",\"AIC\",\"BIC\",\"AICc\")\n  \n  temp\n  \n}\n# q=0,1,; Q=0,1,2,3 and PACF plot: p=0,1; P=0,1,2, D=1 and d=1\noutput=SARIMA.c(p1=2,q1=2,P1=1,P2=3,Q1=1,Q2=3,data=myts)\n#output\nknitr::kable(output)\n\n\n\n\n\np\nd\nq\nP\nD\nQ\nAIC\nBIC\nAICc\n\n\n\n\n1\n1\n1\n0\n1\n0\n86.02072\n94.950919\n86.19093\n\n\n1\n1\n1\n0\n1\n1\n-26.80019\n-14.893256\n-26.51448\n\n\n1\n1\n1\n1\n1\n0\n-16.22925\n-4.322316\n-15.94354\n\n\n\n\n\n\n\n\n\nCode\nauto.arima(myts)\n\n\nSeries: myts \nARIMA(4,1,1)(0,0,1)[12] with drift \n\nCoefficients:\n         ar1      ar2      ar3     ar4      ma1     sma1   drift\n      1.1757  -0.2352  -0.3924  0.3603  -0.6966  -0.5922  0.0215\ns.e.  0.1166   0.1251   0.1162  0.0757   0.1037   0.0807  0.0173\n\nsigma^2 = 0.02376:  log likelihood = 71.61\nAIC=-127.23   AICc=-126.25   BIC=-102.78\n\n\n\n\n\nSorting the table reveals that the AIC and BIC with the parameters (1,1,1) and seasonal parameters are the least (1,1,0). There is also a function that enables the automatic selection of an ARIMA model. According to R’s auto.arima technique, the model’s parameters should be (4,1,1) and (0,0,1).\nSince there are different models to choose from, it is important to perform model diagnostics to determine the best model for the data. These diagnostics can include checking for residual normality, checking for autocorrelation and partial autocorrelation in the residuals, and comparing the forecasted values to the actual values. By conducting these tests, we can determine which model provides the best fit for the data and is therefore the most appropriate to use for forecasting future values.\n\n\nModel Diagnostic\n\nModel 1 PlotModel 1Model 2 PlotModel 2\n\n\n\n\nCode\nmodel_output <- capture.output(sarima(myts, 1,1,1,1,1,0,12))\n\n\n\n\n\n\n\n\n\nCode\ncat(model_output[22:52], model_output[length(model_output)], sep = \"\\n\") \n\n\niter  11 value -1.502488\niter  11 value -1.502488\niter  11 value -1.502488\nfinal  value -1.502488 \nconverged\n$fit\n\nCall:\narima(x = xdata, order = c(p, d, q), seasonal = list(order = c(P, D, Q), period = S), \n    include.mean = !no.constant, transform.pars = trans, fixed = fixed, optim.control = list(trace = trc, \n        REPORT = 1, reltol = tol))\n\nCoefficients:\n         ar1     ma1     sar1\n      0.3616  0.2288  -0.8036\ns.e.  0.1242  0.1212   0.0530\n\nsigma^2 estimated as 0.04535:  log likelihood = 12.11,  aic = -16.23\n\n$degrees_of_freedom\n[1] 142\n\n$ttable\n     Estimate     SE  t.value p.value\nar1    0.3616 0.1242   2.9123  0.0042\nma1    0.2288 0.1212   1.8883  0.0610\nsar1  -0.8036 0.0530 -15.1732  0.0000\n\n$AIC\n[1] -0.1119259\n\n\n\n\n\n\nCode\nmodel_output <- capture.output(sarima(myts, 4,1,1,0,0,1,12))\n\n\n\n\n\n\n\n\n\nCode\ncat(model_output[46:81], model_output[length(model_output)], sep = \"\\n\") \n\n\n$fit\n\nCall:\narima(x = xdata, order = c(p, d, q), seasonal = list(order = c(P, D, Q), period = S), \n    xreg = constant, transform.pars = trans, fixed = fixed, optim.control = list(trace = trc, \n        REPORT = 1, reltol = tol))\n\nCoefficients:\n         ar1      ar2      ar3     ar4      ma1     sma1  constant\n      1.1757  -0.2352  -0.3924  0.3603  -0.6966  -0.5922    0.0215\ns.e.  0.1166   0.1251   0.1162  0.0757   0.1037   0.0807    0.0173\n\nsigma^2 estimated as 0.0227:  log likelihood = 71.61,  aic = -127.23\n\n$degrees_of_freedom\n[1] 150\n\n$ttable\n         Estimate     SE t.value p.value\nar1        1.1757 0.1166 10.0868  0.0000\nar2       -0.2352 0.1251 -1.8803  0.0620\nar3       -0.3924 0.1162 -3.3768  0.0009\nar4        0.3603 0.0757  4.7621  0.0000\nma1       -0.6966 0.1037 -6.7199  0.0000\nsma1      -0.5922 0.0807 -7.3380  0.0000\nconstant   0.0215 0.0173  1.2380  0.2177\n\n$AIC\n[1] -0.8103599\n\n$AICc\n[1] -0.8055721\n\n$BIC\n[1] -0.6546276\n\n\n\n\n\nA smaller p-value indicates that a coefficient is more statistically significant. Looking at the output provided, the second model SARMIA (4,1,1)(0,0,1)[12] appears to have lower AIC and BIC value and when comparing p-values for all of the coefficients, which suggests that it is a better model in terms of statistical significance.\nBy analyzing the standardized residuals plot for the model (SARIMA(4,1,1)(0,0,1)[12]), it can be observed that the mean is close to 0 and the variance is slightly higher than 1. Deviations from these values could indicate poor model fit. However, in this case, the model seems to be well-fitted. The ACF plot of residuals shows very low significant lags, which is a high sign for the model. The qq-plot also suggests high normality in the residuals. Additionally, the p-values for the Ljung-Box test are near than 0.05, indicating that the residuals are independent, which is favorable for the model’s accuracy.\nThe equation for this model: \\[\\Delta Y_t = \\phi_1 \\Delta Y_{t-1} + \\phi_2 \\Delta Y_{t-2} + \\phi_3 \\Delta Y_{t-3} + \\phi_4 \\Delta Y_{t-4} + \\theta_1 \\epsilon_{t-1} + \\epsilon_t + \\theta_{12} \\epsilon_{t-12}\\]\n\n\nForcast\n\nShort Term ForcastLong term Forcast\n\n\n\n\nCode\nmyts %>%\n  Arima(order=c(4,1,1),seasonal = c(0,0,1),include.drift = TRUE) %>%\n  forecast %>%\n  autoplot(main = \"Inflation Rate Prediction\") +\n  ylab(\"Inflation Rate\") + xlab(\"Year\")\n\n\n\n\n\n\n\n\n\nCode\nsarima.for(myts,36, 4,1,1,0,0,1,12, main = \"Inflation Rate Prediction\")\n\n\n\n\n\n$pred\n          Jan      Feb      Mar      Apr      May      Jun      Jul      Aug\n2023                   5.495776 5.389748 5.344802 5.205688 5.249631 5.164470\n2024 5.283954 5.283987 5.290514 5.314089 5.337350 5.358570 5.373114 5.386553\n2025 5.476534 5.494819 5.513071 5.531901 5.551331 5.571139 5.591010 5.610839\n2026 5.711529 5.732022                                                      \n          Sep      Oct      Nov      Dec\n2023 5.064247 5.138831 5.273497 5.297473\n2024 5.400955 5.418630 5.437955 5.457675\n2025 5.630673 5.650633 5.670779 5.691099\n2026                                    \n\n$se\n           Jan       Feb       Mar       Apr       May       Jun       Jul\n2023                     0.1506640 0.2690050 0.3827598 0.4595446 0.5219644\n2024 0.9329297 1.0004244 1.0394805 1.0695190 1.0948899 1.1240493 1.1549882\n2025 1.3042817 1.3255537 1.3460871 1.3656904 1.3844121 1.4024993 1.4201440\n2026 1.5180451 1.5331652                                                  \n           Aug       Sep       Oct       Nov       Dec\n2023 0.5789069 0.6431488 0.7142951 0.7894332 0.8627722\n2024 1.1860724 1.2138042 1.2384755 1.2609465 1.2827317\n2025 1.4374323 1.4543438 1.4708448 1.4869326 1.5026483\n2026                                                  \n\n\n\n\n\nThe two figures above show an idea of the short term and the long-term forecasted Inflation Rate. The forecasted values is be based on the estimated parameters of the model, which were obtained by fitting it to the historical data. One way to assess the accuracy of the forecasts is to use cross-validation, which involves splitting the data into training and testing sets and comparing the predicted values to the actual values in the test set. This can help to identify any potential problems with the model and to fine-tune the model parameters.\n\n12 Step ahead Cross Validation1 Step ahead Cross Validation\n\n\n\n\nCode\n#a seasonal cross validation using 12 steps ahead forecasts\nfarima1 <- function(x, h){forecast(Arima(x, order=c(4,1,1),seasonal = c(0,0,1)), h=h)}\n\n# Compute cross-validated errors for up to 12 steps ahead\ne <- tsCV(myts, forecastfunction = farima1, h = 12)\n\n# Compute the MSE values and remove missing values\nmse <- colMeans(e^2, na.rm = TRUE)\n\n# Plot the MSE values against the forecast horizon\ndata.frame(h = 1:12, MSE = mse) %>%\n  ggplot(aes(x = h, y = MSE)) + geom_point()+geom_line()+ggtitle(\"12 Step Ahead Cross Validation\")\n\n\n\n\n\n\n\n\n\nCode\n#a seasonal cross validation using 1 steps ahead forecasts\nfarima1 <- function(x, h){forecast(Arima(x, order=c(4,1,1),seasonal = c(0,0,1)),h=h)}\n\n# Compute cross-validated errors for up to 1 steps ahead\ne <- tsCV(myts, forecastfunction = farima1, h = 1)\nmse <-abs(mean(e,na.rm=TRUE))\n\n# Plot the MSE values against the forecast horizon\ndata.frame(h = 1:12, MSE = mse) %>%\n  ggplot(aes(x = h, y = MSE)) + geom_point()+geom_line()+ggtitle(\"1 Step Ahead Cross Validation\")\n\n\n\n\n\n\n\n\nThe one-step-ahead forecasting plot shows that the MSE seems to be stable. The MSE is tend to rise by 12 steps at each step of cross-validation. Lastly, the above-mentioned ARIMA model should be compared to benchmark methods before being used for a long time.\n\n\nModel Comparison\n\nPlotModel Error Table\n\n\n\n\nCode\nfit <- Arima(myts, order=c(4,1,1),seasonal = c(0,0,1))\nautoplot(myts) +\n  autolayer(meanf(myts, h=36),\n            series=\"Mean\", PI=FALSE) +\n  autolayer(naive(myts, h=36),\n            series=\"Naïve\", PI=FALSE) +\n  autolayer(snaive(myts, h=36),\n            series=\"SNaïve\", PI=FALSE)+\n  autolayer(rwf(myts, h=36, drift=TRUE),\n            series=\"Drift\", PI=FALSE)+\n  autolayer(forecast(fit,36), \n            series=\"fit\",PI=FALSE) +\n  guides(colour=guide_legend(title=\"Forecast\"))\n\n\n\n\n\n\n\n\n\nCode\nsummary <- summary(fit)\nsnaive <- snaive(myts, h=36)\n#accuracy(snaive)\n\n\n\n\n\nError\nModel\nSnavie\n\n\n\n\nME\n0.01203582\n0.4178082\n\n\nRMSE\n0.1508245\n1.215031\n\n\nMAE\n0.1068308\n0.7410959\n\n\nMPE\n0.3265318\n6.891819\n\n\nMAPE\n5.342487\n26.54941\n\n\nMASE\n0.1441525\n1.0000000\n\n\nACF1\n-0.0333594\n0.9389579\n\n\n\n\n\n\nThe ARIMA forecast tracks the actual data points very closely when compared to other model. The other forecast methods are less accurate when compared to other model.The trend is visible in the fitted model, which indicates it is a good fit. From the table, Model error measurements of fit are much lower than snaive method.We can conclude that the fitted model is good."
  },
  {
    "objectID": "arima_macroeconomic_interest.html",
    "href": "arima_macroeconomic_interest.html",
    "title": "SARIMA Model for Interest Rate",
    "section": "",
    "text": "From the exploratory data analysis (EDA), it may be observed that the raw data for certain macroeconomic factors, such as interest rates or stock prices, is non-stationary and requires differencing to achieve stationarity. By differencing the data, we remove the trend and other non-stationary components, making it easier to model and forecast accurately.\n\nDifferentiated Time Series Plot\n\nPlotACFPACFADF TestSeasonal ACF PlotSeasonal PACF Ploy\n\n\n\n\nCode\n#import the data\ndf <- read.csv(\"DATA/CLEANED DATA/interest_rate_clean_data.csv\")\n#convert to ts data\nmyts<-ts(df$value,frequency=12,start=c(2010/1/1))\n#First order differentiation\ndf1 <- diff(myts)\n# Plot \nmyts  %>% diff() %>% ggtsdisplay(main = \"First order differentiation\") \n\n\n\n\n\n\n\n\n\nCode\n#ACF plot \nggAcf(df1,36,main=\"ACF Plot: First order differentiation\") \n\n\n\n\n\n\n\n\n\nCode\nggPacf(df1,60,main=\"PACF Plot: First order differentiation\") \n\n\n\n\n\n\n\n\n\nCode\ntseries::adf.test(df1)\n\n\n\n    Augmented Dickey-Fuller Test\n\ndata:  df1\nDickey-Fuller = -4.1249, Lag order = 5, p-value = 0.01\nalternative hypothesis: stationary\n\n\n\n\n\n\nCode\n# Seasonal differencing (period = 12)\ndf1_seasonal = myts %>%diff(differences = 1, lag = 12)\nggAcf(df1_seasonal,main=\"ACF Plot: Seasonality\")\n\n\n\n\n\n\n\n\n\nCode\n# Seasonal differencing (period = 12)\nggPacf(df1_seasonal,main=\"PACF Plot: Seasonality\")\n\n\n\n\n\n\n\n\nAfter analyzing the ACF and PACF plots, it can be observed that most of the bar lines lie between the blue lines, indicating that the time series is stationary. This has been further confirmed through an Augmented Dickey-Fuller test as the p-value is less than 0.05. Once the data is stationary and there is presence of seasonality, the next step is to model it using SARIMA. The combination of ACF and PACF plots can help determine the appropriate values for p, d, and q in the ARIMA model. The order of differencing required to achieve stationarity gives the d value. The p value is determined by examining significant spikes at various lags in the PACF plot, while the q value is determined through significant spikes in the ACF plot. P and Q are determined similarly, from seasonal plot.\nHere the parameters are d = 1 p = 0 (PACF Plot) q = 0 (ACF Plot) P = 1 (PACF Seasonality Plot) Q = 1,2,3,4 (ACF Seasonality Plot) D = 1\n\n\nModel Selection\n\nSARIMA ResultAuto Arima Model\n\n\n\n\nCode\n######################## Check for different combinations ########\n\n\n#write a funtion\nSARIMA.c=function(p1,q1,P1,P2,Q1,Q2,data){\n  \n  #K=(p2+1)*(q2+1)*(P2+1)*(Q2+1)\n  \n  temp=c()\n  d=1\n  D=1\n  s=12\n  \n  i=1\n  temp= data.frame()\n  ls=matrix(rep(NA,9*7),nrow=7)\n  \n  \n  for (p in p1)\n  {\n    for(q in q1)\n    {\n      for(P in P1:P2)\n      {\n        for(Q in Q1:Q2)\n        {\n          if(p+d+q+P+D+Q<=9)\n          {\n            \n            model<- Arima(data,order=c(p-1,d,q-1),seasonal=c(P-1,D,Q-1))\n            ls[i,]= c(p-1,d,q-1,P-1,D,Q-1,model$aic,model$bic,model$aicc)\n            i=i+1\n            #print(i)\n            \n          }\n          \n        }\n      }\n    }\n    \n  }\n\n  \n  temp= as.data.frame(ls)\n  names(temp)= c(\"p\",\"d\",\"q\",\"P\",\"D\",\"Q\",\"AIC\",\"BIC\",\"AICc\")\n  \n  temp\n  \n}\n# q=0,; Q=0,1,2,3,4 and PACF plot: p=0; P=0,1, D=0 and d=O\noutput=SARIMA.c(p1=1,q1=1,P1=1,P2=2,Q1=1,Q2=5,data=myts)\n#output\nknitr::kable(output)\n\n\n\n\n\np\nd\nq\nP\nD\nQ\nAIC\nBIC\nAICc\n\n\n\n\n0\n1\n0\n0\n1\n0\n-38.60554\n-35.62193\n-38.57776\n\n\n0\n1\n0\n0\n1\n1\n-61.11557\n-55.14836\n-61.03165\n\n\n0\n1\n0\n0\n1\n2\n-59.78470\n-50.83388\n-59.61569\n\n\n0\n1\n0\n0\n1\n3\n-60.70802\n-48.77359\n-60.42433\n\n\n0\n1\n0\n1\n1\n0\n-57.49539\n-51.52817\n-57.41147\n\n\n0\n1\n0\n1\n1\n1\n-60.79010\n-51.83928\n-60.62109\n\n\n0\n1\n0\n1\n1\n2\n-60.05246\n-48.11803\n-59.76877\n\n\n\n\n\n\n\n\n\nCode\nauto.arima(myts)\n\n\nSeries: myts \nARIMA(1,0,0)(1,0,0)[12] with non-zero mean \n\nCoefficients:\n         ar1    sar1    mean\n      0.9548  0.2623  0.8646\ns.e.  0.0267  0.0940  0.3954\n\nsigma^2 = 0.03157:  log likelihood = 48.82\nAIC=-89.64   AICc=-89.38   BIC=-77.36\n\n\n\n\n\nSorting the table reveals that the AIC and BIC with the parameters (1,1,0) and seasonal parameters are the least (0,1,1). There is also a function that enables the automatic selection of an ARIMA model. According to R’s auto.arima technique, the model’s parameters should be (1,0,0) and (1,0,0).\nSince there are different models to choose from, it is important to perform model diagnostics to determine the best model for the data. These diagnostics can include checking for residual normality, checking for autocorrelation and partial autocorrelation in the residuals, and comparing the forecasted values to the actual values. By conducting these tests, we can determine which model provides the best fit for the data and is therefore the most appropriate to use for forecasting future values.\n\n\nModel Diagnostic\n\nModel 1 PlotModel 1Model 2 PlotModel 2\n\n\n\n\nCode\nmodel_output <- capture.output(sarima(myts, 1,1,0,0,1,1,12))\n\n\n\n\n\n\n\n\n\nCode\ncat(model_output[18:48], model_output[length(model_output)], sep = \"\\n\") \n\n\n$fit\n\nCall:\narima(x = xdata, order = c(p, d, q), seasonal = list(order = c(P, D, Q), period = S), \n    include.mean = !no.constant, transform.pars = trans, fixed = fixed, optim.control = list(trace = trc, \n        REPORT = 1, reltol = tol))\n\nCoefficients:\n          ar1     sma1\n      -0.0786  -0.5590\ns.e.   0.0869   0.1293\n\nsigma^2 estimated as 0.03614:  log likelihood = 32.97,  aic = -59.93\n\n$degrees_of_freedom\n[1] 144\n\n$ttable\n     Estimate     SE t.value p.value\nar1   -0.0786 0.0869 -0.9045  0.3673\nsma1  -0.5590 0.1293 -4.3227  0.0000\n\n$AIC\n[1] -0.4104909\n\n$AICc\n[1] -0.4099161\n\n$BIC\n[1] -0.3491839\n\n\n\n\n\n\nCode\nmodel_output <- capture.output(sarima(myts, 1,0,0,1,0,0,12))\n\n\n\n\n\n\n\n\n\nCode\ncat(model_output[60:91], model_output[length(model_output)], sep = \"\\n\") \n\n\n$fit\n\nCall:\narima(x = xdata, order = c(p, d, q), seasonal = list(order = c(P, D, Q), period = S), \n    xreg = xmean, include.mean = FALSE, transform.pars = trans, fixed = fixed, \n    optim.control = list(trace = trc, REPORT = 1, reltol = tol))\n\nCoefficients:\n         ar1    sar1   xmean\n      0.9548  0.2623  0.8646\ns.e.  0.0267  0.0940  0.3954\n\nsigma^2 estimated as 0.03097:  log likelihood = 48.82,  aic = -89.64\n\n$degrees_of_freedom\n[1] 156\n\n$ttable\n      Estimate     SE t.value p.value\nar1     0.9548 0.0267 35.7690  0.0000\nsar1    0.2623 0.0940  2.7898  0.0059\nxmean   0.8646 0.3954  2.1866  0.0303\n\n$AIC\n[1] -0.5637746\n\n$AICc\n[1] -0.5628008\n\n$BIC\n[1] -0.4865695\n\n\n\n\n\nLooking at the p-values of the coefficients in the two models, we can see that all the coefficients in the second model have p-values less than 0.05, indicating that they are statistically significant at the 5% level. In contrast, the p-value of the AR(1) coefficient in the first model is 0.3673, indicating that it is not statistically significant at the 5% level.Therefore, based on the p-values alone, the first model with SARIMA(1,1,0)(0,1,1)[12] appears to be the better model.\nBy analyzing the standardized residuals plot for the model (SARIMA(1,1,0)(0,1,1)[12]), it can be observed that the mean is close to 0 and the variance is higher than 1. Deviations from these values could indicate poor model fit. However, in this case, the model seems to be well-fitted. The ACF plot of residuals shows no significant lags, which is a high sign for the model. The qq-plot also suggests high normality in the residuals. Additionally, the p-values for the Ljung-Box test are near than 0.05, indicating that the residuals are independent, which is favorable for the model’s accuracy.\nThe equation for this model: \\[ (1-\\phi_1B)(1-B^{12})(1-\\Phi_1B^{12}))y_t = \\epsilon_t \\]\n\n\nForcast\n\nShort Term ForcastLong term Forcast\n\n\n\n\nCode\nmyts %>%\n  Arima(order=c(1,1,0), seasonal = c(0,1,1),include.drift = TRUE) %>%\n  forecast %>%\n  autoplot(main = \"Interest Rate Prediction\") +\n  ylab(\"Interest Rate\") + xlab(\"Year\")\n\n\n\n\n\n\n\n\n\nCode\nsarima.for(myts,36, 1,1,0,0,1,1,12, main = \"Interest Rate Prediction\")\n\n\n\n\n\n$pred\n          Jan      Feb      Mar      Apr      May      Jun      Jul      Aug\n2023                            2.282059 2.409382 2.295342 2.289331 2.117888\n2024 2.704908 2.565338 2.802065 3.054416 3.179239 3.065396 3.059369 2.887927\n2025 3.474948 3.335377 3.572105 3.824456 3.949279 3.835436 3.829409 3.657967\n2026 4.244988 4.105417 4.342144                                             \n          Sep      Oct      Nov      Dec\n2023 2.273762 2.634956 2.731161 2.557040\n2024 3.043801 3.404995 3.501201 3.327079\n2025 3.813841 4.175035 4.271240 4.097119\n2026                                    \n\n$se\n           Jan       Feb       Mar       Apr       May       Jun       Jul\n2023                               0.1901105 0.2585052 0.3129230 0.3591436\n2024 0.5615857 0.5885951 0.6144183 0.6672023 0.7137387 0.7575948 0.7990346\n2025 1.0126712 1.0440358 1.0744853 1.1263407 1.1740351 1.2200070 1.2642976\n2026 1.5028805 1.5390527 1.5743941                                        \n           Aug       Sep       Oct       Nov       Dec\n2023 0.4000625 0.4371678 0.4713612 0.5032366 0.5332099\n2024 0.8384296 0.8760547 0.9121292 0.9468302 0.9803037\n2025 1.3070892 1.3485235 1.3887222 1.4277895 1.4658159\n2026                                                  \n\n\n\n\n\nThe two figures above show an idea of the short term and the long-term forecasted Interest Rate. The forecasted values is be based on the estimated parameters of the model, which were obtained by fitting it to the historical data. One way to assess the accuracy of the forecasts is to use cross-validation, which involves splitting the data into training and testing sets and comparing the predicted values to the actual values in the test set. This can help to identify any potential problems with the model and to fine-tune the model parameters.\n\n12 Step ahead Cross Validation1 Step ahead Cross Validation\n\n\n\n\nCode\n#a seasonal cross validation using 12 steps ahead forecasts\nfarima1 <- function(x, h){forecast(Arima(x, order=c(1,1,0),seasonal = c(0,1,1)),lambda = 12, h=h)}\n\n# Compute cross-validated errors for up to 12 steps ahead\ne <- tsCV(myts, forecastfunction = farima1, h = 12)\n\n# Compute the MSE values and remove missing values\nmse <- colMeans(e^2, na.rm = TRUE)\n\n# Plot the MSE values against the forecast horizon\ndata.frame(h = 1:12, MSE = mse) %>%\n  ggplot(aes(x = h, y = MSE)) + geom_point()+geom_line()+ggtitle(\"12 Step Ahead Cross Validation\")\n\n\n\n\n\n\n\n\n\nCode\n#a seasonal cross validation using 1 steps ahead forecasts\nfarima1 <- function(x, h){forecast(Arima(x, order=c(1,1,0),seasonal = c(0,1,1)),lambda = 12,h=h)}\n\n# Compute cross-validated errors for up to 1 steps ahead\ne <- tsCV(myts, forecastfunction = farima1, h = 1)\nmse <-abs(mean(e,na.rm=TRUE))\n\n# Plot the MSE values against the forecast horizon\ndata.frame(h = 1:12, MSE = mse) %>%\n  ggplot(aes(x = h, y = MSE)) + geom_point()+geom_line()+ggtitle(\"1 Step Ahead Cross Validation\")\n\n\n\n\n\n\n\n\nThe one-step-ahead forecasting plot shows that the MSE seems to be stable. The MSE is tend to rise by 12 steps at each step of cross-validation. Lastly, the above-mentioned ARIMA model should be compared to benchmark methods before being used for a long time.\n\n\nModel Comparison\n\nPlotModel Error Table\n\n\n\n\nCode\nfit <- Arima(myts, order=c(1,1,0),seasonal = c(0,1,1),lambda = 12)\nautoplot(myts) +\n  autolayer(meanf(myts, h=36),\n            series=\"Mean\", PI=FALSE) +\n  autolayer(naive(myts, h=36),\n            series=\"Naïve\", PI=FALSE) +\n  autolayer(snaive(myts, h=36),\n            series=\"SNaïve\", PI=FALSE)+\n  autolayer(rwf(myts, h=36, drift=TRUE),\n            series=\"Drift\", PI=FALSE)+\n  autolayer(forecast(fit,36), \n            series=\"fit\",PI=FALSE) +\n  guides(colour=guide_legend(title=\"Forecast\"))\n\n\n\n\n\n\n\n\n\nCode\nsummary <- summary(fit)\nsnaive <- snaive(myts, h=36)\n#accuracy(snaive)\n\n\n\n\n\nError\nModel\nSnavie\n\n\n\n\nME\n0.01857939\n0.04380615\n\n\nRMSE\n0.1821734\n0.584916\n\n\nMAE\n0.1288216\n0.4637065\n\n\nMPE\n8.065525\n54.48896\n\n\nMAPE\n101.6013\n232.775\n\n\nMASE\n0.2778084\n1.0000000\n\n\nACF1\n-0.01083644\n0.9050999\n\n\n\n\n\n\nThe ARIMA forecast doesn’t track the actual data points very closely. The other forecast methods are less accurate when compared to other model.The trend is visible in the fitted model, which indicates it is a good fit. From the table, Model error measurements of fit are much lower than snaive method.We can conclude that the fitted model is good."
  },
  {
    "objectID": "arima_macroeconomic_unemployment.html",
    "href": "arima_macroeconomic_unemployment.html",
    "title": "ARIMA Model for Unemployment Rate",
    "section": "",
    "text": "During exploratory data analysis (EDA), it was observed that the raw data is often non-stationary, exhibiting trends, seasonality, and other complex patterns that can make modeling and forecasting difficult. To address this issue, the data is often differentiated to achieve stationarity, which is a key assumption of the ARIMA model.\nDifferencing involves calculating the difference between each data point and the previous data point, which can help to remove trends and seasonality from the data. The goal is to create a new series of data that is stationary, meaning that the statistical properties of the data do not change over time.\n\nDifferentiated Time Series Plot\n\nPlotACFPACFADF Test\n\n\n\n\nCode\n#import the data\ndf <- read.csv(\"DATA/CLEANED DATA/unemployment_rate_clean_data.csv\")\n#convert the data to ts data\nmyts<-ts(df$Value,frequency=12,start=c(2010/1/1))\n#First order differentiation\ndf1 <- diff(myts)\n# Plot \nmyts  %>% diff() %>% ggtsdisplay(main = \"First order differentiation\") \n\n\n\n\n\n\n\n\n\nCode\n#ACF plot \nggAcf(df1,36,main=\"ACF Plot: First order differentiation\") \n\n\n\n\n\n\n\n\n\nCode\nggPacf(df1,36,main=\"PACF Plot: First order differentiation\") \n\n\n\n\n\n\n\n\n\nCode\ntseries::adf.test(df1)\n\n\n\n    Augmented Dickey-Fuller Test\n\ndata:  df1\nDickey-Fuller = -6.457, Lag order = 5, p-value = 0.01\nalternative hypothesis: stationary\n\n\n\n\n\nAfter analyzing the ACF and PACF plots, it can be observed that most of the bar lines lie between the blue lines, indicating that the time series is stationary. This has been further confirmed through an Augmented Dickey-Fuller test as the p-value is less than 0.05. Once the data is stationary, the next step is to model it using ARIMA. The combination of ACF and PACF plots can help determine the appropriate values for p, d, and q in the ARIMA model. The order of differencing required to achieve stationarity gives the d value. The p value is determined by examining significant spikes at various lags in the PACF plot, while the q value is determined through significant spikes in the ACF plot. It is important to consider other methods such as grid search and information criteria to ensure the most appropriate model is selected.\nHere the parameters are d = 1 p = 0 (PACF Plot) q = 0 (ACF Plot)\n\n\nModel Selection\n\nARIMA ResultAuto Arima Model\n\n\n\n\nCode\nmodel <- arima(myts,c(0,1,0))\nmodel\n\n\n\nCall:\narima(x = myts, order = c(0, 1, 0))\n\n\nsigma^2 estimated as 0.7845:  log likelihood = -203.72,  aic = 409.43\n\n\n\n\n\n\nCode\nauto.arima(myts)\n\n\nSeries: myts \nARIMA(0,1,0) \n\nsigma^2 = 0.7845:  log likelihood = -203.72\nAIC=409.43   AICc=409.46   BIC=412.49\n\n\n\n\n\nThe given table provides different ARIMA models with their corresponding AIC and BIC values. By sorting the table according to each criterion, we can identify the models with the lowest values. The smallest AIC value and BIC value corresponds to an ARIMA (0,1,0) model. Additionally, the auto.arima function in R suggests an ARIMA (0,1,0) model as the best fit for the data. Since the model parameter are the same, we can proceed with model diagnostic for the parameters.\n\n\nModel Diagnostic\n\nModel PlotModel 1Residual\n\n\n\n\nCode\nmodel_output <- capture.output(sarima(myts, 1,0,1))\n\n\n\n\n\n\n\n\n\nCode\ncat(model_output[62:93], model_output[length(model_output)], sep = \"\\n\") \n\n\n$fit\n\nCall:\narima(x = xdata, order = c(p, d, q), seasonal = list(order = c(P, D, Q), period = S), \n    xreg = xmean, include.mean = FALSE, transform.pars = trans, fixed = fixed, \n    optim.control = list(trace = trc, REPORT = 1, reltol = tol))\n\nCoefficients:\n         ar1     ma1   xmean\n      0.9156  0.0771  6.1460\ns.e.  0.0355  0.0950  0.8277\n\nsigma^2 estimated as 0.7549:  log likelihood = -202.96,  aic = 413.93\n\n$degrees_of_freedom\n[1] 155\n\n$ttable\n      Estimate     SE t.value p.value\nar1     0.9156 0.0355 25.7721  0.0000\nma1     0.0771 0.0950  0.8118  0.4182\nxmean   6.1460 0.8277  7.4254  0.0000\n\n$AIC\n[1] 2.619783\n\n$AICc\n[1] 2.620769\n\n$BIC\n[1] 2.697317\n\n\n\n\n\n\nCode\narima <- auto.arima(myts)\ncheckresiduals(arima)\n\n\n\n\n\n\n    Ljung-Box test\n\ndata:  Residuals from ARIMA(0,1,0)\nQ* = 7.2289, df = 24, p-value = 0.9996\n\nModel df: 0.   Total lags used: 24\n\n\n\n\n\nThe best model is ARIMA(1,0,1). By analyzing the standardized residuals plot, it can be observed that the mean is close to 0 and the variance is slightly higher than 1. Deviations from these values could indicate poor model fit. However, in this case, the model seems to be well-fitted. The ACF plot of residuals shows no significant lags, which is a positive sign for the model. The qq-plot also suggests high normality in the residuals. Additionally, the p-values for the Ljung-Box test are near than 0, indicating that the residuals are independent, which is favorable for the model’s accuracy.\nThe equation for this model: \\[\\Delta Y_t = \\phi_1 \\Delta Y_{t-1} + \\theta_1 \\epsilon_{t-1} + \\epsilon_t\\]\n\n\nForcast\n\nShort Term ForcastLong term Forcast\n\n\n\n\nCode\nmyts %>%\n  Arima(order=c(1,0,1),include.drift = TRUE) %>%\n  forecast %>%\n  autoplot(main = \"Unemployment Rate Prediction\") +\n  ylab(\"Unemployment Rate\") + xlab(\"Year\")\n\n\n\n\n\n\n\n\n\nCode\nsarima.for(myts,36, 1,0,1, main = \"Unemployment Rate Prediction\")\n\n\n\n\n\n$pred\n          Jan      Feb      Mar      Apr      May      Jun      Jul      Aug\n2023                   3.814151 4.010889 4.191028 4.355968 4.506993 4.645275\n2024 5.180167 5.261654 5.336266 5.404583 5.467136 5.524412 5.576855 5.624873\n2025 5.810614 5.838911 5.864819 5.888542 5.910264 5.930153 5.948364 5.965038\n2026 6.029536 6.039362                                                      \n          Sep      Oct      Nov      Dec\n2023 4.771891 4.887824 4.993976 5.091172\n2024 5.668841 5.709098 5.745959 5.779711\n2025 5.980306 5.994285 6.007085 6.018805\n2026                                    \n\n$se\n           Jan       Feb       Mar       Apr       May       Jun       Jul\n2023                     0.8688527 1.2243105 1.4569544 1.6265546 1.7561668\n2024 2.1374499 2.1671027 2.1916538 2.2120269 2.2289639 2.2430649 2.2548189\n2025 2.2941710 2.2975230 2.3003295 2.3026798 2.3046483 2.3062974 2.3076791\n2026 2.3123513 2.3127525                                                  \n           Aug       Sep       Oct       Nov       Dec\n2023 1.8578756 1.9390393 2.0045542 2.0578738 2.1015336\n2024 2.2646262 2.2728158 2.2796591 2.2853807 2.2901664\n2025 2.3088368 2.3098070 2.3106200 2.3113014 2.3118726\n2026                                                  \n\n\n\n\n\nThe two figures above show an idea of the short term and the long-term forecasted unemployment rate. The forecasted values is be based on the estimated parameters of the model, which were obtained by fitting it to the historical data. One way to assess the accuracy of the forecasts is to use cross-validation, which involves splitting the data into training and testing sets and comparing the predicted values to the actual values in the test set. This can help to identify any potential problems with the model and to fine-tune the model parameters.\n\n12 Step ahead Cross Validation1 Step ahead Cross Validation\n\n\n\n\nCode\n#a seasonal cross validation using 12 steps ahead forecasts\nfarima1 <- function(x, h){forecast(Arima(x, order=c(1,0,1)), h=h)}\n\n# Compute cross-validated errors for up to 12 steps ahead\ne <- tsCV(myts, forecastfunction = farima1, h = 12)\n\n# Compute the MSE values and remove missing values\nmse <- colMeans(e^2, na.rm = TRUE)\n\n# Plot the MSE values against the forecast horizon\ndata.frame(h = 1:12, MSE = mse) %>%\n  ggplot(aes(x = h, y = MSE)) + geom_point()+geom_line()+ggtitle(\"12 Step Ahead Cross Validation\")\n\n\n\n\n\n\n\n\n\nCode\n#a seasonal cross validation using 1 steps ahead forecasts\nfarima1 <- function(x, h){forecast(Arima(x, order=c(1,0,1)),h=h)}\n\n# Compute cross-validated errors for up to 1 steps ahead\ne <- tsCV(myts, forecastfunction = farima1, h = 1)\nmse <-abs(mean(e,na.rm=TRUE))\n\n# Plot the MSE values against the forecast horizon\ndata.frame(h = 1:12, MSE = mse) %>%\n  ggplot(aes(x = h, y = MSE)) + geom_point()+geom_line()+ggtitle(\"1 Step Ahead Cross Validation\")\n\n\n\n\n\n\n\n\nThe one-step-ahead forecasting plot shows that the MSE seems to be stable. The MSE is tend to rise by 12 steps at each step of cross-validation. Lastly, the above-mentioned ARIMA model should be compared to benchmark methods before being used for a long time.\n\n\nModel Comparison\n\nPlotModel Error Table\n\n\n\n\nCode\nfit <- Arima(myts, order=c(1,0,1))\nautoplot(myts) +\n  autolayer(meanf(myts, h=36),\n            series=\"Mean\", PI=FALSE) +\n  autolayer(naive(myts, h=36),\n            series=\"Naïve\", PI=FALSE) +\n  autolayer(snaive(myts, h=36),\n            series=\"SNaïve\", PI=FALSE)+\n  autolayer(rwf(myts, h=36, drift=TRUE),\n            series=\"Drift\", PI=FALSE)+\n  autolayer(forecast(fit,36), \n            series=\"fit\",PI=FALSE) +\n  guides(colour=guide_legend(title=\"Forecast\"))\n\n\n\n\n\n\n\n\n\nCode\nsummary <- summary(fit)\nsnaive <- snaive(myts, h=36)\n#accuracy(snaive)\n\n\n\n\n\nError\nModel\nSnavie\n\n\n\n\nME\n-0.03281026\n0.4958904\n\n\nRMSE\n0.8688527\n2.221162\n\n\nMAE\n0.2861976\n1.330137\n\n\nMPE\n-1.977255\n-13.26146\n\n\nMAPE\n4.412742\n22.30239\n\n\nMASE\n0.215164\n1.0000000\n\n\nACF1\n-0.0139776\n0.819433\n\n\n\n\n\n\nThe benchmark models are all close to each other and showing different trend. From the table, Model error measurements of fit are much lower than snaive method which indicates that it is better fit then snaive method, but further analysis should be done to check on the prediction."
  },
  {
    "objectID": "arima_sector_communication_services.html",
    "href": "arima_sector_communication_services.html",
    "title": "ARIMA Model for Communication Services Sector Fund",
    "section": "",
    "text": "During exploratory data analysis (EDA), it was observed that the raw data is often non-stationary, exhibiting trends, seasonality, and other complex patterns that can make modeling and forecasting difficult. To address this issue, the data is often differentiated to achieve stationarity, which is a key assumption of the ARIMA model.\nDifferencing involves calculating the difference between each data point and the previous data point, which can help to remove trends and seasonality from the data. The goal is to create a new series of data that is stationary, meaning that the statistical properties of the data do not change over time.\n\nDifferentiated Time Series Plot\n::: panel-tabset ##### Plot\n\n\nCode\n#import the data\ndf <- read.csv(\"DATA/CLEANED DATA/XLC_raw_data.csv\")\n#convert to time series data\nmyts<-ts(df$XLC.Adjusted,frequency=252,start=c(2018,6,19), end = c(2023,3,1)) \n#First order differentiation\ndf1 <- diff(myts)\n# Plot \nmyts  %>% diff() %>% ggtsdisplay(main = \"First order differentiation\") \n\n\n\n\n\n\nACFPACFADF Test\n\n\n\n\nCode\n#ACF plot \nggAcf(df1,60,main=\"ACF Plot: First order differentiation\") \n\n\n\n\n\n\n\n\n\nCode\nggPacf(df1,60,main=\"PACF Plot: First order differentiation\") \n\n\n\n\n\n\n\n\n\nCode\ntseries::adf.test(df1)\n\n\n\n    Augmented Dickey-Fuller Test\n\ndata:  df1\nDickey-Fuller = -10.487, Lag order = 10, p-value = 0.01\nalternative hypothesis: stationary\n\n\n\n\n\nAfter analyzing the ACF and PACF plots, it can be observed that most of the bar lines lie between the blue lines, indicating that the time series is stationary. This has been further confirmed through an Augmented Dickey-Fuller test as the p-value is less than 0.05. Once the data is stationary, the next step is to model it using ARIMA. The combination of ACF and PACF plots can help determine the appropriate values for p, d, and q in the ARIMA model. The order of differencing required to achieve stationarity gives the d value. The p value is determined by examining significant spikes at various lags in the PACF plot, while the q value is determined through significant spikes in the ACF plot. It is important to consider other methods such as grid search and information criteria to ensure the most appropriate model is selected.\nHere the parameters are d = 1 p = 0,1,2 (PACF Plot) q = 0,1,2 (ACF Plot)\n\n\nModel Selection\n\nARIMA ResultAuto Arima Model\n\n\n\n\nCode\n######################## Check for different combinations ########\nd=1\ni=1\ntemp = data.frame()\nls=matrix(rep(NA,6*9),nrow=9) #nrow = 3x3x1\n\n\nfor (p in 1:3)# p=0,1,2 :3\n{\n  for(q in 1:3)# q=0,1,2 :3\n  {\n    for(d in 1)# d=1 :1\n    {\n      \n      if(p-1+d+q-1<=8)\n      {\n        \n        model<- Arima(myts,order=c(p-1,d,q-1),include.drift=TRUE) \n        ls[i,]= c(p-1,d,q-1,model$aic,model$bic,model$aicc)\n        i=i+1\n        \n      }\n      \n    }\n  }\n}\n\nmodel = as.data.frame(ls)\nnames(model)= c(\"p\",\"d\",\"q\",\"AIC\",\"BIC\",\"AICc\")\n\n#temp\nknitr::kable(model)\n\n\n\n\n\np\nd\nq\nAIC\nBIC\nAICc\n\n\n\n\n0\n1\n0\n3171.471\n3181.744\n3171.481\n\n\n0\n1\n1\n3163.739\n3179.149\n3163.759\n\n\n0\n1\n2\n3164.010\n3184.556\n3164.042\n\n\n1\n1\n0\n3163.046\n3178.455\n3163.065\n\n\n1\n1\n1\n3163.240\n3183.786\n3163.272\n\n\n1\n1\n2\n3165.106\n3190.789\n3165.154\n\n\n2\n1\n0\n3163.526\n3184.072\n3163.558\n\n\n2\n1\n1\n3165.132\n3190.815\n3165.180\n\n\n2\n1\n2\n3166.632\n3197.451\n3166.699\n\n\n\n\n\n\n\n\n\nCode\nauto.arima(myts)\n\n\nSeries: myts \nARIMA(1,1,0) \n\nCoefficients:\n          ar1\n      -0.0909\ns.e.   0.0281\n\nsigma^2 = 0.7222:  log likelihood = -1578.52\nAIC=3161.05   AICc=3161.06   BIC=3171.32\n\n\n\n\n\nThe given table provides different ARIMA models with their corresponding AIC and BIC values. By sorting the table according to each criterion, we can identify the models with the lowest values. The smallest AIC and BIC value corresponds to an ARIMA (2,1,2) mode. Additionally, the auto.arima function in R suggests an ARIMA (2,1,2) model as the best fit for the data. We can proceed with the further model diagnostic for the chosen best model i.e. (2,1,2), to look into the details of the model.\n\n\nModel Diagnostic\n\nModelModel\n\n\n\n\nCode\nmodel_output <- capture.output(sarima(myts,2,1,2))\n\n\n\n\n\n\n\n\n\nCode\ncat(model_output[201:234], model_output[length(model_output)], sep = \"\\n\") \n\n\nNA\nNA\nNA\nNA\nNA\nNA\nNA\nNA\nNA\nNA\nNA\nNA\nNA\nNA\nNA\nNA\nNA\nNA\nNA\nNA\nNA\nNA\nNA\nNA\nNA\nNA\nNA\nNA\nNA\nNA\nNA\nNA\nNA\nNA\n\n\n\n\n\nModel diagnostic is done for ARIMA(2,1,2). By analyzing the standardized residuals plot, it can be observed that the mean is close to 0 and the variance is slightly higher than 1. Deviations from these values could indicate poor model fit. However, in this case, the model seems to be well-fitted. The ACF plot of residuals shows very few significant lags, which is a positive sign for the model. The qq-plot also suggests normality in the residuals. Additionally, the p-values for the Ljung-Box test are greater than 0.05, indicating that the residuals are independent, which is favorable for the model’s accuracy.\nThe equation for this model: \\[(1-\\phi_1B-\\phi_2B^2)(1-B)(Yt - \\mu) = (1+\\theta_1B+\\theta_2B^2)\\epsilon_t\\]\n\n\nForcast\n\nShort Term ForcastLong term Forcast\n\n\n\n\nCode\nmyts %>%\n  Arima(order=c(2,1,2),include.drift = TRUE) %>%\n  forecast %>%\n  autoplot(main = \"Communication Services Sector Fund Stock Prices Prediction\") +\n  ylab(\"stock prices\") + xlab(\"Year\")\n\n\n\n\n\n\n\n\n\nCode\nsarima.for(myts,182, 2,1,2, main='Communication Services Sector Fund Stock Prices Prediction')\n\n\n\n\n\n$pred\nTime Series:\nStart = c(2023, 4) \nEnd = c(2023, 185) \nFrequency = 252 \n  [1] 46.14665 46.12414 46.13983 46.13609 46.14004 46.13944 46.14021 46.13980\n  [9] 46.13952 46.13887 46.13816 46.13730 46.13637 46.13538 46.13435 46.13328\n [17] 46.13218 46.13107 46.12994 46.12880 46.12765 46.12650 46.12534 46.12418\n [25] 46.12301 46.12185 46.12068 46.11951 46.11834 46.11717 46.11600 46.11483\n [33] 46.11366 46.11249 46.11131 46.11014 46.10897 46.10780 46.10663 46.10546\n [41] 46.10428 46.10311 46.10194 46.10077 46.09960 46.09842 46.09725 46.09608\n [49] 46.09491 46.09373 46.09256 46.09139 46.09022 46.08905 46.08787 46.08670\n [57] 46.08553 46.08436 46.08319 46.08201 46.08084 46.07967 46.07850 46.07733\n [65] 46.07615 46.07498 46.07381 46.07264 46.07146 46.07029 46.06912 46.06795\n [73] 46.06678 46.06560 46.06443 46.06326 46.06209 46.06092 46.05974 46.05857\n [81] 46.05740 46.05623 46.05506 46.05388 46.05271 46.05154 46.05037 46.04919\n [89] 46.04802 46.04685 46.04568 46.04451 46.04333 46.04216 46.04099 46.03982\n [97] 46.03865 46.03747 46.03630 46.03513 46.03396 46.03279 46.03161 46.03044\n[105] 46.02927 46.02810 46.02692 46.02575 46.02458 46.02341 46.02224 46.02106\n[113] 46.01989 46.01872 46.01755 46.01638 46.01520 46.01403 46.01286 46.01169\n[121] 46.01052 46.00934 46.00817 46.00700 46.00583 46.00465 46.00348 46.00231\n[129] 46.00114 45.99997 45.99879 45.99762 45.99645 45.99528 45.99411 45.99293\n[137] 45.99176 45.99059 45.98942 45.98825 45.98707 45.98590 45.98473 45.98356\n[145] 45.98238 45.98121 45.98004 45.97887 45.97770 45.97652 45.97535 45.97418\n[153] 45.97301 45.97184 45.97066 45.96949 45.96832 45.96715 45.96597 45.96480\n[161] 45.96363 45.96246 45.96129 45.96011 45.95894 45.95777 45.95660 45.95543\n[169] 45.95425 45.95308 45.95191 45.95074 45.94957 45.94839 45.94722 45.94605\n[177] 45.94488 45.94370 45.94253 45.94136 45.94019 45.93902\n\n$se\nTime Series:\nStart = c(2023, 4) \nEnd = c(2023, 185) \nFrequency = 252 \n  [1]  0.8486421  1.1449043  1.3905681  1.5882432  1.7642976  1.9213970\n  [7]  2.0659293  2.1999377  2.3257517  2.4446400  2.5577291  2.6658023\n [13]  2.7695106  2.8693594  2.9657680  3.0590804  3.1495866  3.2375323\n [19]  3.3231284  3.4065576  3.4879796  3.5675347  3.6453475  3.7215288\n [25]  3.7961780  3.8693847  3.9412299  4.0117872  4.0811239  4.1493012\n [31]  4.2163758  4.2823994  4.3474202  4.4114825  4.4746276  4.5368937\n [37]  4.5983167  4.6589299  4.7187646  4.7778499  4.8362134  4.8938808\n [43]  4.9508766  5.0072237  5.0629437  5.1180571  5.1725833  5.2265407\n [49]  5.2799467  5.3328179  5.3851700  5.4370180  5.4883763  5.5392584\n [55]  5.5896774  5.6396456  5.6891750  5.7382769  5.7869622  5.8352412\n [61]  5.8831242  5.9306205  5.9777394  6.0244898  6.0708803  6.1169189\n [67]  6.1626136  6.2079719  6.2530013  6.2977087  6.3421009  6.3861845\n [73]  6.4299660  6.4734513  6.5166465  6.5595572  6.6021890  6.6445473\n [79]  6.6866373  6.7284640  6.7700323  6.8113469  6.8524124  6.8932333\n [85]  6.9338139  6.9741583  7.0142707  7.0541550  7.0938151  7.1332547\n [91]  7.1724774  7.2114867  7.2502862  7.2888792  7.3272689  7.3654585\n [97]  7.4034511  7.4412497  7.4788573  7.5162767  7.5535108  7.5905622\n[103]  7.6274336  7.6641277  7.7006469  7.7369937  7.7731706  7.8091799\n[109]  7.8450239  7.8807049  7.9162251  7.9515865  7.9867915  8.0218419\n[115]  8.0567398  8.0914873  8.1260861  8.1605383  8.1948456  8.2290099\n[121]  8.2630329  8.2969164  8.3306621  8.3642717  8.3977468  8.4310889\n[127]  8.4642997  8.4973807  8.5303334  8.5631593  8.5958599  8.6284365\n[133]  8.6608906  8.6932235  8.7254366  8.7575313  8.7895087  8.8213702\n[139]  8.8531171  8.8847505  8.9162717  8.9476818  8.9789821  9.0101736\n[145]  9.0412576  9.0722350  9.1031070  9.1338747  9.1645390  9.1951012\n[151]  9.2255620  9.2559226  9.2861840  9.3163471  9.3464128  9.3763821\n[157]  9.4062559  9.4360352  9.4657208  9.4953135  9.5248144  9.5542241\n[163]  9.5835436  9.6127736  9.6419151  9.6709687  9.6999353  9.7288157\n[169]  9.7576105  9.7863207  9.8149469  9.8434898  9.8719502  9.9003288\n[175]  9.9286263  9.9568433  9.9849807 10.0130389 10.0410187 10.0689208\n[181] 10.0967458 10.1244943\n\n\n\n\n\nThe two figures above show an idea of the short term and the long-term forecasted stock price. The forecasted values is be based on the estimated parameters of the model, which were obtained by fitting it to the historical data. One way to assess the accuracy of the forecasts is to use cross-validation, which involves splitting the data into training and testing sets and comparing the predicted values to the actual values in the test set. This can help to identify any potential problems with the model and to fine-tune the model parameters.\n\n12 Step ahead Cross Validation1 Step ahead Cross Validation\n\n\n\n\nCode\n#a seasonal cross validation using 12 steps ahead forecasts\nfarima1 <- function(x, h){forecast(Arima(x, order=c(2,1,2)), h=h)}\n\n# Compute cross-validated errors for up to 12 steps ahead\ne <- tsCV(myts, forecastfunction = farima1, h = 12)\n\n# Compute the MSE values and remove missing values\nmse <- colMeans(e^2, na.rm = TRUE)\n\n# Plot the MSE values against the forecast horizon\ndata.frame(h = 1:12, MSE = mse) %>%\n  ggplot(aes(x = h, y = MSE)) + geom_point()+geom_line()+ggtitle(\"12 Step Ahead Cross Validation\")\n\n\n\n\n\n\n\n\n\nCode\n#a seasonal cross validation using 1 steps ahead forecasts\nfarima1 <- function(x, h){forecast(Arima(x, order=c(2,1,2)),h=h)}\n\n# Compute cross-validated errors for up to 1 steps ahead\ne <- tsCV(myts, forecastfunction = farima1, h = 1)\nmse <-abs(mean(e,na.rm=TRUE))\n\n# Plot the MSE values against the forecast horizon\ndata.frame(h = 1:12, MSE = mse) %>%\n  ggplot(aes(x = h, y = MSE)) + geom_point()+geom_line()+ggtitle(\"1 Step Ahead Cross Validation\")\n\n\n\n\n\n\n\n\nThe one-step-ahead forecasting plot shows that the MSE seems to be stable. The MSE is tend to rise by 12 steps at each step of cross-validation. Lastly, the above-mentioned ARIMA model should be compared to benchmark methods before being used for a long time.\n\n\nModel Comparison\n\nPlotModel Error Table\n\n\n\n\nCode\nfit <- Arima(myts, order=c(2,1,2))\nautoplot(myts) +\n  autolayer(meanf(myts, h=182),\n            series=\"Mean\", PI=FALSE) +\n  autolayer(naive(myts, h=182),\n            series=\"Naïve\", PI=FALSE) +\n  autolayer(snaive(myts, h=182),\n            series=\"SNaïve\", PI=FALSE)+\n  autolayer(rwf(myts, h=182, drift=TRUE),\n            series=\"Drift\", PI=FALSE)+\n  autolayer(forecast(fit,182), \n            series=\"fit\",PI=FALSE) +\n  guides(colour=guide_legend(title=\"Forecast\"))\n\n\n\n\n\n\n\n\n\nCode\nsummary <- summary(fit)\nsnaive <- snaive(myts, h=182)\naccuracy(snaive)\n\n\n                   ME     RMSE      MAE       MPE    MAPE MASE      ACF1\nTraining set 1.356329 17.34553 14.68822 -1.229349 24.7035    1 0.9972269\n\n\nCode\nsummary\n\n\nSeries: myts \nARIMA(2,1,2) \n\nCoefficients:\n         ar1     ar2      ma1      ma2\n      0.2870  0.3467  -0.3814  -0.2952\ns.e.  0.3431  0.1725   0.3431   0.1641\n\nsigma^2 = 0.7225:  log likelihood = -1577.32\nAIC=3164.63   AICc=3164.68   BIC=3190.32\n\nTraining set error measures:\n                       ME      RMSE       MAE         MPE     MAPE       MASE\nTraining set -0.001229776 0.8483069 0.5992102 -0.01655992 1.087716 0.04079529\n                    ACF1\nTraining set 0.006457781\n\n\n\n\n\nError\nModel\nSnavie\n\n\n\n\nME\n0.01010937\n1.761967\n\n\nRMSE\n0.8192804\n18.67951\n\n\nMAE\n0.5816893\n15.71119\n\n\nMPE\n0.005100481\n-1.616813\n\n\nMAPE\n1.084491\n27.61046\n\n\nMASE\n0.03702389\n1.0000000\n\n\nACF1\n-0.001399621\n0.9976193\n\n\n\n\n\n\nThe ARIMA forecast tracks the actual data points very closely when compared to other model. The other forecast methods are less accurate when compared. From the table, Model error measurements of fit are much lower than snaive method.We can conclude that the fitted model is good."
  },
  {
    "objectID": "arima_sector_consumer_discretionary.html",
    "href": "arima_sector_consumer_discretionary.html",
    "title": "ARIMA Model for Consumer Discretionary Sector Fund",
    "section": "",
    "text": "During exploratory data analysis (EDA), it was observed that the raw data is often non-stationary, exhibiting trends, seasonality, and other complex patterns that can make modeling and forecasting difficult. To address this issue, the data is often differentiated to achieve stationarity, which is a key assumption of the ARIMA model.\nDifferencing involves calculating the difference between each data point and the previous data point, which can help to remove trends and seasonality from the data. The goal is to create a new series of data that is stationary, meaning that the statistical properties of the data do not change over time.\n\nDifferentiated Time Series Plot\n\nPlotACFPACFADF Test\n\n\n\n\nCode\n#import the data\ndf <- read.csv(\"DATA/CLEANED DATA/XLY_raw_data.csv\")\n#convert to time series data\nmyts<-ts(df$XLY.Adjusted,frequency=252,start=c(2010,1,1), end = c(2023,3,1)) \n#First order differentiation\ndf1 <- diff(myts)\n# Plot \nmyts  %>% diff() %>% ggtsdisplay(main = \"First order differentiation\") \n\n\n\n\n\n\n\n\n\nCode\n#ACF plot \nggAcf(df1,60,main=\"ACF Plot: First order differentiation\") \n\n\n\n\n\n\n\n\n\nCode\nggPacf(df1,60,main=\"PACF Plot: First order differentiation\") \n\n\n\n\n\n\n\n\n\nCode\ntseries::adf.test(df1)\n\n\n\n    Augmented Dickey-Fuller Test\n\ndata:  df1\nDickey-Fuller = -14.88, Lag order = 14, p-value = 0.01\nalternative hypothesis: stationary\n\n\n\n\n\nAfter analyzing the ACF and PACF plots, it can be observed that most of the bar lines lie between the blue lines, indicating that the time series is stationary. This has been further confirmed through an Augmented Dickey-Fuller test as the p-value is less than 0.05. Once the data is stationary, the next step is to model it using ARIMA. The combination of ACF and PACF plots can help determine the appropriate values for p, d, and q in the ARIMA model. The order of differencing required to achieve stationarity gives the d value. The p value is determined by examining significant spikes at various lags in the PACF plot, while the q value is determined through significant spikes in the ACF plot. It is important to consider other methods such as grid search and information criteria to ensure the most appropriate model is selected.\nHere the parameters are d = 1 p = 0,10 (PACF Plot) q = 0,10 (ACF Plot)\n\n\nModel Selection\n\nARIMA(0,1,0) ResultARIMA(10,1,0) ResultARIMA(0,1,10) ResultARIMA(10,1,10) ResultAuto Arima Model\n\n\n\n\nCode\nArima(myts,order=c(0,1,0),include.drift=TRUE) \n\n\nSeries: myts \nARIMA(0,1,0) with drift \n\nCoefficients:\n       drift\n      0.0341\ns.e.  0.0244\n\nsigma^2 = 1.948:  log likelihood = -5743.71\nAIC=11491.42   AICc=11491.43   BIC=11503.61\n\n\n\n\n\n\nCode\nArima(myts,order=c(10,1,0),include.drift=TRUE) \n\n\nSeries: myts \nARIMA(10,1,0) with drift \n\nCoefficients:\n          ar1     ar2      ar3      ar4     ar5      ar6     ar7      ar8\n      -0.0082  0.0387  -0.0221  -0.0175  0.0059  -0.0478  0.0702  -0.0725\ns.e.   0.0175  0.0174   0.0174   0.0173  0.0173   0.0173  0.0173   0.0174\n         ar9     ar10   drift\n      0.0895  -0.0418  0.0341\ns.e.  0.0174   0.0175  0.0239\n\nsigma^2 = 1.896:  log likelihood = -5694.26\nAIC=11412.52   AICc=11412.61   BIC=11485.66\n\n\n\n\n\n\nCode\nArima(myts,order=c(0,1,10),include.drift=TRUE) \n\n\nSeries: myts \nARIMA(0,1,10) with drift \n\nCoefficients:\n          ma1     ma2      ma3      ma4     ma5      ma6     ma7      ma8\n      -0.0100  0.0346  -0.0145  -0.0274  0.0106  -0.0478  0.0692  -0.0614\ns.e.   0.0175  0.0175   0.0173   0.0173  0.0176   0.0180  0.0176   0.0177\n         ma9     ma10   drift\n      0.0789  -0.0362  0.0341\ns.e.  0.0167   0.0184  0.0240\n\nsigma^2 = 1.905:  log likelihood = -5701.93\nAIC=11427.85   AICc=11427.95   BIC=11500.99\n\n\n\n\n\n\nCode\nArima(myts,order=c(10,1,10),include.drift=TRUE) \n\n\nSeries: myts \nARIMA(10,1,10) with drift \n\nCoefficients:\n          ar1     ar2      ar3     ar4     ar5      ar6      ar7      ar8\n      -0.3213  0.3264  -0.4687  -0.193  0.9261  -0.2793  -0.7444  -0.0770\ns.e.   0.0472     NaN   0.2209     NaN  0.0845   0.1075   0.0853   0.0725\n          ar9     ar10     ma1      ma2     ma3     ma4      ma5     ma6\n      -0.2497  -0.4612  0.3097  -0.3004  0.4597  0.1453  -0.9148  0.2552\ns.e.   0.0174   0.1686  0.0602      NaN  0.2119     NaN   0.0865  0.1060\n         ma7     ma8     ma9    ma10   drift\n      0.7464  0.0639  0.3156  0.4264  0.0341\ns.e.  0.0652  0.0929     NaN  0.1614  0.0234\n\nsigma^2 = 1.863:  log likelihood = -5662.96\nAIC=11369.91   AICc=11370.23   BIC=11504\n\n\n\n\n\n\nCode\nauto.arima(myts)\n\n\nSeries: myts \nARIMA(2,1,0) \n\nCoefficients:\n          ar1     ar2\n      -0.0305  0.0546\ns.e.   0.0175  0.0175\n\nsigma^2 = 1.942:  log likelihood = -5738.09\nAIC=11482.18   AICc=11482.19   BIC=11500.46\n\n\n\n\n\nIn the Model selection, when comparing the AIC, AICc and BIC values of all the 4 models, model (0,1,0) has the least BIC and AIc value. The model generated by auto.arima is also (0,1,0). We can proceed with the further model diagnostic for the chosen best model i.e. (0,1,0), to look into the details of the model.\n\n\nModel Diagnostic\n\nModel PlotModel\n\n\n\n\nCode\nmodel_output <- capture.output(sarima(myts,0,1,0))\n\n\n\n\n\n\n\n\n\nCode\ncat(model_output[9:38], model_output[length(model_output)], sep = \"\\n\") \n\n\n$fit\n\nCall:\narima(x = xdata, order = c(p, d, q), seasonal = list(order = c(P, D, Q), period = S), \n    xreg = constant, transform.pars = trans, fixed = fixed, optim.control = list(trace = trc, \n        REPORT = 1, reltol = tol))\n\nCoefficients:\n      constant\n        0.0341\ns.e.    0.0244\n\nsigma^2 estimated as 1.947:  log likelihood = -5743.71,  aic = 11491.42\n\n$degrees_of_freedom\n[1] 3277\n\n$ttable\n         Estimate     SE t.value p.value\nconstant   0.0341 0.0244  1.4005  0.1615\n\n$AIC\n[1] 3.50562\n\n$AICc\n[1] 3.50562\n\n$BIC\n[1] 3.509339\n\n\n\n\n\nThe model diagnostic is done for ARIMA(0,1,0). By analyzing the standardized residuals plot, it can be observed that the mean is close to 0 and the variance is slightly higher than 1. Deviations from these values could indicate poor model fit. However, in this case, the model seems to be well-fitted. The ACF plot of residuals shows very few significant lags, which is a positive sign for the model. The qq-plot also suggests normality in the residuals. Additionally, the p-values for the Ljung-Box test are greater than 0.05, indicating that the residuals are independent, which is favorable for the model’s accuracy.\nThe equation for this model: \\[(1-\\phi_1B-\\phi_2B^2)(1-B)(Y_t-\\mu) = (1+\\theta_1B+\\theta_2B^2)\\epsilon_t\\]\n\n\nForcast\n\nShort Term ForcastLong term Forcast\n\n\n\n\nCode\nmyts %>%\n  Arima(order=c(0,1,0),include.drift = TRUE) %>%\n  forecast %>%\n  autoplot(main = \"Consumer Discretionary Sector Fund Stock Prices Prediction\") +\n  ylab(\"stock prices\") + xlab(\"Year\")\n\n\n\n\n\n\n\n\n\nCode\nsarima.for(myts,182, 0,1,0, main='Consumer Discretionary Sector Fund Stock Prices Prediction')\n\n\n\n\n\n$pred\nTime Series:\nStart = c(2023, 4) \nEnd = c(2023, 185) \nFrequency = 252 \n  [1] 137.0181 137.0523 137.0864 137.1206 137.1547 137.1888 137.2230 137.2571\n  [9] 137.2912 137.3254 137.3595 137.3936 137.4278 137.4619 137.4960 137.5302\n [17] 137.5643 137.5985 137.6326 137.6667 137.7009 137.7350 137.7691 137.8033\n [25] 137.8374 137.8715 137.9057 137.9398 137.9739 138.0081 138.0422 138.0764\n [33] 138.1105 138.1446 138.1788 138.2129 138.2470 138.2812 138.3153 138.3494\n [41] 138.3836 138.4177 138.4518 138.4860 138.5201 138.5543 138.5884 138.6225\n [49] 138.6567 138.6908 138.7249 138.7591 138.7932 138.8273 138.8615 138.8956\n [57] 138.9297 138.9639 138.9980 139.0322 139.0663 139.1004 139.1346 139.1687\n [65] 139.2028 139.2370 139.2711 139.3052 139.3394 139.3735 139.4076 139.4418\n [73] 139.4759 139.5101 139.5442 139.5783 139.6125 139.6466 139.6807 139.7149\n [81] 139.7490 139.7831 139.8173 139.8514 139.8855 139.9197 139.9538 139.9880\n [89] 140.0221 140.0562 140.0904 140.1245 140.1586 140.1928 140.2269 140.2610\n [97] 140.2952 140.3293 140.3635 140.3976 140.4317 140.4659 140.5000 140.5341\n[105] 140.5683 140.6024 140.6365 140.6707 140.7048 140.7389 140.7731 140.8072\n[113] 140.8414 140.8755 140.9096 140.9438 140.9779 141.0120 141.0462 141.0803\n[121] 141.1144 141.1486 141.1827 141.2168 141.2510 141.2851 141.3193 141.3534\n[129] 141.3875 141.4217 141.4558 141.4899 141.5241 141.5582 141.5923 141.6265\n[137] 141.6606 141.6947 141.7289 141.7630 141.7972 141.8313 141.8654 141.8996\n[145] 141.9337 141.9678 142.0020 142.0361 142.0702 142.1044 142.1385 142.1726\n[153] 142.2068 142.2409 142.2751 142.3092 142.3433 142.3775 142.4116 142.4457\n[161] 142.4799 142.5140 142.5481 142.5823 142.6164 142.6505 142.6847 142.7188\n[169] 142.7530 142.7871 142.8212 142.8554 142.8895 142.9236 142.9578 142.9919\n[177] 143.0260 143.0602 143.0943 143.1284 143.1626 143.1967\n\n$se\nTime Series:\nStart = c(2023, 4) \nEnd = c(2023, 185) \nFrequency = 252 \n  [1]  1.395512  1.973552  2.417098  2.791024  3.120460  3.418292  3.692178\n  [8]  3.947104  4.186536  4.412996  4.628390  4.834195  5.031590  5.221528\n [15]  5.404795  5.582048  5.753843  5.920656  6.082896  6.240919  6.395039\n [22]  6.545531  6.692640  6.836585  6.977560  7.115743  7.251293  7.384355\n [29]  7.515062  7.643534  7.769882  7.894208  8.016606  8.137163  8.255960\n [36]  8.373072  8.488568  8.602514  8.714969  8.825993  8.935637  9.043951\n [43]  9.150984  9.256779  9.361379  9.464823  9.567148  9.668391  9.768584\n [50]  9.867760  9.965949 10.063180 10.159480 10.254877 10.349394 10.443055\n [57] 10.535884 10.627903 10.719131 10.809589 10.899297 10.988272 11.076533\n [64] 11.164096 11.250977 11.337193 11.422758 11.507687 11.591993 11.675691\n [71] 11.758793 11.841312 11.923259 12.004648 12.085488 12.165791 12.245568\n [78] 12.324828 12.403582 12.481839 12.559608 12.636898 12.713719 12.790078\n [85] 12.865985 12.941446 13.016469 13.091063 13.165234 13.238989 13.312336\n [92] 13.385281 13.457830 13.529991 13.601768 13.673169 13.744199 13.814864\n [99] 13.885169 13.955120 14.024722 14.093980 14.162900 14.231486 14.299742\n[106] 14.367675 14.435288 14.502586 14.569573 14.636253 14.702631 14.768710\n[113] 14.834496 14.899991 14.965199 15.030124 15.094770 15.159140 15.223238\n[120] 15.287068 15.350632 15.413934 15.476976 15.539764 15.602298 15.664583\n[127] 15.726621 15.788416 15.849969 15.911285 15.972365 16.033212 16.093829\n[134] 16.154219 16.214384 16.274326 16.334049 16.393554 16.452843 16.511920\n[141] 16.570787 16.629444 16.687896 16.746144 16.804189 16.862035 16.919683\n[148] 16.977136 17.034394 17.091461 17.148338 17.205027 17.261530 17.317848\n[155] 17.373984 17.429939 17.485715 17.541313 17.596736 17.651985 17.707062\n[162] 17.761968 17.816704 17.871273 17.925676 17.979914 18.033989 18.087902\n[169] 18.141656 18.195250 18.248687 18.301968 18.355094 18.408067 18.460888\n[176] 18.513558 18.566079 18.618452 18.670678 18.722758 18.774693 18.826486\n\n\n\n\n\nThe two figures above show an idea of the short term and the long-term forecasted stock price. The forecasted values is be based on the estimated parameters of the model, which were obtained by fitting it to the historical data. One way to assess the accuracy of the forecasts is to use cross-validation, which involves splitting the data into training and testing sets and comparing the predicted values to the actual values in the test set. This can help to identify any potential problems with the model and to fine-tune the model parameters.\n\n12 Step ahead Cross Validation1 Step ahead Cross Validation\n\n\n\n\nCode\n#a seasonal cross validation using 12 steps ahead forecasts\nfarima1 <- function(x, h){forecast(Arima(x, order=c(0,1,0)), h=h)}\n\n# Compute cross-validated errors for up to 12 steps ahead\ne <- tsCV(myts, forecastfunction = farima1, h = 12)\n\n# Compute the MSE values and remove missing values\nmse <- colMeans(e^2, na.rm = TRUE)\n\n# Plot the MSE values against the forecast horizon\ndata.frame(h = 1:12, MSE = mse) %>%\n  ggplot(aes(x = h, y = MSE)) + geom_point()+geom_line()+ggtitle(\"12 Step Ahead Cross Validation\")\n\n\n\n\n\n\n\n\n\nCode\n#a seasonal cross validation using 1 steps ahead forecasts\nfarima1 <- function(x, h){forecast(Arima(x, order=c(0,1,0)),h=h)}\n\n# Compute cross-validated errors for up to 1 steps ahead\ne <- tsCV(myts, forecastfunction = farima1, h = 1)\nmse <-abs(mean(e,na.rm=TRUE))\n\n# Plot the MSE values against the forecast horizon\ndata.frame(h = 1:12, MSE = mse) %>%\n  ggplot(aes(x = h, y = MSE)) + geom_point()+geom_line()+ggtitle(\"1 Step Ahead Cross Validation\")\n\n\n\n\n\n\n\n\nThe one-step-ahead forecasting plot shows that the MSE seems to be stable. The MSE is tend to rise by 12 steps at each step of cross-validation. Lastly, the above-mentioned ARIMA model should be compared to benchmark methods before being used for a long time.\n\n\nModel Comparison\n\nPlotModel Error Table\n\n\n\n\nCode\nfit <- Arima(myts, order=c(0,1,0))\nautoplot(myts) +\n  autolayer(meanf(myts, h=182),\n            series=\"Mean\", PI=FALSE) +\n  autolayer(naive(myts, h=182),\n            series=\"Naïve\", PI=FALSE) +\n  autolayer(snaive(myts, h=182),\n            series=\"SNaïve\", PI=FALSE)+\n  autolayer(rwf(myts, h=182, drift=TRUE),\n            series=\"Drift\", PI=FALSE)+\n  autolayer(forecast(fit,182), \n            series=\"fit\",PI=FALSE) +\n  guides(colour=guide_legend(title=\"Forecast\"))\n\n\n\n\n\n\n\n\n\nCode\nsummary <- summary(fit)\nsnaive <- snaive(myts, h=182)\naccuracy(snaive)\n\n\n                   ME     RMSE      MAE    MPE     MAPE MASE      ACF1\nTraining set 10.61178 21.13059 15.33656 12.482 15.97199    1 0.9927812\n\n\nCode\nsummary\n\n\nSeries: myts \nARIMA(0,1,0) \n\nsigma^2 = 1.949:  log likelihood = -5744.69\nAIC=11491.38   AICc=11491.38   BIC=11497.48\n\nTraining set error measures:\n                     ME     RMSE       MAE        MPE      MAPE       MASE\nTraining set 0.03413301 1.395717 0.8033052 0.04362309 0.8758898 0.05237845\n                    ACF1\nTraining set -0.03291003\n\n\n\n\n\nError\nModel\nSnavie\n\n\n\n\nME\n0.009462746\n2.91434\n\n\nRMSE\n2.118625\n45.1313\n\n\nMAE\n0.6990421\n27.56965\n\n\nMPE\n-0.0492247\n-21.0712\n\n\nMAPE\n0.9543524\n58.49983\n\n\nMASE\n0.02535549\n1.0000000\n\n\nACF1\n-0.01001972\n0.9975806\n\n\n\n\n\n\nThe ARIMA forecast tracks the actual data points very closely when compared to other model. The other forecast methods are less accurate when compared. From the table, Model error measurements of fit are much lower than snaive method.We can conclude that the fitted model is good."
  },
  {
    "objectID": "arima_sector_energy.html",
    "href": "arima_sector_energy.html",
    "title": "ARIMA Model for Energy Sector Fund",
    "section": "",
    "text": "During exploratory data analysis (EDA), it was observed that the raw data is often non-stationary, exhibiting trends, seasonality, and other complex patterns that can make modeling and forecasting difficult. To address this issue, the data is often differentiated to achieve stationarity, which is a key assumption of the ARIMA model.\nDifferencing involves calculating the difference between each data point and the previous data point, which can help to remove trends and seasonality from the data. The goal is to create a new series of data that is stationary, meaning that the statistical properties of the data do not change over time.\n\nDifferentiated Time Series Plot\n\nPlotPACFADF Test\n\n\n\n\nCode\n#import the data\ndf <- read.csv(\"DATA/CLEANED DATA/XLE_raw_data.csv\")\n#convert to time series data\nmyts<-ts(df$XLE.Adjusted,frequency=252,start=c(2010,1,1), end = c(2023,3,1)) \n#First order differentiation\ndf1 <- diff(myts)\n# Plot \nmyts  %>% diff() %>% ggtsdisplay(main = \"First order differentiation\") \n\n\n\n\n\n\n\nCode\n#ACF plot \nggAcf(df1,60,main=\"ACF Plot: First order differentiation\") \n\n\n\n\n\n\n\n\n\nCode\nggPacf(df1,60,main=\"PACF Plot: First order differentiation\") \n\n\n\n\n\n\n\n\n\nCode\ntseries::adf.test(df1)\n\n\n\n    Augmented Dickey-Fuller Test\n\ndata:  df1\nDickey-Fuller = -14.575, Lag order = 14, p-value = 0.01\nalternative hypothesis: stationary\n\n\n\n\n\nAfter analyzing the ACF and PACF plots, it can be observed that most of the bar lines lie between the blue lines, indicating that the time series is stationary. This has been further confirmed through an Augmented Dickey-Fuller test as the p-value is less than 0.05. Once the data is stationary, the next step is to model it using ARIMA. The combination of ACF and PACF plots can help determine the appropriate values for p, d, and q in the ARIMA model. The order of differencing required to achieve stationarity gives the d value. The p value is determined by examining significant spikes at various lags in the PACF plot, while the q value is determined through significant spikes in the ACF plot. It is important to consider other methods such as grid search and information criteria to ensure the most appropriate model is selected.\nHere the parameters are d = 1 p = 0,7 (PACF Plot) q = 0,7 (ACF Plot)\n\n\nModel Selection\n\nARIMA(0,1,0) ResultARIMA(7,1,0) ResultARIMA(0,1,7) ResultARIMA(7,1,7) ResultAuto Arima Model\n\n\n\n\nCode\nArima(myts,order=c(0,1,0),include.drift=TRUE) \n\n\nSeries: myts \nARIMA(0,1,0) with drift \n\nCoefficients:\n       drift\n      0.0146\ns.e.  0.0144\n\nsigma^2 = 0.6812:  log likelihood = -4021.58\nAIC=8047.16   AICc=8047.17   BIC=8059.35\n\n\n\n\n\n\nCode\nArima(myts,order=c(7,1,0),include.drift=TRUE) \n\n\nSeries: myts \nARIMA(7,1,0) with drift \n\nCoefficients:\n         ar1     ar2      ar3     ar4      ar5      ar6     ar7   drift\n      0.0040  0.0241  -0.0245  0.0231  -0.0258  -0.0054  0.0363  0.0146\ns.e.  0.0175  0.0175   0.0174  0.0175   0.0175   0.0175  0.0175  0.0149\n\nsigma^2 = 0.6802:  log likelihood = -4015.65\nAIC=8049.31   AICc=8049.36   BIC=8104.16\n\n\n\n\n\n\nCode\nArima(myts,order=c(0,1,7),include.drift=TRUE) \n\n\nSeries: myts \nARIMA(0,1,7) with drift \n\nCoefficients:\n         ma1     ma2      ma3     ma4      ma5      ma6     ma7   drift\n      0.0046  0.0238  -0.0247  0.0234  -0.0283  -0.0021  0.0345  0.0146\ns.e.  0.0175  0.0175   0.0174  0.0176   0.0175   0.0173  0.0177  0.0148\n\nsigma^2 = 0.6802:  log likelihood = -4015.55\nAIC=8049.1   AICc=8049.16   BIC=8103.96\n\n\n\n\n\n\nCode\nArima(myts,order=c(7,1,7),include.drift=TRUE) \n\n\nSeries: myts \nARIMA(7,1,7) with drift \n\nCoefficients:\n          ar1     ar2     ar3     ar4    ar5      ar6      ar7    ma1      ma2\n      -0.3250  0.4772  0.4285  0.7847  0.255  -0.5981  -0.6461  0.320  -0.4699\ns.e.   0.1691  0.0758  0.1537  0.0310  0.158   0.0826   0.1657  0.163   0.0785\n          ma3      ma4      ma5     ma6     ma7   drift\n      -0.4531  -0.7665  -0.2422  0.5771  0.6864  0.0174\ns.e.   0.1467   0.0301   0.1512  0.0881  0.1567  0.0149\n\nsigma^2 = 0.6734:  log likelihood = -3997.25\nAIC=8026.51   AICc=8026.67   BIC=8124.03\n\n\n\n\n\n\nCode\nauto.arima(myts)\n\n\nSeries: myts \nARIMA(0,1,0) \n\nsigma^2 = 0.6812:  log likelihood = -4022.09\nAIC=8046.19   AICc=8046.19   BIC=8052.28\n\n\n\n\n\nIn the Model selection, when comparing the AIC, AICc and BIC values of all the 4 models, model (0,1,0) has the least BIC and AIc value. The model generated by auto.arima is also (0,1,0). We can proceed with the further model diagnostic for the chosen best model i.e. (0,1,0), to look into the details of the model.\n\n\nModel Diagnostic\n\nModel PlotModel\n\n\n\n\nCode\nmodel_output <- capture.output(sarima(myts,0,1,0))\n\n\n\n\n\n\n\n\n\nCode\ncat(model_output[9:38], model_output[length(model_output)], sep = \"\\n\") \n\n\n$fit\n\nCall:\narima(x = xdata, order = c(p, d, q), seasonal = list(order = c(P, D, Q), period = S), \n    xreg = constant, transform.pars = trans, fixed = fixed, optim.control = list(trace = trc, \n        REPORT = 1, reltol = tol))\n\nCoefficients:\n      constant\n        0.0146\ns.e.    0.0144\n\nsigma^2 estimated as 0.681:  log likelihood = -4021.58,  aic = 8047.16\n\n$degrees_of_freedom\n[1] 3277\n\n$ttable\n         Estimate     SE t.value p.value\nconstant   0.0146 0.0144  1.0139  0.3107\n\n$AIC\n[1] 2.4549\n\n$AICc\n[1] 2.4549\n\n$BIC\n[1] 2.458618\n\n\n\n\n\nThe model diagnostic is done for ARIMA(0,1,0). By analyzing the standardized residuals plot, it can be observed that the mean is close to 0 and the variance is slightly higher than 1. Deviations from these values could indicate poor model fit. However, in this case, the model seems to be well-fitted. The ACF plot of residuals shows very few significant lags, which is a positive sign for the model. The qq-plot also suggests normality in the residuals. Additionally, the p-values for the Ljung-Box test are greater than 0.05, indicating that the residuals are independent, which is favorable for the model’s accuracy.\nThe equation for this model: \\[(1-\\phi_1B-\\phi_2B^2)(1-B)(Y_t-\\mu) = (1+\\theta_1B+\\theta_2B^2)\\epsilon_t\\]\n\n\nForcast\n\nShort Term ForcastLong term Forcast\n\n\n\n\nCode\nmyts %>%\n  Arima(order=c(0,1,0),include.drift = TRUE) %>%\n  forecast %>%\n  autoplot(main = \"Energy Sector Fund Stock Prices Prediction\") +\n  ylab(\"stock prices\") + xlab(\"Year\")\n\n\n\n\n\n\n\n\n\nCode\nsarima.for(myts,182, 0,1,0, main='Energy Sector Fund Stock Prices Prediction')\n\n\n\n\n\n$pred\nTime Series:\nStart = c(2023, 4) \nEnd = c(2023, 185) \nFrequency = 252 \n  [1] 85.03263 85.04724 85.06185 85.07647 85.09108 85.10569 85.12031 85.13492\n  [9] 85.14954 85.16415 85.17876 85.19338 85.20799 85.22260 85.23722 85.25183\n [17] 85.26644 85.28106 85.29567 85.31028 85.32490 85.33951 85.35413 85.36874\n [25] 85.38335 85.39797 85.41258 85.42719 85.44181 85.45642 85.47103 85.48565\n [33] 85.50026 85.51487 85.52949 85.54410 85.55872 85.57333 85.58794 85.60256\n [41] 85.61717 85.63178 85.64640 85.66101 85.67562 85.69024 85.70485 85.71946\n [49] 85.73408 85.74869 85.76331 85.77792 85.79253 85.80715 85.82176 85.83637\n [57] 85.85099 85.86560 85.88021 85.89483 85.90944 85.92405 85.93867 85.95328\n [65] 85.96790 85.98251 85.99712 86.01174 86.02635 86.04096 86.05558 86.07019\n [73] 86.08480 86.09942 86.11403 86.12865 86.14326 86.15787 86.17249 86.18710\n [81] 86.20171 86.21633 86.23094 86.24555 86.26017 86.27478 86.28939 86.30401\n [89] 86.31862 86.33324 86.34785 86.36246 86.37708 86.39169 86.40630 86.42092\n [97] 86.43553 86.45014 86.46476 86.47937 86.49398 86.50860 86.52321 86.53783\n[105] 86.55244 86.56705 86.58167 86.59628 86.61089 86.62551 86.64012 86.65473\n[113] 86.66935 86.68396 86.69857 86.71319 86.72780 86.74242 86.75703 86.77164\n[121] 86.78626 86.80087 86.81548 86.83010 86.84471 86.85932 86.87394 86.88855\n[129] 86.90316 86.91778 86.93239 86.94701 86.96162 86.97623 86.99085 87.00546\n[137] 87.02007 87.03469 87.04930 87.06391 87.07853 87.09314 87.10775 87.12237\n[145] 87.13698 87.15160 87.16621 87.18082 87.19544 87.21005 87.22466 87.23928\n[153] 87.25389 87.26850 87.28312 87.29773 87.31235 87.32696 87.34157 87.35619\n[161] 87.37080 87.38541 87.40003 87.41464 87.42925 87.44387 87.45848 87.47309\n[169] 87.48771 87.50232 87.51694 87.53155 87.54616 87.56078 87.57539 87.59000\n[177] 87.60462 87.61923 87.63384 87.64846 87.66307 87.67768\n\n$se\nTime Series:\nStart = c(2023, 4) \nEnd = c(2023, 185) \nFrequency = 252 \n  [1]  0.8252253  1.1670449  1.4293322  1.6504507  1.8452600  2.0213810\n  [7]  2.1833410  2.3340897  2.4756760  2.6095917  2.7369628  2.8586644\n [13]  2.9753923  3.0877105  3.1960840  3.3009014  3.4024912  3.5011346\n [19]  3.5970739  3.6905199  3.7816576  3.8706499  3.9576417  4.0427620\n [25]  4.1261267  4.2078401  4.2879967  4.3666821  4.4439745  4.5199453\n [31]  4.5946602  4.6681795  4.7405587  4.8118493  4.8820990  4.9513520\n [37]  5.0196498  5.0870307  5.1535306  5.2191833  5.2840204  5.3480715\n [43]  5.4113644  5.4739257  5.5357799  5.5969506  5.6574599  5.7173289\n [49]  5.7765774  5.8352244  5.8932877  5.9507846  6.0077312  6.0641430\n [55]  6.1200349  6.1754210  6.2303147  6.2847290  6.3386761  6.3921680\n [61]  6.4452160  6.4978308  6.5500231  6.6018027  6.6531794  6.7041624\n [67]  6.7547605  6.8049825  6.8548365  6.9043306  6.9534723  7.0022692\n [73]  7.0507284  7.0988568  7.1466611  7.1941477  7.2413230  7.2881929\n [79]  7.3347633  7.3810398  7.4270281  7.4727333  7.5181607  7.5633152\n [85]  7.6082017  7.6528250  7.6971896  7.7412999  7.7851603  7.8287750\n [91]  7.8721480  7.9152834  7.9581850  8.0008565  8.0433017  8.0855240\n [97]  8.1275270  8.1693141  8.2108885  8.2522534  8.2934120  8.3343674\n[103]  8.3751225  8.4156802  8.4560434  8.4962149  8.5361973  8.5759933\n[109]  8.6156055  8.6550364  8.6942885  8.7333641  8.7722657  8.8109955\n[115]  8.8495559  8.8879489  8.9261768  8.9642417  9.0021457  9.0398907\n[121]  9.0774788  9.1149118  9.1521918  9.1893205  9.2262998  9.2631315\n[127]  9.2998173  9.3363590  9.3727582  9.4090165  9.4451357  9.4811173\n[133]  9.5169629  9.5526740  9.5882520  9.6236985  9.6590150  9.6942028\n[139]  9.7292633  9.7641979  9.7990080  9.8336949  9.8682598  9.9027041\n[145]  9.9370290  9.9712357 10.0053255 10.0392996 10.0731590 10.1069050\n[151] 10.1405388 10.1740613 10.2074737 10.2407772 10.2739726 10.3070612\n[157] 10.3400439 10.3729217 10.4056956 10.4383666 10.4709357 10.5034038\n[163] 10.5357719 10.5680408 10.6002114 10.6322848 10.6642616 10.6961429\n[169] 10.7279294 10.7596221 10.7912216 10.8227289 10.8541447 10.8854699\n[175] 10.9167051 10.9478513 10.9789091 11.0098793 11.0407626 11.0715598\n[181] 11.1022715 11.1328985\n\n\n\n\n\nThe two figures above show an idea of the short term and the long-term forecasted stock price. The forecasted values is be based on the estimated parameters of the model, which were obtained by fitting it to the historical data. One way to assess the accuracy of the forecasts is to use cross-validation, which involves splitting the data into training and testing sets and comparing the predicted values to the actual values in the test set. This can help to identify any potential problems with the model and to fine-tune the model parameters.\n\n12 Step ahead Cross Validation1 Step ahead Cross Validation\n\n\n\n\nCode\n#a seasonal cross validation using 12 steps ahead forecasts\nfarima1 <- function(x, h){forecast(Arima(x, order=c(0,1,0)), h=h)}\n\n# Compute cross-validated errors for up to 12 steps ahead\ne <- tsCV(myts, forecastfunction = farima1, h = 12)\n\n# Compute the MSE values and remove missing values\nmse <- colMeans(e^2, na.rm = TRUE)\n\n# Plot the MSE values against the forecast horizon\ndata.frame(h = 1:12, MSE = mse) %>%\n  ggplot(aes(x = h, y = MSE)) + geom_point()+geom_line()+ggtitle(\"12 Step Ahead Cross Validation\")\n\n\n\n\n\n\n\n\n\nCode\n#a seasonal cross validation using 1 steps ahead forecasts\nfarima1 <- function(x, h){forecast(Arima(x, order=c(0,1,0)),h=h)}\n\n# Compute cross-validated errors for up to 1 steps ahead\ne <- tsCV(myts, forecastfunction = farima1, h = 1)\nmse <-abs(mean(e,na.rm=TRUE))\n\n# Plot the MSE values against the forecast horizon\ndata.frame(h = 1:12, MSE = mse) %>%\n  ggplot(aes(x = h, y = MSE)) + geom_point()+geom_line()+ggtitle(\"1 Step Ahead Cross Validation\")\n\n\n\n\n\n\n\n\nThe one-step-ahead forecasting plot shows that the MSE seems to be stable. The MSE is tend to rise by 12 steps at each step of cross-validation. Lastly, the above-mentioned ARIMA model should be compared to benchmark methods before being used for a long time.\n\n\nModel Comparison\n\nPlotModel Error Table\n\n\n\n\nCode\nfit <- Arima(myts, order=c(0,1,0))\nautoplot(myts) +\n  autolayer(meanf(myts, h=182),\n            series=\"Mean\", PI=FALSE) +\n  autolayer(naive(myts, h=182),\n            series=\"Naïve\", PI=FALSE) +\n  autolayer(snaive(myts, h=182),\n            series=\"SNaïve\", PI=FALSE)+\n  autolayer(rwf(myts, h=182, drift=TRUE),\n            series=\"Drift\", PI=FALSE)+\n  autolayer(forecast(fit,182), \n            series=\"fit\",PI=FALSE) +\n  guides(colour=guide_legend(title=\"Forecast\"))\n\n\n\n\n\n\n\n\n\nCode\nsummary <- summary(fit)\nsnaive <- snaive(myts, h=182)\naccuracy(snaive)\n\n\n                   ME     RMSE      MAE      MPE    MAPE MASE      ACF1\nTraining set 3.144901 13.08413 10.36343 2.192255 21.1083    1 0.9957227\n\n\nCode\nsummary\n\n\nSeries: myts \nARIMA(0,1,0) \n\nsigma^2 = 0.6812:  log likelihood = -4022.09\nAIC=8046.19   AICc=8046.19   BIC=8052.28\n\nTraining set error measures:\n                     ME      RMSE       MAE         MPE     MAPE       MASE\nTraining set 0.01462044 0.8252291 0.5803309 0.009107036 1.210244 0.05599795\n                    ACF1\nTraining set 0.002310955\n\n\n\n\n\nError\nModel\nSnavie\n\n\n\n\nME\n0.001858803\n1.488227\n\n\nRMSE\n1.032129\n16.98363\n\n\nMAE\n0.5766939\n12.5794\n\n\nMPE\n-0.01822702\n-2.493712\n\n\nMAPE\n1.173334\n25.42997\n\n\nMASE\n0.0458443\n1.0000000\n\n\nACF1\n0.0093721\n0.9959312\n\n\n\n\n\n\nThe ARIMA forecast tracks the actual data points very closely when compared to other model. The other forecast methods are less accurate when compared. From the table, Model error measurements of fit are much lower than snaive method.We can conclude that the fitted model is good."
  },
  {
    "objectID": "arima_sector_financial.html",
    "href": "arima_sector_financial.html",
    "title": "ARIMA Model for Financial Sector Fund",
    "section": "",
    "text": "During exploratory data analysis (EDA), it was observed that the raw data is often non-stationary, exhibiting trends, seasonality, and other complex patterns that can make modeling and forecasting difficult. To address this issue, the data is often differentiated to achieve stationarity, which is a key assumption of the ARIMA model.\nDifferencing involves calculating the difference between each data point and the previous data point, which can help to remove trends and seasonality from the data. The goal is to create a new series of data that is stationary, meaning that the statistical properties of the data do not change over time.\n\nDifferentiated Time Series Plot\n\nPlotACFPACFADF Test\n\n\n\n\nCode\n#import the data\ndf <- read.csv(\"DATA/CLEANED DATA/XLF_raw_data.csv\")\n#convert to time series data\nmyts<-ts(df$XLF.Adjusted,frequency=252,start=c(2010,1,1), end = c(2023,3,1)) \n#First order differentiation\ndf1 <- diff(myts)\n# Plot \nmyts  %>% diff() %>% ggtsdisplay(main = \"First order differentiation\") \n\n\n\n\n\n\n\n\n\nCode\n#ACF plot \nggAcf(df1,60,main=\"ACF Plot: First order differentiation\") \n\n\n\n\n\n\n\n\n\nCode\nggPacf(df1,60,main=\"PACF Plot: First order differentiation\") \n\n\n\n\n\n\n\n\n\nCode\ntseries::adf.test(df1)\n\n\n\n    Augmented Dickey-Fuller Test\n\ndata:  df1\nDickey-Fuller = -15.915, Lag order = 14, p-value = 0.01\nalternative hypothesis: stationary\n\n\n\n\n\nAfter analyzing the ACF and PACF plots, it can be observed that most of the bar lines lie between the blue lines, indicating that the time series is stationary. This has been further confirmed through an Augmented Dickey-Fuller test as the p-value is less than 0.05. Once the data is stationary, the next step is to model it using ARIMA. The combination of ACF and PACF plots can help determine the appropriate values for p, d, and q in the ARIMA model. The order of differencing required to achieve stationarity gives the d value. The p value is determined by examining significant spikes at various lags in the PACF plot, while the q value is determined through significant spikes in the ACF plot. It is important to consider other methods such as grid search and information criteria to ensure the most appropriate model is selected.\nHere the parameters are d = 1 p = 0,1 (PACF Plot) q = 0,1 (ACF Plot)\n\n\nModel Selection\n\nARIMA ResultAuto Arima Model\n\n\n\n\nCode\n######################## Check for different combinations ########\nd=1\ni=1\ntemp = data.frame()\nls=matrix(rep(NA,6*4),nrow=4) #nrow = 2x2x1\n\n\nfor (p in 1:2)# p=0,1 :2\n{\n  for(q in 1:2)# q=0,1 :2\n  {\n    for(d in 1)# d=1 :1\n    {\n      \n      if(p-1+d+q-1<=8)\n      {\n        \n        model<- Arima(myts,order=c(p-1,d,q-1),include.drift=TRUE) \n        ls[i,]= c(p-1,d,q-1,model$aic,model$bic,model$aicc)\n        i=i+1\n        \n      }\n      \n    }\n  }\n}\n\nmodel = as.data.frame(ls)\nnames(model)= c(\"p\",\"d\",\"q\",\"AIC\",\"BIC\",\"AICc\")\n\n#temp\nknitr::kable(model)\n\n\n\n\n\np\nd\nq\nAIC\nBIC\nAICc\n\n\n\n\n0\n1\n0\n1341.341\n1353.531\n1341.344\n\n\n0\n1\n1\n1327.281\n1345.566\n1327.288\n\n\n1\n1\n0\n1325.483\n1343.768\n1325.491\n\n\n1\n1\n1\n1322.460\n1346.840\n1322.472\n\n\n\n\n\n\n\n\n\nCode\nauto.arima(myts)\n\n\nSeries: myts \nARIMA(4,1,2) with drift \n\nCoefficients:\n          ar1      ar2     ar3      ar4     ma1     ma2   drift\n      -1.6560  -0.7806  0.0264  -0.0017  1.6035  0.7386  0.0079\ns.e.   0.0683   0.0695  0.0354   0.0229  0.0660  0.0573  0.0050\n\nsigma^2 = 0.08589:  log likelihood = -624.61\nAIC=1265.21   AICc=1265.26   BIC=1313.97\n\n\n\n\n\nThe given table provides different ARIMA models with their corresponding AIC and BIC values. By sorting the table according to each criterion, we can identify the models with the lowest values. The smallest AIC value corresponds to an ARIMA (1,1,0) model, while the ARIMA (0,1,0) model has the lowest BIC value. Additionally, the auto.arima function in R suggests an ARIMA (0,1,0) model as the best fit for the data. Since there are different models to choose from, it is important to perform model diagnostics to determine the best model for the data. These diagnostics can include checking for residual normality, checking for autocorrelation and partial autocorrelation in the residuals, and comparing the forecasted values to the actual values. By conducting these tests, we can determine which model provides the best fit for the data and is therefore the most appropriate to use for forecasting future values.\n\n\nModel Diagnostic\n\nModel 1 PlotModel 1Model 2 PlotModel 2\n\n\n\n\nCode\nmodel_output <- capture.output(sarima(myts,1,1,0))\n\n\n\n\n\n\n\n\n\nCode\ncat(model_output[11:41], model_output[length(model_output)], sep = \"\\n\") \n\n\n$fit\n\nCall:\narima(x = xdata, order = c(p, d, q), seasonal = list(order = c(P, D, Q), period = S), \n    xreg = constant, transform.pars = trans, fixed = fixed, optim.control = list(trace = trc, \n        REPORT = 1, reltol = tol))\n\nCoefficients:\n          ar1  constant\n      -0.0737    0.0079\ns.e.   0.0174    0.0048\n\nsigma^2 estimated as 0.08757:  log likelihood = -659.74,  aic = 1325.48\n\n$degrees_of_freedom\n[1] 3276\n\n$ttable\n         Estimate     SE t.value p.value\nar1       -0.0737 0.0174 -4.2313  0.0000\nconstant   0.0079 0.0048  1.6375  0.1016\n\n$AIC\n[1] 0.4043573\n\n$AICc\n[1] 0.4043584\n\n$BIC\n[1] 0.4099354\n\n\n\n\n\n\nCode\nmodel_output <- capture.output(sarima(myts,0,1,0))\n\n\n\n\n\n\n\n\n\nCode\ncat(model_output[9:38], model_output[length(model_output)], sep = \"\\n\") \n\n\n$fit\n\nCall:\narima(x = xdata, order = c(p, d, q), seasonal = list(order = c(P, D, Q), period = S), \n    xreg = constant, transform.pars = trans, fixed = fixed, optim.control = list(trace = trc, \n        REPORT = 1, reltol = tol))\n\nCoefficients:\n      constant\n        0.0079\ns.e.    0.0052\n\nsigma^2 estimated as 0.08805:  log likelihood = -668.67,  aic = 1341.34\n\n$degrees_of_freedom\n[1] 3277\n\n$ttable\n         Estimate     SE t.value p.value\nconstant   0.0079 0.0052   1.522  0.1281\n\n$AIC\n[1] 0.4091949\n\n$AICc\n[1] 0.4091952\n\n$BIC\n[1] 0.4129136\n\n\n\n\n\nTo determine which model diagnosis is the best, we need to consider some key factors such as the coefficient estimates and standard errors, AIC, BIC, p-values, and the log-likelihood. Comparing the two models, model 2 is the best because it has the lowest BIC value when compared and the AIC value are almost the same, which indicate better goodness of fit. Also, all the coefficient estimates in the model are significant, as evidenced by the p-values are better than model 1, which implies that there is a strong correlation between the variables. The t-values are high, indicating that the estimates are highly reliable.\nTherefore, based on the AIC and BIC values, the significance of the coefficient estimates, and the reliability of the estimates, we can conclude that model 2 is the best model diagnosis. Therefore, (0,1,0) is the best model for this time series.\nThe best model identified is ARIMA(0,1,0). By analyzing the standardized residuals plot, it can be observed that the mean is close to 0 and the variance is slightly higher than 1. Deviations from these values could indicate poor model fit. However, in this case, the model seems to be well-fitted. The ACF plot of residuals shows very few significant lags, which is a positive sign for the model. The qq-plot also suggests normality in the residuals. Additionally, the p-values for the Ljung-Box test are greater than 0.05, indicating that the residuals are independent, which is favorable for the model’s accuracy.\nThe equation for this model: \\[(1-\\phi_1B-\\phi_2B^2)(1-B)(Y_t-\\mu) = (1+\\theta_1B+\\theta_2B^2)\\epsilon_t\\]\n\n\nForcast\n\nShort Term ForcastLong term Forcast\n\n\n\n\nCode\nmyts %>%\n  Arima(order=c(0,1,0),include.drift = TRUE) %>%\n  forecast %>%\n  autoplot(main = \"Financial Sector Fund Stock Prices Prediction\") +\n  ylab(\"stock prices\") + xlab(\"Year\")\n\n\n\n\n\n\n\n\n\nCode\nsarima.for(myts,182, 0,1,0, main='Financial Sector Fund Stock Prices Prediction')\n\n\n\n\n\n$pred\nTime Series:\nStart = c(2023, 4) \nEnd = c(2023, 185) \nFrequency = 252 \n  [1] 35.12057 35.12846 35.13635 35.14424 35.15212 35.16001 35.16790 35.17579\n  [9] 35.18368 35.19156 35.19945 35.20734 35.21523 35.22311 35.23100 35.23889\n [17] 35.24678 35.25467 35.26255 35.27044 35.27833 35.28622 35.29410 35.30199\n [25] 35.30988 35.31777 35.32566 35.33354 35.34143 35.34932 35.35721 35.36509\n [33] 35.37298 35.38087 35.38876 35.39665 35.40453 35.41242 35.42031 35.42820\n [41] 35.43608 35.44397 35.45186 35.45975 35.46764 35.47552 35.48341 35.49130\n [49] 35.49919 35.50707 35.51496 35.52285 35.53074 35.53863 35.54651 35.55440\n [57] 35.56229 35.57018 35.57806 35.58595 35.59384 35.60173 35.60962 35.61750\n [65] 35.62539 35.63328 35.64117 35.64905 35.65694 35.66483 35.67272 35.68060\n [73] 35.68849 35.69638 35.70427 35.71216 35.72004 35.72793 35.73582 35.74371\n [81] 35.75159 35.75948 35.76737 35.77526 35.78315 35.79103 35.79892 35.80681\n [89] 35.81470 35.82258 35.83047 35.83836 35.84625 35.85414 35.86202 35.86991\n [97] 35.87780 35.88569 35.89357 35.90146 35.90935 35.91724 35.92513 35.93301\n[105] 35.94090 35.94879 35.95668 35.96456 35.97245 35.98034 35.98823 35.99612\n[113] 36.00400 36.01189 36.01978 36.02767 36.03555 36.04344 36.05133 36.05922\n[121] 36.06711 36.07499 36.08288 36.09077 36.09866 36.10654 36.11443 36.12232\n[129] 36.13021 36.13809 36.14598 36.15387 36.16176 36.16965 36.17753 36.18542\n[137] 36.19331 36.20120 36.20908 36.21697 36.22486 36.23275 36.24064 36.24852\n[145] 36.25641 36.26430 36.27219 36.28007 36.28796 36.29585 36.30374 36.31163\n[153] 36.31951 36.32740 36.33529 36.34318 36.35106 36.35895 36.36684 36.37473\n[161] 36.38262 36.39050 36.39839 36.40628 36.41417 36.42205 36.42994 36.43783\n[169] 36.44572 36.45361 36.46149 36.46938 36.47727 36.48516 36.49304 36.50093\n[177] 36.50882 36.51671 36.52460 36.53248 36.54037 36.54826\n\n$se\nTime Series:\nStart = c(2023, 4) \nEnd = c(2023, 185) \nFrequency = 252 \n  [1] 0.2967245 0.4196318 0.5139419 0.5934490 0.6634961 0.7268236 0.7850592\n  [8] 0.8392636 0.8901735 0.9383252 0.9841238 1.0278838 1.0698554 1.1102414\n [15] 1.1492090 1.1868979 1.2234264 1.2588954 1.2933921 1.3269922 1.3597624\n [22] 1.3917612 1.4230406 1.4536472 1.4836224 1.5130040 1.5418257 1.5701184\n [29] 1.5979103 1.6252269 1.6520920 1.6785272 1.7045524 1.7301862 1.7554457\n [36] 1.7803469 1.8049046 1.8291326 1.8530438 1.8766504 1.8999638 1.9229945\n [43] 1.9457526 1.9682476 1.9904884 2.0124834 2.0342406 2.0557676 2.0770714\n [50] 2.0981590 2.1190367 2.1397107 2.1601869 2.1804708 2.2005677 2.2204827\n [57] 2.2402208 2.2597864 2.2791840 2.2984180 2.3174923 2.3364109 2.3551776\n [64] 2.3737959 2.3922693 2.4106011 2.4287946 2.4468528 2.4647787 2.4825752\n [71] 2.5002450 2.5177908 2.5352151 2.5525206 2.5697094 2.5867841 2.6037468\n [78] 2.6205997 2.6373449 2.6539845 2.6705204 2.6869545 2.7032887 2.7195248\n [85] 2.7356646 2.7517097 2.7676618 2.7835224 2.7992932 2.8149757 2.8305712\n [92] 2.8460813 2.8615073 2.8768506 2.8921126 2.9072944 2.9223973 2.9374226\n [99] 2.9523714 2.9672449 2.9820442 2.9967704 3.0114246 3.0260079 3.0405212\n[106] 3.0549656 3.0693420 3.0836513 3.0978946 3.1120727 3.1261865 3.1402368\n[113] 3.1542246 3.1681506 3.1820156 3.1958205 3.2095661 3.2232530 3.2368820\n[120] 3.2504539 3.2639694 3.2774291 3.2908338 3.3041841 3.3174806 3.3307241\n[127] 3.3439151 3.3570544 3.3701423 3.3831797 3.3961670 3.4091048 3.4219937\n[134] 3.4348343 3.4476270 3.4603724 3.4730711 3.4857235 3.4983301 3.5108915\n[141] 3.5234081 3.5358803 3.5483088 3.5606938 3.5730360 3.5853356 3.5975932\n[148] 3.6098092 3.6219840 3.6341179 3.6462115 3.6582652 3.6702792 3.6822540\n[155] 3.6941901 3.7060877 3.7179472 3.7297690 3.7415534 3.7533009 3.7650117\n[162] 3.7766861 3.7883247 3.7999275 3.8114950 3.8230276 3.8345254 3.8459889\n[169] 3.8574183 3.8688140 3.8801762 3.8915052 3.9028013 3.9140648 3.9252960\n[176] 3.9364952 3.9476625 3.9587984 3.9699031 3.9809767 3.9920197 4.0030322\n\n\n\n\n\nThe two figures above show an idea of the short term and the long-term forecasted stock price. The forecasted values is be based on the estimated parameters of the model, which were obtained by fitting it to the historical data. One way to assess the accuracy of the forecasts is to use cross-validation, which involves splitting the data into training and testing sets and comparing the predicted values to the actual values in the test set. This can help to identify any potential problems with the model and to fine-tune the model parameters.\n\n12 Step ahead Cross Validation1 Step ahead Cross Validation\n\n\n\n\nCode\n#a seasonal cross validation using 12 steps ahead forecasts\nfarima1 <- function(x, h){forecast(Arima(x, order=c(0,1,0)), h=h)}\n\n# Compute cross-validated errors for up to 12 steps ahead\ne <- tsCV(myts, forecastfunction = farima1, h = 12)\n\n# Compute the MSE values and remove missing values\nmse <- colMeans(e^2, na.rm = TRUE)\n\n# Plot the MSE values against the forecast horizon\ndata.frame(h = 1:12, MSE = mse) %>%\n  ggplot(aes(x = h, y = MSE)) + geom_point()+geom_line()+ggtitle(\"12 Step Ahead Cross Validation\")\n\n\n\n\n\n\n\n\n\nCode\n#a seasonal cross validation using 1 steps ahead forecasts\nfarima1 <- function(x, h){forecast(Arima(x, order=c(0,1,0)),h=h)}\n\n# Compute cross-validated errors for up to 1 steps ahead\ne <- tsCV(myts, forecastfunction = farima1, h = 1)\nmse <-abs(mean(e,na.rm=TRUE))\n\n# Plot the MSE values against the forecast horizon\ndata.frame(h = 1:12, MSE = mse) %>%\n  ggplot(aes(x = h, y = MSE)) + geom_point()+geom_line()+ggtitle(\"1 Step Ahead Cross Validation\")\n\n\n\n\n\n\n\n\nThe one-step-ahead forecasting plot shows that the MSE seems to be stable. The MSE is tend to rise by 12 steps at each step of cross-validation. Lastly, the above-mentioned ARIMA model should be compared to benchmark methods before being used for a long time.\n\n\nModel Comparison\n\nPlotModel Error Table\n\n\n\n\nCode\nfit <- Arima(myts, order=c(0,1,0))\nautoplot(myts) +\n  autolayer(meanf(myts, h=182),\n            series=\"Mean\", PI=FALSE) +\n  autolayer(naive(myts, h=182),\n            series=\"Naïve\", PI=FALSE) +\n  autolayer(snaive(myts, h=182),\n            series=\"SNaïve\", PI=FALSE)+\n  autolayer(rwf(myts, h=182, drift=TRUE),\n            series=\"Drift\", PI=FALSE)+\n  autolayer(forecast(fit,182), \n            series=\"fit\",PI=FALSE) +\n  guides(colour=guide_legend(title=\"Forecast\"))\n\n\n\n\n\n\n\n\n\nCode\nsummary <- summary(fit)\nsnaive <- snaive(myts, h=182)\naccuracy(snaive)\n\n\n                   ME     RMSE      MAE      MPE     MAPE MASE      ACF1\nTraining set 2.036155 4.592548 3.271272 8.718375 14.98144    1 0.9947578\n\n\nCode\nsummary\n\n\nSeries: myts \nARIMA(0,1,0) \n\nsigma^2 = 0.08811:  log likelihood = -669.83\nAIC=1341.66   AICc=1341.66   BIC=1347.75\n\nTraining set error measures:\n                      ME      RMSE       MAE        MPE      MAPE       MASE\nTraining set 0.007888177 0.2967841 0.1888321 0.03010762 0.9793237 0.05772436\n                    ACF1\nTraining set -0.07370236\n\n\n\n\n\nError\nModel\nSnavie\n\n\n\n\nME\n0.001430649\n0.5394057\n\n\nRMSE\n0.4622021\n8.718076\n\n\nMAE\n0.17157\n5.297662\n\n\nMPE\n-0.02929688\n-9.401781\n\n\nMAPE\n1.02526\n37.78756\n\n\nMASE\n0.03238599\n1.0000000\n\n\nACF1\n-0.0292269\n0.996959\n\n\n\n\n\n\nThe ARIMA forecast tracks the actual data points very closely when compared to other model. The other forecast methods are less accurate when compared. From the table, Model error measurements of fit are much lower than snaive method.We can conclude that the fitted model is good."
  },
  {
    "objectID": "arima_sector_health_care.html",
    "href": "arima_sector_health_care.html",
    "title": "ARIMA Model for Health Care Sector Fund",
    "section": "",
    "text": "During exploratory data analysis (EDA), it was observed that the raw data is often non-stationary, exhibiting trends, seasonality, and other complex patterns that can make modeling and forecasting difficult. To address this issue, the data is often differentiated to achieve stationarity, which is a key assumption of the ARIMA model.\nDifferencing involves calculating the difference between each data point and the previous data point, which can help to remove trends and seasonality from the data. The goal is to create a new series of data that is stationary, meaning that the statistical properties of the data do not change over time.\n\nDifferentiated Time Series Plot\n\nPlotACFADF Test\n\n\n\n\nCode\n#import the data\ndf <- read.csv(\"DATA/CLEANED DATA/XLV_raw_data.csv\")\n#convert to time series data\nmyts<-ts(df$XLV.Adjusted,frequency=252,start=c(2010,1,1), end = c(2023,3,1)) \n#First order differentiation\ndf1 <- diff(myts)\n# Plot \nmyts  %>% diff() %>% ggtsdisplay(main = \"First order differentiation\") \n\n\n\n\n\n\n\n\n\nCode\n##### PACF\n#ACF plot \nggAcf(df1,60,main=\"ACF Plot: First order differentiation\") \n\n\n\n\n\n\n\nCode\nggPacf(df1,60,main=\"PACF Plot: First order differentiation\") \n\n\n\n\n\n\n\n\n\nCode\ntseries::adf.test(df1)\n\n\n\n    Augmented Dickey-Fuller Test\n\ndata:  df1\nDickey-Fuller = -16.669, Lag order = 14, p-value = 0.01\nalternative hypothesis: stationary\n\n\n\n\n\nAfter analyzing the ACF and PACF plots, it can be observed that most of the bar lines lie between the blue lines, indicating that the time series is stationary. This has been further confirmed through an Augmented Dickey-Fuller test as the p-value is less than 0.05. Once the data is stationary, the next step is to model it using ARIMA. The combination of ACF and PACF plots can help determine the appropriate values for p, d, and q in the ARIMA model. The order of differencing required to achieve stationarity gives the d value. The p value is determined by examining significant spikes at various lags in the PACF plot, while the q value is determined through significant spikes in the ACF plot. It is important to consider other methods such as grid search and information criteria to ensure the most appropriate model is selected.\nHere the parameters are d = 1 p = 0 (PACF Plot) q = 0 (ACF Plot)\n\n\nModel Selection\n\nARIMA ResultAuto Arima Model\n\n\n\n\nCode\nArima(myts,order=c(0,1,0),include.drift=TRUE) \n\n\nSeries: myts \nARIMA(0,1,0) with drift \n\nCoefficients:\n       drift\n      0.0330\ns.e.  0.0143\n\nsigma^2 = 0.6695:  log likelihood = -3993.26\nAIC=7990.52   AICc=7990.52   BIC=8002.71\n\n\n\n\n\n\nCode\nauto.arima(myts)\n\n\nSeries: myts \nARIMA(3,1,4) with drift \n\nCoefficients:\n          ar1     ar2     ar3     ma1      ma2      ma3      ma4   drift\n      -0.7614  0.7830  0.8135  0.7132  -0.7845  -0.7850  -0.0644  0.0331\ns.e.   0.0316  0.0237  0.0306  0.0356   0.0286   0.0348   0.0195  0.0068\n\nsigma^2 = 0.6408:  log likelihood = -3918.09\nAIC=7854.18   AICc=7854.24   BIC=7909.04\n\n\n\n\n\nIn the Model selection, chosen model is the same as the auto.arima generated model. We can proceed with the further model diagnostic for the chosen best model i.e. (0,1,0), to look into the details of the model.\n\n\nModel Diagnostic\n\nModel PlotModel\n\n\n\n\nCode\nmodel_output <- capture.output(sarima(myts,0,1,0))\n\n\n\n\n\n\n\n\n\nCode\ncat(model_output[9:38], model_output[length(model_output)], sep = \"\\n\") \n\n\n$fit\n\nCall:\narima(x = xdata, order = c(p, d, q), seasonal = list(order = c(P, D, Q), period = S), \n    xreg = constant, transform.pars = trans, fixed = fixed, optim.control = list(trace = trc, \n        REPORT = 1, reltol = tol))\n\nCoefficients:\n      constant\n        0.0330\ns.e.    0.0143\n\nsigma^2 estimated as 0.6693:  log likelihood = -3993.26,  aic = 7990.52\n\n$degrees_of_freedom\n[1] 3277\n\n$ttable\n         Estimate     SE t.value p.value\nconstant    0.033 0.0143  2.3091   0.021\n\n$AIC\n[1] 2.437619\n\n$AICc\n[1] 2.43762\n\n$BIC\n[1] 2.441338\n\n\n\n\n\nThe model diagnostic is done for ARIMA(0,1,0). By analyzing the standardized residuals plot, it can be observed that the mean is close to 0 and the variance is slightly higher than 1. Deviations from these values could indicate poor model fit. However, in this case, the model seems to be well-fitted. The ACF plot of residuals shows very few significant lags, which is a positive sign for the model. The qq-plot also suggests normality in the residuals. Additionally, the p-values for the Ljung-Box test are greater than 0.05, indicating that the residuals are independent, which is favorable for the model’s accuracy.\nThe equation for this model: \\[(1-\\phi_1B-\\phi_2B^2)(1-B)(Y_t-\\mu) = (1+\\theta_1B+\\theta_2B^2)\\epsilon_t\\]\n\n\nForcast\n\nShort Term ForcastLong term Forcast\n\n\n\n\nCode\nmyts %>%\n  Arima(order=c(0,1,0),include.drift = TRUE) %>%\n  forecast %>%\n  autoplot(main = \"Health Care Sector Fund Stock Prices Prediction\") +\n  ylab(\"stock prices\") + xlab(\"Year\")\n\n\n\n\n\n\n\n\n\nCode\nsarima.for(myts,182, 0,1,0, main='Health Care Sector Fund Stock Prices Prediction')\n\n\n\n\n\n$pred\nTime Series:\nStart = c(2023, 4) \nEnd = c(2023, 185) \nFrequency = 252 \n  [1] 133.0910 133.1240 133.1570 133.1900 133.2230 133.2560 133.2890 133.3220\n  [9] 133.3550 133.3880 133.4210 133.4540 133.4870 133.5199 133.5529 133.5859\n [17] 133.6189 133.6519 133.6849 133.7179 133.7509 133.7839 133.8169 133.8499\n [25] 133.8829 133.9159 133.9489 133.9819 134.0149 134.0479 134.0809 134.1139\n [33] 134.1469 134.1799 134.2129 134.2459 134.2789 134.3119 134.3449 134.3778\n [41] 134.4108 134.4438 134.4768 134.5098 134.5428 134.5758 134.6088 134.6418\n [49] 134.6748 134.7078 134.7408 134.7738 134.8068 134.8398 134.8728 134.9058\n [57] 134.9388 134.9718 135.0048 135.0378 135.0708 135.1038 135.1368 135.1698\n [65] 135.2028 135.2358 135.2687 135.3017 135.3347 135.3677 135.4007 135.4337\n [73] 135.4667 135.4997 135.5327 135.5657 135.5987 135.6317 135.6647 135.6977\n [81] 135.7307 135.7637 135.7967 135.8297 135.8627 135.8957 135.9287 135.9617\n [89] 135.9947 136.0277 136.0607 136.0937 136.1267 136.1596 136.1926 136.2256\n [97] 136.2586 136.2916 136.3246 136.3576 136.3906 136.4236 136.4566 136.4896\n[105] 136.5226 136.5556 136.5886 136.6216 136.6546 136.6876 136.7206 136.7536\n[113] 136.7866 136.8196 136.8526 136.8856 136.9186 136.9516 136.9846 137.0176\n[121] 137.0505 137.0835 137.1165 137.1495 137.1825 137.2155 137.2485 137.2815\n[129] 137.3145 137.3475 137.3805 137.4135 137.4465 137.4795 137.5125 137.5455\n[137] 137.5785 137.6115 137.6445 137.6775 137.7105 137.7435 137.7765 137.8095\n[145] 137.8425 137.8755 137.9085 137.9414 137.9744 138.0074 138.0404 138.0734\n[153] 138.1064 138.1394 138.1724 138.2054 138.2384 138.2714 138.3044 138.3374\n[161] 138.3704 138.4034 138.4364 138.4694 138.5024 138.5354 138.5684 138.6014\n[169] 138.6344 138.6674 138.7004 138.7334 138.7664 138.7993 138.8323 138.8653\n[177] 138.8983 138.9313 138.9643 138.9973 139.0303 139.0633\n\n$se\nTime Series:\nStart = c(2023, 4) \nEnd = c(2023, 185) \nFrequency = 252 \n  [1]  0.8181259  1.1570047  1.4170356  1.6362517  1.8293851  2.0039909\n  [7]  2.1645576  2.3140094  2.4543776  2.5871412  2.7134165  2.8340712\n [13]  2.9497948  3.0611467  3.1685879  3.2725035  3.3732194  3.4710141\n [19]  3.5661280  3.6587701  3.7491237  3.8373505  3.9235938  4.0079819\n [25]  4.0906294  4.1716398  4.2511067  4.3291152  4.4057427  4.4810599\n [31]  4.5551321  4.6280188  4.6997753  4.7704526  4.8400979  4.9087552\n [37]  4.9764654  5.0432666  5.1091944  5.1742823  5.2385616  5.3020616\n [43]  5.3648101  5.4268331  5.4881552  5.5487996  5.6087884  5.6681423\n [49]  5.7268811  5.7850235  5.8425874  5.8995896  5.9560462  6.0119728\n [55]  6.0673839  6.1222934  6.1767149  6.2306610  6.2841441  6.3371758\n [61]  6.3897673  6.4419296  6.4936728  6.5450070  6.5959416  6.6464860\n [67]  6.6966489  6.7464388  6.7958639  6.8449321  6.8936511  6.9420282\n [73]  6.9900705  7.0377849  7.0851779  7.1322560  7.1790254  7.2254921\n [79]  7.2716618  7.3175403  7.3631328  7.4084449  7.4534814  7.4982475\n [85]  7.5427478  7.5869872  7.6309701  7.6747010  7.7181840  7.7614235\n [91]  7.8044234  7.8471877  7.8897202  7.9320246  7.9741046  8.0159637\n [97]  8.0576054  8.0990329  8.1402496  8.1812587  8.2220633  8.2626663\n[103]  8.3030708  8.3432796  8.3832955  8.4231214  8.4627598  8.5022135\n[109]  8.5414849  8.5805765  8.6194909  8.6582304  8.6967973  8.7351939\n[115]  8.7734226  8.8114853  8.8493843  8.8871218  8.9246996  8.9621199\n[121]  8.9993846  9.0364956  9.0734548  9.1102641  9.1469253  9.1834401\n[127]  9.2198103  9.2560376  9.2921237  9.3280701  9.3638786  9.3995506\n[133]  9.4350878  9.4704917  9.5057636  9.5409052  9.5759178  9.6108029\n[139]  9.6455618  9.6801959  9.7147065  9.7490949  9.7833625  9.8175105\n[145]  9.8515401  9.8854525  9.9192490  9.9529308  9.9864990 10.0199547\n[151] 10.0532990 10.0865332 10.1196581 10.1526751 10.1855850 10.2183889\n[157] 10.2510878 10.2836827 10.3161747 10.3485647 10.3808536 10.4130423\n[163] 10.4451319 10.4771232 10.5090171 10.5408145 10.5725163 10.6041233\n[169] 10.6356363 10.6670563 10.6983840 10.7296202 10.7607658 10.7918214\n[175] 10.8227880 10.8536662 10.8844568 10.9151605 10.9457782 10.9763104\n[181] 11.0067579 11.0371214\n\n\n\n\n\nThe two figures above show an idea of the short term and the long-term forecasted stock price. The forecasted values is be based on the estimated parameters of the model, which were obtained by fitting it to the historical data. One way to assess the accuracy of the forecasts is to use cross-validation, which involves splitting the data into training and testing sets and comparing the predicted values to the actual values in the test set. This can help to identify any potential problems with the model and to fine-tune the model parameters.\n\n12 Step ahead Cross Validation1 Step ahead Cross Validation\n\n\n\n\nCode\n#a seasonal cross validation using 12 steps ahead forecasts\nfarima1 <- function(x, h){forecast(Arima(x, order=c(0,1,0)), h=h)}\n\n# Compute cross-validated errors for up to 12 steps ahead\ne <- tsCV(myts, forecastfunction = farima1, h = 12)\n\n# Compute the MSE values and remove missing values\nmse <- colMeans(e^2, na.rm = TRUE)\n\n# Plot the MSE values against the forecast horizon\ndata.frame(h = 1:12, MSE = mse) %>%\n  ggplot(aes(x = h, y = MSE)) + geom_point()+geom_line()+ggtitle(\"12 Step Ahead Cross Validation\")\n\n\n\n\n\n\n\n\n\nCode\n#a seasonal cross validation using 1 steps ahead forecasts\nfarima1 <- function(x, h){forecast(Arima(x, order=c(0,1,0)),h=h)}\n\n# Compute cross-validated errors for up to 1 steps ahead\ne <- tsCV(myts, forecastfunction = farima1, h = 1)\nmse <-abs(mean(e,na.rm=TRUE))\n\n# Plot the MSE values against the forecast horizon\ndata.frame(h = 1:12, MSE = mse) %>%\n  ggplot(aes(x = h, y = MSE)) + geom_point()+geom_line()+ggtitle(\"1 Step Ahead Cross Validation\")\n\n\n\n\n\n\n\n\nThe one-step-ahead forecasting plot shows that the MSE seems to be stable. The MSE is tend to rise by 12 steps at each step of cross-validation. Lastly, the above-mentioned ARIMA model should be compared to benchmark methods before being used for a long time.\n\n\nModel Comparison\n\nPlotModel Error Table\n\n\n\n\nCode\nfit <- Arima(myts, order=c(0,1,0))\nautoplot(myts) +\n  autolayer(meanf(myts, h=182),\n            series=\"Mean\", PI=FALSE) +\n  autolayer(naive(myts, h=182),\n            series=\"Naïve\", PI=FALSE) +\n  autolayer(snaive(myts, h=182),\n            series=\"SNaïve\", PI=FALSE)+\n  autolayer(rwf(myts, h=182, drift=TRUE),\n            series=\"Drift\", PI=FALSE)+\n  autolayer(forecast(fit,182), \n            series=\"fit\",PI=FALSE) +\n  guides(colour=guide_legend(title=\"Forecast\"))\n\n\n\n\n\n\n\n\n\nCode\nsummary <- summary(fit)\nsnaive <- snaive(myts, h=182)\naccuracy(snaive)\n\n\n                   ME     RMSE      MAE     MPE     MAPE MASE      ACF1\nTraining set 8.627682 11.30817 9.134217 12.5155 13.20282    1 0.9876419\n\n\nCode\nsummary\n\n\nSeries: myts \nARIMA(0,1,0) \n\nsigma^2 = 0.6704:  log likelihood = -3995.92\nAIC=7993.84   AICc=7993.84   BIC=7999.94\n\nTraining set error measures:\n                    ME      RMSE       MAE        MPE     MAPE       MASE\nTraining set 0.0329938 0.8186662 0.5129553 0.04557335 0.737573 0.05615756\n                    ACF1\nTraining set -0.07603346\n\n\n\n\n\nError\nModel\nSnavie\n\n\n\n\nME\n0.008081029\n2.821097\n\n\nRMSE\n1.64206\n33.39996\n\n\nMAE\n0.4678367\n20.38998\n\n\nMPE\n-0.03649654\n-16.47367\n\n\nMAPE\n0.8072921\n52.59869\n\n\nMASE\n0.02294444\n1.0000000\n\n\nACF1\n-0.003904598\n0.9973734\n\n\n\n\n\n\nThe ARIMA forecast tracks the actual data points very closely when compared to other model. The other forecast methods are less accurate when compared. From the table, Model error measurements of fit are much lower than snaive method.We can conclude that the fitted model is good."
  },
  {
    "objectID": "arima_sector_industrial.html",
    "href": "arima_sector_industrial.html",
    "title": "ARIMA Model for Industrial Sector Fund",
    "section": "",
    "text": "During exploratory data analysis (EDA), it was observed that the raw data is often non-stationary, exhibiting trends, seasonality, and other complex patterns that can make modeling and forecasting difficult. To address this issue, the data is often differentiated to achieve stationarity, which is a key assumption of the ARIMA model.\nDifferencing involves calculating the difference between each data point and the previous data point, which can help to remove trends and seasonality from the data. The goal is to create a new series of data that is stationary, meaning that the statistical properties of the data do not change over time.\n\nDifferentiated Time Series Plot\n\nPlotACFPACFADF Test\n\n\n\n\nCode\n#import the data\ndf <- read.csv(\"DATA/CLEANED DATA/XLI_raw_data.csv\")\n#convert to time series data\nmyts<-ts(df$XLI.Adjusted,frequency=252,start=c(2010,1,1), end = c(2023,3,1)) \n#First order differentiation\ndf1 <- diff(myts)\n# Plot \nmyts  %>% diff() %>% ggtsdisplay(main = \"First order differentiation\") \n\n\n\n\n\n\n\n\n\nCode\n#ACF plot \nggAcf(df1,60,main=\"ACF Plot: First order differentiation\") \n\n\n\n\n\n\n\n\n\nCode\nggPacf(df1,60,main=\"PACF Plot: First order differentiation\") \n\n\n\n\n\n\n\n\n\nCode\ntseries::adf.test(df1)\n\n\n\n    Augmented Dickey-Fuller Test\n\ndata:  df1\nDickey-Fuller = -14.662, Lag order = 14, p-value = 0.01\nalternative hypothesis: stationary\n\n\n\n\n\nAfter analyzing the ACF and PACF plots, it can be observed that most of the bar lines lie between the blue lines, indicating that the time series is stationary. This has been further confirmed through an Augmented Dickey-Fuller test as the p-value is less than 0.05. Once the data is stationary, the next step is to model it using ARIMA. The combination of ACF and PACF plots can help determine the appropriate values for p, d, and q in the ARIMA model. The order of differencing required to achieve stationarity gives the d value. The p value is determined by examining significant spikes at various lags in the PACF plot, while the q value is determined through significant spikes in the ACF plot. It is important to consider other methods such as grid search and information criteria to ensure the most appropriate model is selected.\nHere the parameters are d = 1 p = 0 (PACF Plot) q = 0 (ACF Plot)\n\n\nModel Selection\n\nARIMA ResultAuto Arima Model\n\n\n\n\nCode\nArima(myts,order=c(0,1,0),include.drift=TRUE) \n\n\nSeries: myts \nARIMA(0,1,0) with drift \n\nCoefficients:\n       drift\n      0.0240\ns.e.  0.0129\n\nsigma^2 = 0.5429:  log likelihood = -3649.68\nAIC=7303.37   AICc=7303.37   BIC=7315.56\n\n\n\n\n\n\nCode\nauto.arima(myts)\n\n\nSeries: myts \nARIMA(2,1,0) with drift \n\nCoefficients:\n          ar1     ar2  drift\n      -0.0363  0.0503  0.024\ns.e.   0.0174  0.0174  0.013\n\nsigma^2 = 0.5411:  log likelihood = -3643.14\nAIC=7294.28   AICc=7294.29   BIC=7318.66\n\n\n\n\n\nIn the Model selection, chosen model is the same as the auto.arima generated model. We can proceed with the further model diagnostic for the chosen best model i.e. (0,1,0), to look into the details of the model.\n\n\nModel Diagnostic\n\nModel PlotModel\n\n\n\n\nCode\nmodel_output <- capture.output(sarima(myts,0,1,0))\n\n\n\n\n\n\n\n\n\nCode\ncat(model_output[9:38], model_output[length(model_output)], sep = \"\\n\") \n\n\n$fit\n\nCall:\narima(x = xdata, order = c(p, d, q), seasonal = list(order = c(P, D, Q), period = S), \n    xreg = constant, transform.pars = trans, fixed = fixed, optim.control = list(trace = trc, \n        REPORT = 1, reltol = tol))\n\nCoefficients:\n      constant\n        0.0240\ns.e.    0.0129\n\nsigma^2 estimated as 0.5428:  log likelihood = -3649.68,  aic = 7303.37\n\n$degrees_of_freedom\n[1] 3277\n\n$ttable\n         Estimate     SE t.value p.value\nconstant    0.024 0.0129  1.8638  0.0624\n\n$AIC\n[1] 2.227994\n\n$AICc\n[1] 2.227995\n\n$BIC\n[1] 2.231713\n\n\n\n\n\nThe model diagnostic is done for ARIMA(0,1,0). By analyzing the standardized residuals plot, it can be observed that the mean is close to 0 and the variance is slightly higher than 1. Deviations from these values could indicate poor model fit. However, in this case, the model seems to be well-fitted. The ACF plot of residuals shows very few significant lags, which is a positive sign for the model. The qq-plot also suggests normality in the residuals. Additionally, the p-values for the Ljung-Box test are greater than 0.05, indicating that the residuals are independent, which is favorable for the model’s accuracy.\nThe equation for this model: \\[(1-\\phi_1B-\\phi_2B^2)(1-B)(Y_t-\\mu) = (1+\\theta_1B+\\theta_2B^2)\\epsilon_t\\]\n\n\nForcast\n\nShort Term ForcastLong term Forcast\n\n\n\n\nCode\nmyts %>%\n  Arima(order=c(0,1,0),include.drift = TRUE) %>%\n  forecast %>%\n  autoplot(main = \"Industrial Sector Fund Stock Prices Prediction\") +\n  ylab(\"stock prices\") + xlab(\"Year\")\n\n\n\n\n\n\n\n\n\nCode\nsarima.for(myts,182, 0,1,0, main='Industrial Sector Fund Stock Prices Prediction')\n\n\n\n\n\n$pred\nTime Series:\nStart = c(2023, 4) \nEnd = c(2023, 185) \nFrequency = 252 \n  [1] 100.2241 100.2481 100.2721 100.2961 100.3201 100.3441 100.3680 100.3920\n  [9] 100.4160 100.4400 100.4640 100.4879 100.5119 100.5359 100.5599 100.5839\n [17] 100.6079 100.6318 100.6558 100.6798 100.7038 100.7278 100.7517 100.7757\n [25] 100.7997 100.8237 100.8477 100.8717 100.8956 100.9196 100.9436 100.9676\n [33] 100.9916 101.0156 101.0395 101.0635 101.0875 101.1115 101.1355 101.1594\n [41] 101.1834 101.2074 101.2314 101.2554 101.2794 101.3033 101.3273 101.3513\n [49] 101.3753 101.3993 101.4232 101.4472 101.4712 101.4952 101.5192 101.5432\n [57] 101.5671 101.5911 101.6151 101.6391 101.6631 101.6871 101.7110 101.7350\n [65] 101.7590 101.7830 101.8070 101.8309 101.8549 101.8789 101.9029 101.9269\n [73] 101.9509 101.9748 101.9988 102.0228 102.0468 102.0708 102.0948 102.1187\n [81] 102.1427 102.1667 102.1907 102.2147 102.2386 102.2626 102.2866 102.3106\n [89] 102.3346 102.3586 102.3825 102.4065 102.4305 102.4545 102.4785 102.5024\n [97] 102.5264 102.5504 102.5744 102.5984 102.6224 102.6463 102.6703 102.6943\n[105] 102.7183 102.7423 102.7663 102.7902 102.8142 102.8382 102.8622 102.8862\n[113] 102.9101 102.9341 102.9581 102.9821 103.0061 103.0301 103.0540 103.0780\n[121] 103.1020 103.1260 103.1500 103.1739 103.1979 103.2219 103.2459 103.2699\n[129] 103.2939 103.3178 103.3418 103.3658 103.3898 103.4138 103.4378 103.4617\n[137] 103.4857 103.5097 103.5337 103.5577 103.5816 103.6056 103.6296 103.6536\n[145] 103.6776 103.7016 103.7255 103.7495 103.7735 103.7975 103.8215 103.8455\n[153] 103.8694 103.8934 103.9174 103.9414 103.9654 103.9893 104.0133 104.0373\n[161] 104.0613 104.0853 104.1093 104.1332 104.1572 104.1812 104.2052 104.2292\n[169] 104.2531 104.2771 104.3011 104.3251 104.3491 104.3731 104.3970 104.4210\n[177] 104.4450 104.4690 104.4930 104.5170 104.5409 104.5649\n\n$se\nTime Series:\nStart = c(2023, 4) \nEnd = c(2023, 185) \nFrequency = 252 \n  [1] 0.736717 1.041875 1.276031 1.473434 1.647349 1.804581 1.949170 2.083750\n  [9] 2.210151 2.329704 2.443414 2.552062 2.656271 2.756542 2.853293 2.946868\n [17] 3.037562 3.125625 3.211275 3.294698 3.376061 3.455509 3.533170 3.609161\n [25] 3.683585 3.756534 3.828094 3.898340 3.967342 4.035165 4.101866 4.167501\n [33] 4.232117 4.295761 4.358476 4.420302 4.481274 4.541428 4.600796 4.659407\n [41] 4.717290 4.774472 4.830976 4.886828 4.942048 4.996658 5.050677 5.104125\n [49] 5.157019 5.209376 5.261212 5.312542 5.363381 5.413742 5.463639 5.513085\n [57] 5.562091 5.610669 5.658830 5.706585 5.753943 5.800915 5.847510 5.893736\n [65] 5.939602 5.985117 6.030288 6.075124 6.119631 6.163816 6.207688 6.251251\n [73] 6.294513 6.337479 6.380156 6.422550 6.464665 6.506508 6.548084 6.589397\n [81] 6.630453 6.671256 6.711811 6.752123 6.792195 6.832032 6.871638 6.911018\n [89] 6.950174 6.989111 7.027832 7.066341 7.104641 7.142736 7.180629 7.218323\n [97] 7.255821 7.293126 7.330241 7.367170 7.403914 7.440477 7.476861 7.513068\n[105] 7.549103 7.584965 7.620660 7.656187 7.691551 7.726753 7.761795 7.796680\n[113] 7.831409 7.865985 7.900409 7.934685 7.968812 8.002795 8.036633 8.070330\n[121] 8.103887 8.137305 8.170586 8.203733 8.236746 8.269627 8.302379 8.335001\n[129] 8.367496 8.399866 8.432111 8.464234 8.496235 8.528116 8.559878 8.591522\n[137] 8.623051 8.654465 8.685765 8.716953 8.748029 8.778996 8.809854 8.840604\n[145] 8.871247 8.901785 8.932219 8.962549 8.992777 9.022903 9.052930 9.082857\n[153] 9.112686 9.142417 9.172052 9.201592 9.231037 9.260389 9.289648 9.318814\n[161] 9.347890 9.376876 9.405773 9.434581 9.463301 9.491934 9.520481 9.548943\n[169] 9.577321 9.605614 9.633824 9.661952 9.689999 9.717964 9.745849 9.773655\n[177] 9.801382 9.829030 9.856601 9.884095 9.911513 9.938855\n\n\n\n\n\nThe two figures above show an idea of the short term and the long-term forecasted stock price. The forecasted values is be based on the estimated parameters of the model, which were obtained by fitting it to the historical data. One way to assess the accuracy of the forecasts is to use cross-validation, which involves splitting the data into training and testing sets and comparing the predicted values to the actual values in the test set. This can help to identify any potential problems with the model and to fine-tune the model parameters.\n\n12 Step ahead Cross Validation1 Step ahead Cross Validation\n\n\n\n\nCode\n#a seasonal cross validation using 12 steps ahead forecasts\nfarima1 <- function(x, h){forecast(Arima(x, order=c(0,1,0)), h=h)}\n\n# Compute cross-validated errors for up to 12 steps ahead\ne <- tsCV(myts, forecastfunction = farima1, h = 12)\n\n# Compute the MSE values and remove missing values\nmse <- colMeans(e^2, na.rm = TRUE)\n\n# Plot the MSE values against the forecast horizon\ndata.frame(h = 1:12, MSE = mse) %>%\n  ggplot(aes(x = h, y = MSE)) + geom_point()+geom_line()+ggtitle(\"12 Step Ahead Cross Validation\")\n\n\n\n\n\n\n\n\n\nCode\n#a seasonal cross validation using 1 steps ahead forecasts\nfarima1 <- function(x, h){forecast(Arima(x, order=c(0,1,0)),h=h)}\n\n# Compute cross-validated errors for up to 1 steps ahead\ne <- tsCV(myts, forecastfunction = farima1, h = 1)\nmse <-abs(mean(e,na.rm=TRUE))\n\n# Plot the MSE values against the forecast horizon\ndata.frame(h = 1:12, MSE = mse) %>%\n  ggplot(aes(x = h, y = MSE)) + geom_point()+geom_line()+ggtitle(\"1 Step Ahead Cross Validation\")\n\n\n\n\n\n\n\n\nThe one-step-ahead forecasting plot shows that the MSE seems to be stable. The MSE is tend to rise by 12 steps at each step of cross-validation. Lastly, the above-mentioned ARIMA model should be compared to benchmark methods before being used for a long time.\n\n\nModel Comparison\n\nPlotModel Error Table\n\n\n\n\nCode\nfit <- Arima(myts, order=c(0,1,0))\nautoplot(myts) +\n  autolayer(meanf(myts, h=182),\n            series=\"Mean\", PI=FALSE) +\n  autolayer(naive(myts, h=182),\n            series=\"Naïve\", PI=FALSE) +\n  autolayer(snaive(myts, h=182),\n            series=\"SNaïve\", PI=FALSE)+\n  autolayer(rwf(myts, h=182, drift=TRUE),\n            series=\"Drift\", PI=FALSE)+\n  autolayer(forecast(fit,182), \n            series=\"fit\",PI=FALSE) +\n  guides(colour=guide_legend(title=\"Forecast\"))\n\n\n\n\n\n\n\n\n\nCode\nsummary <- summary(fit)\nsnaive <- snaive(myts, h=182)\naccuracy(snaive)\n\n\n                   ME   RMSE      MAE      MPE     MAPE MASE      ACF1\nTraining set 5.781288 10.775 7.847508 9.956244 13.12541    1 0.9937622\n\n\nCode\nsummary\n\n\nSeries: myts \nARIMA(0,1,0) \n\nsigma^2 = 0.5433:  log likelihood = -3651.42\nAIC=7304.84   AICc=7304.84   BIC=7310.93\n\nTraining set error measures:\n                     ME      RMSE       MAE      MPE      MAPE       MASE\nTraining set 0.02398147 0.7369949 0.4704777 0.038742 0.8629876 0.05995249\n                    ACF1\nTraining set -0.03821421\n\n\n\n\n\nError\nModel\nSnavie\n\n\n\n\nME\n0.004904666\n1.759821\n\n\nRMSE\n1.317807\n23.47883\n\n\nMAE\n0.4339241\n14.15936\n\n\nMPE\n-0.03607689\n-9.529885\n\n\nMAPE\n0.931558\n38.60253\n\n\nMASE\n0.03064573\n1.0000000\n\n\nACF1\n-0.006843027\n0.9965928\n\n\n\n\n\n\nThe ARIMA fitted forecast and snaive tracks the actual data points very closely when compared to other model. The other forecast methods are less accurate when compared. From the table, Model error measurements of fit are much lower than snaive method.We can conclude that the fitted model is good."
  },
  {
    "objectID": "arima_sector_market.html",
    "href": "arima_sector_market.html",
    "title": "ARIMA Model for Sector Market",
    "section": "",
    "text": "The ARIMA model is a commonly used time series analysis technique for forecasting the performance of sector markets. However, the raw data is often non-stationary, requiring differentiation to achieve stationarity, a key assumption of the model.\nWith the appropriate order of differencing and seasonality identified using statistical tests, the ARIMA model can be fitted to the data to identify the appropriate AR, I, and MA parameters. It may also be necessary to incorporate relevant exogenous variables into the model to improve forecast accuracy. Overall, the ARIMA model is a powerful tool for modeling and forecasting sector markets, allowing analysts to identify unique patterns and trends and make more accurate predictions about future performance.\nClick to view ARIMA Page for Consumer Staples Sector Fund\nClick to view ARIMA Page for Utilities Sector Fund\nClick to view ARIMA Page for Health Care Sector Fund\nClick to view ARIMA Page for Industrial Sector Fund\nClick to view ARIMA Page for Financial Sector Fund\nClick to view ARIMA Page for Consumer Discretionary Sector Fund\nClick to view ARIMA Page for Communication Services Sector Fund\nClick to view ARIMA Page for Real Estate Sector Fund\nClick to view ARIMA Page for Materials Sector Fund\nClick to view ARIMA Page for Technology Sector Fund\nClick to view ARIMA Page for Energy Sector Fund"
  },
  {
    "objectID": "arima_sector_materials_sector.html",
    "href": "arima_sector_materials_sector.html",
    "title": "ARIMA Model for Materials Sector Fund",
    "section": "",
    "text": "During exploratory data analysis (EDA), it was observed that the raw data is often non-stationary, exhibiting trends, seasonality, and other complex patterns that can make modeling and forecasting difficult. To address this issue, the data is often differentiated to achieve stationarity, which is a key assumption of the ARIMA model.\nDifferencing involves calculating the difference between each data point and the previous data point, which can help to remove trends and seasonality from the data. The goal is to create a new series of data that is stationary, meaning that the statistical properties of the data do not change over time.\n\nDifferentiated Time Series Plot\n\nPlotACFPACFADF Test\n\n\n\n\nCode\n#import the data\ndf <- read.csv(\"DATA/CLEANED DATA/XLB_raw_data.csv\")\n#convert to time series data\nmyts<-ts(df$XLB.Adjusted,frequency=252,start=c(2010,1,1), end = c(2023,3,1)) \n#First order differentiation\ndf1 <- diff(myts)\n# Plot \nmyts  %>% diff() %>% ggtsdisplay(main = \"First order differentiation\") \n\n\n\n\n\n\n\n\n\nCode\n#ACF plot \nggAcf(df1,60,main=\"ACF Plot: First order differentiation\") \n\n\n\n\n\n\n\n\n\nCode\nggPacf(df1,60,main=\"PACF Plot: First order differentiation\") \n\n\n\n\n\n\n\n\n\nCode\ntseries::adf.test(df1)\n\n\n\n    Augmented Dickey-Fuller Test\n\ndata:  df1\nDickey-Fuller = -15.19, Lag order = 14, p-value = 0.01\nalternative hypothesis: stationary\n\n\n\n\n\nAfter analyzing the ACF and PACF plots, it can be observed that most of the bar lines lie between the blue lines, indicating that the time series is stationary. This has been further confirmed through an Augmented Dickey-Fuller test as the p-value is less than 0.05. Once the data is stationary, the next step is to model it using ARIMA. The combination of ACF and PACF plots can help determine the appropriate values for p, d, and q in the ARIMA model. The order of differencing required to achieve stationarity gives the d value. The p value is determined by examining significant spikes at various lags in the PACF plot, while the q value is determined through significant spikes in the ACF plot. It is important to consider other methods such as grid search and information criteria to ensure the most appropriate model is selected.\nHere the parameters are d = 1 p = 0(PACF Plot) q = 0 (ACF Plot)\n\n\nModel Selection\n\nARIMA(0,1,0) ResultAuto Arima Model\n\n\n\n\nCode\nArima(myts,order=c(0,1,0),include.drift=TRUE) \n\n\nSeries: myts \nARIMA(0,1,0) with drift \n\nCoefficients:\n       drift\n      0.0171\ns.e.  0.0112\n\nsigma^2 = 0.4091:  log likelihood = -3185.81\nAIC=6375.63   AICc=6375.63   BIC=6387.82\n\n\n\n\n\n\nCode\nauto.arima(myts)\n\n\nSeries: myts \nARIMA(0,1,0) with drift \n\nCoefficients:\n       drift\n      0.0171\ns.e.  0.0112\n\nsigma^2 = 0.4091:  log likelihood = -3185.81\nAIC=6375.63   AICc=6375.63   BIC=6387.82\n\n\n\n\n\nIn the Model selection, the chosen model and the suto.arima generated model are the same i.e.(0,1,0). So this is the best model for th series. We can proceed with the further model diagnostic for the chosen best model i.e. (0,1,0), to look into the details of the model.\n\n\nModel Diagnostic\n\nModel PlotModel\n\n\n\n\nCode\nmodel_output <- capture.output(sarima(myts,0,1,0))\n\n\n\n\n\n\n\n\n\nCode\ncat(model_output[9:38], model_output[length(model_output)], sep = \"\\n\") \n\n\n$fit\n\nCall:\narima(x = xdata, order = c(p, d, q), seasonal = list(order = c(P, D, Q), period = S), \n    xreg = constant, transform.pars = trans, fixed = fixed, optim.control = list(trace = trc, \n        REPORT = 1, reltol = tol))\n\nCoefficients:\n      constant\n        0.0171\ns.e.    0.0112\n\nsigma^2 estimated as 0.409:  log likelihood = -3185.81,  aic = 6375.63\n\n$degrees_of_freedom\n[1] 3277\n\n$ttable\n         Estimate     SE t.value p.value\nconstant   0.0171 0.0112  1.5343   0.125\n\n$AIC\n[1] 1.944975\n\n$AICc\n[1] 1.944976\n\n$BIC\n[1] 1.948694\n\n\n\n\n\nThe model diagnostic is done for ARIMA(0,1,0). By analyzing the standardized residuals plot, it can be observed that the mean is close to 0 and the variance is slightly higher than 1. Deviations from these values could indicate poor model fit. However, in this case, the model seems to be well-fitted. The ACF plot of residuals shows very few significant lags, which is a positive sign for the model. The qq-plot also suggests normality in the residuals. Additionally, the p-values for the Ljung-Box test are greater than 0.05, indicating that the residuals are independent, which is favorable for the model’s accuracy.\nThe equation for this model: \\[(1-\\phi_1B-\\phi_2B^2)(1-B)(Y_t-\\mu) = (1+\\theta_1B+\\theta_2B^2)\\epsilon_t\\]\n\n\nForcast\n\nShort Term ForcastLong term Forcast\n\n\n\n\nCode\nmyts %>%\n  Arima(order=c(0,1,0),include.drift = TRUE) %>%\n  forecast %>%\n  autoplot(main = \"Materials Sector Fund Stock Prices Prediction\") +\n  ylab(\"stock prices\") + xlab(\"Year\")\n\n\n\n\n\n\n\n\n\nCode\nsarima.for(myts,182, 0,1,0, main='Materials Sector Fund Stock Prices Prediction')\n\n\n\n\n\n$pred\nTime Series:\nStart = c(2023, 4) \nEnd = c(2023, 185) \nFrequency = 252 \n  [1] 81.26254 81.27968 81.29681 81.31395 81.33109 81.34823 81.36536 81.38250\n  [9] 81.39964 81.41678 81.43392 81.45105 81.46819 81.48533 81.50247 81.51960\n [17] 81.53674 81.55388 81.57102 81.58816 81.60529 81.62243 81.63957 81.65671\n [25] 81.67384 81.69098 81.70812 81.72526 81.74240 81.75953 81.77667 81.79381\n [33] 81.81095 81.82808 81.84522 81.86236 81.87950 81.89664 81.91377 81.93091\n [41] 81.94805 81.96519 81.98233 81.99946 82.01660 82.03374 82.05088 82.06801\n [49] 82.08515 82.10229 82.11943 82.13657 82.15370 82.17084 82.18798 82.20512\n [57] 82.22225 82.23939 82.25653 82.27367 82.29081 82.30794 82.32508 82.34222\n [65] 82.35936 82.37649 82.39363 82.41077 82.42791 82.44505 82.46218 82.47932\n [73] 82.49646 82.51360 82.53073 82.54787 82.56501 82.58215 82.59929 82.61642\n [81] 82.63356 82.65070 82.66784 82.68498 82.70211 82.71925 82.73639 82.75353\n [89] 82.77066 82.78780 82.80494 82.82208 82.83922 82.85635 82.87349 82.89063\n [97] 82.90777 82.92490 82.94204 82.95918 82.97632 82.99346 83.01059 83.02773\n[105] 83.04487 83.06201 83.07914 83.09628 83.11342 83.13056 83.14770 83.16483\n[113] 83.18197 83.19911 83.21625 83.23338 83.25052 83.26766 83.28480 83.30194\n[121] 83.31907 83.33621 83.35335 83.37049 83.38763 83.40476 83.42190 83.43904\n[129] 83.45618 83.47331 83.49045 83.50759 83.52473 83.54187 83.55900 83.57614\n[137] 83.59328 83.61042 83.62755 83.64469 83.66183 83.67897 83.69611 83.71324\n[145] 83.73038 83.74752 83.76466 83.78179 83.79893 83.81607 83.83321 83.85035\n[153] 83.86748 83.88462 83.90176 83.91890 83.93603 83.95317 83.97031 83.98745\n[161] 84.00459 84.02172 84.03886 84.05600 84.07314 84.09028 84.10741 84.12455\n[169] 84.14169 84.15883 84.17596 84.19310 84.21024 84.22738 84.24452 84.26165\n[177] 84.27879 84.29593 84.31307 84.33020 84.34734 84.36448\n\n$se\nTime Series:\nStart = c(2023, 4) \nEnd = c(2023, 185) \nFrequency = 252 \n  [1] 0.6395049 0.9043965 1.1076550 1.2790098 1.4299764 1.5664607 1.6919709\n  [8] 1.8087930 1.9185147 2.0222921 2.1209978 2.2153100 2.3057677 2.3928083\n [15] 2.4767918 2.5580196 2.6367463 2.7131895 2.7875373 2.8599529 2.9305796\n [22] 2.9995439 3.0669578 3.1329214 3.1975245 3.2608480 3.3229650 3.3839419\n [29] 3.4438393 3.5027126 3.5606126 3.6175860 3.6736760 3.7289223 3.7833620\n [36] 3.8370294 3.8899565 3.9421730 3.9937069 4.0445841 4.0948294 4.1444655\n [43] 4.1935141 4.2419956 4.2899293 4.3373333 4.3842247 4.4306199 4.4765343\n [50] 4.5219825 4.5669785 4.6115355 4.6556660 4.6993821 4.7426953 4.7856165\n [57] 4.8281562 4.8703243 4.9121304 4.9535837 4.9946930 5.0354667 5.0759128\n [64] 5.1160392 5.1558534 5.1953624 5.2345732 5.2734925 5.3121267 5.3504819\n [71] 5.3885641 5.4263791 5.4639323 5.5012292 5.5382749 5.5750745 5.6116328\n [78] 5.6479544 5.6840439 5.7199058 5.7555441 5.7909632 5.8261670 5.8611593\n [85] 5.8959439 5.9305245 5.9649047 5.9990878 6.0330772 6.0668762 6.1004880\n [92] 6.1339156 6.1671620 6.2002301 6.2331228 6.2658428 6.2983929 6.3307756\n [99] 6.3629935 6.3950491 6.4269448 6.4586829 6.4902659 6.5216960 6.5529753\n[106] 6.5841060 6.6150902 6.6459299 6.6766272 6.7071840 6.7376023 6.7678838\n[113] 6.7980304 6.8280439 6.8579261 6.8876786 6.9173032 6.9468014 6.9761749\n[120] 7.0054252 7.0345540 7.0635625 7.0924525 7.1212252 7.1498822 7.1784248\n[127] 7.2068543 7.2351721 7.2633795 7.2914778 7.3194682 7.3473520 7.3751303\n[134] 7.4028045 7.4303755 7.4578447 7.4852130 7.5124816 7.5396516 7.5667241\n[141] 7.5937000 7.6205804 7.6473664 7.6740589 7.7006588 7.7271672 7.7535849\n[148] 7.7799129 7.8061522 7.8323035 7.8583678 7.8843460 7.9102388 7.9360472\n[155] 7.9617719 7.9874137 8.0129735 8.0384520 8.0638500 8.0891683 8.1144076\n[162] 8.1395686 8.1646521 8.1896587 8.2145892 8.2394443 8.2642247 8.2889309\n[169] 8.3135638 8.3381238 8.3626118 8.3870282 8.4113738 8.4356491 8.4598547\n[176] 8.4839913 8.5080594 8.5320596 8.5559925 8.5798586 8.6036586 8.6273928\n\n\n\n\n\nThe two figures above show an idea of the short term and the long-term forecasted stock price. The forecasted values is be based on the estimated parameters of the model, which were obtained by fitting it to the historical data. One way to assess the accuracy of the forecasts is to use cross-validation, which involves splitting the data into training and testing sets and comparing the predicted values to the actual values in the test set. This can help to identify any potential problems with the model and to fine-tune the model parameters.\n\n12 Step ahead Cross Validation1 Step ahead Cross Validation\n\n\n\n\nCode\n#a seasonal cross validation using 12 steps ahead forecasts\nfarima1 <- function(x, h){forecast(Arima(x, order=c(0,1,0)), h=h)}\n\n# Compute cross-validated errors for up to 12 steps ahead\ne <- tsCV(myts, forecastfunction = farima1, h = 12)\n\n# Compute the MSE values and remove missing values\nmse <- colMeans(e^2, na.rm = TRUE)\n\n# Plot the MSE values against the forecast horizon\ndata.frame(h = 1:12, MSE = mse) %>%\n  ggplot(aes(x = h, y = MSE)) + geom_point()+geom_line()+ggtitle(\"12 Step Ahead Cross Validation\")\n\n\n\n\n\n\n\n\n\nCode\n#a seasonal cross validation using 1 steps ahead forecasts\nfarima1 <- function(x, h){forecast(Arima(x, order=c(0,1,0)),h=h)}\n\n# Compute cross-validated errors for up to 1 steps ahead\ne <- tsCV(myts, forecastfunction = farima1, h = 1)\nmse <-abs(mean(e,na.rm=TRUE))\n\n# Plot the MSE values against the forecast horizon\ndata.frame(h = 1:12, MSE = mse) %>%\n  ggplot(aes(x = h, y = MSE)) + geom_point()+geom_line()+ggtitle(\"1 Step Ahead Cross Validation\")\n\n\n\n\n\n\n\n\nThe one-step-ahead forecasting plot shows that the MSE seems to be stable. The MSE is tend to rise by 12 steps at each step of cross-validation. Lastly, the above-mentioned ARIMA model should be compared to benchmark methods before being used for a long time.\n\n\nModel Comparison\n\nPlotModel Error Table\n\n\n\n\nCode\nfit <- Arima(myts, order=c(0,1,0))\nautoplot(myts) +\n  autolayer(meanf(myts, h=182),\n            series=\"Mean\", PI=FALSE) +\n  autolayer(naive(myts, h=182),\n            series=\"Naïve\", PI=FALSE) +\n  autolayer(snaive(myts, h=182),\n            series=\"SNaïve\", PI=FALSE)+\n  autolayer(rwf(myts, h=182, drift=TRUE),\n            series=\"Drift\", PI=FALSE)+\n  autolayer(forecast(fit,182), \n            series=\"fit\",PI=FALSE) +\n  guides(colour=guide_legend(title=\"Forecast\"))\n\n\n\n\n\n\n\n\n\nCode\nsummary <- summary(fit)\nsnaive <- snaive(myts, h=182)\naccuracy(snaive)\n\n\n                   ME    RMSE      MAE      MPE     MAPE MASE      ACF1\nTraining set 4.379863 8.90169 6.586829 8.128289 13.12814    1 0.9936968\n\n\nCode\nsummary\n\n\nSeries: myts \nARIMA(0,1,0) \n\nsigma^2 = 0.4093:  log likelihood = -3186.99\nAIC=6375.98   AICc=6375.98   BIC=6382.08\n\nTraining set error measures:\n                     ME      RMSE       MAE        MPE      MAPE       MASE\nTraining set 0.01714022 0.6396371 0.4338019 0.02656752 0.9608813 0.06585898\n                    ACF1\nTraining set -0.02017707\n\n\n\n\n\nError\nModel\nSnavie\n\n\n\n\nME\n0.002368649\n1.198017\n\n\nRMSE\n1.001375\n18.26109\n\n\nMAE\n0.4089937\n11.13495\n\n\nMPE\n-0.02319761\n-5.976606\n\n\nMAPE\n1.002138\n30.78121\n\n\nMASE\n0.03673064\n1.0000000\n\n\nACF1\n-0.01059568\n0.9967933\n\n\n\n\n\n\nThe ARIMA forecast tracks the actual data points very closely when compared to other model. The other forecast methods are less accurate when compared. From the table, Model error measurements of fit are much lower than snaive method.We can conclude that the fitted model is good."
  },
  {
    "objectID": "arima_sector_real_estate.html",
    "href": "arima_sector_real_estate.html",
    "title": "ARIMA Model for Real Estate Sector Fund",
    "section": "",
    "text": "During exploratory data analysis (EDA), it was observed that the raw data is often non-stationary, exhibiting trends, seasonality, and other complex patterns that can make modeling and forecasting difficult. To address this issue, the data is often differentiated to achieve stationarity, which is a key assumption of the ARIMA model.\nDifferencing involves calculating the difference between each data point and the previous data point, which can help to remove trends and seasonality from the data. The goal is to create a new series of data that is stationary, meaning that the statistical properties of the data do not change over time.\n\nDifferentiated Time Series Plot\n\nPlotACFPACFADF Test\n\n\n\n\nCode\n#import the data\ndf <- read.csv(\"DATA/CLEANED DATA/XLRE_raw_data.csv\")\n#convert to time series data\nmyts<-ts(df$XLRE.Adjusted,frequency=252,start=c(2016,1,1), end = c(2023,3,1)) \n#First order differentiation\ndf1 <- diff(myts)\n# Plot \nmyts  %>% diff() %>% ggtsdisplay(main = \"First order differentiation\") \n\n\n\n\n\n\n\n\n\nCode\n#ACF plot \nggAcf(df1,60,main=\"ACF Plot: First order differentiation\") \n\n\n\n\n\n\n\n\n\nCode\nggPacf(df1,60,main=\"PACF Plot: First order differentiation\") \n\n\n\n\n\n\n\n\n\nCode\ntseries::adf.test(df1)\n\n\n\n    Augmented Dickey-Fuller Test\n\ndata:  df1\nDickey-Fuller = -11.999, Lag order = 12, p-value = 0.01\nalternative hypothesis: stationary\n\n\n\n\n\nAfter analyzing the ACF and PACF plots, it can be observed that most of the bar lines lie between the blue lines, indicating that the time series is stationary. This has been further confirmed through an Augmented Dickey-Fuller test as the p-value is less than 0.05. Once the data is stationary, the next step is to model it using ARIMA. The combination of ACF and PACF plots can help determine the appropriate values for p, d, and q in the ARIMA model. The order of differencing required to achieve stationarity gives the d value. The p value is determined by examining significant spikes at various lags in the PACF plot, while the q value is determined through significant spikes in the ACF plot. It is important to consider other methods such as grid search and information criteria to ensure the most appropriate model is selected.\nHere the parameters are d = 1 p = 0,1,2,3,4 (PACF Plot) q = 0,1,2,3 (ACF Plot)\n\n\nModel Selection\n\nARIMA ResultAuto Arima Model\n\n\n\n\nCode\n######################## Check for different combinations ########\nd=1\ni=1\ntemp = data.frame()\nls=matrix(rep(NA,6*20),nrow=20) #nrow = 5x4x1\n\n\nfor (p in 1:5)# p=0,1,2,3,4 :5\n{\n  for(q in 1:4)# q=0,1,2,3 :4\n  {\n    for(d in 1)# d=1 :1\n    {\n      \n      if(p-1+d+q-1<=8)\n      {\n        \n        model<- Arima(myts,order=c(p-1,d,q-1),include.drift=TRUE) \n        ls[i,]= c(p-1,d,q-1,model$aic,model$bic,model$aicc)\n        i=i+1\n        \n      }\n      \n    }\n  }\n}\n\nmodel = as.data.frame(ls)\nnames(model)= c(\"p\",\"d\",\"q\",\"AIC\",\"BIC\",\"AICc\")\n\n#temp\nknitr::kable(model)\n\n\n\n\n\np\nd\nq\nAIC\nBIC\nAICc\n\n\n\n\n0\n1\n0\n2095.635\n2106.588\n2095.642\n\n\n0\n1\n1\n2091.010\n2107.440\n2091.024\n\n\n0\n1\n2\n2081.804\n2103.710\n2081.827\n\n\n0\n1\n3\n2083.632\n2111.014\n2083.666\n\n\n1\n1\n0\n2090.113\n2106.543\n2090.127\n\n\n1\n1\n1\n2089.931\n2111.837\n2089.954\n\n\n1\n1\n2\n2083.756\n2111.138\n2083.790\n\n\n1\n1\n3\n2043.011\n2075.870\n2043.059\n\n\n2\n1\n0\n2084.879\n2106.785\n2084.901\n\n\n2\n1\n1\n2086.138\n2113.520\n2086.172\n\n\n2\n1\n2\n2063.625\n2096.484\n2063.673\n\n\n2\n1\n3\n2034.287\n2072.622\n2034.350\n\n\n3\n1\n0\n2083.710\n2111.092\n2083.744\n\n\n3\n1\n1\n2036.952\n2069.811\n2037.000\n\n\n3\n1\n2\n2065.493\n2103.828\n2065.556\n\n\n3\n1\n3\n2067.351\n2111.163\n2067.433\n\n\n4\n1\n0\n2062.455\n2095.313\n2062.502\n\n\n4\n1\n1\n2032.845\n2071.180\n2032.908\n\n\n4\n1\n2\n2034.149\n2077.960\n2034.231\n\n\n4\n1\n3\n2035.702\n2084.990\n2035.805\n\n\n\n\n\n\n\n\n\nCode\nauto.arima(myts)\n\n\nSeries: myts \nARIMA(4,1,1) \n\nCoefficients:\n          ar1     ar2     ar3      ar4     ma1\n      -0.8638  0.0224  0.0905  -0.0666  0.8301\ns.e.   0.0477  0.0315  0.0315   0.0267  0.0422\n\nsigma^2 = 0.1842:  log likelihood = -1009.7\nAIC=2031.4   AICc=2031.45   BIC=2064.26\n\n\n\n\n\nThe given table provides different ARIMA models with their corresponding AIC and BIC values. By sorting the table according to each criterion, we can identify the models with the lowest values. The smallest AIC and BIC value corresponds to an ARIMA (3,1,1) mode. Additionally, the auto.arima function in R suggests an ARIMA (4,1,1) model as the best fit for the data. Since there are different models to choose from, it is important to perform model diagnostics to determine the best model for the data. These diagnostics can include checking for residual normality, checking for autocorrelation and partial autocorrelation in the residuals, and comparing the forecasted values to the actual values. By conducting these tests, we can determine which model provides the best fit for the data and is therefore the most appropriate to use for forecasting future values.\n\n\nModel Diagnostic\n\nModel 1 PlotModel 1Model 2 PlotModel 2\n\n\n\n\nCode\nmodel_output <- capture.output(sarima(myts,3,1,1))\n\n\n\n\n\n\n\n\n\nCode\ncat(model_output[46:79], model_output[length(model_output)], sep = \"\\n\") \n\n\nconverged\n$fit\n\nCall:\narima(x = xdata, order = c(p, d, q), seasonal = list(order = c(P, D, Q), period = S), \n    xreg = constant, transform.pars = trans, fixed = fixed, optim.control = list(trace = trc, \n        REPORT = 1, reltol = tol))\n\nCoefficients:\n          ar1     ar2     ar3     ma1  constant\n      -0.9175  0.0169  0.1407  0.8763    0.0075\ns.e.   0.0376  0.0322  0.0246  0.0307    0.0109\n\nsigma^2 estimated as 0.1843:  log likelihood = -1012.48,  aic = 2036.95\n\n$degrees_of_freedom\n[1] 1761\n\n$ttable\n         Estimate     SE  t.value p.value\nar1       -0.9175 0.0376 -24.4337  0.0000\nar2        0.0169 0.0322   0.5249  0.5997\nar3        0.1407 0.0246   5.7192  0.0000\nma1        0.8763 0.0307  28.5714  0.0000\nconstant   0.0075 0.0109   0.6904  0.4900\n\n$AIC\n[1] 1.153427\n\n$AICc\n[1] 1.153446\n\n$BIC\n[1] 1.172034\n\n\n\n\n\n\nCode\nmodel_output <- capture.output(sarima(myts,4,1,1))\n\n\n\n\n\n\n\n\n\nCode\ncat(model_output[53:86], model_output[length(model_output)], sep = \"\\n\") \n\n\ns.e.   0.0478  0.0315  0.0315   0.0267  0.0422    0.0103\n\nsigma^2 estimated as 0.1836:  log likelihood = -1009.42,  aic = 2032.84\n\n$degrees_of_freedom\n[1] 1760\n\n$ttable\n         Estimate     SE  t.value p.value\nar1       -0.8639 0.0478 -18.0921  0.0000\nar2        0.0219 0.0315   0.6930  0.4884\nar3        0.0899 0.0315   2.8502  0.0044\nar4       -0.0670 0.0267  -2.5066  0.0123\nma1        0.8299 0.0422  19.6468  0.0000\nconstant   0.0077 0.0103   0.7463  0.4556\n\n$AIC\n[1] 1.151101\n\n$AICc\n[1] 1.151128\n\n$BIC\n[1] 1.172809\n\nNA\nNA\nNA\nNA\nNA\nNA\nNA\nNA\nNA\n\n\n\n\n\nTo determine which model diagnosis is the best, we need to consider some key factors such as the coefficient estimates and standard errors, AIC, BIC, p-values, and the log-likelihood. Comparing the two models, model 1 is the best because it has the lowest BIC and AIC value when compared and the AIC value are almost the same, which indicate better goodness of fit. Also, all the coefficient estimates in the model are significant, as evidenced by the p-values are better than model 2, which implies that there is a strong correlation between the variables. The t-values are high, indicating that the estimates are highly reliable.\nTherefore, based on the AIC and BIC values, the significance of the coefficient estimates, and the reliability of the estimates, we can conclude that model 1 is the best model diagnosis. Therefore, (3,1,1) is the best model for this time series.\nThe best model identified is ARIMA(3,1,1). By analyzing the standardized residuals plot, it can be observed that the mean is close to 0 and the variance is slightly higher than 1. Deviations from these values could indicate poor model fit. However, in this case, the model seems to be well-fitted. The ACF plot of residuals shows very few significant lags, which is a positive sign for the model. The qq-plot also suggests normality in the residuals. Additionally, the p-values for the Ljung-Box test are greater than 0.05, indicating that the residuals are independent, which is favorable for the model’s accuracy.\nThe equation for this model: \\[(1-\\phi_1B-\\phi_2B^2-\\phi_3B^3)(Yt - \\mu) = (1+\\theta_1B)\\epsilon_t\\]\n\n\nForcast\n\nShort Term ForcastLong term Forcast\n\n\n\n\nCode\nmyts %>%\n  Arima(order=c(3,1,1),include.drift = TRUE) %>%\n  forecast %>%\n  autoplot(main = \"Real Estate Sector Fund Stock Prices Prediction\") +\n  ylab(\"stock prices\") + xlab(\"Year\")\n\n\n\n\n\n\n\n\n\nCode\nsarima.for(myts,182, 3,1,1, main='Real Estate Sector Fund Stock Prices Prediction')\n\n\n\n\n\n$pred\nTime Series:\nStart = c(2023, 4) \nEnd = c(2023, 185) \nFrequency = 252 \n  [1] 36.49861 36.70498 36.52527 36.69950 36.57886 36.68044 36.62294 36.67368\n  [9] 36.65367 36.67803 36.67571 36.68867 36.69340 36.70219 36.70926 36.71681\n [17] 36.72447 36.73180 36.73950 36.74687 36.75450 36.76194 36.76951 36.77699\n [25] 36.78454 36.79204 36.79957 36.80708 36.81460 36.82212 36.82964 36.83716\n [33] 36.84467 36.85219 36.85971 36.86723 36.87475 36.88227 36.88978 36.89730\n [41] 36.90482 36.91234 36.91986 36.92738 36.93489 36.94241 36.94993 36.95745\n [49] 36.96497 36.97249 36.98000 36.98752 36.99504 37.00256 37.01008 37.01760\n [57] 37.02511 37.03263 37.04015 37.04767 37.05519 37.06271 37.07023 37.07774\n [65] 37.08526 37.09278 37.10030 37.10782 37.11534 37.12285 37.13037 37.13789\n [73] 37.14541 37.15293 37.16045 37.16796 37.17548 37.18300 37.19052 37.19804\n [81] 37.20556 37.21307 37.22059 37.22811 37.23563 37.24315 37.25067 37.25818\n [89] 37.26570 37.27322 37.28074 37.28826 37.29578 37.30330 37.31081 37.31833\n [97] 37.32585 37.33337 37.34089 37.34841 37.35592 37.36344 37.37096 37.37848\n[105] 37.38600 37.39352 37.40103 37.40855 37.41607 37.42359 37.43111 37.43863\n[113] 37.44614 37.45366 37.46118 37.46870 37.47622 37.48374 37.49125 37.49877\n[121] 37.50629 37.51381 37.52133 37.52885 37.53637 37.54388 37.55140 37.55892\n[129] 37.56644 37.57396 37.58148 37.58899 37.59651 37.60403 37.61155 37.61907\n[137] 37.62659 37.63410 37.64162 37.64914 37.65666 37.66418 37.67170 37.67921\n[145] 37.68673 37.69425 37.70177 37.70929 37.71681 37.72432 37.73184 37.73936\n[153] 37.74688 37.75440 37.76192 37.76944 37.77695 37.78447 37.79199 37.79951\n[161] 37.80703 37.81455 37.82206 37.82958 37.83710 37.84462 37.85214 37.85966\n[169] 37.86717 37.87469 37.88221 37.88973 37.89725 37.90477 37.91228 37.91980\n[177] 37.92732 37.93484 37.94236 37.94988 37.95739 37.96491\n\n$se\nTime Series:\nStart = c(2023, 4) \nEnd = c(2023, 185) \nFrequency = 252 \n  [1] 0.4292664 0.5947151 0.7368675 0.8759513 0.9785349 1.0875044 1.1746933\n  [8] 1.2642500 1.3422545 1.4195790 1.4906612 1.5598449 1.6253157 1.6886734\n [15] 1.7495189 1.8084223 1.8654246 1.9207470 1.9745229 2.0268646 2.0778969\n [22] 2.1276981 2.1763657 2.2239645 2.2705684 2.3162330 2.3610158 2.4049640\n [29] 2.4481239 2.4905357 2.5322374 2.5732633 2.6136453 2.6534128 2.6925931\n [36] 2.7312114 2.7692911 2.8068543 2.8439214 2.8805116 2.9166427 2.9523317\n [43] 2.9875944 3.0224458 3.0568998 3.0909698 3.1246683 3.1580073 3.1909979\n [50] 3.2236510 3.2559766 3.2879844 3.3196836 3.3510830 3.3821909 3.4130152\n [57] 3.4435637 3.4738435 3.5038616 3.5336248 3.5631394 3.5924115 3.6214469\n [64] 3.6502515 3.6788305 3.7071892 3.7353326 3.7632655 3.7909926 3.8185184\n [71] 3.8458472 3.8729832 3.8999303 3.9266926 3.9532736 3.9796771 4.0059066\n [78] 4.0319655 4.0578570 4.0835844 4.1091507 4.1345589 4.1598119 4.1849126\n [85] 4.2098635 4.2346675 4.2593270 4.2838446 4.3082227 4.3324635 4.3565695\n [92] 4.3805429 4.4043858 4.4281003 4.4516884 4.4751522 4.4984937 4.5217146\n [99] 4.5448170 4.5678024 4.5906728 4.6134298 4.6360751 4.6586104 4.6810371\n[106] 4.7033569 4.7255713 4.7476817 4.7696897 4.7915966 4.8134037 4.8351126\n[113] 4.8567243 4.8782404 4.8996619 4.9209902 4.9422265 4.9633719 4.9844276\n[120] 5.0053947 5.0262744 5.0470677 5.0677756 5.0883993 5.1089398 5.1293980\n[127] 5.1497749 5.1700715 5.1902887 5.2104275 5.2304888 5.2504734 5.2703822\n[134] 5.2902161 5.3099759 5.3296625 5.3492766 5.3688191 5.3882906 5.4076921\n[141] 5.4270242 5.4462877 5.4654833 5.4846117 5.5036736 5.5226697 5.5416008\n[148] 5.5604673 5.5792701 5.5980097 5.6166868 5.6353020 5.6538559 5.6723491\n[155] 5.6907822 5.7091558 5.7274704 5.7457267 5.7639252 5.7820663 5.8001508\n[162] 5.8181790 5.8361515 5.8540689 5.8719316 5.8897401 5.9074949 5.9251965\n[169] 5.9428454 5.9604421 5.9779869 5.9954804 6.0129230 6.0303152 6.0476573\n[176] 6.0649499 6.0821933 6.0993879 6.1165342 6.1336326 6.1506834 6.1676871\n\n\n\n\n\nThe two figures above show an idea of the short term and the long-term forecasted stock price. The forecasted values is be based on the estimated parameters of the model, which were obtained by fitting it to the historical data. One way to assess the accuracy of the forecasts is to use cross-validation, which involves splitting the data into training and testing sets and comparing the predicted values to the actual values in the test set. This can help to identify any potential problems with the model and to fine-tune the model parameters.\n\n12 Step ahead Cross Validation1 Step ahead Cross Validation\n\n\n\n\nCode\n#a seasonal cross validation using 12 steps ahead forecasts\nfarima1 <- function(x, h){forecast(Arima(x, order=c(3,1,1)), h=h)}\n\n# Compute cross-validated errors for up to 12 steps ahead\ne <- tsCV(myts, forecastfunction = farima1, h = 12)\n\n# Compute the MSE values and remove missing values\nmse <- colMeans(e^2, na.rm = TRUE)\n\n# Plot the MSE values against the forecast horizon\ndata.frame(h = 1:12, MSE = mse) %>%\n  ggplot(aes(x = h, y = MSE)) + geom_point()+geom_line()+ggtitle(\"12 Step Ahead Cross Validation\")\n\n\n\n\n\n\n\n\n\nCode\n#a seasonal cross validation using 1 steps ahead forecasts\nfarima1 <- function(x, h){forecast(Arima(x, order=c(3,1,1)),h=h)}\n\n# Compute cross-validated errors for up to 1 steps ahead\ne <- tsCV(myts, forecastfunction = farima1, h = 1)\nmse <-abs(mean(e,na.rm=TRUE))\n\n# Plot the MSE values against the forecast horizon\ndata.frame(h = 1:12, MSE = mse) %>%\n  ggplot(aes(x = h, y = MSE)) + geom_point()+geom_line()+ggtitle(\"1 Step Ahead Cross Validation\")\n\n\n\n\n\n\n\n\nThe one-step-ahead forecasting plot shows that the MSE seems to be stable. The MSE is tend to rise by 12 steps at each step of cross-validation. Lastly, the above-mentioned ARIMA model should be compared to benchmark methods before being used for a long time.\n\n\nModel Comparison\n\nPlotModel Error Table\n\n\n\n\nCode\nfit <- Arima(myts, order=c(3,1,1))\nautoplot(myts) +\n  autolayer(meanf(myts, h=182),\n            series=\"Mean\", PI=FALSE) +\n  autolayer(naive(myts, h=182),\n            series=\"Naïve\", PI=FALSE) +\n  autolayer(snaive(myts, h=182),\n            series=\"SNaïve\", PI=FALSE)+\n  autolayer(rwf(myts, h=182, drift=TRUE),\n            series=\"Drift\", PI=FALSE)+\n  autolayer(forecast(fit,182), \n            series=\"fit\",PI=FALSE) +\n  guides(colour=guide_legend(title=\"Forecast\"))\n\n\n\n\n\n\n\n\n\nCode\nsummary <- summary(fit)\nsnaive <- snaive(myts, h=182)\naccuracy(snaive)\n\n\n                   ME     RMSE      MAE      MPE     MAPE MASE      ACF1\nTraining set 2.601795 5.786741 4.294961 6.922133 11.96095    1 0.9911468\n\n\nCode\nsummary\n\n\nSeries: myts \nARIMA(3,1,1) \n\nCoefficients:\n          ar1     ar2     ar3     ma1\n      -0.9171  0.0175  0.1410  0.8762\ns.e.   0.0376  0.0322  0.0246  0.0307\n\nsigma^2 = 0.1847:  log likelihood = -1012.72\nAIC=2035.44   AICc=2035.48   BIC=2062.82\n\nTraining set error measures:\n                      ME      RMSE       MAE        MPE      MAPE      MASE\nTraining set 0.007159992 0.4292049 0.2877009 0.01592699 0.9017147 0.0669857\n                    ACF1\nTraining set 0.007050705\n\n\n\n\n\nError\nModel\nSnavie\n\n\n\n\nME\n0.001165962\n0.3348181\n\n\nRMSE\n0.4838699\n8.889729\n\n\nMAE\n0.2666752\n6.46935\n\n\nMPE\n-0.007797867\n-2.538989\n\n\nMAPE\n0.8598005\n21.31273\n\n\nMASE\n0.04122133\n1.0000000\n\n\nACF1\n-0.001108128\n0.9965303\n\n\n\n\n\n\nThe ARIMA fitted and snaive forecast tracks the actual data points very closely when compared to other model. The other forecast methods are less accurate when compared. From the table, Model error measurements of fit are much lower than snaive method.We can conclude that the fitted model is good."
  },
  {
    "objectID": "arima_sector_technology.html",
    "href": "arima_sector_technology.html",
    "title": "ARIMA Model for Technology Sector Fund",
    "section": "",
    "text": "During exploratory data analysis (EDA), it was observed that the raw data is often non-stationary, exhibiting trends, seasonality, and other complex patterns that can make modeling and forecasting difficult. To address this issue, the data is often differentiated to achieve stationarity, which is a key assumption of the ARIMA model.\nDifferencing involves calculating the difference between each data point and the previous data point, which can help to remove trends and seasonality from the data. The goal is to create a new series of data that is stationary, meaning that the statistical properties of the data do not change over time.\n\nDifferentiated Time Series Plot\n\nPlotACFPACFADF Test\n\n\n\n\nCode\n#import the data\ndf <- read.csv(\"DATA/CLEANED DATA/XLK_raw_data.csv\")\n#convert to time series data\nmyts<-ts(df$XLK.Adjusted,frequency=252,start=c(2010,1,1), end = c(2023,3,1)) \n#First order differentiation\ndf1 <- diff(myts)\n# Plot \nmyts  %>% diff() %>% ggtsdisplay(main = \"First order differentiation\") \n\n\n\n\n\n\n\n\n\nCode\n#ACF plot \nggAcf(df1,60,main=\"ACF Plot: First order differentiation\") \n\n\n\n\n\n\n\n\n\nCode\nggPacf(df1,60,main=\"PACF Plot: First order differentiation\") \n\n\n\n\n\n\n\n\n\nCode\ntseries::adf.test(df1)\n\n\n\n    Augmented Dickey-Fuller Test\n\ndata:  df1\nDickey-Fuller = -15.488, Lag order = 14, p-value = 0.01\nalternative hypothesis: stationary\n\n\n\n\n\nAfter analyzing the ACF and PACF plots, it can be observed that most of the bar lines lie between the blue lines, indicating that the time series is stationary. This has been further confirmed through an Augmented Dickey-Fuller test as the p-value is less than 0.05. Once the data is stationary, the next step is to model it using ARIMA. The combination of ACF and PACF plots can help determine the appropriate values for p, d, and q in the ARIMA model. The order of differencing required to achieve stationarity gives the d value. The p value is determined by examining significant spikes at various lags in the PACF plot, while the q value is determined through significant spikes in the ACF plot. It is important to consider other methods such as grid search and information criteria to ensure the most appropriate model is selected.\nHere the parameters are d = 1 p = 0,7 (PACF Plot) q = 0,7 (ACF Plot)\n\n\nModel Selection\n\nARIMA(0,1,0) ResultARIMA(7,1,0) ResultARIMA(0,1,7) ResultARIMA(7,1,7) ResultAuto Arima Model\n\n\n\n\nCode\nArima(myts,order=c(0,1,0),include.drift=TRUE) \n\n\nSeries: myts \nARIMA(0,1,0) with drift \n\nCoefficients:\n       drift\n      0.0333\ns.e.  0.0207\n\nsigma^2 = 1.409:  log likelihood = -5213.33\nAIC=10430.66   AICc=10430.67   BIC=10442.85\n\n\n\n\n\n\nCode\nArima(myts,order=c(7,1,0),include.drift=TRUE) \n\n\nSeries: myts \nARIMA(7,1,0) with drift \n\nCoefficients:\n          ar1      ar2      ar3      ar4     ar5      ar6     ar7   drift\n      -0.1023  -0.0056  -0.0268  -0.0353  0.0269  -0.0618  0.0702  0.0333\ns.e.   0.0174   0.0175   0.0175   0.0175  0.0175   0.0175  0.0175  0.0180\n\nsigma^2 = 1.377:  log likelihood = -5171.97\nAIC=10361.93   AICc=10361.99   BIC=10416.79\n\n\n\n\n\n\nCode\nArima(myts,order=c(0,1,7),include.drift=TRUE) \n\n\nSeries: myts \nARIMA(0,1,7) with drift \n\nCoefficients:\n          ma1      ma2     ma3      ma4     ma5      ma6     ma7   drift\n      -0.0918  -0.0186  0.0008  -0.0610  0.0541  -0.0752  0.0686  0.0332\ns.e.   0.0176   0.0177  0.0177   0.0191  0.0185   0.0174  0.0172  0.0180\n\nsigma^2 = 1.378:  log likelihood = -5172.92\nAIC=10363.84   AICc=10363.89   BIC=10418.69\n\n\n\n\n\n\nCode\nArima(myts,order=c(7,1,7),include.drift=TRUE) \n\n\nSeries: myts \nARIMA(7,1,7) with drift \n\nCoefficients:\n          ar1     ar2      ar3      ar4     ar5     ar6     ar7     ma1\n      -0.6453  0.2792  -0.2041  -0.3923  0.5153  0.5485  0.2153  0.5622\ns.e.      NaN  0.1652      NaN   0.0769  0.1360     NaN     NaN     NaN\n          ma2     ma3     ma4      ma5      ma6      ma7   drift\n      -0.3557  0.2074  0.3462  -0.5458  -0.5473  -0.1313  0.0341\ns.e.   0.1681     NaN  0.0839   0.1448      NaN      NaN  0.0159\n\nsigma^2 = 1.349:  log likelihood = -5134.9\nAIC=10301.81   AICc=10301.97   BIC=10399.33\n\n\n\n\n\n\nCode\nauto.arima(myts)\n\n\nSeries: myts \nARIMA(0,1,1) with drift \n\nCoefficients:\n          ma1   drift\n      -0.1095  0.0332\ns.e.   0.0173  0.0184\n\nsigma^2 = 1.393:  log likelihood = -5193.58\nAIC=10393.16   AICc=10393.17   BIC=10411.44\n\n\n\n\n\nIn the Model selection, when comparing the AIC, AICc and BIC values of all the 4 models, model (0,1,0) has the least BIC and and (7,1,7) has the least AIC value. The model generated by auto.arima is also (0,1,0). We can proceed with the further model diagnostic comparison for both the model to find the best model.\n\n\nModel Diagnostic\n\nModel 1 PlotModel 1Model 2 PlotModel 2\n\n\n\n\nCode\nmodel_output <- capture.output(sarima(myts,7,1,7))\n\n\n\n\n\n\n\n\n\nCode\ncat(model_output[104:149], model_output[length(model_output)], sep = \"\\n\") \n\n\ninitial  value 0.171461 \niter   2 value 0.162864\niter   3 value 0.159782\niter   4 value 0.158708\niter   5 value 0.158307\niter   6 value 0.156015\niter   7 value 0.155472\niter   8 value 0.154363\niter   9 value 0.153948\niter  10 value 0.153606\niter  11 value 0.153151\niter  12 value 0.152418\niter  13 value 0.151507\niter  14 value 0.150418\niter  15 value 0.150141\niter  16 value 0.149644\niter  17 value 0.149471\niter  18 value 0.149398\niter  19 value 0.149363\niter  20 value 0.149357\niter  21 value 0.149353\niter  22 value 0.149352\niter  23 value 0.149351\niter  24 value 0.149348\niter  25 value 0.149341\niter  26 value 0.149321\niter  27 value 0.149300\niter  28 value 0.149239\niter  29 value 0.149184\niter  30 value 0.149094\niter  31 value 0.149077\niter  32 value 0.149054\niter  33 value 0.149047\niter  34 value 0.149045\niter  35 value 0.149044\niter  36 value 0.149042\niter  37 value 0.149039\niter  38 value 0.149030\niter  39 value 0.149020\niter  40 value 0.149014\niter  41 value 0.149003\niter  42 value 0.148985\niter  43 value 0.148980\niter  44 value 0.148975\niter  45 value 0.148967\niter  46 value 0.148956\n\n\n\n\n\n\nCode\nmodel_output <- capture.output(sarima(myts,0,1,0))\n\n\n\n\n\n\n\n\n\nCode\ncat(model_output[9:38], model_output[length(model_output)], sep = \"\\n\") \n\n\n$fit\n\nCall:\narima(x = xdata, order = c(p, d, q), seasonal = list(order = c(P, D, Q), period = S), \n    xreg = constant, transform.pars = trans, fixed = fixed, optim.control = list(trace = trc, \n        REPORT = 1, reltol = tol))\n\nCoefficients:\n      constant\n        0.0333\ns.e.    0.0207\n\nsigma^2 estimated as 1.409:  log likelihood = -5213.33,  aic = 10430.66\n\n$degrees_of_freedom\n[1] 3277\n\n$ttable\n         Estimate     SE t.value p.value\nconstant   0.0333 0.0207  1.6041  0.1088\n\n$AIC\n[1] 3.18202\n\n$AICc\n[1] 3.182021\n\n$BIC\n[1] 3.185739\n\n\n\n\n\nTo determine which model diagnosis is the best, we need to consider some key factors such as the coefficient estimates and standard errors, AIC, BIC, p-values, and the log-likelihood. Comparing the two models, model 2 is the best because it has the lowest BIC value when compared and the AIC value are almost the same, which indicate better goodness of fit. Also, all the coefficient estimates in the model are significant, as evidenced by the p-values are better than model 1, which implies that there is a strong correlation between the variables. The t-values are high, indicating that the estimates are highly reliable.\nTherefore, based on the AIC and BIC values, the significance of the coefficient estimates, and the reliability of the estimates, we can conclude that model 2 is the best model diagnosis. Therefore, (0,1,0) is the best model for this time series.\nThe best model identified is ARIMA(0,1,0). By analyzing the standardized residuals plot, it can be observed that the mean is close to 0 and the variance is slightly higher than 1. Deviations from these values could indicate poor model fit. However, in this case, the model seems to be well-fitted. The ACF plot of residuals shows very few significant lags, which is a positive sign for the model. The qq-plot also suggests normality in the residuals. Additionally, the p-values for the Ljung-Box test are greater than 0.05, indicating that the residuals are independent, which is favorable for the model’s accuracy.\nThe equation for this model: \\[(1-\\phi_1B-\\phi_2B^2)(1-B)(Y_t-\\mu) = (1+\\theta_1B+\\theta_2B^2)\\epsilon_t\\]\n\n\nForcast\n\nShort Term ForcastLong term Forcast\n\n\n\n\nCode\nmyts %>%\n  Arima(order=c(0,1,0),include.drift = TRUE) %>%\n  forecast %>%\n  autoplot(main = \"Technology Sector Fund Stock Prices Prediction\") +\n  ylab(\"stock prices\") + xlab(\"Year\")\n\n\n\n\n\n\n\n\n\nCode\nsarima.for(myts,182, 0,1,0, main='Technology Sector Fund Stock Prices Prediction')\n\n\n\n\n\n$pred\nTime Series:\nStart = c(2023, 4) \nEnd = c(2023, 185) \nFrequency = 252 \n  [1] 128.0828 128.1160 128.1493 128.1825 128.2158 128.2491 128.2823 128.3156\n  [9] 128.3488 128.3821 128.4153 128.4486 128.4819 128.5151 128.5484 128.5816\n [17] 128.6149 128.6482 128.6814 128.7147 128.7479 128.7812 128.8144 128.8477\n [25] 128.8810 128.9142 128.9475 128.9807 129.0140 129.0473 129.0805 129.1138\n [33] 129.1470 129.1803 129.2136 129.2468 129.2801 129.3133 129.3466 129.3798\n [41] 129.4131 129.4464 129.4796 129.5129 129.5461 129.5794 129.6127 129.6459\n [49] 129.6792 129.7124 129.7457 129.7790 129.8122 129.8455 129.8787 129.9120\n [57] 129.9452 129.9785 130.0118 130.0450 130.0783 130.1115 130.1448 130.1781\n [65] 130.2113 130.2446 130.2778 130.3111 130.3443 130.3776 130.4109 130.4441\n [73] 130.4774 130.5106 130.5439 130.5772 130.6104 130.6437 130.6769 130.7102\n [81] 130.7435 130.7767 130.8100 130.8432 130.8765 130.9097 130.9430 130.9763\n [89] 131.0095 131.0428 131.0760 131.1093 131.1426 131.1758 131.2091 131.2423\n [97] 131.2756 131.3089 131.3421 131.3754 131.4086 131.4419 131.4751 131.5084\n[105] 131.5417 131.5749 131.6082 131.6414 131.6747 131.7080 131.7412 131.7745\n[113] 131.8077 131.8410 131.8742 131.9075 131.9408 131.9740 132.0073 132.0405\n[121] 132.0738 132.1071 132.1403 132.1736 132.2068 132.2401 132.2734 132.3066\n[129] 132.3399 132.3731 132.4064 132.4396 132.4729 132.5062 132.5394 132.5727\n[137] 132.6059 132.6392 132.6725 132.7057 132.7390 132.7722 132.8055 132.8388\n[145] 132.8720 132.9053 132.9385 132.9718 133.0050 133.0383 133.0716 133.1048\n[153] 133.1381 133.1713 133.2046 133.2379 133.2711 133.3044 133.3376 133.3709\n[161] 133.4041 133.4374 133.4707 133.5039 133.5372 133.5704 133.6037 133.6370\n[169] 133.6702 133.7035 133.7367 133.7700 133.8033 133.8365 133.8698 133.9030\n[177] 133.9363 133.9695 134.0028 134.0361 134.0693 134.1026\n\n$se\nTime Series:\nStart = c(2023, 4) \nEnd = c(2023, 185) \nFrequency = 252 \n  [1]  1.187038  1.678726  2.056011  2.374077  2.654299  2.907638  3.140608\n  [8]  3.357452  3.561115  3.753745  3.936961  4.112022  4.279928  4.441491\n [15]  4.597380  4.748154  4.894285  5.036178  5.174181  5.308597  5.439693\n [22]  5.567704  5.692836  5.815277  5.935192  6.052732  6.168033  6.281217\n [29]  6.392398  6.501677  6.609150  6.714903  6.819017  6.921564  7.022614\n [36]  7.122231  7.220473  7.317396  7.413053  7.507490  7.600755  7.692888\n [43]  7.783932  7.873922  7.962896  8.050886  8.137925  8.224043  8.309269\n [50]  8.393629  8.477150  8.559856  8.641770  8.722915  8.803313  8.882982\n [57]  8.961944  9.040215  9.117815  9.194760  9.271066  9.346750  9.421825\n [64]  9.496307  9.570210  9.643546  9.716328  9.788570  9.860282  9.931476\n [71] 10.002164 10.072355 10.142061 10.211291 10.280054 10.348361 10.416220\n [78] 10.483640 10.550628 10.617194 10.683346 10.749090 10.814435 10.879387\n [85] 10.943954 11.008142 11.071957 11.135407 11.198498 11.261235 11.323625\n [92] 11.385673 11.447384 11.508765 11.569819 11.630554 11.690973 11.751081\n [99] 11.810883 11.870384 11.929589 11.988500 12.047124 12.105464 12.163524\n[106] 12.221309 12.278821 12.336065 12.393045 12.449764 12.506226 12.562434\n[113] 12.618392 12.674102 12.729569 12.784795 12.839784 12.894538 12.949060\n[120] 13.003354 13.057423 13.111268 13.164893 13.218300 13.271493 13.324473\n[127] 13.377244 13.429807 13.482165 13.534320 13.586276 13.638033 13.689595\n[134] 13.740963 13.792140 13.843128 13.893929 13.944544 13.994977 14.045228\n[141] 14.095300 14.145195 14.194915 14.244461 14.293835 14.343040 14.392076\n[148] 14.440946 14.489651 14.538192 14.586572 14.634793 14.682854 14.730759\n[155] 14.778509 14.826105 14.873549 14.920842 14.967985 15.014980 15.061829\n[162] 15.108533 15.155092 15.201509 15.247785 15.293920 15.339917 15.385776\n[169] 15.431500 15.477088 15.522542 15.567863 15.613053 15.658112 15.703042\n[176] 15.747844 15.792519 15.837068 15.881492 15.925792 15.969969 16.014024\n\n\n\n\n\nThe two figures above show an idea of the short term and the long-term forecasted stock price. The forecasted values is be based on the estimated parameters of the model, which were obtained by fitting it to the historical data. One way to assess the accuracy of the forecasts is to use cross-validation, which involves splitting the data into training and testing sets and comparing the predicted values to the actual values in the test set. This can help to identify any potential problems with the model and to fine-tune the model parameters.\n\n12 Step ahead Cross Validation1 Step ahead Cross Validation\n\n\n\n\nCode\n#a seasonal cross validation using 12 steps ahead forecasts\nfarima1 <- function(x, h){forecast(Arima(x, order=c(0,1,0)), h=h)}\n\n# Compute cross-validated errors for up to 12 steps ahead\ne <- tsCV(myts, forecastfunction = farima1, h = 12)\n\n# Compute the MSE values and remove missing values\nmse <- colMeans(e^2, na.rm = TRUE)\n\n# Plot the MSE values against the forecast horizon\ndata.frame(h = 1:12, MSE = mse) %>%\n  ggplot(aes(x = h, y = MSE)) + geom_point()+geom_line()+ggtitle(\"12 Step Ahead Cross Validation\")\n\n\n\n\n\n\n\n\n\nCode\n#a seasonal cross validation using 1 steps ahead forecasts\nfarima1 <- function(x, h){forecast(Arima(x, order=c(0,1,0)),h=h)}\n\n# Compute cross-validated errors for up to 1 steps ahead\ne <- tsCV(myts, forecastfunction = farima1, h = 1)\nmse <-abs(mean(e,na.rm=TRUE))\n\n# Plot the MSE values against the forecast horizon\ndata.frame(h = 1:12, MSE = mse) %>%\n  ggplot(aes(x = h, y = MSE)) + geom_point()+geom_line()+ggtitle(\"1 Step Ahead Cross Validation\")\n\n\n\n\n\n\n\n\nThe one-step-ahead forecasting plot shows that the MSE seems to be stable. The MSE is tend to rise by 12 steps at each step of cross-validation. Lastly, the above-mentioned ARIMA model should be compared to benchmark methods before being used for a long time.\n\n\nModel Comparison\n\nPlotModel Error Table\n\n\n\n\nCode\nfit <- Arima(myts, order=c(0,1,0))\nautoplot(myts) +\n  autolayer(meanf(myts, h=182),\n            series=\"Mean\", PI=FALSE) +\n  autolayer(naive(myts, h=182),\n            series=\"Naïve\", PI=FALSE) +\n  autolayer(snaive(myts, h=182),\n            series=\"SNaïve\", PI=FALSE)+\n  autolayer(rwf(myts, h=182, drift=TRUE),\n            series=\"Drift\", PI=FALSE)+\n  autolayer(forecast(fit,182), \n            series=\"fit\",PI=FALSE) +\n  guides(colour=guide_legend(title=\"Forecast\"))\n\n\n\n\n\n\n\n\n\nCode\nsummary <- summary(fit)\nsnaive <- snaive(myts, h=182)\naccuracy(snaive)\n\n\n                   ME     RMSE     MAE      MPE     MAPE MASE      ACF1\nTraining set 9.824487 18.21606 12.4263 14.42715 16.56907    1 0.9931727\n\n\nCode\nsummary\n\n\nSeries: myts \nARIMA(0,1,0) \n\nsigma^2 = 1.41:  log likelihood = -5214.62\nAIC=10431.23   AICc=10431.24   BIC=10437.33\n\nTraining set error measures:\n                     ME     RMSE       MAE        MPE     MAPE      MASE\nTraining set 0.03325436 1.187323 0.6341665 0.04900745 0.915725 0.0510342\n                   ACF1\nTraining set -0.1098027\n\n\n\n\n\nError\nModel\nSnavie\n\n\n\n\nME\n0.003719744\n1.368119\n\n\nRMSE\n1.980341\n40.85699\n\n\nMAE\n0.5360759\n22.68148\n\n\nMPE\n-0.08174304\n-34.13581\n\n\nMAPE\n0.9990605\n71.76659\n\n\nMASE\n0.02363496\n1.0000000\n\n\nACF1\n-0.02466494\n0.9974566\n\n\n\n\n\n\nThe ARIMA forecast tracks the actual data points very closely when compared to other model. The other forecast methods are less accurate when compared. From the table, Model error measurements of fit are much lower than snaive method.We can conclude that the fitted model is good."
  },
  {
    "objectID": "arima_sector_utilites.html",
    "href": "arima_sector_utilites.html",
    "title": "ARIMA Model for Utilities Sector Fund",
    "section": "",
    "text": "During exploratory data analysis (EDA), it was observed that the raw data is often non-stationary, exhibiting trends, seasonality, and other complex patterns that can make modeling and forecasting difficult. To address this issue, the data is often differentiated to achieve stationarity, which is a key assumption of the ARIMA model.\nDifferencing involves calculating the difference between each data point and the previous data point, which can help to remove trends and seasonality from the data. The goal is to create a new series of data that is stationary, meaning that the statistical properties of the data do not change over time.\n\nDifferentiated Time Series Plot\n\nPlotACFPACFADF Test\n\n\n\n\nCode\n#import the data\ndf <- read.csv(\"DATA/CLEANED DATA/XLU_raw_data.csv\")\n#convert to time series data\nmyts<-ts(df$XLU.Adjusted,frequency=252,start=c(2010,1,1), end = c(2023,3,1)) \n#First order differentiation\ndf1 <- diff(myts)\n# Plot \nmyts  %>% diff() %>% ggtsdisplay(main = \"First order differentiation\") \n\n\n\n\n\n\n\n\n\nCode\n#ACF plot \nggAcf(df1,60,main=\"ACF Plot: First order differentiation\") \n\n\n\n\n\n\n\n\n\nCode\nggPacf(df1,60,main=\"PACF Plot: First order differentiation\") \n\n\n\n\n\n\n\n\n\nCode\ntseries::adf.test(df1)\n\n\n\n    Augmented Dickey-Fuller Test\n\ndata:  df1\nDickey-Fuller = -15.702, Lag order = 14, p-value = 0.01\nalternative hypothesis: stationary\n\n\n\n\n\nAfter analyzing the ACF and PACF plots, it can be observed that most of the bar lines lie between the blue lines, indicating that the time series is stationary. This has been further confirmed through an Augmented Dickey-Fuller test as the p-value is less than 0.05. Once the data is stationary, the next step is to model it using ARIMA. The combination of ACF and PACF plots can help determine the appropriate values for p, d, and q in the ARIMA model. The order of differencing required to achieve stationarity gives the d value. The p value is determined by examining significant spikes at various lags in the PACF plot, while the q value is determined through significant spikes in the ACF plot. It is important to consider other methods such as grid search and information criteria to ensure the most appropriate model is selected.\nHere the parameters are d = 1 p = 0,9 (PACF Plot) q = 0,9 (ACF Plot)\n\n\nModel Selection\n\nARIMA(0,1,0) ResultARIMA(9,1,0) ResultARIMA(9,1,9) ResultARIMA(0,1,9) ResultAuto Arima Model\n\n\n\n\nCode\nArima(myts,order=c(0,1,0),include.drift=TRUE) \n\n\nSeries: myts \nARIMA(0,1,0) with drift \n\nCoefficients:\n       drift\n      0.0155\ns.e.  0.0090\n\nsigma^2 = 0.2669:  log likelihood = -2485.69\nAIC=4975.37   AICc=4975.38   BIC=4987.56\n\n\n\n\n\n\nCode\nArima(myts,order=c(9,1,0),include.drift=TRUE) \n\n\nSeries: myts \nARIMA(9,1,0) with drift \n\nCoefficients:\n          ar1     ar2     ar3      ar4     ar5      ar6     ar7      ar8\n      -0.0552  0.0417  0.0000  -0.0828  0.0718  -0.0601  0.0634  -0.0421\ns.e.   0.0173  0.0174  0.0173   0.0173  0.0173   0.0173  0.0174   0.0174\n         ar9   drift\n      0.1179  0.0155\ns.e.  0.0174  0.0093\n\nsigma^2 = 0.2547:  log likelihood = -2405\nAIC=4832   AICc=4832.08   BIC=4899.04\n\n\n\n\n\n\nCode\nArima(myts,order=c(9,1,9),include.drift=TRUE) \n\n\nSeries: myts \nARIMA(9,1,9) with drift \n\nCoefficients:\n          ar1     ar2      ar3      ar4      ar5     ar6      ar7      ar8\n      -0.3311  0.3528  -0.4807  -1.1841  -0.4268  0.0256  -0.1490  -0.5778\ns.e.   0.1624  0.2935   0.1519   0.2882   0.5276  0.2699   0.3303   0.1196\n          ar9     ma1      ma2     ma3     ma4     ma5     ma6     ma7     ma8\n      -0.3263  0.2793  -0.3212  0.5307  1.0766  0.4447  0.0015  0.1837  0.5359\ns.e.   0.2495  0.1599   0.2873  0.1613  0.2724  0.5351  0.2084  0.3143  0.1441\n         ma9   drift\n      0.3921  0.0150\ns.e.  0.2200  0.0088\n\nsigma^2 = 0.2518:  log likelihood = -2381.61\nAIC=4803.22   AICc=4803.48   BIC=4925.12\n\n\n\n\n\n\nCode\nArima(myts,order=c(0,1,9),include.drift=TRUE) \n\n\nSeries: myts \nARIMA(0,1,9) with drift \n\nCoefficients:\n          ma1     ma2     ma3      ma4     ma5      ma6     ma7      ma8\n      -0.0511  0.0451  0.0073  -0.0783  0.0864  -0.0794  0.0671  -0.0362\ns.e.   0.0174  0.0173  0.0173   0.0175  0.0180   0.0181  0.0174   0.0168\n         ma9   drift\n      0.0963  0.0155\ns.e.  0.0180  0.0093\n\nsigma^2 = 0.2559:  log likelihood = -2412.45\nAIC=4846.91   AICc=4846.99   BIC=4913.95\n\n\n\n\n\n\nCode\nauto.arima(myts)\n\n\nSeries: myts \nARIMA(2,1,0) with drift \n\nCoefficients:\n          ar1     ar2   drift\n      -0.0804  0.0579  0.0155\ns.e.   0.0174  0.0174  0.0088\n\nsigma^2 = 0.2642:  log likelihood = -2468.21\nAIC=4944.42   AICc=4944.43   BIC=4968.8\n\n\n\n\n\nIn the Model selection, when comparing the AIC, AICc and BIC values of all the 4 models, model (0,1,0) has the least BIC and AIc value. The model generated by auto.arima is also (0,1,0). We can proceed with the further model diagnostic for the chosen best model i.e. (0,1,0), to look into the details of the model.\n\n\nModel Diagnostic\n\nModel PlotModel\n\n\n\n\nCode\nmodel_output <- capture.output(sarima(myts,0,1,0))\n\n\n\n\n\n\n\n\n\nCode\ncat(model_output[9:38], model_output[length(model_output)], sep = \"\\n\") \n\n\n$fit\n\nCall:\narima(x = xdata, order = c(p, d, q), seasonal = list(order = c(P, D, Q), period = S), \n    xreg = constant, transform.pars = trans, fixed = fixed, optim.control = list(trace = trc, \n        REPORT = 1, reltol = tol))\n\nCoefficients:\n      constant\n        0.0155\ns.e.    0.0090\n\nsigma^2 estimated as 0.2668:  log likelihood = -2485.69,  aic = 4975.37\n\n$degrees_of_freedom\n[1] 3277\n\n$ttable\n         Estimate    SE t.value p.value\nconstant   0.0155 0.009  1.7165  0.0862\n\n$AIC\n[1] 1.517807\n\n$AICc\n[1] 1.517808\n\n$BIC\n[1] 1.521526\n\n\n\n\n\nThe model diagnostic is done for ARIMA(0,1,0). By analyzing the standardized residuals plot, it can be observed that the mean is close to 0 and the variance is slightly higher than 1. Deviations from these values could indicate poor model fit. However, in this case, the model seems to be well-fitted. The ACF plot of residuals shows very few significant lags, which is a positive sign for the model. The qq-plot also suggests normality in the residuals. Additionally, the p-values for the Ljung-Box test are greater than 0.05, indicating that the residuals are independent, which is favorable for the model’s accuracy.\nThe equation for this model: \\[(1-\\phi_1B-\\phi_2B^2)(1-B)(Y_t-\\mu) = (1+\\theta_1B+\\theta_2B^2)\\epsilon_t\\]\n\n\nForcast\n\nShort Term ForcastLong term Forcast\n\n\n\n\nCode\nmyts %>%\n  Arima(order=c(0,1,0),include.drift = TRUE) %>%\n  forecast %>%\n  autoplot(main = \"Utilities Sector Fund Stock Prices Prediction\") +\n  ylab(\"stock prices\") + xlab(\"Year\")\n\n\n\n\n\n\n\n\n\nCode\nsarima.for(myts,182, 0,1,0, main='Utilities Sector Fund Stock Prices Prediction')\n\n\n\n\n\n$pred\nTime Series:\nStart = c(2023, 4) \nEnd = c(2023, 185) \nFrequency = 252 \n  [1] 69.76949 69.78498 69.80046 69.81595 69.83143 69.84692 69.86240 69.87789\n  [9] 69.89338 69.90886 69.92435 69.93983 69.95532 69.97080 69.98629 70.00178\n [17] 70.01726 70.03275 70.04823 70.06372 70.07920 70.09469 70.11017 70.12566\n [25] 70.14115 70.15663 70.17212 70.18760 70.20309 70.21857 70.23406 70.24954\n [33] 70.26503 70.28052 70.29600 70.31149 70.32697 70.34246 70.35794 70.37343\n [41] 70.38892 70.40440 70.41989 70.43537 70.45086 70.46634 70.48183 70.49731\n [49] 70.51280 70.52829 70.54377 70.55926 70.57474 70.59023 70.60571 70.62120\n [57] 70.63668 70.65217 70.66766 70.68314 70.69863 70.71411 70.72960 70.74508\n [65] 70.76057 70.77606 70.79154 70.80703 70.82251 70.83800 70.85348 70.86897\n [73] 70.88445 70.89994 70.91543 70.93091 70.94640 70.96188 70.97737 70.99285\n [81] 71.00834 71.02382 71.03931 71.05480 71.07028 71.08577 71.10125 71.11674\n [89] 71.13222 71.14771 71.16320 71.17868 71.19417 71.20965 71.22514 71.24062\n [97] 71.25611 71.27159 71.28708 71.30257 71.31805 71.33354 71.34902 71.36451\n[105] 71.37999 71.39548 71.41096 71.42645 71.44194 71.45742 71.47291 71.48839\n[113] 71.50388 71.51936 71.53485 71.55034 71.56582 71.58131 71.59679 71.61228\n[121] 71.62776 71.64325 71.65873 71.67422 71.68971 71.70519 71.72068 71.73616\n[129] 71.75165 71.76713 71.78262 71.79810 71.81359 71.82908 71.84456 71.86005\n[137] 71.87553 71.89102 71.90650 71.92199 71.93748 71.95296 71.96845 71.98393\n[145] 71.99942 72.01490 72.03039 72.04587 72.06136 72.07685 72.09233 72.10782\n[153] 72.12330 72.13879 72.15427 72.16976 72.18524 72.20073 72.21622 72.23170\n[161] 72.24719 72.26267 72.27816 72.29364 72.30913 72.32462 72.34010 72.35559\n[169] 72.37107 72.38656 72.40204 72.41753 72.43301 72.44850 72.46399 72.47947\n[177] 72.49496 72.51044 72.52593 72.54141 72.55690 72.57238\n\n$se\nTime Series:\nStart = c(2023, 4) \nEnd = c(2023, 185) \nFrequency = 252 \n  [1] 0.5165181 0.7304668 0.8946355 1.0330361 1.1549695 1.2652057 1.3665783\n  [8] 1.4609337 1.5495542 1.6333735 1.7130966 1.7892710 1.8623323 1.9326336\n [15] 2.0004658 2.0660722 2.1296585 2.1914005 2.2514500 2.3099390 2.3669831\n [22] 2.4226844 2.4771336 2.5304113 2.5825903 2.6337356 2.6839065 2.7331566\n [29] 2.7815348 2.8290859 2.8758508 2.9218673 2.9671703 3.0117919 3.0557620\n [36] 3.0991083 3.1418567 3.1840311 3.2256542 3.2667470 3.3073293 3.3474196\n [43] 3.3870354 3.4261932 3.4649084 3.5031959 3.5410694 3.5785420 3.6156264\n [50] 3.6523342 3.6886767 3.7246646 3.7603082 3.7956170 3.8306004 3.8652672\n [57] 3.8996258 3.9336843 3.9674504 4.0009316 4.0341350 4.0670672 4.0997349\n [64] 4.1321444 4.1643017 4.1962125 4.2278825 4.2593170 4.2905212 4.3215001\n [71] 4.3522585 4.3828010 4.4131322 4.4432563 4.4731776 4.5029000 4.5324275\n [78] 4.5617639 4.5909129 4.6198779 4.6486625 4.6772699 4.7057034 4.7339661\n [85] 4.7620611 4.7899914 4.8177597 4.8453688 4.8728216 4.9001205 4.9272682\n [92] 4.9542671 4.9811197 5.0078283 5.0343952 5.0608227 5.0871129 5.1132678\n [99] 5.1392897 5.1651805 5.1909422 5.2165766 5.2420857 5.2674713 5.2927351\n[106] 5.3178788 5.3429043 5.3678131 5.3926068 5.4172870 5.4418554 5.4663133\n[113] 5.4906622 5.5149037 5.5390390 5.5630697 5.5869970 5.6108222 5.6345467\n[120] 5.6581718 5.6816986 5.7051284 5.7284623 5.7517016 5.7748474 5.7979008\n[127] 5.8208628 5.8437347 5.8665174 5.8892119 5.9118193 5.9343406 5.9567768\n[134] 5.9791287 6.0013974 6.0235838 6.0456888 6.0677132 6.0896580 6.1115240\n[141] 6.1333121 6.1550230 6.1766576 6.1982166 6.2197010 6.2411114 6.2624486\n[148] 6.2837133 6.3049063 6.3260284 6.3470801 6.3680622 6.3889755 6.4098205\n[155] 6.4305979 6.4513084 6.4719526 6.4925312 6.5130448 6.5334940 6.5538794\n[162] 6.5742015 6.5944610 6.6146585 6.6347945 6.6548696 6.6748843 6.6948391\n[169] 6.7147347 6.7345715 6.7543500 6.7740708 6.7937343 6.8133411 6.8328916\n[176] 6.8523863 6.8718257 6.8912103 6.9105406 6.9298169 6.9490397 6.9682095\n\n\n\n\n\nThe two figures above show an idea of the short term and the long-term forecasted stock price. The forecasted values is be based on the estimated parameters of the model, which were obtained by fitting it to the historical data. One way to assess the accuracy of the forecasts is to use cross-validation, which involves splitting the data into training and testing sets and comparing the predicted values to the actual values in the test set. This can help to identify any potential problems with the model and to fine-tune the model parameters.\n\n12 Step ahead Cross Validation1 Step ahead Cross Validation\n\n\n\n\nCode\n#a seasonal cross validation using 12 steps ahead forecasts\nfarima1 <- function(x, h){forecast(Arima(x, order=c(0,1,0)), h=h)}\n\n# Compute cross-validated errors for up to 12 steps ahead\ne <- tsCV(myts, forecastfunction = farima1, h = 12)\n\n# Compute the MSE values and remove missing values\nmse <- colMeans(e^2, na.rm = TRUE)\n\n# Plot the MSE values against the forecast horizon\ndata.frame(h = 1:12, MSE = mse) %>%\n  ggplot(aes(x = h, y = MSE)) + geom_point()+geom_line()+ggtitle(\"12 Step Ahead Cross Validation\")\n\n\n\n\n\n\n\n\n\nCode\n#a seasonal cross validation using 1 steps ahead forecasts\nfarima1 <- function(x, h){forecast(Arima(x, order=c(0,1,0)),h=h)}\n\n# Compute cross-validated errors for up to 1 steps ahead\ne <- tsCV(myts, forecastfunction = farima1, h = 1)\nmse <-abs(mean(e,na.rm=TRUE))\n\n# Plot the MSE values against the forecast horizon\ndata.frame(h = 1:12, MSE = mse) %>%\n  ggplot(aes(x = h, y = MSE)) + geom_point()+geom_line()+ggtitle(\"1 Step Ahead Cross Validation\")\n\n\n\n\n\n\n\n\nThe one-step-ahead forecasting plot shows that the MSE seems to be stable. The MSE is tend to rise by 12 steps at each step of cross-validation. Lastly, the above-mentioned ARIMA model should be compared to benchmark methods before being used for a long time.\n\n\nModel Comparison\n\nPlotModel Error Table\n\n\n\n\nCode\nfit <- Arima(myts, order=c(0,1,0))\nautoplot(myts) +\n  autolayer(meanf(myts, h=182),\n            series=\"Mean\", PI=FALSE) +\n  autolayer(naive(myts, h=182),\n            series=\"Naïve\", PI=FALSE) +\n  autolayer(snaive(myts, h=182),\n            series=\"SNaïve\", PI=FALSE)+\n  autolayer(rwf(myts, h=182, drift=TRUE),\n            series=\"Drift\", PI=FALSE)+\n  autolayer(forecast(fit,182), \n            series=\"fit\",PI=FALSE) +\n  guides(colour=guide_legend(title=\"Forecast\"))\n\n\n\n\n\n\n\n\n\nCode\nsummary <- summary(fit)\nsnaive <- snaive(myts, h=182)\naccuracy(snaive)\n\n\n                   ME     RMSE      MAE      MPE     MAPE MASE      ACF1\nTraining set 4.032984 5.327367 4.329826 9.786299 10.44789    1 0.9775654\n\n\nCode\nsummary\n\n\nSeries: myts \nARIMA(0,1,0) \n\nsigma^2 = 0.267:  log likelihood = -2487.16\nAIC=4976.32   AICc=4976.32   BIC=4982.41\n\nTraining set error measures:\n                     ME      RMSE       MAE        MPE      MAPE       MASE\nTraining set 0.01548667 0.5166714 0.3138152 0.03344792 0.7580014 0.07247755\n                    ACF1\nTraining set -0.08532622\n\n\n\n\n\nError\nModel\nSnavie\n\n\n\n\nME\n0.002908013\n1.118668\n\n\nRMSE\n0.8014312\n15.21741\n\n\nMAE\n0.2891511\n9.130549\n\n\nMPE\n-0.01774937\n-6.87212\n\n\nMAPE\n0.780873\n33.35526\n\n\nMASE\n0.03166853\n1.0000000\n\n\nACF1\n-0.006536676\n0.9970383\n\n\n\n\n\n\nThe ARIMA forecast tracks the actual data points very closely when compared to other model. The other forecast methods are less accurate when compared. From the table, Model error measurements of fit are much lower than snaive method.We can conclude that the fitted model is good."
  },
  {
    "objectID": "arima_stock_index.html",
    "href": "arima_stock_index.html",
    "title": "ARIMA Model for US Stock Indices",
    "section": "",
    "text": "The US stock market is one of the largest and most influential markets in the world, with the Dow Jones, Nasdaq, and S&P 500 indices serving as key indicators of market performance. To accurately forecast future values of these indices, it is essential to identify and model the complex patterns and trends that are often present in the data. The ARIMA model is a commonly used time series analysis technique that is well-suited for modeling and forecasting stock market indices, as it allows analysts to capture the underlying patterns and dynamics of the data. By fitting an ARIMA model to historical data, analysts can make more accurate predictions about future stock market performance, helping investors and financial institutions make informed decisions.\nClick to view the ARIMA Page for Dow Jones Index\nClick to view the ARIMA Page for NASDAQ Composite Index\nClick to view the ARIMA Page for S&P 500 Index"
  },
  {
    "objectID": "arimax_dow_jones_macroeconomic.html",
    "href": "arimax_dow_jones_macroeconomic.html",
    "title": "ARIMAX/SARIMAX Model for Dow Jones index and macroeconomic factors",
    "section": "",
    "text": "Endogenous variable: Dow Jones Index Exogenous variables: Macroeconomic indicators such as GDP, inflation rate, unemployment rate and interest rates\nBefore proceeding with model selection, it can be useful to examine the feature variables and select the appropriate ones. This can be done by creating cross-correlation function plots, which can serve as a verification of the correlation heatmap for the variables.\n\nCCF Plot for Dow Jones and Exogenous Variables\n\nGDPInterest RateInflation RateUnemployment Rate\n\n\n\n\nCode\npar(mfrow=c(1,1))\nccf_result <- ccf(normalized_index_factor_data[, c(\"DJI.Adjusted\")], normalized_index_factor_data[, c(\"gdp\")], \n    lag.max = 300,\n    main = \"Cros-Correlation Plot for Dow Jones Index and GDP Growth Rate \",\n    ylab = \"CCF\")\n\n\n\n\n\nCode\ncat(\"The sum of cross correlation function is\", sum(abs(ccf_result$acf)))\n\n\nThe sum of cross correlation function is 4.745225\n\n\n\n\n\n\nCode\npar(mfrow=c(1,1))\nccf_result <- ccf(normalized_index_factor_data[, c(\"DJI.Adjusted\")], normalized_index_factor_data[, c(\"interest\")], \n    lag.max = 300,\n    main = \"Cros-Correlation Plot for Dow Jones Index and Interest Rate\",\n    ylab = \"CCF\")\n\n\n\n\n\nCode\ncat(\"The sum of cross correlation function is\", sum(abs(ccf_result$acf)))\n\n\nThe sum of cross correlation function is 9.845121\n\n\n\n\n\n\nCode\npar(mfrow=c(1,1))\nccf_result <- ccf(normalized_index_factor_data[, c(\"DJI.Adjusted\")], normalized_index_factor_data[, c(\"inflation\")], \n    lag.max = 300,\n    main = \"Cros-Correlation Plot for Dow Jones Index and Inflation Rate\",\n    ylab = \"CCF\")\n\n\n\n\n\nCode\ncat(\"The sum of cross correlation function is\", sum(abs(ccf_result$acf)))\n\n\nThe sum of cross correlation function is 19.95814\n\n\n\n\n\n\nCode\npar(mfrow=c(1,1))\nccf_result <- ccf(normalized_index_factor_data[, c(\"DJI.Adjusted\")], normalized_index_factor_data[, c(\"unemployment\")], \n    lag.max = 300,\n    main = \"Cros-Correlation Plot for Dow Jones Index and Unemployment Rate\",\n    ylab = \"CCF\")\n\n\n\n\n\nCode\ncat(\"The sum of cross correlation function is\", sum(abs(ccf_result$acf)))\n\n\nThe sum of cross correlation function is 22.80055\n\n\n\n\n\nThe cross-correlation feature plots reaffirm the findings from the heatmap analysis, indicating that inflation and unemployment rate exhibit stronger correlations with the index compared to GDP and interest rates. The cross-correlation plots for GDP and interest rates show weaker and more scattered patterns, with strikes between the blue lines indicating lower correlation coefficients. This suggests that inflation and unemployment rate are more suitable feature variables for the ARIMAXmodel when predicting Dow Jones movements.\nFinal Exogenous variables: Macroeconomic indicators: Inflation rate and unemployment rate.\n\n\nEndogenous and Exogenous Variables Plot\n\n\nCode\nfinal_dj_factor_data <- index_factor_data %>%dplyr::select( Date,DJI.Adjusted, inflation,unemployment)\nnumeric_vars_dj_factor_data <- c(\"DJI.Adjusted\", \"inflation\", \"unemployment\")\nnumeric_dj_factor_data <- final_dj_factor_data[, numeric_vars_dj_factor_data]\nnormalized_dj_factor_data_numeric <- scale(numeric_dj_factor_data)\nnormalized_dj_factor_data_numeric_df <- data.frame(normalized_dj_factor_data_numeric)\nnormalized_dj_factor_data_ts <- ts(normalized_dj_factor_data_numeric, start = c(2010, 1), frequency = 4)\n\nautoplot(normalized_dj_factor_data_ts, facets=TRUE) +\n  xlab(\"Year\") + ylab(\"\") +\n  ggtitle(\"Dow Jones Index Stock Price, Inflation Rate and Unemployment Rate in USA 2010-2023\")\n\n\n\n\n\n\n\nCheck the stationarity\nBefore proceeding with further analysis, it’s important to check the stationarity of the time series data. If the series is not stationary, it needs to be differentiated before performing ARIMAX model. The stationarity of a multivariate time series data can be checked using the Phillips-Perron test, which is specifically designed for multivariate data. By performing the Phillips-Perron test on the multivariate time series data, we can determine if the data is stationary or if it requires differencing to make it suitable for ARIMAX modeling.\n\n\nCode\n# Convert your multivariate time series data to a matrix\nfinal_dj_factor_data_ts_multivariate <- as.matrix(normalized_dj_factor_data_ts)\n\n# Check for stationarity using Phillips-Perron test\nphillips_perron_test <- ur.pp(final_dj_factor_data_ts_multivariate)  \nsummary(phillips_perron_test)\n\n\n\n################################## \n# Phillips-Perron Unit Root Test # \n################################## \n\nTest regression with intercept \n\n\nCall:\nlm(formula = y ~ y.l1)\n\nResiduals:\n    Min      1Q  Median      3Q     Max \n-1.5621 -0.1561 -0.0549  0.0940  4.4921 \n\nCoefficients:\n            Estimate Std. Error t value Pr(>|t|)    \n(Intercept) 0.002691   0.040890   0.066    0.948    \ny.l1        0.857196   0.041305  20.753   <2e-16 ***\n---\nSignif. codes:  0 '***' 0.001 '**' 0.01 '*' 0.05 '.' 0.1 ' ' 1\n\nResidual standard error: 0.5091 on 153 degrees of freedom\nMultiple R-squared:  0.7379,    Adjusted R-squared:  0.7362 \nF-statistic: 430.7 on 1 and 153 DF,  p-value: < 2.2e-16\n\n\nValue of test-statistic, type: Z-alpha  is: -22.979 \n\n         aux. Z statistics\nZ-tau-mu            0.0656\n\n\nThe Phillips-Perron unit root test output suggests that the data is stationary. The test regression includes an intercept term and tests whether the coefficient on the lagged value is significantly different from one. The estimated coefficients indicate that the intercept is not significant (p-value > 0.05), while the coefficient on the lagged value is highly significant (p-value < 2.2e-16), suggesting that the data is stationary.\nAdditionally, the “Z-alpha” value is the test statistic, which is compared to critical values of the standard normal distribution to determine significance. In this case, the test statistic is -22.979, indicating strong evidence against the null hypothesis of a unit root. The “Z-tau-mu” value is an auxiliary test statistic used to correct for autocorrelation in the residuals, and it is close to zero, suggesting that there is no evidence of residual autocorrelation.\n\n\nFitting a ARIMAX model\nFitting an ARIMAX model will be a useful approach to modeling and predicting time series data when external factors are believed to have an impact on the response variable. In this case, we will be using an ARIMAX model to predict the DJI Index, taking into account the effects of inflation and unemployment on the Index. By including these exogenous variables in our model, we can improve the accuracy of our predictions and gain a better understanding of the underlying dynamics of the time series.\n\nAuto ARIMAResiduals\n\n\n\n\nCode\nxreg <- cbind(Inflation = normalized_dj_factor_data_ts[, \"inflation\"],\n              Unemployment = normalized_dj_factor_data_ts[, \"unemployment\"])\n\nfit <- auto.arima(normalized_dj_factor_data_ts[, \"DJI.Adjusted\"], xreg = xreg)\nsummary(fit)\n\n\nSeries: normalized_dj_factor_data_ts[, \"DJI.Adjusted\"] \nRegression with ARIMA(0,1,0) errors \n\nCoefficients:\n      Inflation  Unemployment\n         0.0999       -0.2337\ns.e.     0.0804        0.0365\n\nsigma^2 = 0.03424:  log likelihood = 14.7\nAIC=-23.4   AICc=-22.89   BIC=-17.6\n\nTraining set error measures:\n                     ME      RMSE       MAE      MPE     MAPE      MASE\nTraining set 0.03018628 0.1796281 0.1389231 4.691153 32.83951 0.3988319\n                    ACF1\nTraining set -0.07188426\n\n\n\n\n\n\nCode\ncheckresiduals(fit)\n\n\n\n\n\n\n    Ljung-Box test\n\ndata:  Residuals from Regression with ARIMA(0,1,0) errors\nQ* = 6.4181, df = 8, p-value = 0.6005\n\nModel df: 0.   Total lags used: 8\n\n\n\n\n\nThe auto.arima function reveals that the best-fit model is (0,1,0). Analysis of the residuals plot indicates that the fluctuates a lot and lies between -2000 to 2000.The ACF plot of the residuals shows significant lags, indicating that the model is performing well. The qq-plot also suggests normality in the residuals. The p-value greater than 0.05 actually suggests that there is no evidence of significant autocorrelation in the residuals, which means that the model is performing well in terms of capturing the temporal patterns in the data.\n\n\nFitting the model manually\n\nLinear ModelACF of ResidualsACF of ResidualsDifferentiated Residual\n\n\n\n\nCode\nnormalized_dj_factor_data_numeric_df$DJI.Adjusted<-ts(normalized_dj_factor_data_numeric_df$DJI.Adjusted,star=decimal_date(as.Date(\"2010-01-01\",format = \"%Y-%m-%d\")),frequency = 4)\nnormalized_dj_factor_data_numeric_df$inflation<-ts(normalized_dj_factor_data_numeric_df$inflation,star=decimal_date(as.Date(\"2010-01-01\",format = \"%Y-%m-%d\")),frequency = 4)\nnormalized_dj_factor_data_numeric_df$unemployment<-ts(normalized_dj_factor_data_numeric_df$unemployment,star=decimal_date(as.Date(\"2010-01-01\",format = \"%Y-%m-%d\")),frequency = 4)\n\n############# First fit the linear model##########\nfit.reg <- lm(DJI.Adjusted ~ inflation+unemployment, data=normalized_dj_factor_data_numeric_df)\nsummary(fit.reg)\n\n\n\nCall:\nlm(formula = DJI.Adjusted ~ inflation + unemployment, data = normalized_dj_factor_data_numeric_df)\n\nResiduals:\n    Min      1Q  Median      3Q     Max \n-0.8660 -0.4391 -0.2518  0.3505  1.6797 \n\nCoefficients:\n               Estimate Std. Error t value Pr(>|t|)    \n(Intercept)   1.180e-16  9.263e-02   0.000   1.0000    \ninflation     5.296e-01  1.067e-01   4.963 8.79e-06 ***\nunemployment -3.416e-01  1.067e-01  -3.202   0.0024 ** \n---\nSignif. codes:  0 '***' 0.001 '**' 0.01 '*' 0.05 '.' 0.1 ' ' 1\n\nResidual standard error: 0.668 on 49 degrees of freedom\nMultiple R-squared:  0.5713,    Adjusted R-squared:  0.5538 \nF-statistic: 32.65 on 2 and 49 DF,  p-value: 9.73e-10\n\n\n\n\n\n\nCode\nres.fit<-ts(residuals(fit.reg),star=decimal_date(as.Date(\"2010-01-01\",format = \"%Y-%m-%d\")),frequency = 4)\n############## Then look at the residuals ############\nacf(res.fit)\n\n\n\n\n\n\n\n\n\nCode\nPacf(res.fit)\n\n\n\n\n\n\n\n\n\nCode\nres.fit %>% diff() %>% ggtsdisplay()\n\n\n\n\n\n\n\n\nThe output displays the results of a linear regression model using two predictor variables, inflation and unemployment, to explain the variation in the response variable, DJI.Adjusted. The coefficients of inflation and unemployment have p-values of less than 0.05, indicating that both variables significantly impact DJI.Adjusted. The R-squared value of approximately 57% suggests that the model explains a moderate amount of the variation in the response variable. Examination of the ACF plot of residuals reveals some autocorrelation, and therefore, the series is differentiated to achieve stationarity. By analyzing the differentiated ACF and PACF plots, the parameters if the ARIMAX model are that p=4, q=1, and d=1.\n\n\nFinding the model parameters.\n\nARIMAX ResultModel 1 PlotModel 1Model 2 PlotModel 2\n\n\n\n\nCode\nARIMA.c=function(p1,p2,q1,q2,data){\ntemp=c()\nd=1\ni=1\ntemp= data.frame()\nls=matrix(rep(NA,6*30),nrow=30)\n\n\nfor (p in p1:p2)#\n{\n  for(q in q1:q2)#\n  {\n    for(d in 0:1)\n    {\n      \n      if(p+d+q<=6)\n      {\n        \n        model<- Arima(data,order=c(p,d,q))\n        ls[i,]= c(p,d,q,model$aic,model$bic,model$aicc)\n        i=i+1\n        #print(i)\n        \n      }\n      \n    }\n  }\n}\n\n\ntemp= as.data.frame(ls)\nnames(temp)= c(\"p\",\"d\",\"q\",\"AIC\",\"BIC\",\"AICc\")\n\ntemp\n}\n\noutput <- ARIMA.c(1,5,1,5,data=residuals(fit.reg))\n\noutput[which.min(output$AIC),] \n\n\n  p d q      AIC      BIC     AICc\n6 1 1 3 7.439979 17.09911 8.773313\n\n\nCode\noutput[which.min(output$BIC),]\n\n\n  p d q      AIC      BIC     AICc\n6 1 1 3 7.439979 17.09911 8.773313\n\n\nCode\noutput[which.min(output$AICc),]\n\n\n  p d q      AIC      BIC     AICc\n6 1 1 3 7.439979 17.09911 8.773313\n\n\n\n\n\n\nCode\nset.seed(1234)\n\nmodel_output_1 <- capture.output(sarima(res.fit, 1,1,3)) \n\n\n\n\n\n\n\n\n\nCode\ncat(model_output_1[45:78], model_output_1[length(model_output_1)], sep = \"\\n\")\n\n\n$fit\n\nCall:\narima(x = xdata, order = c(p, d, q), seasonal = list(order = c(P, D, Q), period = S), \n    xreg = constant, transform.pars = trans, fixed = fixed, optim.control = list(trace = trc, \n        REPORT = 1, reltol = tol))\n\nCoefficients:\n         ar1      ma1     ma2     ma3  constant\n      0.4565  -0.4496  0.1422  0.4319   -0.0068\ns.e.  0.2077   0.2094  0.1524  0.1736    0.0654\n\nsigma^2 estimated as 0.05442:  log likelihood = 1.29,  aic = 9.43\n\n$degrees_of_freedom\n[1] 46\n\n$ttable\n         Estimate     SE t.value p.value\nar1        0.4565 0.2077  2.1978  0.0330\nma1       -0.4496 0.2094 -2.1469  0.0371\nma2        0.1422 0.1524  0.9331  0.3557\nma3        0.4319 0.1736  2.4882  0.0165\nconstant  -0.0068 0.0654 -0.1040  0.9176\n\n$AIC\n[1] 0.1848859\n\n$AICc\n[1] 0.2110297\n\n$BIC\n[1] 0.4121595\n\n\n\n\n\n\nCode\nmodel_output_2 <- capture.output(sarima(res.fit,0,1,0)) \n\n\n\n\n\n\n\n\n\nCode\ncat(model_output_2[9:38], model_output_2[length(model_output_2)], sep = \"\\n\")\n\n\n$fit\n\nCall:\narima(x = xdata, order = c(p, d, q), seasonal = list(order = c(P, D, Q), period = S), \n    xreg = constant, transform.pars = trans, fixed = fixed, optim.control = list(trace = trc, \n        REPORT = 1, reltol = tol))\n\nCoefficients:\n      constant\n       -0.0052\ns.e.    0.0365\n\nsigma^2 estimated as 0.06791:  log likelihood = -3.78,  aic = 11.56\n\n$degrees_of_freedom\n[1] 50\n\n$ttable\n         Estimate     SE t.value p.value\nconstant  -0.0052 0.0365 -0.1417  0.8879\n\n$AIC\n[1] 0.2267275\n\n$AICc\n[1] 0.2283282\n\n$BIC\n[1] 0.3024854\n\n\n\n\n\nAfter manual fitting, we have identified that the ARIMAX model most suitable for our data is likely (1,1,3). A comparison of its AIC, AICc, and BIC values to those of other models indicates that it has the minimum values. We proceed to fit this model using both manual and auto.arima methods. After fitting, we observe that both models have p-values greater than 0, indicating that they are statistically significant. To determine the best fit model, we use cross-validation.\n\n\nCross Validation\n\nRMSE PlotRMSE Results\n\n\n\n\nCode\nn=length(res.fit)\nk= 51\n \n \nrmse1 <- matrix(NA, (n-k),4)\nrmse2 <- matrix(NA, (n-k),4)\n\n\nst <- tsp(res.fit)[1]+(k-5)/4 \n\nfor(i in 1:(n-k))\n{\n  xtrain <- window(res.fit, end=st + i/4)\n  xtest <- window(res.fit, start=st + (i+1)/4, end=st + (i+4)/4)\n  \n  #ARIMA(0,1,0) ARIMA(1,1,3)\n  \n  fit <- Arima(xtrain, order=c(1,1,3),\n                include.drift=TRUE, method=\"ML\")\n  fcast <- forecast(fit, h=4)\n  \n  fit2 <- Arima(xtrain, order=c(0,1,0),\n                include.drift=TRUE, method=\"ML\")\n  fcast2 <- forecast(fit2, h=4)\n\n  \n\n  rmse1[i,1:length(xtest)]   <- sqrt((fcast$mean-xtest)^2)\n  rmse2[i,1:length(xtest)] <- sqrt((fcast2$mean-xtest)^2)\n  \n}\n\nplot(1:4,colMeans(rmse1,na.rm=TRUE), type=\"l\",col=2, xlab=\"horizon\", ylab=\"RMSE\")\nlines(1:4, colMeans(rmse2,na.rm=TRUE), type=\"l\",col=3)\nlegend(\"topleft\",legend=c(\"fit1\",\"fit2\"),col=2:3,lty=1)\n\n\n\n\n\n\n\n\n\nCode\ncat(\"RMSE values for Model 1\\n\", colMeans(rmse1))\n\n\nRMSE values for Model 1\n 0.2600182 0.512597 0.8190045 1.189145\n\n\nCode\ncat(\"RMSE values for Model 2\\n\", colMeans(rmse2))\n\n\nRMSE values for Model 2\n 0.3674812 0.7675162 1.181429 1.556654\n\n\n\n\n\nThe RMSE cross-validation plot shows that the RMSE of Model 1 (1,1,3) is lower than that of Model 2 (0,1,0). This suggests that Model 1 (1,1,3) is a better fit for the data when compared to Model 2 (0,1,0).\n\n\nForecast\n\nForecast for DJI with feature variablesARIMA Model for InflationARIMA Model for EmploymentARIMA Model for DJI with feature variables\n\n\n\n\nCode\n#fiting an ARIMA model to the Inflation variable\ninflation_fit<-auto.arima(normalized_dj_factor_data_numeric_df$inflation) \nfinflation<-forecast(inflation_fit)\n\n#fitting an ARIMA model to the Unemployment variable\nunemployment_fit<-auto.arima(normalized_dj_factor_data_numeric_df$unemployment) \nfunemployment<-forecast(unemployment_fit)\n\n# best model fit for forcasting\nxreg <- cbind(Inflation = normalized_dj_factor_data_ts[, \"inflation\"],\n              Unemployment = normalized_dj_factor_data_ts[, \"unemployment\"])\n\nfit <- Arima(normalized_dj_factor_data_ts[, \"DJI.Adjusted\"],order=c(1,1,3),xreg=xreg)\n\n# Forcast the stock price using feature variables\nfxreg <- cbind(Inflation = finflation$mean,\n              Unemployment = funemployment$mean)\nfcast <- forecast(fit, xreg=fxreg) \nautoplot(fcast) + xlab(\"Year\") +\n  ylab(\"Normalised DJI.Adjusted\")\n\n\n\n\n\n\n\n\n\nCode\n#fiting an ARIMA model to the Inflation variable\ninflation_fit<-auto.arima(normalized_dj_factor_data_numeric_df$inflation) \nfinflation<-forecast(inflation_fit)\nsummary(inflation_fit)\n\n\nSeries: normalized_dj_factor_data_numeric_df$inflation \nARIMA(0,1,3)(0,0,1)[4] with drift \n\nCoefficients:\n         ma1     ma2     ma3     sma1   drift\n      0.2153  0.5354  0.4353  -0.3707  0.0567\ns.e.  0.1719  0.1915  0.1455   0.1942  0.0499\n\nsigma^2 = 0.0708:  log likelihood = -3.48\nAIC=18.96   AICc=20.87   BIC=30.55\n\nTraining set error measures:\n                      ME     RMSE       MAE       MPE    MAPE      MASE\nTraining set 0.002515126 0.250258 0.1802161 -17.11236 114.163 0.3180481\n                    ACF1\nTraining set -0.06141191\n\n\n\n\n\n\nCode\n#fitting an ARIMA model to the Unemployment variable\nunemployment_fit<-auto.arima(normalized_dj_factor_data_numeric_df$unemployment) \nfunemployment<-forecast(unemployment_fit)\nsummary(unemployment_fit)\n\n\nSeries: normalized_dj_factor_data_numeric_df$unemployment \nARIMA(0,1,2) \n\nCoefficients:\n          ma1      ma2\n      -0.3027  -0.2410\ns.e.   0.1376   0.1472\n\nsigma^2 = 0.4982:  log likelihood = -53.72\nAIC=113.45   AICc=113.96   BIC=119.24\n\nTraining set error measures:\n                     ME      RMSE      MAE      MPE     MAPE      MASE\nTraining set -0.1001611 0.6851533 0.281119 140.1191 219.8632 0.4565275\n                    ACF1\nTraining set -0.02079058\n\n\n\n\n\n\nCode\n# best model fit for forcasting\nxreg <- cbind(Inflation = normalized_dj_factor_data_ts[, \"inflation\"],\n              Unemployment = normalized_dj_factor_data_ts[, \"unemployment\"])\n\nfit <- Arima(normalized_dj_factor_data_ts[, \"DJI.Adjusted\"],order=c(1,1,3),xreg=xreg)\nsummary(fit)\n\n\nSeries: normalized_dj_factor_data_ts[, \"DJI.Adjusted\"] \nRegression with ARIMA(1,1,3) errors \n\nCoefficients:\n         ar1      ma1     ma2     ma3  Inflation  Unemployment\n      0.4207  -0.6167  0.3593  0.5494     0.2413       -0.2258\ns.e.  0.2317   0.2680  0.2682  0.2372     0.0521        0.0188\n\nsigma^2 = 0.02576:  log likelihood = 21.19\nAIC=-28.38   AICc=-25.77   BIC=-14.85\n\nTraining set error measures:\n                      ME      RMSE       MAE       MPE     MAPE      MASE\nTraining set 0.001852359 0.1492948 0.1197532 -2.903386 19.90147 0.3437973\n                    ACF1\nTraining set -0.02545389\n\n\n\n\n\nThe ARIMAX model may have incorporated exogenous variables such as macroeconomic factors, which could indicate the potential impact of external factors on the forecasted values. The findings suggest that inflation and unemployment rates significantly impact the DJI stock price, underscoring the importance of including macroeconomic factors in stock market forecasting. This is further supported by the comparison of RMSE, MAE, AIC and BIC values for forecasts with and without the feature variable. It is found that including the feature variable leads to a lower values, indicating that the feature variable makes a significant impact and should be considered in stock market forecasting."
  },
  {
    "objectID": "arimax_nasdaq_macroeconomic.html",
    "href": "arimax_nasdaq_macroeconomic.html",
    "title": "ARIMAX/SARIMAX Model for NASDAQ Composite index and macroeconomic factors",
    "section": "",
    "text": "Endogenous variable: NASDAQ Composite Index Exogenous variables: Macroeconomic indicators such as GDP, inflation rate, unemployment rate and interest rates\nBefore proceeding with model selection, it can be useful to examine the feature variables and select the appropriate ones. This can be done by creating cross-correlation function plots, which can serve as a verification of the correlation heatmap for the variables.\n\nCCF Plot for NASDAQ Composite and Exogenous Variables\n\nGDPInterest RateInflation RateUnemployment Rate\n\n\n\n\nCode\npar(mfrow=c(1,1))\nccf_result <- ccf(normalized_index_factor_data[, c(\"IXIC.Adjusted\")], normalized_index_factor_data[, c(\"gdp\")], \n    lag.max = 300,\n    main = \"Cros-Correlation Plot for NASDAQ Composite Index and GDP Growth Rate \",\n    ylab = \"CCF\")\n\n\n\n\n\nCode\ncat(\"The sum of cross correlation function is\", sum(abs(ccf_result$acf)))\n\n\nThe sum of cross correlation function is 4.754054\n\n\n\n\n\n\nCode\npar(mfrow=c(1,1))\nccf_result <- ccf(normalized_index_factor_data[, c(\"IXIC.Adjusted\")], normalized_index_factor_data[, c(\"interest\")], \n    lag.max = 300,\n    main = \"Cros-Correlation Plot for NASDAQ Composite Index and Interest Rate\",\n    ylab = \"CCF\")\n\n\n\n\n\nCode\ncat(\"The sum of cross correlation function is\", sum(abs(ccf_result$acf)))\n\n\nThe sum of cross correlation function is 10.60738\n\n\n\n\n\n\nCode\npar(mfrow=c(1,1))\nccf_result <- ccf(normalized_index_factor_data[, c(\"IXIC.Adjusted\")], normalized_index_factor_data[, c(\"inflation\")], \n    lag.max = 300,\n    main = \"Cros-Correlation Plot for NASDAQ Composite Index and Inflation Rate\",\n    ylab = \"CCF\")\n\n\n\n\n\nCode\ncat(\"The sum of cross correlation function is\", sum(abs(ccf_result$acf)))\n\n\nThe sum of cross correlation function is 18.798\n\n\n\n\n\n\nCode\npar(mfrow=c(1,1))\nccf_result <- ccf(normalized_index_factor_data[, c(\"IXIC.Adjusted\")], normalized_index_factor_data[, c(\"unemployment\")], \n    lag.max = 300,\n    main = \"Cros-Correlation Plot for NASDAQ Composite Index and Unemployment Rate\",\n    ylab = \"CCF\")\n\n\n\n\n\nCode\ncat(\"The sum of cross correlation function is\", sum(abs(ccf_result$acf)))\n\n\nThe sum of cross correlation function is 22.22389\n\n\n\n\n\nThe cross-correlation feature plots reaffirm the findings from the heatmap analysis, indicating that inflation and unemployment rate exhibit stronger correlations with the index compared to GDP and interest rates. The cross-correlation plots for GDP and interest rates show weaker and more scattered patterns, with strikes between the blue lines indicating lower correlation coefficients. This suggests that inflation and unemployment rate are more suitable feature variables for the ARIMAX model when predicting NASDAQ Composite movements.\nFinal Exogenous variables: Macroeconomic indicators: Inflation rate and unemployment rate.\n\n\nEndogenous and Exogenous Variables Plot\n\n\nCode\nfinal_nadsaq_factor_data <- index_factor_data %>%dplyr::select( Date,IXIC.Adjusted, inflation,unemployment)\nnumeric_vars_nadsaq_factor_data <- c(\"IXIC.Adjusted\", \"inflation\", \"unemployment\")\nnumeric_nadsaq_factor_data <- final_nadsaq_factor_data[, numeric_vars_nadsaq_factor_data]\nnormalized_nadsaq_factor_data_numeric <- scale(numeric_nadsaq_factor_data)\nnormalized_nadsaq_factor_data_numeric_df <- data.frame(normalized_nadsaq_factor_data_numeric)\nnormalized_nadsaq_factor_data_ts <- ts(normalized_nadsaq_factor_data_numeric, start = c(2010, 1), frequency = 4)\n\nautoplot(normalized_nadsaq_factor_data_ts, facets=TRUE) +\n  xlab(\"Year\") + ylab(\"\") +\n  ggtitle(\"NASDAQ Composite Index Stock Price, Inflation Rate and Unemployment Rate in USA 2010-2023\")\n\n\n\n\n\n\n\nCheck the stationarity\nBefore proceeding with further analysis, it’s important to check the stationarity of the time series data. If the series is not stationary, it needs to be differentiated before performing ARIMAX model. The stationarity of a multivariate time series data can be checked using the Phillips-Perron test, which is specifically designed for multivariate data. By performing the Phillips-Perron test on the multivariate time series data, we can determine if the data is stationary or if it requires differencing to make it suitable for ARIMAX modeling.\n\n\nCode\n# Convert your multivariate time series data to a matrix\nfinal_nadsaq_factor_data_ts_multivariate <- as.matrix(normalized_nadsaq_factor_data_ts)\n\n# Check for stationarity using Phillips-Perron test\nphillips_perron_test <- ur.pp(final_nadsaq_factor_data_ts_multivariate)  \nsummary(phillips_perron_test)\n\n\n\n################################## \n# Phillips-Perron Unit Root Test # \n################################## \n\nTest regression with intercept \n\n\nCall:\nlm(formula = y ~ y.l1)\n\nResiduals:\n    Min      1Q  Median      3Q     Max \n-1.5440 -0.1528 -0.0519  0.0850  4.4989 \n\nCoefficients:\n            Estimate Std. Error t value Pr(>|t|)    \n(Intercept) 0.001161   0.040544   0.029    0.977    \ny.l1        0.861900   0.040955  21.045   <2e-16 ***\n---\nSignif. codes:  0 '***' 0.001 '**' 0.01 '*' 0.05 '.' 0.1 ' ' 1\n\nResidual standard error: 0.5048 on 153 degrees of freedom\nMultiple R-squared:  0.7432,    Adjusted R-squared:  0.7416 \nF-statistic: 442.9 on 1 and 153 DF,  p-value: < 2.2e-16\n\n\nValue of test-statistic, type: Z-alpha  is: -23.0718 \n\n         aux. Z statistics\nZ-tau-mu            0.0293\n\n\nThe Phillips-Perron unit root test output suggests that the data is stationary. The test regression includes an intercept term and tests whether the coefficient on the lagged value is significantly different from one. The estimated coefficients indicate that the intercept is not significant (p-value > 0.05), while the coefficient on the lagged value is highly significant (p-value < 2e-16), suggesting that the data is stationary.\nAdditionally, the “Z-alpha” value is the test statistic, which is compared to critical values of the standard normal distribution to determine significance. In this case, the test statistic is -23.0718, indicating strong evidence against the null hypothesis of a unit root. The “Z-tau-mu” value is an auxiliary test statistic used to correct for autocorrelation in the residuals, and it is close to zero, suggesting that there is no evidence of residual autocorrelation.\n\n\nFitting a ARIMAX model\nFitting an ARIMAX model will be a useful approach to modeling and predicting time series data when external factors are believed to have an impact on the response variable. In this case, we will be using an ARIMAX model to predict the NASDAQ Composite Index, taking into account the effects of inflation and unemployment on the Index. By including these exogenous variables in our model, we can improve the accuracy of our predictions and gain a better understanding of the underlying dynamics of the time series.\n\nAuto ARIMAResiduals\n\n\n\n\nCode\nxreg <- cbind(Inflation = normalized_nadsaq_factor_data_ts[, \"inflation\"],\n              Unemployment = normalized_nadsaq_factor_data_ts[, \"unemployment\"])\n\nfit <- auto.arima(normalized_nadsaq_factor_data_ts[, \"IXIC.Adjusted\"], xreg = xreg)\nsummary(fit)\n\n\nSeries: normalized_nadsaq_factor_data_ts[, \"IXIC.Adjusted\"] \nRegression with ARIMA(0,1,0) errors \n\nCoefficients:\n      Inflation  Unemployment\n         0.1224       -0.1363\ns.e.     0.0863        0.0392\n\nsigma^2 = 0.0394:  log likelihood = 11.12\nAIC=-16.24   AICc=-15.73   BIC=-10.44\n\nTraining set error measures:\n                    ME      RMSE       MAE       MPE     MAPE      MASE\nTraining set 0.0284036 0.1926934 0.1243624 -4.567455 34.48622 0.3764756\n                  ACF1\nTraining set 0.2577338\n\n\n\n\n\n\nCode\ncheckresiduals(fit)\n\n\n\n\n\n\n    Ljung-Box test\n\ndata:  Residuals from Regression with ARIMA(0,1,0) errors\nQ* = 13.819, df = 8, p-value = 0.0866\n\nModel df: 0.   Total lags used: 8\n\n\n\n\n\nThe auto.arima function reveals that the best-fit model is (0,1,0). Analysis of the residuals plot indicates that the fluctuates a lot and lies between -2000 to 2000.The ACF plot of the residuals shows significant lags, indicating that the model is performing well. The qq-plot also suggests normality in the residuals. The p-value greater than 0.05 actually suggests that there is no evidence of significant autocorrelation in the residuals, which means that the model is performing well in terms of capturing the temporal patterns in the data.\n\n\nFitting the model manually\n\nLinear ModelACF of ResidualsACF of ResidualsDifferentiated Residual\n\n\n\n\nCode\nnormalized_nadsaq_factor_data_numeric_df$IXIC.Adjusted<-ts(normalized_nadsaq_factor_data_numeric_df$IXIC.Adjusted,star=decimal_date(as.Date(\"2010-01-01\",format = \"%Y-%m-%d\")),frequency = 4)\nnormalized_nadsaq_factor_data_numeric_df$inflation<-ts(normalized_nadsaq_factor_data_numeric_df$inflation,star=decimal_date(as.Date(\"2010-01-01\",format = \"%Y-%m-%d\")),frequency = 4)\nnormalized_nadsaq_factor_data_numeric_df$unemployment<-ts(normalized_nadsaq_factor_data_numeric_df$unemployment,star=decimal_date(as.Date(\"2010-01-01\",format = \"%Y-%m-%d\")),frequency = 4)\n\n############# First fit the linear model##########\nfit.reg <- lm(IXIC.Adjusted ~ inflation+unemployment, data=normalized_nadsaq_factor_data_numeric_df)\nsummary(fit.reg)\n\n\n\nCall:\nlm(formula = IXIC.Adjusted ~ inflation + unemployment, data = normalized_nadsaq_factor_data_numeric_df)\n\nResiduals:\n    Min      1Q  Median      3Q     Max \n-0.9537 -0.4333 -0.2053  0.2360  2.1227 \n\nCoefficients:\n               Estimate Std. Error t value Pr(>|t|)    \n(Intercept)   3.654e-17  9.763e-02   0.000     1.00    \ninflation     6.353e-01  1.125e-01   5.649  8.1e-07 ***\nunemployment -1.566e-01  1.125e-01  -1.392     0.17    \n---\nSignif. codes:  0 '***' 0.001 '**' 0.01 '*' 0.05 '.' 0.1 ' ' 1\n\nResidual standard error: 0.704 on 49 degrees of freedom\nMultiple R-squared:  0.5238,    Adjusted R-squared:  0.5044 \nF-statistic: 26.95 on 2 and 49 DF,  p-value: 1.276e-08\n\n\n\n\n\n\nCode\nres.fit<-ts(residuals(fit.reg),star=decimal_date(as.Date(\"2010-01-01\",format = \"%Y-%m-%d\")),frequency = 4)\n############## Then look at the residuals ############\nacf(res.fit)\n\n\n\n\n\n\n\n\n\nCode\nPacf(res.fit)\n\n\n\n\n\n\n\n\n\nCode\nres.fit %>% diff() %>% ggtsdisplay()\n\n\n\n\n\n\n\n\nThe output displays the results of a linear regression model using two predictor variables, inflation and unemployment, to explain the variation in the response variable, IXIC.Adjusted. The coefficients of inflation and unemployment have p-values of less than 0.05, indicating that both variables significantly impact IXIC.Adjusted. The R-squared value of approximately 52% suggests that the model explains a moderate amount of the variation in the response variable. Examination of the ACF plot of residuals reveals some autocorrelation, and therefore, the series is differentiated to achieve stationarity. By analyzing the differentiated ACF and PACF plots, the parameters if the ARIMAX model are that p=1, q=1,2, and d=1.\n\n\nFinding the model parameters.\n\nARIMAX ResultModel 1 PlotModel 1Model 2 PlotModel 2Model 3 PlotModel 3\n\n\n\n\nCode\nARIMA.c=function(p1,q1,q2,data){\ntemp=c()\nd=1\ni=1\ntemp= data.frame()\nls=matrix(rep(NA,6*30),nrow=30)\n\n\nfor (p in p1)#\n{\n  for(q in q1:q2)#\n  {\n    for(d in 0:1)\n    {\n      \n      if(p+d+q<=6)\n      {\n        \n        model<- Arima(data,order=c(p,d,q))\n        ls[i,]= c(p,d,q,model$aic,model$bic,model$aicc)\n        i=i+1\n        #print(i)\n        \n      }\n      \n    }\n  }\n}\n\n\ntemp= as.data.frame(ls)\nnames(temp)= c(\"p\",\"d\",\"q\",\"AIC\",\"BIC\",\"AICc\")\n\ntemp\n}\n\noutput <- ARIMA.c(1,1,2,data=residuals(fit.reg))\n\noutput[which.min(output$AIC),] \n\n\n  p d q      AIC      BIC     AICc\n4 1 1 2 5.193728 12.92103 6.063294\n\n\nCode\noutput[which.min(output$BIC),]\n\n\n  p d q     AIC      BIC     AICc\n2 1 1 1 5.89386 11.68934 6.404499\n\n\nCode\noutput[which.min(output$AICc),]\n\n\n  p d q      AIC      BIC     AICc\n4 1 1 2 5.193728 12.92103 6.063294\n\n\n\n\n\n\nCode\nset.seed(1234)\n\nmodel_output_1 <- capture.output(sarima(res.fit, 1,1,1)) \n\n\n\n\n\n\n\n\n\nCode\ncat(model_output_1[32:63], model_output_1[length(model_output_1)], sep = \"\\n\")\n\n\n$fit\n\nCall:\narima(x = xdata, order = c(p, d, q), seasonal = list(order = c(P, D, Q), period = S), \n    xreg = constant, transform.pars = trans, fixed = fixed, optim.control = list(trace = trc, \n        REPORT = 1, reltol = tol))\n\nCoefficients:\n         ar1      ma1  constant\n      0.7210  -0.3976   -0.0146\ns.e.  0.1802   0.2033    0.0708\n\nsigma^2 estimated as 0.05812:  log likelihood = 0.07,  aic = 7.85\n\n$degrees_of_freedom\n[1] 48\n\n$ttable\n         Estimate     SE t.value p.value\nar1        0.7210 0.1802  4.0008  0.0002\nma1       -0.3976 0.2033 -1.9557  0.0563\nconstant  -0.0146 0.0708 -0.2055  0.8381\n\n$AIC\n[1] 0.1539454\n\n$AICc\n[1] 0.1639579\n\n$BIC\n[1] 0.3054612\n\n\n\n\n\n\nCode\nmodel_output_2 <- capture.output(sarima(res.fit,1,1,2)) \n\n\n\n\n\n\n\n\n\nCode\ncat(model_output_2[31:63], model_output_2[length(model_output_2)], sep = \"\\n\")\n\n\n$fit\n\nCall:\narima(x = xdata, order = c(p, d, q), seasonal = list(order = c(P, D, Q), period = S), \n    xreg = constant, transform.pars = trans, fixed = fixed, optim.control = list(trace = trc, \n        REPORT = 1, reltol = tol))\n\nCoefficients:\n         ar1      ma1     ma2  constant\n      0.6080  -0.4268  0.3364   -0.0163\ns.e.  0.2276   0.2165  0.1899    0.0737\n\nsigma^2 estimated as 0.0548:  log likelihood = 1.43,  aic = 7.14\n\n$degrees_of_freedom\n[1] 47\n\n$ttable\n         Estimate     SE t.value p.value\nar1        0.6080 0.2276  2.6717  0.0103\nma1       -0.4268 0.2165 -1.9715  0.0546\nma2        0.3364 0.1899  1.7714  0.0830\nconstant  -0.0163 0.0737 -0.2218  0.8254\n\n$AIC\n[1] 0.1400875\n\n$AICc\n[1] 0.1571378\n\n$BIC\n[1] 0.3294822\n\n\n\n\n\n\nCode\nmodel_output_3 <- capture.output(sarima(res.fit,0,1,0)) \n\n\n\n\n\n\n\n\n\nCode\ncat(model_output_3[9:38], model_output_3[length(model_output_3)], sep = \"\\n\")\n\n\n$fit\n\nCall:\narima(x = xdata, order = c(p, d, q), seasonal = list(order = c(P, D, Q), period = S), \n    xreg = constant, transform.pars = trans, fixed = fixed, optim.control = list(trace = trc, \n        REPORT = 1, reltol = tol))\n\nCoefficients:\n      constant\n       -0.0086\ns.e.    0.0369\n\nsigma^2 estimated as 0.06942:  log likelihood = -4.34,  aic = 12.69\n\n$degrees_of_freedom\n[1] 50\n\n$ttable\n         Estimate     SE t.value p.value\nconstant  -0.0086 0.0369 -0.2318  0.8176\n\n$AIC\n[1] 0.2487808\n\n$AICc\n[1] 0.2503814\n\n$BIC\n[1] 0.3245387\n\n\n\n\n\nFollowing the manual fitting process, we identified that the ARIMAX model with the minimum AIC and AICc has the parameters (1,1,2), while the model with the minimum BIC has the parameters (1,1,1). We fitted both models using both the manual and auto.arima methods and observed that all the models have p-values greater than 0, indicating statistical significance. To determine the best fit model, we performed cross-validation.\n\n\nCross Validation\n\nRMSE PlotRMSE Results\n\n\n\n\nCode\nn=length(res.fit)\nk= 51\n \n \nrmse1 <- matrix(NA, (n-k),4)\nrmse2 <- matrix(NA, (n-k),4)\nrmse3 <- matrix(NA, (n-k),4)\n\n\nst <- tsp(res.fit)[1]+(k-5)/4 \n\nfor(i in 1:(n-k))\n{\n  xtrain <- window(res.fit, end=st + i/4)\n  xtest <- window(res.fit, start=st + (i+1)/4, end=st + (i+4)/4)\n  \n  #ARIMA(0,1,0) ARIMA(1,1,1) ARIMA(1,1,2)\n  \n  fit <- Arima(xtrain, order=c(1,1,2),\n                include.drift=TRUE, method=\"ML\")\n  fcast <- forecast(fit, h=4)\n  \n  fit2 <- Arima(xtrain, order=c(1,1,1),\n                include.drift=TRUE, method=\"ML\")\n  fcast2 <- forecast(fit2, h=4)\n\n  fit3 <- Arima(xtrain, order=c(0,1,0),\n                include.drift=TRUE, method=\"ML\")\n  fcast3 <- forecast(fit3, h=4)\n\n  rmse1[i,1:length(xtest)]   <- sqrt((fcast$mean-xtest)^2)\n  rmse2[i,1:length(xtest)] <- sqrt((fcast2$mean-xtest)^2)\n  rmse3[i,1:length(xtest)] <- sqrt((fcast3$mean-xtest)^2)\n}\n\nplot(1:4,colMeans(rmse1,na.rm=TRUE), type=\"l\",col=2, xlab=\"horizon\", ylab=\"RMSE\")\nlines(1:4, colMeans(rmse2,na.rm=TRUE), type=\"l\",col=3)\nlines(1:4, colMeans(rmse3,na.rm=TRUE), type=\"l\",col=4)\nlegend(\"topleft\",legend=c(\"fit1\",\"fit2\",\"fit3\"),col=2:4,lty=1)\n\n\n\n\n\n\n\n\n\nCode\ncat(\"RMSE values for Model 1\\n\", colMeans(rmse1))\n\n\nRMSE values for Model 1\n 0.02597127 0.5330269 1.236045 1.521616\n\n\nCode\ncat(\"RMSE values for Model 2\\n\", colMeans(rmse2))\n\n\nRMSE values for Model 2\n 0.3579776 0.9094041 1.62559 1.919108\n\n\nCode\ncat(\"RMSE values for Model 3\\n\", colMeans(rmse3))\n\n\nRMSE values for Model 3\n 0.4130706 0.9870188 1.712512 2.009977\n\n\n\n\n\nThe RMSE cross-validation plot shows that the RMSE of Model 1 (1,1,2) is lower than that of Model 2 (1,1,1) and Model 3 (0,1,0). This suggests that Model 1 (1,1,2) is a better fit for the data when compared other models.\n\n\nForecast\n\nForecast for NASDAQ Index with feature variablesARIMA Model for InflationARIMA Model for EmploymentARIMA Model for NASDAQ Composite Index with feature variables\n\n\n\n\nCode\n#fiting an ARIMA model to the Inflation variable\ninflation_fit<-auto.arima(normalized_nadsaq_factor_data_numeric_df$inflation) \nfinflation<-forecast(inflation_fit)\n\n#fitting an ARIMA model to the Unemployment variable\nunemployment_fit<-auto.arima(normalized_nadsaq_factor_data_numeric_df$unemployment) \nfunemployment<-forecast(unemployment_fit)\n\n# best model fit for forcasting\nxreg <- cbind(Inflation = normalized_nadsaq_factor_data_ts[, \"inflation\"],\n              Unemployment = normalized_nadsaq_factor_data_ts[, \"unemployment\"])\n\nfit <- Arima(normalized_nadsaq_factor_data_ts[, \"IXIC.Adjusted\"],order=c(1,1,3),xreg=xreg)\n\n# Forcast the stock price using feature variables\nfxreg <- cbind(Inflation = finflation$mean,\n              Unemployment = funemployment$mean)\nfcast <- forecast(fit, xreg=fxreg) \nautoplot(fcast) + xlab(\"Year\") +\n  ylab(\"Normalised IXIC.Adjusted\")\n\n\n\n\n\n\n\n\n\nCode\n#fiting an ARIMA model to the Inflation variable\ninflation_fit<-auto.arima(normalized_nadsaq_factor_data_numeric_df$inflation) \nfinflation<-forecast(inflation_fit)\nsummary(inflation_fit)\n\n\nSeries: normalized_nadsaq_factor_data_numeric_df$inflation \nARIMA(0,1,3)(0,0,1)[4] with drift \n\nCoefficients:\n         ma1     ma2     ma3     sma1   drift\n      0.2153  0.5354  0.4353  -0.3707  0.0567\ns.e.  0.1719  0.1915  0.1455   0.1942  0.0499\n\nsigma^2 = 0.0708:  log likelihood = -3.48\nAIC=18.96   AICc=20.87   BIC=30.55\n\nTraining set error measures:\n                      ME     RMSE       MAE       MPE    MAPE      MASE\nTraining set 0.002515126 0.250258 0.1802161 -17.11236 114.163 0.3180481\n                    ACF1\nTraining set -0.06141191\n\n\n\n\n\n\nCode\n#fitting an ARIMA model to the Unemployment variable\nunemployment_fit<-auto.arima(normalized_nadsaq_factor_data_numeric_df$unemployment) \nfunemployment<-forecast(unemployment_fit)\nsummary(unemployment_fit)\n\n\nSeries: normalized_nadsaq_factor_data_numeric_df$unemployment \nARIMA(0,1,2) \n\nCoefficients:\n          ma1      ma2\n      -0.3027  -0.2410\ns.e.   0.1376   0.1472\n\nsigma^2 = 0.4982:  log likelihood = -53.72\nAIC=113.45   AICc=113.96   BIC=119.24\n\nTraining set error measures:\n                     ME      RMSE      MAE      MPE     MAPE      MASE\nTraining set -0.1001611 0.6851533 0.281119 140.1191 219.8632 0.4565275\n                    ACF1\nTraining set -0.02079058\n\n\n\n\n\n\nCode\n# best model fit for forcasting\nxreg <- cbind(Inflation = normalized_nadsaq_factor_data_ts[, \"inflation\"],\n              Unemployment = normalized_nadsaq_factor_data_ts[, \"unemployment\"])\n\nfit <- Arima(normalized_nadsaq_factor_data_ts[, \"IXIC.Adjusted\"],order=c(1,1,3),xreg=xreg)\nsummary(fit)\n\n\nSeries: normalized_nadsaq_factor_data_ts[, \"IXIC.Adjusted\"] \nRegression with ARIMA(1,1,3) errors \n\nCoefficients:\n          ar1     ma1     ma2     ma3  Inflation  Unemployment\n      -0.2800  0.6402  0.3728  0.7326     0.0955       -0.1747\ns.e.   0.1965  0.1478  0.2092  0.2020     0.0880        0.0282\n\nsigma^2 = 0.03115:  log likelihood = 17.37\nAIC=-20.74   AICc=-18.13   BIC=-7.21\n\nTraining set error measures:\n                      ME      RMSE       MAE       MPE     MAPE      MASE\nTraining set 0.007283442 0.1641978 0.1086579 -5.307647 28.99016 0.3289342\n                     ACF1\nTraining set -0.004903313\n\n\n\n\n\nThe ARIMAX model may have incorporated exogenous variables such as macroeconomic factors, which could indicate the potential impact of external factors on the forecasted values. The findings suggest that inflation and unemployment rates significantly impact the NADSAQ Composite Index stock price, underscoring the importance of including macroeconomic factors in stock market forecasting. This is further supported by the comparison of RMSE, MAE, AIC and BIC values for forecasts with and without the feature variable. It is found that including the feature variable leads to a lower values, indicating that the feature variable makes a significant impact and should be considered in stock market forecasting."
  },
  {
    "objectID": "arimax_sector_communication_services_macroeconomic.html",
    "href": "arimax_sector_communication_services_macroeconomic.html",
    "title": "ARIMAX/SARIMAX Page for Communication Services Sector Fund and macroeconomic factors",
    "section": "",
    "text": "Endogenous variable: Communication Services Sector Fund Exogenous variables: Macroeconomic indicators such as GDP, inflation rate, unemployment rate and interest rates\nBefore proceeding with model selection, it can be useful to examine the feature variables and select the appropriate ones. This can be done by creating cross-correlation function plots, which can serve as a verification of the correlation heatmap for the variables.\n\nCCF Plot for Communication Services Sector Fund and Exogenous Variables\n\nGDPInterest RateInflation RateUnemployment Rate\n\n\n\n\nCode\npar(mfrow=c(1,1))\nccf_result <- ccf(normalized_sector_factor_data[, c(\"XLC.Adjusted\")], normalized_sector_factor_data[, c(\"gdp\")], \n    lag.max = 300,\n    main = \"Cros-Correlation Plot for Communication Services Sector Fund and GDP Growth Rate \",\n    ylab = \"CCF\")\n\n\n\n\n\nCode\ncat(\"The sum of cross correlation function is\", sum(abs(ccf_result$acf)))\n\n\nThe sum of cross correlation function is 5.453043\n\n\n\n\n\n\nCode\npar(mfrow=c(1,1))\nccf_result <- ccf(normalized_sector_factor_data[, c(\"XLC.Adjusted\")], normalized_sector_factor_data[, c(\"interest\")], \n    lag.max = 300,\n    main = \"Cros-Correlation Plot for Communication Services Sector Fund and Interest Rate\",\n    ylab = \"CCF\")\n\n\n\n\n\nCode\ncat(\"The sum of cross correlation function is\", sum(abs(ccf_result$acf)))\n\n\nThe sum of cross correlation function is 9.647901\n\n\n\n\n\n\nCode\npar(mfrow=c(1,1))\nccf_result <- ccf(normalized_sector_factor_data[, c(\"XLC.Adjusted\")], normalized_sector_factor_data[, c(\"inflation\")], \n    lag.max = 300,\n    main = \"Cros-Correlation Plot for Communication Services Sector Fund and Inflation Rate\",\n    ylab = \"CCF\")\n\n\n\n\n\nCode\ncat(\"The sum of cross correlation function is\", sum(abs(ccf_result$acf)))\n\n\nThe sum of cross correlation function is 9.872718\n\n\n\n\n\n\nCode\npar(mfrow=c(1,1))\nccf_result <- ccf(normalized_sector_factor_data[, c(\"XLC.Adjusted\")], normalized_sector_factor_data[, c(\"unemployment\")], \n    lag.max = 300,\n    main = \"Cros-Correlation Plot for Communication Services Sector Fund and Unemployment Rate\",\n    ylab = \"CCF\")\n\n\n\n\n\nCode\ncat(\"The sum of cross correlation function is\", sum(abs(ccf_result$acf)))\n\n\nThe sum of cross correlation function is 7.151748\n\n\n\n\n\nThe cross-correlation feature plots reaffirm the findings from the heatmap analysis, indicating that inflation rate exhibit stronger correlations with the index compared to GDP, interest rate and unemployment rate. The cross-correlation plots for GDP, unemployment and interest rates show weaker and more scattered patterns, with strikes between the blue lines indicating lower correlation coefficients. This suggests that inflation rate are more suitable feature variables for the ARIMAX model when predicting Communication Services Sector Fund movements.\nFinal Exogenous variables: Macroeconomic indicators: Inflation rate\n\n\nEndogenous and Exogenous Variables Plot\n\n\nCode\nfinal_XLC_factor_data <- sector_factor_data %>%dplyr::select( Date,XLC.Adjusted, inflation)\nnumeric_vars_XLC_factor_data <- c(\"XLC.Adjusted\", \"inflation\")\nnumeric_XLC_factor_data <- final_XLC_factor_data[, numeric_vars_XLC_factor_data]\nnormalized_XLC_factor_data_numeric <- scale(numeric_XLC_factor_data)\nnormalized_XLC_factor_data_numeric_df <- data.frame(normalized_XLC_factor_data_numeric)\nnormalized_XLC_factor_data_ts <- ts(normalized_XLC_factor_data_numeric, start = c(2018, 7), frequency = 4)\n\nautoplot(normalized_XLC_factor_data_ts, facets=TRUE) +\n  xlab(\"Year\") + ylab(\"\") +\n  ggtitle(\"Communication Services Sector Fund Stock Price, Inflation Rate and Unemployment Rate in USA 2018-2023\")\n\n\n\n\n\n\n\nCheck the stationarity\nBefore proceeding with further analysis, it’s important to check the stationarity of the time series data. If the series is not stationary, it needs to be differentiated before performing a ARIMAX model. The stationarity of a multivariate time series data can be checked using the Phillips-Perron test, which is specifically designed for multivariate data. By performing the Phillips-Perron test on the multivariate time series data, we can determine if the data is stationary or if it requires differencing to make it suitable for ARIMAX modeling.\n\n\nCode\n# Convert your multivariate time series data to a matrix\nfinal_XLC_factor_data_ts_multivariate <- as.matrix(normalized_XLC_factor_data_ts)\n\n# Check for stationarity using Phillips-Perron test\nphillips_perron_test <- ur.pp(final_XLC_factor_data_ts_multivariate)  \nsummary(phillips_perron_test)\n\n\n\n################################## \n# Phillips-Perron Unit Root Test # \n################################## \n\nTest regression with intercept \n\n\nCall:\nlm(formula = y ~ y.l1)\n\nResiduals:\n     Min       1Q   Median       3Q      Max \n-1.06019 -0.17368 -0.05236  0.26744  0.79058 \n\nCoefficients:\n            Estimate Std. Error t value Pr(>|t|)    \n(Intercept)  0.06560    0.07770   0.844    0.405    \ny.l1         0.92511    0.08246  11.218 8.43e-13 ***\n---\nSignif. codes:  0 '***' 0.001 '**' 0.01 '*' 0.05 '.' 0.1 ' ' 1\n\nResidual standard error: 0.4591 on 33 degrees of freedom\nMultiple R-squared:  0.7923,    Adjusted R-squared:  0.786 \nF-statistic: 125.8 on 1 and 33 DF,  p-value: 8.433e-13\n\n\nValue of test-statistic, type: Z-alpha  is: -5.1925 \n\n         aux. Z statistics\nZ-tau-mu            0.6613\n\n\nThe Phillips-Perron unit root test output suggests that the data is stationary. The test regression includes an intercept term and tests whether the coefficient on the lagged value is significantly different from one. The estimated coefficients indicate that the intercept is not significant (p-value > 0.05), while the coefficient on the lagged value is highly significant (p-value < 8.433e-13), suggesting that the data is stationary.\nAdditionally, the “Z-alpha” value is the test statistic, which is compared to critical values of the standard normal distribution to determine significance. In this case, the test statistic is -5.1925, indicating strong evidence against the null hypothesis of a unit root. The “Z-tau-mu” value is an auxiliary test statistic used to correct for autocorrelation in the residuals, and it is close to zero, suggesting that there is no evidence of residual autocorrelation.\n\n\nFitting a ARIMAX model\nFitting an ARIMAX model will be a useful approach to modeling and predicting time series data when external factors are believed to have an impact on the response variable. In this case, we will be using an ARIMAX model to predict the Communication Services Sector Fund Stock Price, taking into account the effects of inflation on the sector. By including these exogenous variables in our model, we can improve the accuracy of our predictions and gain a better understanding of the underlying dynamics of the time series.\n\nAuto ARIMAResiduals\n\n\n\n\nCode\nxreg <- cbind(Inflation = normalized_XLC_factor_data_ts[, \"inflation\"])\n\nfit <- auto.arima(normalized_XLC_factor_data_ts[, \"XLC.Adjusted\"], xreg = xreg)\nsummary(fit)\n\n\nSeries: normalized_XLC_factor_data_ts[, \"XLC.Adjusted\"] \nRegression with ARIMA(1,2,0) errors \n\nCoefficients:\n          ar1    xreg\n      -0.6425  0.7675\ns.e.   0.1844  0.2688\n\nsigma^2 = 0.2251:  log likelihood = -9.97\nAIC=25.94   AICc=27.94   BIC=28.26\n\nTraining set error measures:\n                    ME      RMSE       MAE       MPE     MAPE     MASE\nTraining set -0.063185 0.4183969 0.3514946 -14.76584 69.18403 0.328731\n                    ACF1\nTraining set -0.07403479\n\n\n\n\n\n\nCode\ncheckresiduals(fit)\n\n\n\n\n\n\n    Ljung-Box test\n\ndata:  Residuals from Regression with ARIMA(1,2,0) errors\nQ* = 0.22825, df = 3, p-value = 0.9729\n\nModel df: 1.   Total lags used: 4\n\n\n\n\n\nThe auto.arima function reveals that the best-fit model is (0,1,0). Analysis of the residuals plot indicates that the fluctuates a lot and lies between -0.6 to 0.6. The ACF plot of the residuals shows significant lags, indicating that the model is performing well. The qq-plot also suggests normality in the residuals. The p-value greater than 0.05 actually suggests that there is no evidence of significant autocorrelation in the residuals, which means that the model is performing well in terms of capturing the temporal patterns in the data.\n\n\nFitting the model manually\n\nLinear ModelACF of ResidualsACF of ResidualsDifferentiated Residual\n\n\n\n\nCode\nnormalized_XLC_factor_data_numeric_df$XLC.Adjusted<-ts(normalized_XLC_factor_data_numeric_df$XLC.Adjusted,star=decimal_date(as.Date(\"2018-07-01\",format = \"%Y-%m-%d\")),frequency = 4)\nnormalized_XLC_factor_data_numeric_df$inflation<-ts(normalized_XLC_factor_data_numeric_df$inflation,star=decimal_date(as.Date(\"2018-07-01\",format = \"%Y-%m-%d\")),frequency = 4)\n\n\n############# First fit the linear model##########\nfit.reg <- lm(XLC.Adjusted ~ inflation, data=normalized_XLC_factor_data_numeric_df)\nsummary(fit.reg)\n\n\n\nCall:\nlm(formula = XLC.Adjusted ~ inflation, data = normalized_XLC_factor_data_numeric_df)\n\nResiduals:\n    Min      1Q  Median      3Q     Max \n-1.4465 -0.5415 -0.2586  0.6946  1.4192 \n\nCoefficients:\n              Estimate Std. Error t value Pr(>|t|)  \n(Intercept) -5.759e-16  2.129e-01   0.000    1.000  \ninflation    4.817e-01  2.191e-01   2.199    0.043 *\n---\nSignif. codes:  0 '***' 0.001 '**' 0.01 '*' 0.05 '.' 0.1 ' ' 1\n\nResidual standard error: 0.9033 on 16 degrees of freedom\nMultiple R-squared:  0.232, Adjusted R-squared:  0.184 \nF-statistic: 4.834 on 1 and 16 DF,  p-value: 0.04295\n\n\n\n\n\n\nCode\nres.fit<-ts(residuals(fit.reg),star=decimal_date(as.Date(\"2018-01-01\",format = \"%Y-%m-%d\")),frequency = 4)\n############## Then look at the residuals ############\nacf(res.fit)\n\n\n\n\n\n\n\n\n\nCode\nPacf(res.fit)\n\n\n\n\n\n\n\n\n\nCode\nres.fit %>% diff() %>% ggtsdisplay()\n\n\n\n\n\n\n\n\nThe output displays the results of a linear regression model using two predictor variables, inflation and unemployment, to explain the variation in the response variable, XLC.Adjusted The coefficients of inflation have p-values of less than 0.05, indicating that both variables significantly impact XLC.Adjusted The R-squared value of approximately 23% suggests that the model explains a moderate amount of the variation in the response variable. Examination of the ACF plot of residuals reveals some autocorrelation, and therefore, the series is differentiated to achieve stationarity. By analyzing the differentiated ACF and PACF plots, the parameters if the ARIMAX modle are that p=0, q=0, and d=1.\n\n\nFitting Model.\n\nModel Plot 1Model 1Model Plot 2Model 2\n\n\n\n\nCode\nset.seed(1234)\n\nmodel_output <- capture.output(sarima(res.fit, 0,1,0)) \n\n\n\n\n\n\n\n\n\nCode\ncat(model_output[9:38], model_output[length(model_output)], sep = \"\\n\")\n\n\n$fit\n\nCall:\narima(x = xdata, order = c(p, d, q), seasonal = list(order = c(P, D, Q), period = S), \n    xreg = constant, transform.pars = trans, fixed = fixed, optim.control = list(trace = trc, \n        REPORT = 1, reltol = tol))\n\nCoefficients:\n      constant\n       -0.0560\ns.e.    0.1215\n\nsigma^2 estimated as 0.251:  log likelihood = -12.37,  aic = 28.74\n\n$degrees_of_freedom\n[1] 16\n\n$ttable\n         Estimate     SE t.value p.value\nconstant   -0.056 0.1215 -0.4606  0.6513\n\n$AIC\n[1] 1.690689\n\n$AICc\n[1] 1.706376\n\n$BIC\n[1] 1.788714\n\n\n\n\n\n\nCode\nset.seed(1234)\n\nmodel_output <- capture.output(sarima(res.fit, 1,2,0)) \n\n\n\n\n\n\n\n\n\nCode\ncat(model_output[15:43], model_output[length(model_output)], sep = \"\\n\")\n\n\n$fit\n\nCall:\narima(x = xdata, order = c(p, d, q), seasonal = list(order = c(P, D, Q), period = S), \n    include.mean = !no.constant, transform.pars = trans, fixed = fixed, optim.control = list(trace = trc, \n        REPORT = 1, reltol = tol))\n\nCoefficients:\n          ar1\n      -0.6352\ns.e.   0.1860\n\nsigma^2 estimated as 0.211:  log likelihood = -10.52,  aic = 25.03\n\n$degrees_of_freedom\n[1] 15\n\n$ttable\n    Estimate    SE t.value p.value\nar1  -0.6352 0.186 -3.4143  0.0038\n\n$AIC\n[1] 1.564486\n\n$AICc\n[1] 1.582343\n\n$BIC\n[1] 1.66106\n\n\n\n\n\nWe fitted both models using both the manual and auto.arima methods and observed that all the models have p-values greater than 0, indicating statistical significance. To determine the best fit model, we performed cross-validation.\n\n\nCross Validation\n\nRMSE PlotRMSE Results\n\n\n\n\nCode\nn=length(res.fit)\nk= 10\n \n \nrmse1 <- matrix(NA, (n-k),4)\nrmse2 <- matrix(NA, (n-k),4)\nrmse3 <- matrix(NA, (n-k),4)\n\nst <- tsp(res.fit)[1]+(k-5)/4 \n\nfor(i in 1:(n-k))\n{\n  xtrain <- window(res.fit, end=st + i/4)\n  xtest <- window(res.fit, start=st + (i+1)/4, end=st + (i+4)/4)\n\n  \n  fit <- Arima(xtrain, order=c(0,1,0),\n                include.drift=TRUE, method=\"ML\")\n  fcast <- forecast(fit, h=4)\n  \n  fit2 <- Arima(xtrain, order=c(1,2,0),\n                include.drift=TRUE, method=\"ML\")\n  fcast2 <- forecast(fit2, h=4)\n\n\n  rmse1[i,1:length(xtest)]   <- sqrt((fcast$mean-xtest)^2)\n  rmse2[i,1:length(xtest)] <- sqrt((fcast2$mean-xtest)^2)\n}\n\nplot(1:4,colMeans(rmse1,na.rm=TRUE), type=\"l\",col=2, xlab=\"horizon\", ylab=\"RMSE\")\nlines(1:4, colMeans(rmse2,na.rm=TRUE), type=\"l\",col=3)\nlegend(\"topleft\",legend=c(\"fit1\",\"fit2\"),col=2:4,lty=1)\n\n\n\n\n\n\n\n\n\nCode\ncat(\"RMSE values for Model 1\\n\", colMeans(rmse1))\n\n\nRMSE values for Model 1\n 0.4100462 0.6829554 1.168707 1.662351\n\n\nCode\ncat(\"RMSE values for Model 2\\n\", colMeans(rmse2))\n\n\nRMSE values for Model 2\n 0.4866858 0.7792442 1.380504 1.962777\n\n\n\n\n\nThe RMSE cross-validation plot shows that the RMSE of Model 2 (1,2,0) is lower than that of Model 1 (0,1,0). This suggests that Model 2 (1,2,0) is a better fit for the data when compared other model.\n\n\nForecast\n::: panel-tabset ### Forecast for Communication Services Sector Fund with feature variable\n\n\nCode\n#fiting an ARIMA model to the Inflation variable\ninflation_fit<-auto.arima(normalized_XLC_factor_data_numeric_df$inflation) \nfinflation<-forecast(inflation_fit)\n\n\n# best model fit for forcasting\nxreg <- cbind(Inflation = normalized_XLC_factor_data_ts[, \"inflation\"])\n\nfit <- Arima(normalized_XLC_factor_data_ts[, \"XLC.Adjusted\"],order=c(1,2,0),xreg=xreg)\n\n# Forcast the stock price using feature variables\nfxreg <- cbind(Inflation = finflation$mean)\nfcast <- forecast(fit, xreg=fxreg) \nautoplot(fcast) + xlab(\"Year\") +\n  ylab(\"Normalised XLC.Adjusted\")\n\n\n\n\n\n\n\nARIMA Model for Inflation\n\n\nCode\n#fiting an ARIMA model to the Inflation variable\ninflation_fit<-auto.arima(normalized_XLC_factor_data_numeric_df$inflation) \nfinflation<-forecast(inflation_fit)\nsummary(inflation_fit)\n\n\nSeries: normalized_XLC_factor_data_numeric_df$inflation \nARIMA(0,1,0) \n\nsigma^2 = 0.1378:  log likelihood = -7.28\nAIC=16.55   AICc=16.82   BIC=17.38\n\nTraining set error measures:\n                    ME      RMSE       MAE       MPE     MAPE      MASE\nTraining set 0.1201871 0.3607553 0.2312026 -26.91442 61.15158 0.2676132\n                  ACF1\nTraining set 0.2431721\n\n\n\n\nARIMA Model for Communication Services Sector Fund with feature variable\n\n\nCode\n# best model fit for forcasting\nxreg <- cbind(Inflation = normalized_XLC_factor_data_ts[, \"inflation\"])\n\nfit <- Arima(normalized_XLC_factor_data_ts[, \"XLC.Adjusted\"],order=c(1,2,0),xreg=xreg)\nsummary(fit)\n\n\nSeries: normalized_XLC_factor_data_ts[, \"XLC.Adjusted\"] \nRegression with ARIMA(1,2,0) errors \n\nCoefficients:\n          ar1    xreg\n      -0.6425  0.7675\ns.e.   0.1844  0.2688\n\nsigma^2 = 0.2251:  log likelihood = -9.97\nAIC=25.94   AICc=27.94   BIC=28.26\n\nTraining set error measures:\n                    ME      RMSE       MAE       MPE     MAPE     MASE\nTraining set -0.063185 0.4183969 0.3514946 -14.76584 69.18403 0.328731\n                    ACF1\nTraining set -0.07403479\n\n\nThe ARIMAX model may have incorporated exogenous variables such as macroeconomic factors, which could indicate the potential impact of external factors on the forecasted values. The findings suggest that inflation rates significantly impact the Communication Services Sector Fund stock price, underscoring the importance of including macroeconomic factors in stock market forecasting. This is further supported by the comparison of RMSE, MAE, AIC and BIC values for forecasts with and without the feature variable. It is found that including the feature variable leads to a lower values, indicating that the feature variable makes a significant impact and should be considered in stock market forecasting."
  },
  {
    "objectID": "arimax_sector_consumer_discretionary_macroeconomic.html",
    "href": "arimax_sector_consumer_discretionary_macroeconomic.html",
    "title": "ARIMAX/SARIMAX Page for Consumer Discretionary Sector Fund and macroeconomic factors",
    "section": "",
    "text": "Endogenous variable: Consumer Discretionary Sector Fund Exogenous variables: Macroeconomic indicators such as GDP, inflation rate, unemployment rate and interest rates\nBefore proceeding with model selection, it can be useful to examine the feature variables and select the appropriate ones. This can be done by creating cross-correlation function plots, which can serve as a verification of the correlation heatmap for the variables.\n\nCCF Plot for Consumer Discretionary Sector Fund and Exogenous Variables\n\nGDPInterest RateInflation RateUnemployment Rate\n\n\n\n\nCode\npar(mfrow=c(1,1))\nccf_result <- ccf(normalized_sector_factor_data[, c(\"XLY.Adjusted\")], normalized_sector_factor_data[, c(\"gdp\")], \n    lag.max = 300,\n    main = \"Cros-Correlation Plot for Consumer Discretionary Sector Fund and GDP Growth Rate \",\n    ylab = \"CCF\")\n\n\n\n\n\nCode\ncat(\"The sum of cross correlation function is\", sum(abs(ccf_result$acf)))\n\n\nThe sum of cross correlation function is 4.929662\n\n\n\n\n\n\nCode\npar(mfrow=c(1,1))\nccf_result <- ccf(normalized_sector_factor_data[, c(\"XLY.Adjusted\")], normalized_sector_factor_data[, c(\"interest\")], \n    lag.max = 300,\n    main = \"Cros-Correlation Plot for Consumer Discretionary Sector Fund and Interest Rate\",\n    ylab = \"CCF\")\n\n\n\n\n\nCode\ncat(\"The sum of cross correlation function is\", sum(abs(ccf_result$acf)))\n\n\nThe sum of cross correlation function is 9.474914\n\n\n\n\n\n\nCode\npar(mfrow=c(1,1))\nccf_result <- ccf(normalized_sector_factor_data[, c(\"XLY.Adjusted\")], normalized_sector_factor_data[, c(\"inflation\")], \n    lag.max = 300,\n    main = \"Cros-Correlation Plot for Consumer Discretionary Sector Fund and Inflation Rate\",\n    ylab = \"CCF\")\n\n\n\n\n\nCode\ncat(\"The sum of cross correlation function is\", sum(abs(ccf_result$acf)))\n\n\nThe sum of cross correlation function is 10.30298\n\n\n\n\n\n\nCode\npar(mfrow=c(1,1))\nccf_result <- ccf(normalized_sector_factor_data[, c(\"XLY.Adjusted\")], normalized_sector_factor_data[, c(\"unemployment\")], \n    lag.max = 300,\n    main = \"Cros-Correlation Plot for Consumer Discretionary Sector Fund and Unemployment Rate\",\n    ylab = \"CCF\")\n\n\n\n\n\nCode\ncat(\"The sum of cross correlation function is\", sum(abs(ccf_result$acf)))\n\n\nThe sum of cross correlation function is 6.993324\n\n\n\n\n\nThe cross-correlation feature plots reaffirm the findings from the heatmap analysis, indicating that inflation rate exhibit stronger correlations with the index compared to GDP, interest rate and unemployment rate. The cross-correlation plots for GDP, unemployment and interest rates show weaker and more scattered patterns, with strikes between the blue lines indicating lower correlation coefficients. This suggests that inflation rate are more suitable feature variables for the ARIMAX model when predicting Consumer Discretionary Sector Fund movements.\nFinal Exogenous variables: Macroeconomic indicators: Inflation rate\n\n\nEndogenous and Exogenous Variables Plot\n\n\nCode\nfinal_XLY_factor_data <- sector_factor_data %>%dplyr::select( Date,XLY.Adjusted, inflation)\nnumeric_vars_XLY_factor_data <- c(\"XLY.Adjusted\", \"inflation\")\nnumeric_XLY_factor_data <- final_XLY_factor_data[, numeric_vars_XLY_factor_data]\nnormalized_XLY_factor_data_numeric <- scale(numeric_XLY_factor_data)\nnormalized_XLY_factor_data_numeric_df <- data.frame(normalized_XLY_factor_data_numeric)\nnormalized_XLY_factor_data_ts <- ts(normalized_XLY_factor_data_numeric, start = c(2018, 7), frequency = 4)\n\nautoplot(normalized_XLY_factor_data_ts, facets=TRUE) +\n  xlab(\"Year\") + ylab(\"\") +\n  ggtitle(\"Consumer Discretionary Sector Fund Stock Price, Inflation Rate and Unemployment Rate in USA 2018-2023\")\n\n\n\n\n\n\n\nCheck the stationarity\nBefore proceeding with further analysis, it’s important to check the stationarity of the time series data. If the series is not stationary, it needs to be differentiated before performing a ARIMAX model. The stationarity of a multivariate time series data can be checked using the Phillips-Perron test, which is specifically designed for multivariate data. By performing the Phillips-Perron test on the multivariate time series data, we can determine if the data is stationary or if it requires differencing to make it suitable for ARIMAX modeling.\n\n\nCode\n# Convert your multivariate time series data to a matrix\nfinal_XLY_factor_data_ts_multivariate <- as.matrix(normalized_XLY_factor_data_ts)\n\n# Check for stationarity using Phillips-Perron test\nphillips_perron_test <- ur.pp(final_XLY_factor_data_ts_multivariate)  \nsummary(phillips_perron_test)\n\n\n\n################################## \n# Phillips-Perron Unit Root Test # \n################################## \n\nTest regression with intercept \n\n\nCall:\nlm(formula = y ~ y.l1)\n\nResiduals:\n    Min      1Q  Median      3Q     Max \n-1.2506 -0.1793 -0.0025  0.2765  0.9314 \n\nCoefficients:\n            Estimate Std. Error t value Pr(>|t|)    \n(Intercept)  0.07270    0.08474   0.858    0.397    \ny.l1         0.89206    0.08994   9.918 1.99e-11 ***\n---\nSignif. codes:  0 '***' 0.001 '**' 0.01 '*' 0.05 '.' 0.1 ' ' 1\n\nResidual standard error: 0.5007 on 33 degrees of freedom\nMultiple R-squared:  0.7488,    Adjusted R-squared:  0.7412 \nF-statistic: 98.38 on 1 and 33 DF,  p-value: 1.987e-11\n\n\nValue of test-statistic, type: Z-alpha  is: -5.3574 \n\n         aux. Z statistics\nZ-tau-mu            0.7495\n\n\nThe Phillips-Perron unit root test output suggests that the data is stationary. The test regression includes an intercept term and tests whether the coefficient on the lagged value is significantly different from one. The estimated coefficients indicate that the intercept is not significant (p-value > 0.05), while the coefficient on the lagged value is highly significant (p-value < 1.99e-11), suggesting that the data is stationary.\nAdditionally, the “Z-alpha” value is the test statistic, which is compared to critical values of the standard normal distribution to determine significance. In this case, the test statistic is -5.3574, indicating strong evidence against the null hypothesis of a unit root. The “Z-tau-mu” value is an auxiliary test statistic used to correct for autocorrelation in the residuals, and it is close to zero, suggesting that there is no evidence of residual autocorrelation.\n\n\nFitting a ARIMAX model\nFitting an ARIMAX model will be a useful approach to modeling and predicting time series data when external factors are believed to have an impact on the response variable. In this case, we will be using an ARIMAX model to predict the Consumer Discretionary Sector Fund Stock Price, taking into account the effects of inflation on the sector. By including these exogenous variables in our model, we can improve the accuracy of our predictions and gain a better understanding of the underlying dynamics of the time series.\n\nAuto ARIMAResiduals\n\n\n\n\nCode\nxreg <- cbind(Inflation = normalized_XLY_factor_data_ts[, \"inflation\"])\n\nfit <- auto.arima(normalized_XLY_factor_data_ts[, \"XLY.Adjusted\"], xreg = xreg)\nsummary(fit)\n\n\nSeries: normalized_XLY_factor_data_ts[, \"XLY.Adjusted\"] \nRegression with ARIMA(0,1,0) errors \n\nCoefficients:\n        xreg\n      0.7705\ns.e.  0.3474\n\nsigma^2 = 0.3005:  log likelihood = -13.39\nAIC=30.77   AICc=31.63   BIC=32.44\n\nTraining set error measures:\n                      ME      RMSE       MAE       MPE     MAPE      MASE\nTraining set -0.03054598 0.5168065 0.4073284 -466.6557 515.1414 0.4379855\n                  ACF1\nTraining set 0.1083258\n\n\n\n\n\n\nCode\ncheckresiduals(fit)\n\n\n\n\n\n\n    Ljung-Box test\n\ndata:  Residuals from Regression with ARIMA(0,1,0) errors\nQ* = 0.62692, df = 4, p-value = 0.96\n\nModel df: 0.   Total lags used: 4\n\n\n\n\n\nThe auto.arima function reveals that the best-fit model is (0,1,0). Analysis of the residuals plot indicates that the fluctuates a lot and lies between -1 to 1. The ACF plot of the residuals shows significant lags, indicating that the model is performing well. The qq-plot also suggests normality in the residuals. The p-value greater than 0.05 actually suggests that there is no evidence of significant autocorrelation in the residuals, which means that the model is performing well in terms of capturing the temporal patterns in the data.\n\n\nFitting the model manually\n\nLinear ModelACF of ResidualsACF of ResidualsDifferentiated Residual\n\n\n\n\nCode\nnormalized_XLY_factor_data_numeric_df$XLY.Adjusted<-ts(normalized_XLY_factor_data_numeric_df$XLY.Adjusted,star=decimal_date(as.Date(\"2018-07-01\",format = \"%Y-%m-%d\")),frequency = 4)\nnormalized_XLY_factor_data_numeric_df$inflation<-ts(normalized_XLY_factor_data_numeric_df$inflation,star=decimal_date(as.Date(\"2018-07-01\",format = \"%Y-%m-%d\")),frequency = 4)\n\n\n############# First fit the linear model##########\nfit.reg <- lm(XLY.Adjusted ~ inflation, data=normalized_XLY_factor_data_numeric_df)\nsummary(fit.reg)\n\n\n\nCall:\nlm(formula = XLY.Adjusted ~ inflation, data = normalized_XLY_factor_data_numeric_df)\n\nResiduals:\n    Min      1Q  Median      3Q     Max \n-1.0287 -0.6438 -0.2041  0.7331  1.2059 \n\nCoefficients:\n              Estimate Std. Error t value Pr(>|t|)   \n(Intercept) -2.129e-16  1.826e-01    0.00   1.0000   \ninflation    6.595e-01  1.879e-01    3.51   0.0029 **\n---\nSignif. codes:  0 '***' 0.001 '**' 0.01 '*' 0.05 '.' 0.1 ' ' 1\n\nResidual standard error: 0.7748 on 16 degrees of freedom\nMultiple R-squared:  0.435, Adjusted R-squared:  0.3997 \nF-statistic: 12.32 on 1 and 16 DF,  p-value: 0.002903\n\n\n\n\n\n\nCode\nres.fit<-ts(residuals(fit.reg),star=decimal_date(as.Date(\"2018-07-01\",format = \"%Y-%m-%d\")),frequency = 4)\n############## Then look at the residuals ############\nacf(res.fit)\n\n\n\n\n\n\n\n\n\nCode\nPacf(res.fit)\n\n\n\n\n\n\n\n\n\nCode\nres.fit %>% diff() %>% ggtsdisplay()\n\n\n\n\n\n\n\n\nThe output displays the results of a linear regression model using two predictor variables, inflation and unemployment, to explain the variation in the response variable, XLY.Adjusted The coefficients of inflation have p-values of less than 0.05, indicating that both variables significantly impact XLY.Adjusted The R-squared value of approximately 43% suggests that the model explains not good amount of the variation in the response variable. Examination of the ACF plot of residuals reveals some autocorrelation, and therefore, the series is differentiated to achieve stationarity. By analyzing the differentiated ACF and PACF plots, the parameters if the ARIMAX modle are that p=0, q=0, and d=1.\nAs both the model fitted manually and the auto.arima parameters are the same, i.e. p = 0, q = 0 and d = 1. We proceed with fitting the model for the parameters.\n\n\nModel Fitting\n\nModel PlotModel\n\n\n\n\nCode\nset.seed(1234)\n\nmodel_output <- capture.output(sarima(res.fit, 0,1,0)) \n\n\n\n\n\n\n\n\n\nCode\ncat(model_output[9:38], model_output[length(model_output)], sep = \"\\n\")\n\n\n$fit\n\nCall:\narima(x = xdata, order = c(p, d, q), seasonal = list(order = c(P, D, Q), period = S), \n    xreg = constant, transform.pars = trans, fixed = fixed, optim.control = list(trace = trc, \n        REPORT = 1, reltol = tol))\n\nCoefficients:\n      constant\n       -0.0182\ns.e.    0.1293\n\nsigma^2 estimated as 0.2842:  log likelihood = -13.43,  aic = 30.85\n\n$degrees_of_freedom\n[1] 16\n\n$ttable\n         Estimate     SE t.value p.value\nconstant  -0.0182 0.1293 -0.1406  0.8899\n\n$AIC\n[1] 1.814973\n\n$AICc\n[1] 1.830659\n\n$BIC\n[1] 1.912998\n\n\n\n\n\nThe best model identified is ARIMA(0,1,0). By analyzing the standardized residuals plot, it can be observed that the mean is close to 0. Deviations from these values could indicate poor model fit. However, in this case, the model seems to be well-fitted. The ACF plot of residuals shows very few significant lags, which is a positive sign for the model. The qq-plot also suggests normality in the residuals. Additionally, the p-values for the Ljung-Box test are greater than 0.05, indicating that the residuals are independent, which is favorable for the model’s accuracy.\n\n\nForecast\n::: panel-tabset ### Forecast for Consumer Discretionary Sector Fund with feature variable\n\n\nCode\n#fiting an ARIMA model to the Inflation variable\ninflation_fit<-auto.arima(normalized_XLY_factor_data_numeric_df$inflation) \nfinflation<-forecast(inflation_fit)\n\n\n# best model fit for forcasting\nxreg <- cbind(Inflation = normalized_XLY_factor_data_ts[, \"inflation\"])\n\nfit <- Arima(normalized_XLY_factor_data_ts[, \"XLY.Adjusted\"],order=c(0,1,0),xreg=xreg)\n\n# Forcast the stock price using feature variables\nfxreg <- cbind(Inflation = finflation$mean)\nfcast <- forecast(fit, xreg=fxreg) \nautoplot(fcast) + xlab(\"Year\") +\n  ylab(\"Normalised XLY.Adjusted\")\n\n\n\n\n\n\n\nARIMA Model for Inflation\n\n\nCode\n#fiting an ARIMA model to the Inflation variable\ninflation_fit<-auto.arima(normalized_XLY_factor_data_numeric_df$inflation) \nfinflation<-forecast(inflation_fit)\nsummary(inflation_fit)\n\n\nSeries: normalized_XLY_factor_data_numeric_df$inflation \nARIMA(0,1,0) \n\nsigma^2 = 0.1378:  log likelihood = -7.28\nAIC=16.55   AICc=16.82   BIC=17.38\n\nTraining set error measures:\n                    ME      RMSE       MAE       MPE     MAPE      MASE\nTraining set 0.1201871 0.3607553 0.2312026 -26.91442 61.15158 0.2676132\n                  ACF1\nTraining set 0.2431721\n\n\n\n\nARIMA Model for Consumer Discretionary Sector Fund with feature variable\n\n\nCode\n# best model fit for forcasting\nxreg <- cbind(Inflation = normalized_XLY_factor_data_ts[, \"inflation\"])\n\nfit <- Arima(normalized_XLY_factor_data_ts[, \"XLY.Adjusted\"],order=c(0,1,0),xreg=xreg)\nsummary(fit)\n\n\nSeries: normalized_XLY_factor_data_ts[, \"XLY.Adjusted\"] \nRegression with ARIMA(0,1,0) errors \n\nCoefficients:\n        xreg\n      0.7705\ns.e.  0.3474\n\nsigma^2 = 0.3005:  log likelihood = -13.39\nAIC=30.77   AICc=31.63   BIC=32.44\n\nTraining set error measures:\n                      ME      RMSE       MAE       MPE     MAPE      MASE\nTraining set -0.03054598 0.5168065 0.4073284 -466.6557 515.1414 0.4379855\n                  ACF1\nTraining set 0.1083258\n\n\nThe ARIMAX model may have incorporated exogenous variables such as macroeconomic factors, which could indicate the potential impact of external factors on the forecasted values. The findings suggest that inflation rates significantly impact the Consumer Discretionary Sector Fund stock price, underscoring the importance of including macroeconomic factors in stock market forecasting. This is further supported by the comparison of RMSE, MAE, AIC and BIC values for forecasts with and without the feature variable. It is found that including the feature variable leads to a lower values, indicating that the feature variable makes a significant impact and should be considered in stock market forecasting."
  },
  {
    "objectID": "arimax_sector_consumer_staples_macroeconomic.html",
    "href": "arimax_sector_consumer_staples_macroeconomic.html",
    "title": "ARIMAX/SARIMAX for Consumer Staples Sector Fund and macroeconomic factors",
    "section": "",
    "text": "Endogenous variable: Consumer Staples Sector Fund Exogenous variables: Macroeconomic indicators such as GDP, inflation rate, unemployment rate and interest rates\nBefore proceeding with model selection, it can be useful to examine the feature variables and select the appropriate ones. This can be done by creating cross-correlation function plots, which can serve as a verification of the correlation heatmap for the variables.\n\nCCF Plot for Consumer Staples Sector Fund and Exogenous Variables\n\nGDPInterest RateInflation RateUnemployment Rate\n\n\n\n\nCode\npar(mfrow=c(1,1))\nccf_result <- ccf(normalized_sector_factor_data[, c(\"XLP.Adjusted\")], normalized_sector_factor_data[, c(\"gdp\")], \n    lag.max = 300,\n    main = \"Cros-Correlation Plot for Consumer Staples Sector Fund and GDP Growth Rate \",\n    ylab = \"CCF\")\n\n\n\n\n\nCode\ncat(\"The sum of cross correlation function is\", sum(abs(ccf_result$acf)))\n\n\nThe sum of cross correlation function is 5.056662\n\n\n\n\n\n\nCode\npar(mfrow=c(1,1))\nccf_result <- ccf(normalized_sector_factor_data[, c(\"XLP.Adjusted\")], normalized_sector_factor_data[, c(\"interest\")], \n    lag.max = 300,\n    main = \"Cros-Correlation Plot for Consumer Staples Sector Fund and Interest Rate\",\n    ylab = \"CCF\")\n\n\n\n\n\nCode\ncat(\"The sum of cross correlation function is\", sum(abs(ccf_result$acf)))\n\n\nThe sum of cross correlation function is 9.343761\n\n\n\n\n\n\nCode\npar(mfrow=c(1,1))\nccf_result <- ccf(normalized_sector_factor_data[, c(\"XLP.Adjusted\")], normalized_sector_factor_data[, c(\"inflation\")], \n    lag.max = 300,\n    main = \"Cros-Correlation Plot for Consumer Staples Sector Fund and Inflation Rate\",\n    ylab = \"CCF\")\n\n\n\n\n\nCode\ncat(\"The sum of cross correlation function is\", sum(abs(ccf_result$acf)))\n\n\nThe sum of cross correlation function is 11.02386\n\n\n\n\n\n\nCode\npar(mfrow=c(1,1))\nccf_result <- ccf(normalized_sector_factor_data[, c(\"XLP.Adjusted\")], normalized_sector_factor_data[, c(\"unemployment\")], \n    lag.max = 300,\n    main = \"Cros-Correlation Plot for Consumer Staples Sector Fund and Unemployment Rate\",\n    ylab = \"CCF\")\n\n\n\n\n\nCode\ncat(\"The sum of cross correlation function is\", sum(abs(ccf_result$acf)))\n\n\nThe sum of cross correlation function is 6.397481\n\n\n\n\n\nThe cross-correlation feature plots reaffirm the findings from the heatmap analysis, indicating that inflation rate exhibit stronger correlations with the index compared to GDP, interest rate and unemployment rate. The cross-correlation plots for GDP, unemployment and interest rates show weaker and more scattered patterns, with strikes between the blue lines indicating lower correlation coefficients. This suggests that inflation rate are more suitable feature variables for the ARIMAX model when predicting Consumer Staples Sector Fund movements.\nFinal Exogenous variables: Macroeconomic indicators: Inflation rate\n\n\nEndogenous and Exogenous Variables Plot\n\n\nCode\nfinal_XLP_factor_data <- sector_factor_data %>%dplyr::select( Date,XLP.Adjusted, inflation)\nnumeric_vars_XLP_factor_data <- c(\"XLP.Adjusted\", \"inflation\")\nnumeric_XLP_factor_data <- final_XLP_factor_data[, numeric_vars_XLP_factor_data]\nnormalized_XLP_factor_data_numeric <- scale(numeric_XLP_factor_data)\nnormalized_XLP_factor_data_numeric_df <- data.frame(normalized_XLP_factor_data_numeric)\nnormalized_XLP_factor_data_ts <- ts(normalized_XLP_factor_data_numeric, start = c(2018, 7), frequency = 4)\n\nautoplot(normalized_XLP_factor_data_ts, facets=TRUE) +\n  xlab(\"Year\") + ylab(\"\") +\n  ggtitle(\"Consumer Staples Sector Fund Stock Price, Inflation Rate and Unemployment Rate in USA 2018-2023\")\n\n\n\n\n\n\n\nCheck the stationarity\nBefore proceeding with further analysis, it’s important to check the stationarity of the time series data. If the series is not stationary, it needs to be differentiated before performing a ARIMAX model. The stationarity of a multivariate time series data can be checked using the Phillips-Perron test, which is specifically designed for multivariate data. By performing the Phillips-Perron test on the multivariate time series data, we can determine if the data is stationary or if it requires differencing to make it suitable for ARIMAX modeling.\n\n\nCode\n# Convert your multivariate time series data to a matrix\nfinal_XLP_factor_data_ts_multivariate <- as.matrix(normalized_XLP_factor_data_ts)\n\n# Check for stationarity using Phillips-Perron test\nphillips_perron_test <- ur.pp(final_XLP_factor_data_ts_multivariate)  \nsummary(phillips_perron_test)\n\n\n\n################################## \n# Phillips-Perron Unit Root Test # \n################################## \n\nTest regression with intercept \n\n\nCall:\nlm(formula = y ~ y.l1)\n\nResiduals:\n     Min       1Q   Median       3Q      Max \n-1.24887 -0.18245  0.04207  0.18828  0.82983 \n\nCoefficients:\n            Estimate Std. Error t value Pr(>|t|)    \n(Intercept)  0.08589    0.07449   1.153    0.257    \ny.l1         0.90580    0.07905  11.458 4.82e-13 ***\n---\nSignif. codes:  0 '***' 0.001 '**' 0.01 '*' 0.05 '.' 0.1 ' ' 1\n\nResidual standard error: 0.4401 on 33 degrees of freedom\nMultiple R-squared:  0.7991,    Adjusted R-squared:  0.793 \nF-statistic: 131.3 on 1 and 33 DF,  p-value: 4.819e-13\n\n\nValue of test-statistic, type: Z-alpha  is: -5.2349 \n\n         aux. Z statistics\nZ-tau-mu            0.9514\n\n\nThe Phillips-Perron unit root test output suggests that the data is stationary. The test regression includes an intercept term and tests whether the coefficient on the lagged value is significantly different from one. The estimated coefficients indicate that the intercept is not significant (p-value > 0.05), while the coefficient on the lagged value is highly significant (p-value < 4.82e-13), suggesting that the data is stationary.\nAdditionally, the “Z-alpha” value is the test statistic, which is compared to critical values of the standard normal distribution to determine significance. In this case, the test statistic is -5.2349, indicating strong evidence against the null hypothesis of a unit root. The “Z-tau-mu” value is an auxiliary test statistic used to correct for autocorrelation in the residuals, and it is close to zero, suggesting that there is no evidence of residual autocorrelation.\n\n\nFitting a ARIMAX model\nFitting an ARIMAX model will be a useful approach to modeling and predicting time series data when external factors are believed to have an impact on the response variable. In this case, we will be using an ARIMAX model to predict the Consumer Staples Sector Fund Stock Price, taking into account the effects of inflation on the sector. By including these exogenous variables in our model, we can improve the accuracy of our predictions and gain a better understanding of the underlying dynamics of the time series.\n\nAuto ARIMAResiduals\n\n\n\n\nCode\nxreg <- cbind(Inflation = normalized_XLP_factor_data_ts[, \"inflation\"])\n\nfit <- auto.arima(normalized_XLP_factor_data_ts[, \"XLP.Adjusted\"], xreg = xreg)\nsummary(fit)\n\n\nSeries: normalized_XLP_factor_data_ts[, \"XLP.Adjusted\"] \nRegression with ARIMA(0,1,0) errors \n\nCoefficients:\n        xreg\n      0.5390\ns.e.  0.2455\n\nsigma^2 = 0.15:  log likelihood = -7.48\nAIC=18.96   AICc=19.82   BIC=20.63\n\nTraining set error measures:\n                     ME     RMSE       MAE      MPE    MAPE      MASE\nTraining set 0.05960426 0.365138 0.3186148 7.758641 72.3286 0.4301663\n                    ACF1\nTraining set 0.005736426\n\n\n\n\n\n\nCode\ncheckresiduals(fit)\n\n\n\n\n\n\n    Ljung-Box test\n\ndata:  Residuals from Regression with ARIMA(0,1,0) errors\nQ* = 3.4594, df = 4, p-value = 0.4841\n\nModel df: 0.   Total lags used: 4\n\n\n\n\n\nThe auto.arima function reveals that the best-fit model is (0,1,0). Analysis of the residuals plot indicates that the fluctuates a lot and lies between -0.6 to 0.6. The ACF plot of the residuals shows significant lags, indicating that the model is performing well. The qq-plot also suggests normality in the residuals. The p-value greater than 0.05 actually suggests that there is no evidence of significant autocorrelation in the residuals, which means that the model is performing well in terms of capturing the temporal patterns in the data.\n\n\nFitting the model manually\n\nLinear ModelACF of ResidualsACF of ResidualsDifferentiated Residual\n\n\n\n\nCode\nnormalized_XLP_factor_data_numeric_df$XLP.Adjusted<-ts(normalized_XLP_factor_data_numeric_df$XLP.Adjusted,star=decimal_date(as.Date(\"2018-07-01\",format = \"%Y-%m-%d\")),frequency = 4)\nnormalized_XLP_factor_data_numeric_df$inflation<-ts(normalized_XLP_factor_data_numeric_df$inflation,star=decimal_date(as.Date(\"2018-07-01\",format = \"%Y-%m-%d\")),frequency = 4)\n\n\n############# First fit the linear model##########\nfit.reg <- lm(XLP.Adjusted ~ inflation, data=normalized_XLP_factor_data_numeric_df)\nsummary(fit.reg)\n\n\n\nCall:\nlm(formula = XLP.Adjusted ~ inflation, data = normalized_XLP_factor_data_numeric_df)\n\nResiduals:\n    Min      1Q  Median      3Q     Max \n-1.0945 -0.3724  0.0981  0.2660  1.1861 \n\nCoefficients:\n             Estimate Std. Error t value Pr(>|t|)    \n(Intercept) 3.255e-16  1.460e-01    0.00        1    \ninflation   7.993e-01  1.502e-01    5.32  6.9e-05 ***\n---\nSignif. codes:  0 '***' 0.001 '**' 0.01 '*' 0.05 '.' 0.1 ' ' 1\n\nResidual standard error: 0.6194 on 16 degrees of freedom\nMultiple R-squared:  0.6389,    Adjusted R-squared:  0.6163 \nF-statistic: 28.31 on 1 and 16 DF,  p-value: 6.9e-05\n\n\n\n\n\n\nCode\nres.fit<-ts(residuals(fit.reg),star=decimal_date(as.Date(\"2018-01-01\",format = \"%Y-%m-%d\")),frequency = 4)\n############## Then look at the residuals ############\nacf(res.fit)\n\n\n\n\n\n\n\n\n\nCode\nPacf(res.fit)\n\n\n\n\n\n\n\n\n\nCode\nres.fit %>% diff() %>% ggtsdisplay()\n\n\n\n\n\n\n\n\nThe output displays the results of a linear regression model using two predictor variables, inflation and unemployment, to explain the variation in the response variable, XLP.Adjusted The coefficients of inflation have p-values of less than 0.05, indicating that both variables significantly impact XLP.Adjusted The R-squared value of approximately 64% suggests that the model explains a moderate amount of the variation in the response variable. Examination of the ACF plot of residuals reveals some autocorrelation, and therefore, the series is differentiated to achieve stationarity. By analyzing the differentiated ACF and PACF plots, the parameters if the ARIMAX modle are that p=0, q=0, and d=1.\nAs both the model fitted manually and the auto.arima parameters are the same, i.e. p = 0, q = 0 and d = 1. We proceed with fitting the model for the parameters.\n\n\nModel Fitting\n\nModel PlotModel\n\n\n\n\nCode\nset.seed(1234)\n\nmodel_output <- capture.output(sarima(res.fit, 0,1,0)) \n\n\n\n\n\n\n\n\n\nCode\ncat(model_output[9:38], model_output[length(model_output)], sep = \"\\n\")\n\n\n$fit\n\nCall:\narima(x = xdata, order = c(p, d, q), seasonal = list(order = c(P, D, Q), period = S), \n    xreg = constant, transform.pars = trans, fixed = fixed, optim.control = list(trace = trc, \n        REPORT = 1, reltol = tol))\n\nCoefficients:\n      constant\n        0.0300\ns.e.    0.0938\n\nsigma^2 estimated as 0.1496:  log likelihood = -7.97,  aic = 19.95\n\n$degrees_of_freedom\n[1] 16\n\n$ttable\n         Estimate     SE t.value p.value\nconstant     0.03 0.0938  0.3203  0.7529\n\n$AIC\n[1] 1.1734\n\n$AICc\n[1] 1.189086\n\n$BIC\n[1] 1.271425\n\n\n\n\n\nThe best model identified is ARIMA(0,1,0). By analyzing the standardized residuals plot, it can be observed that the mean is close to 0. Deviations from these values could indicate poor model fit. However, in this case, the model seems to be well-fitted. The ACF plot of residuals shows very few significant lags, which is a positive sign for the model. The qq-plot also suggests normality in the residuals. Additionally, the p-values for the Ljung-Box test are greater than 0.05, indicating that the residuals are independent, which is favorable for the model’s accuracy.\n\n\nForecast\n::: panel-tabset ### Forecast for Consumer Staples Sector Fund with feature variable\n\n\nCode\n#fiting an ARIMA model to the Inflation variable\ninflation_fit<-auto.arima(normalized_XLP_factor_data_numeric_df$inflation) \nfinflation<-forecast(inflation_fit)\n\n\n# best model fit for forcasting\nxreg <- cbind(Inflation = normalized_XLP_factor_data_ts[, \"inflation\"])\n\nfit <- Arima(normalized_XLP_factor_data_ts[, \"XLP.Adjusted\"],order=c(0,1,0),xreg=xreg)\n\n# Forcast the stock price using feature variables\nfxreg <- cbind(Inflation = finflation$mean)\nfcast <- forecast(fit, xreg=fxreg) \nautoplot(fcast) + xlab(\"Year\") +\n  ylab(\"Normalised XLP.Adjusted\")\n\n\n\n\n\n\n\nARIMA Model for Inflation\n\n\nCode\n#fiting an ARIMA model to the Inflation variable\ninflation_fit<-auto.arima(normalized_XLP_factor_data_numeric_df$inflation) \nfinflation<-forecast(inflation_fit)\nsummary(inflation_fit)\n\n\nSeries: normalized_XLP_factor_data_numeric_df$inflation \nARIMA(0,1,0) \n\nsigma^2 = 0.1378:  log likelihood = -7.28\nAIC=16.55   AICc=16.82   BIC=17.38\n\nTraining set error measures:\n                    ME      RMSE       MAE       MPE     MAPE      MASE\nTraining set 0.1201871 0.3607553 0.2312026 -26.91442 61.15158 0.2676132\n                  ACF1\nTraining set 0.2431721\n\n\n\n\nARIMA Model for Consumer Staples Sector Fund with feature variable\n\n\nCode\n# best model fit for forcasting\nxreg <- cbind(Inflation = normalized_XLP_factor_data_ts[, \"inflation\"])\n\nfit <- Arima(normalized_XLP_factor_data_ts[, \"XLP.Adjusted\"],order=c(0,1,0),xreg=xreg)\nsummary(fit)\n\n\nSeries: normalized_XLP_factor_data_ts[, \"XLP.Adjusted\"] \nRegression with ARIMA(0,1,0) errors \n\nCoefficients:\n        xreg\n      0.5390\ns.e.  0.2455\n\nsigma^2 = 0.15:  log likelihood = -7.48\nAIC=18.96   AICc=19.82   BIC=20.63\n\nTraining set error measures:\n                     ME     RMSE       MAE      MPE    MAPE      MASE\nTraining set 0.05960426 0.365138 0.3186148 7.758641 72.3286 0.4301663\n                    ACF1\nTraining set 0.005736426\n\n\nThe ARIMAX model may have incorporated exogenous variables such as macroeconomic factors, which could indicate the potential impact of external factors on the forecasted values. The findings suggest that inflation rates significantly impact the Consumer Staples Sector Fund stock price, underscoring the importance of including macroeconomic factors in stock market forecasting. This is further supported by the comparison of RMSE, MAE, AIC and BIC values for forecasts with and without the feature variable. It is found that including the feature variable leads to a lower values, indicating that the feature variable makes a significant impact and should be considered in stock market forecasting."
  },
  {
    "objectID": "arimax_sector_energy_macroeconomic.html",
    "href": "arimax_sector_energy_macroeconomic.html",
    "title": "ARIMAX/SARIMAX Page for Energy Sector Fund and macroeconomic factors",
    "section": "",
    "text": "Endogenous variable: Energy Sector Fund Exogenous variables: Macroeconomic indicators such as GDP, inflation rate, unemployment rate and interest rates\nBefore proceeding with model selection, it can be useful to examine the feature variables and select the appropriate ones. This can be done by creating cross-correlation function plots, which can serve as a verification of the correlation heatmap for the variables.\n\nCCF Plot for Energy Sector Fund and Exogenous Variables\n\nGDPInterest RateInflation RateUnemployment Rate\n\n\n\n\nCode\npar(mfrow=c(1,1))\nccf_result <- ccf(normalized_sector_factor_data[, c(\"XLE.Adjusted\")], normalized_sector_factor_data[, c(\"gdp\")], \n    lag.max = 300,\n    main = \"Cros-Correlation Plot for Energy Sector Fund and GDP Growth Rate \",\n    ylab = \"CCF\")\n\n\n\n\n\nCode\ncat(\"The sum of cross correlation function is\", sum(abs(ccf_result$acf)))\n\n\nThe sum of cross correlation function is 5.945692\n\n\n\n\n\n\nCode\npar(mfrow=c(1,1))\nccf_result <- ccf(normalized_sector_factor_data[, c(\"XLE.Adjusted\")], normalized_sector_factor_data[, c(\"interest\")], \n    lag.max = 300,\n    main = \"Cros-Correlation Plot for Energy Sector Fund and Interest Rate\",\n    ylab = \"CCF\")\n\n\n\n\n\nCode\ncat(\"The sum of cross correlation function is\", sum(abs(ccf_result$acf)))\n\n\nThe sum of cross correlation function is 9.147159\n\n\n\n\n\n\nCode\npar(mfrow=c(1,1))\nccf_result <- ccf(normalized_sector_factor_data[, c(\"XLE.Adjusted\")], normalized_sector_factor_data[, c(\"inflation\")], \n    lag.max = 300,\n    main = \"Cros-Correlation Plot for Energy Sector Fund and Inflation Rate\",\n    ylab = \"CCF\")\n\n\n\n\n\nCode\ncat(\"The sum of cross correlation function is\", sum(abs(ccf_result$acf)))\n\n\nThe sum of cross correlation function is 8.754575\n\n\n\n\n\n\nCode\npar(mfrow=c(1,1))\nccf_result <- ccf(normalized_sector_factor_data[, c(\"XLE.Adjusted\")], normalized_sector_factor_data[, c(\"unemployment\")], \n    lag.max = 300,\n    main = \"Cros-Correlation Plot for Energy Sector Fund and Unemployment Rate\",\n    ylab = \"CCF\")\n\n\n\n\n\nCode\ncat(\"The sum of cross correlation function is\", sum(abs(ccf_result$acf)))\n\n\nThe sum of cross correlation function is 7.2758\n\n\n\n\n\nThe cross-correlation feature plots reaffirm the findings from the heatmap analysis, indicating that inflation rate exhibit stronger correlations with the index compared to GDP, interest rate and unemployment rate. The cross-correlation plots for GDP, unemployment and interest rates show weaker and more scattered patterns, with strikes between the blue lines indicating lower correlation coefficients. This suggests that inflation rate are more suitable feature variables for the ARIMAX model when predicting Energy Sector Fund movements.\nFinal Exogenous variables: Macroeconomic indicators: Inflation rate\n\n\nEndogenous and Exogenous Variables Plot\n\n\nCode\nfinal_XLE_factor_data <- sector_factor_data %>%dplyr::select( Date,XLE.Adjusted, inflation)\nnumeric_vars_XLE_factor_data <- c(\"XLE.Adjusted\", \"inflation\")\nnumeric_XLE_factor_data <- final_XLE_factor_data[, numeric_vars_XLE_factor_data]\nnormalized_XLE_factor_data_numeric <- scale(numeric_XLE_factor_data)\nnormalized_XLE_factor_data_numeric_df <- data.frame(normalized_XLE_factor_data_numeric)\nnormalized_XLE_factor_data_ts <- ts(normalized_XLE_factor_data_numeric, start = c(2018, 7), frequency = 4)\n\nautoplot(normalized_XLE_factor_data_ts, facets=TRUE) +\n  xlab(\"Year\") + ylab(\"\") +\n  ggtitle(\"Energy Sector Fund Stock Price, Inflation Rate and Unemployment Rate in USA 2018-2023\")\n\n\n\n\n\n\n\nCheck the stationarity\nBefore proceeding with further analysis, it’s important to check the stationarity of the time series data. If the series is not stationary, it needs to be differentiated before performing a ARIMAX model. The stationarity of a multivariate time series data can be checked using the Phillips-Perron test, which is specifically designed for multivariate data. By performing the Phillips-Perron test on the multivariate time series data, we can determine if the data is stationary or if it requires differencing to make it suitable for ARIMAX modeling.\n\n\nCode\n# Convert your multivariate time series data to a matrix\nfinal_XLE_factor_data_ts_multivariate <- as.matrix(normalized_XLE_factor_data_ts)\n\n# Check for stationarity using Phillips-Perron test\nphillips_perron_test <- ur.pp(final_XLE_factor_data_ts_multivariate)  \nsummary(phillips_perron_test)\n\n\n\n################################## \n# Phillips-Perron Unit Root Test # \n################################## \n\nTest regression with intercept \n\n\nCall:\nlm(formula = y ~ y.l1)\n\nResiduals:\n     Min       1Q   Median       3Q      Max \n-1.88632 -0.19841  0.06096  0.33275  1.36548 \n\nCoefficients:\n            Estimate Std. Error t value Pr(>|t|)    \n(Intercept)  0.02291    0.11006   0.208    0.836    \ny.l1         0.79774    0.11681   6.830 8.58e-08 ***\n---\nSignif. codes:  0 '***' 0.001 '**' 0.01 '*' 0.05 '.' 0.1 ' ' 1\n\nResidual standard error: 0.6502 on 33 degrees of freedom\nMultiple R-squared:  0.5857,    Adjusted R-squared:  0.5731 \nF-statistic: 46.64 on 1 and 33 DF,  p-value: 8.582e-08\n\n\nValue of test-statistic, type: Z-alpha  is: -8.8291 \n\n         aux. Z statistics\nZ-tau-mu            0.1801\n\n\nThe Phillips-Perron unit root test output suggests that the data is stationary. The test regression includes an intercept term and tests whether the coefficient on the lagged value is significantly different from one. The estimated coefficients indicate that the intercept is not significant (p-value > 0.05), while the coefficient on the lagged value is highly significant (p-value < 8.582e-08), suggesting that the data is stationary.\nAdditionally, the “Z-alpha” value is the test statistic, which is compared to critical values of the standard normal distribution to determine significance. In this case, the test statistic is -8.8291, indicating strong evidence against the null hypothesis of a unit root. The “Z-tau-mu” value is an auxiliary test statistic used to correct for autocorrelation in the residuals, and it is close to zero, suggesting that there is no evidence of residual autocorrelation.\n\n\nFitting a ARIMAX model\nFitting an ARIMAX model will be a useful approach to modeling and predicting time series data when external factors are believed to have an impact on the response variable. In this case, we will be using an ARIMAX model to predict the Energy Sector Fund Stock Price, taking into account the effects of inflation on the sector. By including these exogenous variables in our model, we can improve the accuracy of our predictions and gain a better understanding of the underlying dynamics of the time series.\n\nAuto ARIMAResiduals\n\n\n\n\nCode\nxreg <- cbind(Inflation = normalized_XLE_factor_data_ts[, \"inflation\"])\n\nfit <- auto.arima(normalized_XLE_factor_data_ts[, \"XLE.Adjusted\"], xreg = xreg)\nsummary(fit)\n\n\nSeries: normalized_XLE_factor_data_ts[, \"XLE.Adjusted\"] \nRegression with ARIMA(0,1,0) errors \n\nCoefficients:\n        xreg\n      0.9398\ns.e.  0.4018\n\nsigma^2 = 0.4018:  log likelihood = -15.86\nAIC=35.71   AICc=36.57   BIC=37.38\n\nTraining set error measures:\n                      ME      RMSE       MAE       MPE     MAPE      MASE\nTraining set -0.05259885 0.5976308 0.4515601 -349.5647 506.4671 0.3444568\n                   ACF1\nTraining set -0.4988802\n\n\n\n\n\n\nCode\ncheckresiduals(fit)\n\n\n\n\n\n\n    Ljung-Box test\n\ndata:  Residuals from Regression with ARIMA(0,1,0) errors\nQ* = 8.3288, df = 4, p-value = 0.08025\n\nModel df: 0.   Total lags used: 4\n\n\n\n\n\nThe auto.arima function reveals that the best-fit model is (0,1,0). Analysis of the residuals plot indicates that the fluctuates a lot and lies between -1.5 to 1.5. The ACF plot of the residuals shows significant lags, indicating that the model is performing well. The qq-plot also suggests normality in the residuals. The p-value greater than 0.05 actually suggests that there is no evidence of significant autocorrelation in the residuals, which means that the model is performing well in terms of capturing the temporal patterns in the data.\n\n\nFitting the model manually\n\nLinear ModelACF of ResidualsACF of ResidualsDifferentiated Residual\n\n\n\n\nCode\nnormalized_XLE_factor_data_numeric_df$XLE.Adjusted<-ts(normalized_XLE_factor_data_numeric_df$XLE.Adjusted,star=decimal_date(as.Date(\"2018-07-01\",format = \"%Y-%m-%d\")),frequency = 4)\nnormalized_XLE_factor_data_numeric_df$inflation<-ts(normalized_XLE_factor_data_numeric_df$inflation,star=decimal_date(as.Date(\"2018-07-01\",format = \"%Y-%m-%d\")),frequency = 4)\n\n\n############# First fit the linear model##########\nfit.reg <- lm(XLE.Adjusted ~ inflation, data=normalized_XLE_factor_data_numeric_df)\nsummary(fit.reg)\n\n\n\nCall:\nlm(formula = XLE.Adjusted ~ inflation, data = normalized_XLE_factor_data_numeric_df)\n\nResiduals:\n    Min      1Q  Median      3Q     Max \n-1.0438 -0.5173  0.1668  0.4337  1.1635 \n\nCoefficients:\n              Estimate Std. Error t value Pr(>|t|)    \n(Intercept) -6.443e-17  1.597e-01   0.000 1.000000    \ninflation    7.537e-01  1.643e-01   4.588 0.000303 ***\n---\nSignif. codes:  0 '***' 0.001 '**' 0.01 '*' 0.05 '.' 0.1 ' ' 1\n\nResidual standard error: 0.6774 on 16 degrees of freedom\nMultiple R-squared:  0.5681,    Adjusted R-squared:  0.5411 \nF-statistic: 21.05 on 1 and 16 DF,  p-value: 0.0003034\n\n\n\n\n\n\nCode\nres.fit<-ts(residuals(fit.reg),star=decimal_date(as.Date(\"2018-07-01\",format = \"%Y-%m-%d\")),frequency = 4)\n############## Then look at the residuals ############\nacf(res.fit)\n\n\n\n\n\n\n\n\n\nCode\nPacf(res.fit)\n\n\n\n\n\n\n\n\n\nCode\nres.fit %>% diff() %>% ggtsdisplay()\n\n\n\n\n\n\n\n\nThe output displays the results of a linear regression model using two predictor variables, inflation and unemployment, to explain the variation in the response variable, XLE.Adjusted The coefficients of inflation have p-values of less than 0.05, indicating that both variables significantly impact XLE.Adjusted The R-squared value of approximately 57% suggests that the model explains a moderate amount of the variation in the response variable. Examination of the ACF plot of residuals reveals some autocorrelation, and therefore, the series is differentiated to achieve stationarity. By analyzing the differentiated ACF and PACF plots, the parameters if the ARIMAX modle are that p=0, q=0, and d=1.\nAs both the model fitted manually and the auto.arima parameters are the same, i.e. p = 0, q = 0 and d = 1. We proceed with fitting the model for the parameters.\n\n\nModel Fitting\n\nModel PlotModel\n\n\n\n\nCode\nset.seed(1234)\n\nmodel_output <- capture.output(sarima(res.fit, 0,1,0)) \n\n\n\n\n\n\n\n\n\nCode\ncat(model_output[9:38], model_output[length(model_output)], sep = \"\\n\")\n\n\n$fit\n\nCall:\narima(x = xdata, order = c(p, d, q), seasonal = list(order = c(P, D, Q), period = S), \n    xreg = constant, transform.pars = trans, fixed = fixed, optim.control = list(trace = trc, \n        REPORT = 1, reltol = tol))\n\nCoefficients:\n      constant\n       -0.0321\ns.e.    0.1499\n\nsigma^2 estimated as 0.3819:  log likelihood = -15.94,  aic = 35.88\n\n$degrees_of_freedom\n[1] 16\n\n$ttable\n         Estimate     SE t.value p.value\nconstant  -0.0321 0.1499 -0.2139  0.8333\n\n$AIC\n[1] 2.11062\n\n$AICc\n[1] 2.126306\n\n$BIC\n[1] 2.208645\n\n\n\n\n\nThe best model identified is ARIMA(0,1,0). By analyzing the standardized residuals plot, it can be observed that the mean is close to 0. Deviations from these values could indicate poor model fit. However, in this case, the model seems to be well-fitted. The ACF plot of residuals shows very few significant lags, which is a positive sign for the model. The qq-plot also suggests normality in the residuals. Additionally, the p-values for the Ljung-Box test are greater than 0.05, indicating that the residuals are independent, which is favorable for the model’s accuracy.\n\n\nForecast\n::: panel-tabset ### Forecast for Energy Sector Fund with feature variable\n\n\nCode\n#fiting an ARIMA model to the Inflation variable\ninflation_fit<-auto.arima(normalized_XLE_factor_data_numeric_df$inflation) \nfinflation<-forecast(inflation_fit)\n\n\n# best model fit for forcasting\nxreg <- cbind(Inflation = normalized_XLE_factor_data_ts[, \"inflation\"])\n\nfit <- Arima(normalized_XLE_factor_data_ts[, \"XLE.Adjusted\"],order=c(0,1,0),xreg=xreg)\n\n# Forcast the stock price using feature variables\nfxreg <- cbind(Inflation = finflation$mean)\nfcast <- forecast(fit, xreg=fxreg) \nautoplot(fcast) + xlab(\"Year\") +\n  ylab(\"Normalised XLE.Adjusted\")\n\n\n\n\n\n\n\nARIMA Model for Inflation\n\n\nCode\n#fiting an ARIMA model to the Inflation variable\ninflation_fit<-auto.arima(normalized_XLE_factor_data_numeric_df$inflation) \nfinflation<-forecast(inflation_fit)\nsummary(inflation_fit)\n\n\nSeries: normalized_XLE_factor_data_numeric_df$inflation \nARIMA(0,1,0) \n\nsigma^2 = 0.1378:  log likelihood = -7.28\nAIC=16.55   AICc=16.82   BIC=17.38\n\nTraining set error measures:\n                    ME      RMSE       MAE       MPE     MAPE      MASE\nTraining set 0.1201871 0.3607553 0.2312026 -26.91442 61.15158 0.2676132\n                  ACF1\nTraining set 0.2431721\n\n\n\n\nARIMA Model for Energy Sector Fund with feature variable\n\n\nCode\n# best model fit for forecasting\nxreg <- cbind(Inflation = normalized_XLE_factor_data_ts[, \"inflation\"])\n\nfit <- Arima(normalized_XLE_factor_data_ts[, \"XLE.Adjusted\"],order=c(0,1,0),xreg=xreg)\nsummary(fit)\n\n\nSeries: normalized_XLE_factor_data_ts[, \"XLE.Adjusted\"] \nRegression with ARIMA(0,1,0) errors \n\nCoefficients:\n        xreg\n      0.9398\ns.e.  0.4018\n\nsigma^2 = 0.4018:  log likelihood = -15.86\nAIC=35.71   AICc=36.57   BIC=37.38\n\nTraining set error measures:\n                      ME      RMSE       MAE       MPE     MAPE      MASE\nTraining set -0.05259885 0.5976308 0.4515601 -349.5647 506.4671 0.3444568\n                   ACF1\nTraining set -0.4988802\n\n\nThe ARIMAX model may have incorporated exogenous variables such as macroeconomic factors, which could indicate the potential impact of external factors on the forecasted values. The findings suggest that inflation rates significantly impact the Energy Sector Fund stock price, underscoring the importance of including macroeconomic factors in stock market forecasting. This is further supported by the comparison of RMSE, MAE, AIC and BIC values for forecasts with and without the feature variable. It is found that including the feature variable leads to a lower values, indicating that the feature variable makes a significant impact and should be considered in stock market forecasting."
  },
  {
    "objectID": "arimax_sector_financial_macroeconomic.html",
    "href": "arimax_sector_financial_macroeconomic.html",
    "title": "ARIMAX/SARIMAX Page for Financial Sector Fund and macroeconomic factors",
    "section": "",
    "text": "Endogenous variable: Financial Sector Fund Exogenous variables: Macroeconomic indicators such as GDP, inflation rate, unemployment rate and interest rates\nBefore proceeding with model selection, it can be useful to examine the feature variables and select the appropriate ones. This can be done by creating cross-correlation function plots, which can serve as a verification of the correlation heatmap for the variables.\n\nCCF Plot for Financial Sector Fund and Exogenous Variables\n\nGDPInterest RateInflation RateUnemployment Rate\n\n\n\n\nCode\npar(mfrow=c(1,1))\nccf_result <- ccf(normalized_sector_factor_data[, c(\"XLF.Adjusted\")], normalized_sector_factor_data[, c(\"gdp\")], \n    lag.max = 300,\n    main = \"Cros-Correlation Plot for Financial Sector Fund and GDP Growth Rate \",\n    ylab = \"CCF\")\n\n\n\n\n\nCode\ncat(\"The sum of cross correlation function is\", sum(abs(ccf_result$acf)))\n\n\nThe sum of cross correlation function is 5.162983\n\n\n\n\n\n\nCode\npar(mfrow=c(1,1))\nccf_result <- ccf(normalized_sector_factor_data[, c(\"XLF.Adjusted\")], normalized_sector_factor_data[, c(\"interest\")], \n    lag.max = 300,\n    main = \"Cros-Correlation Plot for Financial Sector Fund and Interest Rate\",\n    ylab = \"CCF\")\n\n\n\n\n\nCode\ncat(\"The sum of cross correlation function is\", sum(abs(ccf_result$acf)))\n\n\nThe sum of cross correlation function is 8.914493\n\n\n\n\n\n\nCode\npar(mfrow=c(1,1))\nccf_result <- ccf(normalized_sector_factor_data[, c(\"XLF.Adjusted\")], normalized_sector_factor_data[, c(\"inflation\")], \n    lag.max = 300,\n    main = \"Cros-Correlation Plot for Financial Sector Fund and Inflation Rate\",\n    ylab = \"CCF\")\n\n\n\n\n\nCode\ncat(\"The sum of cross correlation function is\", sum(abs(ccf_result$acf)))\n\n\nThe sum of cross correlation function is 9.547857\n\n\n\n\n\n\nCode\npar(mfrow=c(1,1))\nccf_result <- ccf(normalized_sector_factor_data[, c(\"XLF.Adjusted\")], normalized_sector_factor_data[, c(\"unemployment\")], \n    lag.max = 300,\n    main = \"Cros-Correlation Plot for Financial Sector Fund and Unemployment Rate\",\n    ylab = \"CCF\")\n\n\n\n\n\nCode\ncat(\"The sum of cross correlation function is\", sum(abs(ccf_result$acf)))\n\n\nThe sum of cross correlation function is 6.49013\n\n\n\n\n\nThe cross-correlation feature plots reaffirm the findings from the heatmap analysis, indicating that inflation rate exhibit stronger correlations with the index compared to GDP, interest rate and unemployment rate. The cross-correlation plots for GDP, unemployment and interest rates show weaker and more scattered patterns, with strikes between the blue lines indicating lower correlation coefficients. This suggests that inflation rate are more suitable feature variables for the ARIMAX model when predicting Financial Sector Fund movements.\nFinal Exogenous variables: Macroeconomic indicators: Inflation rate\n\n\nEndogenous and Exogenous Variables Plot\n\n\nCode\nfinal_XLF_factor_data <- sector_factor_data %>%dplyr::select( Date,XLF.Adjusted, inflation)\nnumeric_vars_XLF_factor_data <- c(\"XLF.Adjusted\", \"inflation\")\nnumeric_XLF_factor_data <- final_XLF_factor_data[, numeric_vars_XLF_factor_data]\nnormalized_XLF_factor_data_numeric <- scale(numeric_XLF_factor_data)\nnormalized_XLF_factor_data_numeric_df <- data.frame(normalized_XLF_factor_data_numeric)\nnormalized_XLF_factor_data_ts <- ts(normalized_XLF_factor_data_numeric, start = c(2018, 7), frequency = 4)\n\nautoplot(normalized_XLF_factor_data_ts, facets=TRUE) +\n  xlab(\"Year\") + ylab(\"\") +\n  ggtitle(\"Financial Sector Fund Stock Price, Inflation Rate and Unemployment Rate in USA 2018-2023\")\n\n\n\n\n\n\n\nCheck the stationarity\nBefore proceeding with further analysis, it’s important to check the stationarity of the time series data. If the series is not stationary, it needs to be differentiated before performing a ARIMAX model. The stationarity of a multivariate time series data can be checked using the Phillips-Perron test, which is specifically designed for multivariate data. By performing the Phillips-Perron test on the multivariate time series data, we can determine if the data is stationary or if it requires differencing to make it suitable for ARIMAX modeling.\n\n\nCode\n# Convert your multivariate time series data to a matrix\nfinal_XLF_factor_data_ts_multivariate <- as.matrix(normalized_XLF_factor_data_ts)\n\n# Check for stationarity using Phillips-Perron test\nphillips_perron_test <- ur.pp(final_XLF_factor_data_ts_multivariate)  \nsummary(phillips_perron_test)\n\n\n\n################################## \n# Phillips-Perron Unit Root Test # \n################################## \n\nTest regression with intercept \n\n\nCall:\nlm(formula = y ~ y.l1)\n\nResiduals:\n     Min       1Q   Median       3Q      Max \n-1.79948 -0.17048 -0.03191  0.31535  0.80545 \n\nCoefficients:\n            Estimate Std. Error t value Pr(>|t|)    \n(Intercept)  0.06281    0.09066   0.693    0.493    \ny.l1         0.88078    0.09622   9.154 1.41e-10 ***\n---\nSignif. codes:  0 '***' 0.001 '**' 0.01 '*' 0.05 '.' 0.1 ' ' 1\n\nResidual standard error: 0.5356 on 33 degrees of freedom\nMultiple R-squared:  0.7175,    Adjusted R-squared:  0.7089 \nF-statistic:  83.8 on 1 and 33 DF,  p-value: 1.411e-10\n\n\nValue of test-statistic, type: Z-alpha  is: -6.3819 \n\n         aux. Z statistics\nZ-tau-mu            0.5878\n\n\nThe Phillips-Perron unit root test output suggests that the data is stationary. The test regression includes an intercept term and tests whether the coefficient on the lagged value is significantly different from one. The estimated coefficients indicate that the intercept is not significant (p-value > 0.05), while the coefficient on the lagged value is highly significant (p-value < 1.411e-10), suggesting that the data is stationary.\nAdditionally, the “Z-alpha” value is the test statistic, which is compared to critical values of the standard normal distribution to determine significance. In this case, the test statistic is -6.3819, indicating strong evidence against the null hypothesis of a unit root. The “Z-tau-mu” value is an auxiliary test statistic used to correct for autocorrelation in the residuals, and it is close to zero, suggesting that there is no evidence of residual autocorrelation.\n\n\nFitting a ARIMAX model\nFitting an ARIMAX model will be a useful approach to modeling and predicting time series data when external factors are believed to have an impact on the response variable. In this case, we will be using an ARIMAX model to predict the Financial Sector Fund Stock Price, taking into account the effects of inflation on the sector. By including these exogenous variables in our model, we can improve the accuracy of our predictions and gain a better understanding of the underlying dynamics of the time series.\n\nAuto ARIMAResiduals\n\n\n\n\nCode\nxreg <- cbind(Inflation = normalized_XLF_factor_data_ts[, \"inflation\"])\n\nfit <- auto.arima(normalized_XLF_factor_data_ts[, \"XLF.Adjusted\"], xreg = xreg)\nsummary(fit)\n\n\nSeries: normalized_XLF_factor_data_ts[, \"XLF.Adjusted\"] \nRegression with ARIMA(0,0,0) errors \n\nCoefficients:\n        xreg\n      0.8126\ns.e.  0.1374\n\nsigma^2 = 0.3397:  log likelihood = -15.31\nAIC=34.62   AICc=35.42   BIC=36.4\n\nTraining set error measures:\n                       ME      RMSE       MAE       MPE     MAPE      MASE\nTraining set -3.94746e-16 0.5664279 0.4726719 -13.14624 125.0325 0.4424444\n                 ACF1\nTraining set 0.479335\n\n\n\n\n\n\nCode\ncheckresiduals(fit)\n\n\n\n\n\n\n    Ljung-Box test\n\ndata:  Residuals from Regression with ARIMA(0,0,0) errors\nQ* = 8.8247, df = 4, p-value = 0.06563\n\nModel df: 0.   Total lags used: 4\n\n\n\n\n\nThe auto.arima function reveals that the best-fit model is (0,0,0). Analysis of the residuals plot indicates that the fluctuates a lot and lies between -0.1 to 0.6. The ACF plot of the residuals shows significant lags, indicating that the model is performing well. The qq-plot also suggests normality in the residuals. The p-value greater than 0.05 actually suggests that there is no evidence of significant autocorrelation in the residuals, which means that the model is performing well in terms of capturing the temporal patterns in the data.\n\n\nFitting the model manually\n\nLinear ModelACF of ResidualsACF of ResidualsDifferentiated Residual\n\n\n\n\nCode\nnormalized_XLF_factor_data_numeric_df$XLF.Adjusted<-ts(normalized_XLF_factor_data_numeric_df$XLF.Adjusted,star=decimal_date(as.Date(\"2018-07-01\",format = \"%Y-%m-%d\")),frequency = 4)\nnormalized_XLF_factor_data_numeric_df$inflation<-ts(normalized_XLF_factor_data_numeric_df$inflation,star=decimal_date(as.Date(\"2018-07-01\",format = \"%Y-%m-%d\")),frequency = 4)\n\n\n############# First fit the linear model##########\nfit.reg <- lm(XLF.Adjusted ~ inflation, data=normalized_XLF_factor_data_numeric_df)\nsummary(fit.reg)\n\n\n\nCall:\nlm(formula = XLF.Adjusted ~ inflation, data = normalized_XLF_factor_data_numeric_df)\n\nResiduals:\n     Min       1Q   Median       3Q      Max \n-0.98522 -0.35561 -0.04648  0.49344  0.88641 \n\nCoefficients:\n              Estimate Std. Error t value Pr(>|t|)    \n(Intercept) -4.073e-16  1.416e-01   0.000        1    \ninflation    8.126e-01  1.457e-01   5.577 4.17e-05 ***\n---\nSignif. codes:  0 '***' 0.001 '**' 0.01 '*' 0.05 '.' 0.1 ' ' 1\n\nResidual standard error: 0.6008 on 16 degrees of freedom\nMultiple R-squared:  0.6603,    Adjusted R-squared:  0.6391 \nF-statistic:  31.1 on 1 and 16 DF,  p-value: 4.173e-05\n\n\n\n\n\n\nCode\nres.fit<-ts(residuals(fit.reg),star=decimal_date(as.Date(\"2018-07-01\",format = \"%Y-%m-%d\")),frequency = 4)\n############## Then look at the residuals ############\nacf(res.fit)\n\n\n\n\n\n\n\n\n\nCode\nPacf(res.fit)\n\n\n\n\n\n\n\n\n\nCode\nres.fit %>% diff() %>% ggtsdisplay()\n\n\n\n\n\n\n\n\nThe output displays the results of a linear regression model using two predictor variables, inflation and unemployment, to explain the variation in the response variable, XLF.Adjusted The coefficients of inflation have p-values of less than 0.05, indicating that both variables significantly impact XLF.Adjusted The R-squared value of approximately 64% suggests that the model explains a moderate amount of the variation in the response variable. Examination of the ACF plot of residuals reveals some autocorrelation, and therefore, the series is differentiated to achieve stationarity. By analyzing the differentiated ACF and PACF plots, the parameters if the ARIMAX modle are that p=0, q=0, and d=1.\n\n\nFinding the model parameters.\n\nARIMAX ResultModel Plot 1Model 1Model Plot 2Model 2\n\n\n\n\nCode\nARIMA.c=function(p1,p2,q1,q2,data){\ntemp=c()\nd=1\ni=1\ntemp= data.frame()\nls=matrix(rep(NA,6*30),nrow=30)\n\n\nfor (p in p1:p2)#\n{\n  for(q in q1:q2)#\n  {\n    for(d in 0:1)\n    {\n      \n      if(p+d+q<=6)\n      {\n        \n        model<- Arima(data,order=c(p,d,q))\n        ls[i,]= c(p,d,q,model$aic,model$bic,model$aicc)\n        i=i+1\n        #print(i)\n        \n      }\n      \n    }\n  }\n}\n\n\ntemp= as.data.frame(ls)\nnames(temp)= c(\"p\",\"d\",\"q\",\"AIC\",\"BIC\",\"AICc\")\n\ntemp\n}\n\noutput <- ARIMA.c(0,1,0,1,data=residuals(fit.reg))\n\noutput[which.min(output$AIC),] \n\n\n  p d q      AIC      BIC    AICc\n2 0 1 0 29.22974 30.06295 29.4964\n\n\nCode\noutput[which.min(output$BIC),]\n\n\n  p d q      AIC      BIC    AICc\n2 0 1 0 29.22974 30.06295 29.4964\n\n\nCode\noutput[which.min(output$AICc),]\n\n\n  p d q      AIC      BIC    AICc\n2 0 1 0 29.22974 30.06295 29.4964\n\n\n\n\n\n\nCode\nset.seed(1234)\n\nmodel_output <- capture.output(sarima(res.fit, 0,1,0)) \n\n\n\n\n\n\n\n\n\nCode\ncat(model_output[9:38], model_output[length(model_output)], sep = \"\\n\")\n\n\n$fit\n\nCall:\narima(x = xdata, order = c(p, d, q), seasonal = list(order = c(P, D, Q), period = S), \n    xreg = constant, transform.pars = trans, fixed = fixed, optim.control = list(trace = trc, \n        REPORT = 1, reltol = tol))\n\nCoefficients:\n      constant\n       -0.0389\ns.e.    0.1304\n\nsigma^2 estimated as 0.289:  log likelihood = -13.57,  aic = 31.14\n\n$degrees_of_freedom\n[1] 16\n\n$ttable\n         Estimate     SE t.value p.value\nconstant  -0.0389 0.1304 -0.2986  0.7691\n\n$AIC\n[1] 1.831812\n\n$AICc\n[1] 1.847498\n\n$BIC\n[1] 1.929837\n\n\n\n\n\n\nCode\nset.seed(1234)\n\nmodel_output <- capture.output(sarima(res.fit, 0,0,0)) \n\n\n\n\n\n\n\n\n\nCode\ncat(model_output[9:38], model_output[length(model_output)], sep = \"\\n\")\n\n\n$fit\n\nCall:\narima(x = xdata, order = c(p, d, q), seasonal = list(order = c(P, D, Q), period = S), \n    xreg = xmean, include.mean = FALSE, transform.pars = trans, fixed = fixed, \n    optim.control = list(trace = trc, REPORT = 1, reltol = tol))\n\nCoefficients:\n       xmean\n      0.0000\ns.e.  0.1335\n\nsigma^2 estimated as 0.3208:  log likelihood = -15.31,  aic = 34.62\n\n$degrees_of_freedom\n[1] 17\n\n$ttable\n      Estimate     SE t.value p.value\nxmean        0 0.1335       0       1\n\n$AIC\n[1] 1.923288\n\n$AICc\n[1] 1.937177\n\n$BIC\n[1] 2.022218\n\n\n\n\n\nFollowing the manual fitting process, we identified that the ARIMAX model with the minimum AIC, bic and AICc has the parameters (0,1,0). We fitted both models using both the manual and auto.arima methods and observed that all the models have p-values greater than 0, indicating statistical significance. To determine the best fit model, we performed cross-validation.\n\n\nCross Validation\n\nRMSE PlotRMSE Results\n\n\n\n\nCode\nn=length(res.fit)\nk= 10\n \n \nrmse1 <- matrix(NA, (n-k),4)\nrmse2 <- matrix(NA, (n-k),4)\nrmse3 <- matrix(NA, (n-k),4)\n\nst <- tsp(res.fit)[1]+(k-5)/4 \n\nfor(i in 1:(n-k))\n{\n  xtrain <- window(res.fit, end=st + i/4)\n  xtest <- window(res.fit, start=st + (i+1)/4, end=st + (i+4)/4)\n\n  \n  fit <- Arima(xtrain, order=c(0,1,0),\n                include.drift=TRUE, method=\"ML\")\n  fcast <- forecast(fit, h=4)\n  \n  fit2 <- Arima(xtrain, order=c(0,0,0),\n                include.drift=TRUE, method=\"ML\")\n  fcast2 <- forecast(fit2, h=4)\n\n\n  rmse1[i,1:length(xtest)]   <- sqrt((fcast$mean-xtest)^2)\n  rmse2[i,1:length(xtest)] <- sqrt((fcast2$mean-xtest)^2)\n}\n\nplot(1:4,colMeans(rmse1,na.rm=TRUE), type=\"l\",col=2, xlab=\"horizon\", ylab=\"RMSE\")\nlines(1:4, colMeans(rmse2,na.rm=TRUE), type=\"l\",col=3)\nlegend(\"topleft\",legend=c(\"fit1\",\"fit2\"),col=2:4,lty=1)\n\n\n\n\n\n\n\n\n\nCode\ncat(\"RMSE values for Model 1\\n\", colMeans(rmse1))\n\n\nRMSE values for Model 1\n 0.4874805 0.7412223 1.079872 1.272875\n\n\nCode\ncat(\"RMSE values for Model 2\\n\", colMeans(rmse2))\n\n\nRMSE values for Model 2\n 0.4958342 0.5855811 0.8209764 0.8966811\n\n\n\n\n\nThe RMSE cross-validation plot shows that the RMSE of Model 2 (0,0,0) is lower than that of Model 1 (0,1,0). This suggests that Model 2 (0,0,0) is a better fit for the data when compared other model.\n\n\nForecast\n::: panel-tabset ### Forecast for Financial Sector Fund with feature variable\n\n\nCode\n#fiting an ARIMA model to the Inflation variable\ninflation_fit<-auto.arima(normalized_XLF_factor_data_numeric_df$inflation) \nfinflation<-forecast(inflation_fit)\n\n\n# best model fit for forcasting\nxreg <- cbind(Inflation = normalized_XLF_factor_data_ts[, \"inflation\"])\n\nfit <- Arima(normalized_XLF_factor_data_ts[, \"XLF.Adjusted\"],order=c(0,0,0),xreg=xreg)\n\n# Forcast the stock price using feature variables\nfxreg <- cbind(Inflation = finflation$mean)\nfcast <- forecast(fit, xreg=fxreg) \nautoplot(fcast) + xlab(\"Year\") +\n  ylab(\"Normalised XLF.Adjusted\")\n\n\n\n\n\n\n\nARIMA Model for Inflation\n\n\nCode\n#fiting an ARIMA model to the Inflation variable\ninflation_fit<-auto.arima(normalized_XLF_factor_data_numeric_df$inflation) \nfinflation<-forecast(inflation_fit)\nsummary(inflation_fit)\n\n\nSeries: normalized_XLF_factor_data_numeric_df$inflation \nARIMA(0,1,0) \n\nsigma^2 = 0.1378:  log likelihood = -7.28\nAIC=16.55   AICc=16.82   BIC=17.38\n\nTraining set error measures:\n                    ME      RMSE       MAE       MPE     MAPE      MASE\nTraining set 0.1201871 0.3607553 0.2312026 -26.91442 61.15158 0.2676132\n                  ACF1\nTraining set 0.2431721\n\n\n\n\nARIMA Model for Financial Sector Fund with feature variable\n\n\nCode\n# best model fit for forcasting\nxreg <- cbind(Inflation = normalized_XLF_factor_data_ts[, \"inflation\"])\n\nfit <- Arima(normalized_XLF_factor_data_ts[, \"XLF.Adjusted\"],order=c(0,0,0),xreg=xreg)\nsummary(fit)\n\n\nSeries: normalized_XLF_factor_data_ts[, \"XLF.Adjusted\"] \nRegression with ARIMA(0,0,0) errors \n\nCoefficients:\n      intercept    xreg\n         0.0000  0.8126\ns.e.     0.1335  0.1374\n\nsigma^2 = 0.3609:  log likelihood = -15.31\nAIC=36.62   AICc=38.33   BIC=39.29\n\nTraining set error measures:\n                       ME      RMSE       MAE       MPE     MAPE      MASE\nTraining set 1.572816e-13 0.5664279 0.4726719 -13.14624 125.0325 0.4424444\n                 ACF1\nTraining set 0.479335\n\n\nThe ARIMAX model may have incorporated exogenous variables such as macroeconomic factors, which could indicate the potential impact of external factors on the forecasted values. The findings suggest that inflation rates significantly impact the Financial Sector Fund stock price, underscoring the importance of including macroeconomic factors in stock market forecasting. This is further supported by the comparison of RMSE, MAE, AIC and BIC values for forecasts with and without the feature variable. It is found that including the feature variable leads to a lower values for AIC and BIC values but the RMSE and MAE values arehigher than the arima model, this might be not enough information to make conclude which model is better for the variables."
  },
  {
    "objectID": "arimax_sector_health_care_macroeconomic.html",
    "href": "arimax_sector_health_care_macroeconomic.html",
    "title": "ARIMAX/SARIMAX Page for Health Care Sector Fund and macroeconomic factors",
    "section": "",
    "text": "Endogenous variable: Health Care Sector Fund Exogenous variables: Macroeconomic indicators such as GDP, inflation rate, unemployment rate and interest rates\nBefore proceeding with model selection, it can be useful to examine the feature variables and select the appropriate ones. This can be done by creating cross-correlation function plots, which can serve as a verification of the correlation heatmap for the variables.\n\nCCF Plot for Health Care Sector Fund and Exogenous Variables\n\nGDPInterest RateInflation RateUnemployment Rate\n\n\n\n\nCode\npar(mfrow=c(1,1))\nccf_result <- ccf(normalized_sector_factor_data[, c(\"XLV.Adjusted\")], normalized_sector_factor_data[, c(\"gdp\")], \n    lag.max = 300,\n    main = \"Cros-Correlation Plot for Health Care Sector Fund and GDP Growth Rate \",\n    ylab = \"CCF\")\n\n\n\n\n\nCode\ncat(\"The sum of cross correlation function is\", sum(abs(ccf_result$acf)))\n\n\nThe sum of cross correlation function is 5.024517\n\n\n\n\n\n\nCode\npar(mfrow=c(1,1))\nccf_result <- ccf(normalized_sector_factor_data[, c(\"XLV.Adjusted\")], normalized_sector_factor_data[, c(\"interest\")], \n    lag.max = 300,\n    main = \"Cros-Correlation Plot for Health Care Sector Fund and Interest Rate\",\n    ylab = \"CCF\")\n\n\n\n\n\nCode\ncat(\"The sum of cross correlation function is\", sum(abs(ccf_result$acf)))\n\n\nThe sum of cross correlation function is 9.642841\n\n\n\n\n\n\nCode\npar(mfrow=c(1,1))\nccf_result <- ccf(normalized_sector_factor_data[, c(\"XLV.Adjusted\")], normalized_sector_factor_data[, c(\"inflation\")], \n    lag.max = 300,\n    main = \"Cros-Correlation Plot for Health Care Sector Fund and Inflation Rate\",\n    ylab = \"CCF\")\n\n\n\n\n\nCode\ncat(\"The sum of cross correlation function is\", sum(abs(ccf_result$acf)))\n\n\nThe sum of cross correlation function is 11.25353\n\n\n\n\n\n\nCode\npar(mfrow=c(1,1))\nccf_result <- ccf(normalized_sector_factor_data[, c(\"XLV.Adjusted\")], normalized_sector_factor_data[, c(\"unemployment\")], \n    lag.max = 300,\n    main = \"Cros-Correlation Plot for Health Care Sector Fund and Unemployment Rate\",\n    ylab = \"CCF\")\n\n\n\n\n\nCode\ncat(\"The sum of cross correlation function is\", sum(abs(ccf_result$acf)))\n\n\nThe sum of cross correlation function is 6.769889\n\n\n\n\n\nThe cross-correlation feature plots reaffirm the findings from the heatmap analysis, indicating that inflation rate exhibit stronger correlations with the index compared to GDP, interest rate and unemployment rate. The cross-correlation plots for GDP, unemployment and interest rates show weaker and more scattered patterns, with strikes between the blue lines indicating lower correlation coefficients. This suggests that inflation rate are more suitable feature variables for the ARIMAX model when predicting Health Care Sector Fund movements.\nFinal Exogenous variables: Macroeconomic indicators: Inflation rate\n\n\nEndogenous and Exogenous Variables Plot\n\n\nCode\nfinal_XLV_factor_data <- sector_factor_data %>%dplyr::select( Date,XLV.Adjusted, inflation)\nnumeric_vars_XLV_factor_data <- c(\"XLV.Adjusted\", \"inflation\")\nnumeric_XLV_factor_data <- final_XLV_factor_data[, numeric_vars_XLV_factor_data]\nnormalized_XLV_factor_data_numeric <- scale(numeric_XLV_factor_data)\nnormalized_XLV_factor_data_numeric_df <- data.frame(normalized_XLV_factor_data_numeric)\nnormalized_XLV_factor_data_ts <- ts(normalized_XLV_factor_data_numeric, start = c(2018, 7), frequency = 4)\n\nautoplot(normalized_XLV_factor_data_ts, facets=TRUE) +\n  xlab(\"Year\") + ylab(\"\") +\n  ggtitle(\"Health Care Sector Fund Stock Price, Inflation Rate and Unemployment Rate in USA 2018-2023\")\n\n\n\n\n\n\n\nCheck the stationarity\nBefore proceeding with further analysis, it’s important to check the stationarity of the time series data. If the series is not stationary, it needs to be differentiated before performing a ARIMAX model. The stationarity of a multivariate time series data can be checked using the Phillips-Perron test, which is specifically designed for multivariate data. By performing the Phillips-Perron test on the multivariate time series data, we can determine if the data is stationary or if it requires differencing to make it suitable for ARIMAX modeling.\n\n\nCode\n# Convert your multivariate time series data to a matrix\nfinal_XLV_factor_data_ts_multivariate <- as.matrix(normalized_XLV_factor_data_ts)\n\n# Check for stationarity using Phillips-Perron test\nphillips_perron_test <- ur.pp(final_XLV_factor_data_ts_multivariate)  \nsummary(phillips_perron_test)\n\n\n\n################################## \n# Phillips-Perron Unit Root Test # \n################################## \n\nTest regression with intercept \n\n\nCall:\nlm(formula = y ~ y.l1)\n\nResiduals:\n     Min       1Q   Median       3Q      Max \n-1.35623 -0.21421 -0.06766  0.28914  0.76913 \n\nCoefficients:\n            Estimate Std. Error t value Pr(>|t|)    \n(Intercept)  0.08155    0.07757   1.051    0.301    \ny.l1         0.90369    0.08233  10.977 1.49e-12 ***\n---\nSignif. codes:  0 '***' 0.001 '**' 0.01 '*' 0.05 '.' 0.1 ' ' 1\n\nResidual standard error: 0.4583 on 33 degrees of freedom\nMultiple R-squared:  0.785, Adjusted R-squared:  0.7785 \nF-statistic: 120.5 on 1 and 33 DF,  p-value: 1.492e-12\n\n\nValue of test-statistic, type: Z-alpha  is: -5.2745 \n\n         aux. Z statistics\nZ-tau-mu            0.8787\n\n\nThe Phillips-Perron unit root test output suggests that the data is stationary. The test regression includes an intercept term and tests whether the coefficient on the lagged value is significantly different from one. The estimated coefficients indicate that the intercept is not significant (p-value > 0.05), while the coefficient on the lagged value is highly significant (p-value < 1.49e-12), suggesting that the data is stationary.\nAdditionally, the “Z-alpha” value is the test statistic, which is compared to critical values of the standard normal distribution to determine significance. In this case, the test statistic is -5.2745, indicating strong evidence against the null hypothesis of a unit root. The “Z-tau-mu” value is an auxiliary test statistic used to correct for autocorrelation in the residuals, and it is close to zero, suggesting that there is no evidence of residual autocorrelation.\n\n\nFitting a ARIMAX model\nFitting an ARIMAX model will be a useful approach to modeling and predicting time series data when external factors are believed to have an impact on the response variable. In this case, we will be using an ARIMAX model to predict the Health Care Sector Fund Stock Price, taking into account the effects of inflation on the sector. By including these exogenous variables in our model, we can improve the accuracy of our predictions and gain a better understanding of the underlying dynamics of the time series.\n\nAuto ARIMAResiduals\n\n\n\n\nCode\nxreg <- cbind(Inflation = normalized_XLV_factor_data_ts[, \"inflation\"])\n\nfit <- auto.arima(normalized_XLV_factor_data_ts[, \"XLV.Adjusted\"], xreg = xreg)\nsummary(fit)\n\n\nSeries: normalized_XLV_factor_data_ts[, \"XLV.Adjusted\"] \nRegression with ARIMA(0,1,0) errors \n\nCoefficients:\n        xreg\n      0.5528\ns.e.  0.2538\n\nsigma^2 = 0.1603:  log likelihood = -8.05\nAIC=20.09   AICc=20.95   BIC=21.76\n\nTraining set error measures:\n                     ME      RMSE       MAE       MPE     MAPE      MASE\nTraining set 0.05668425 0.3775153 0.3134693 -28.94416 62.84741 0.4203005\n                   ACF1\nTraining set -0.4581504\n\n\n\n\n\n\nCode\ncheckresiduals(fit)\n\n\n\n\n\n\n    Ljung-Box test\n\ndata:  Residuals from Regression with ARIMA(0,1,0) errors\nQ* = 7.5156, df = 4, p-value = 0.111\n\nModel df: 0.   Total lags used: 4\n\n\n\n\n\nThe auto.arima function reveals that the best-fit model is (0,1,0). Analysis of the residuals plot indicates that the fluctuates a lot and lies between -0.50 to 0.75. The ACF plot of the residuals shows significant lags, indicating that the model is performing well. The qq-plot also suggests normality in the residuals. The p-value greater than 0.05 actually suggests that there is no evidence of significant autocorrelation in the residuals, which means that the model is performing well in terms of capturing the temporal patterns in the data.\n\n\nFitting the model manually\n\nLinear ModelACF of ResidualsACF of ResidualsDifferentiated Residual\n\n\n\n\nCode\nnormalized_XLV_factor_data_numeric_df$XLV.Adjusted<-ts(normalized_XLV_factor_data_numeric_df$XLV.Adjusted,star=decimal_date(as.Date(\"2018-07-01\",format = \"%Y-%m-%d\")),frequency = 4)\nnormalized_XLV_factor_data_numeric_df$inflation<-ts(normalized_XLV_factor_data_numeric_df$inflation,star=decimal_date(as.Date(\"2018-07-01\",format = \"%Y-%m-%d\")),frequency = 4)\n\n\n############# First fit the linear model##########\nfit.reg <- lm(XLV.Adjusted ~ inflation, data=normalized_XLV_factor_data_numeric_df)\nsummary(fit.reg)\n\n\n\nCall:\nlm(formula = XLV.Adjusted ~ inflation, data = normalized_XLV_factor_data_numeric_df)\n\nResiduals:\n     Min       1Q   Median       3Q      Max \n-0.92550 -0.36140  0.00643  0.38970  1.10570 \n\nCoefficients:\n              Estimate Std. Error t value Pr(>|t|)    \n(Intercept) -1.277e-15  1.300e-01   0.000        1    \ninflation    8.448e-01  1.338e-01   6.316 1.03e-05 ***\n---\nSignif. codes:  0 '***' 0.001 '**' 0.01 '*' 0.05 '.' 0.1 ' ' 1\n\nResidual standard error: 0.5515 on 16 degrees of freedom\nMultiple R-squared:  0.7137,    Adjusted R-squared:  0.6958 \nF-statistic: 39.89 on 1 and 16 DF,  p-value: 1.026e-05\n\n\n\n\n\n\nCode\nres.fit<-ts(residuals(fit.reg),star=decimal_date(as.Date(\"2018-01-01\",format = \"%Y-%m-%d\")),frequency = 4)\n############## Then look at the residuals ############\nacf(res.fit)\n\n\n\n\n\n\n\n\n\nCode\nPacf(res.fit)\n\n\n\n\n\n\n\n\n\nCode\nres.fit %>% diff() %>% ggtsdisplay()\n\n\n\n\n\n\n\n\nThe output displays the results of a linear regression model using two predictor variables, inflation and unemployment, to explain the variation in the response variable, XLV.Adjusted The coefficients of inflation have p-values of less than 0.05, indicating that both variables significantly impact XLV.Adjusted The R-squared value of approximately 71% suggests that the model explains a moderate amount of the variation in the response variable. Examination of the ACF plot of residuals reveals some autocorrelation, and therefore, the series is differentiated to achieve stationarity. By analyzing the differentiated ACF and PACF plots, the parameters if the ARIMAX modle are that p=0, q=0, and d=1.\nAs both the model fitted manually and the auto.arima parameters are the same, i.e. p = 0, q = 0 and d = 1. We proceed with fitting the model for the parameters.\n\n\nModel Fitting\n\nModel PlotModel\n\n\n\n\nCode\nset.seed(1234)\n\nmodel_output <- capture.output(sarima(res.fit, 0,1,0)) \n\n\n\n\n\n\n\n\n\nCode\ncat(model_output[9:38], model_output[length(model_output)], sep = \"\\n\")\n\n\n$fit\n\nCall:\narima(x = xdata, order = c(p, d, q), seasonal = list(order = c(P, D, Q), period = S), \n    xreg = constant, transform.pars = trans, fixed = fixed, optim.control = list(trace = trc, \n        REPORT = 1, reltol = tol))\n\nCoefficients:\n      constant\n        0.0229\ns.e.    0.0977\n\nsigma^2 estimated as 0.1621:  log likelihood = -8.66,  aic = 21.32\n\n$degrees_of_freedom\n[1] 16\n\n$ttable\n         Estimate     SE t.value p.value\nconstant   0.0229 0.0977  0.2345  0.8175\n\n$AIC\n[1] 1.253827\n\n$AICc\n[1] 1.269513\n\n$BIC\n[1] 1.351852\n\n\n\n\n\nThe best model identified is ARIMA(0,1,0). By analyzing the standardized residuals plot, it can be observed that the mean is close to 0. Deviations from these values could indicate poor model fit. However, in this case, the model seems to be well-fitted. The ACF plot of residuals shows very few significant lags, which is a positive sign for the model. The qq-plot also suggests normality in the residuals. Additionally, the p-values for the Ljung-Box test are greater than 0.05, indicating that the residuals are independent, which is favorable for the model’s accuracy.\n\n\nForecast\n::: panel-tabset ### Forecast for Health Care Sector Fund with feature variable\n\n\nCode\n#fiting an ARIMA model to the Inflation variable\ninflation_fit<-auto.arima(normalized_XLV_factor_data_numeric_df$inflation) \nfinflation<-forecast(inflation_fit)\n\n\n# best model fit for forcasting\nxreg <- cbind(Inflation = normalized_XLV_factor_data_ts[, \"inflation\"])\n\nfit <- Arima(normalized_XLV_factor_data_ts[, \"XLV.Adjusted\"],order=c(0,1,0),xreg=xreg)\n\n# Forcast the stock price using feature variables\nfxreg <- cbind(Inflation = finflation$mean)\nfcast <- forecast(fit, xreg=fxreg) \nautoplot(fcast) + xlab(\"Year\") +\n  ylab(\"Normalised XLV.Adjusted\")\n\n\n\n\n\n\n\nARIMA Model for Inflation\n\n\nCode\n#fiting an ARIMA model to the Inflation variable\ninflation_fit<-auto.arima(normalized_XLV_factor_data_numeric_df$inflation) \nfinflation<-forecast(inflation_fit)\nsummary(inflation_fit)\n\n\nSeries: normalized_XLV_factor_data_numeric_df$inflation \nARIMA(0,1,0) \n\nsigma^2 = 0.1378:  log likelihood = -7.28\nAIC=16.55   AICc=16.82   BIC=17.38\n\nTraining set error measures:\n                    ME      RMSE       MAE       MPE     MAPE      MASE\nTraining set 0.1201871 0.3607553 0.2312026 -26.91442 61.15158 0.2676132\n                  ACF1\nTraining set 0.2431721\n\n\n\n\nARIMA Model for Health Care Sector Fund with feature variable\n\n\nCode\n# best model fit for forcasting\nxreg <- cbind(Inflation = normalized_XLV_factor_data_ts[, \"inflation\"])\n\nfit <- Arima(normalized_XLV_factor_data_ts[, \"XLV.Adjusted\"],order=c(0,1,0),xreg=xreg)\nsummary(fit)\n\n\nSeries: normalized_XLV_factor_data_ts[, \"XLV.Adjusted\"] \nRegression with ARIMA(0,1,0) errors \n\nCoefficients:\n        xreg\n      0.5528\ns.e.  0.2538\n\nsigma^2 = 0.1603:  log likelihood = -8.05\nAIC=20.09   AICc=20.95   BIC=21.76\n\nTraining set error measures:\n                     ME      RMSE       MAE       MPE     MAPE      MASE\nTraining set 0.05668425 0.3775153 0.3134693 -28.94416 62.84741 0.4203005\n                   ACF1\nTraining set -0.4581504\n\n\nThe ARIMAX model may have incorporated exogenous variables such as macroeconomic factors, which could indicate the potential impact of external factors on the forecasted values. The findings suggest that inflation rates significantly impact the Health Care Sector Fund stock price, underscoring the importance of including macroeconomic factors in stock market forecasting. This is further supported by the comparison of RMSE, MAE, AIC and BIC values for forecasts with and without the feature variable. It is found that including the feature variable leads to a lower values, indicating that the feature variable makes a significant impact and should be considered in stock market forecasting."
  },
  {
    "objectID": "arimax_sector_industrial_macroeconomic.html",
    "href": "arimax_sector_industrial_macroeconomic.html",
    "title": "ARIMAX/SARIMAX Page for Industrial Sector Fund and macroeconomic factors",
    "section": "",
    "text": "Endogenous variable: Industrial Sector Fund Exogenous variables: Macroeconomic indicators such as GDP, inflation rate, unemployment rate and interest rates\nBefore proceeding with model selection, it can be useful to examine the feature variables and select the appropriate ones. This can be done by creating cross-correlation function plots, which can serve as a verification of the correlation heatmap for the variables.\n\nCCF Plot for Industrial Sector Fund and Exogenous Variables\n\nGDPInterest RateInflation RateUnemployment Rate\n\n\n\n\nCode\npar(mfrow=c(1,1))\nccf_result <- ccf(normalized_sector_factor_data[, c(\"XLI.Adjusted\")], normalized_sector_factor_data[, c(\"gdp\")], \n    lag.max = 300,\n    main = \"Cros-Correlation Plot for Industrial Sector Fund and GDP Growth Rate \",\n    ylab = \"CCF\")\n\n\n\n\n\nCode\ncat(\"The sum of cross correlation function is\", sum(abs(ccf_result$acf)))\n\n\nThe sum of cross correlation function is 5.024671\n\n\n\n\n\n\nCode\npar(mfrow=c(1,1))\nccf_result <- ccf(normalized_sector_factor_data[, c(\"XLI.Adjusted\")], normalized_sector_factor_data[, c(\"interest\")], \n    lag.max = 300,\n    main = \"Cros-Correlation Plot for Industrial Sector Fund and Interest Rate\",\n    ylab = \"CCF\")\n\n\n\n\n\nCode\ncat(\"The sum of cross correlation function is\", sum(abs(ccf_result$acf)))\n\n\nThe sum of cross correlation function is 8.965553\n\n\n\n\n\n\nCode\npar(mfrow=c(1,1))\nccf_result <- ccf(normalized_sector_factor_data[, c(\"XLI.Adjusted\")], normalized_sector_factor_data[, c(\"inflation\")], \n    lag.max = 300,\n    main = \"Cros-Correlation Plot for Industrial Sector Fund and Inflation Rate\",\n    ylab = \"CCF\")\n\n\n\n\n\nCode\ncat(\"The sum of cross correlation function is\", sum(abs(ccf_result$acf)))\n\n\nThe sum of cross correlation function is 9.82419\n\n\n\n\n\n\nCode\npar(mfrow=c(1,1))\nccf_result <- ccf(normalized_sector_factor_data[, c(\"XLI.Adjusted\")], normalized_sector_factor_data[, c(\"unemployment\")], \n    lag.max = 300,\n    main = \"Cros-Correlation Plot for Industrial Sector Fund and Unemployment Rate\",\n    ylab = \"CCF\")\n\n\n\n\n\nCode\ncat(\"The sum of cross correlation function is\", sum(abs(ccf_result$acf)))\n\n\nThe sum of cross correlation function is 6.6434\n\n\n\n\n\nThe cross-correlation feature plots reaffirm the findings from the heatmap analysis, indicating that inflation rate exhibit stronger correlations with the index compared to GDP, interest rate and unemployment rate. The cross-correlation plots for GDP, unemployment and interest rates show weaker and more scattered patterns, with strikes between the blue lines indicating lower correlation coefficients. This suggests that inflation rate are more suitable feature variables for the ARIMAX model when predicting Industrial Sector Fund movements.\nFinal Exogenous variables: Macroeconomic indicators: Inflation rate\n\n\nEndogenous and Exogenous Variables Plot\n\n\nCode\nfinal_XLI_factor_data <- sector_factor_data %>%dplyr::select( Date,XLI.Adjusted, inflation)\nnumeric_vars_XLI_factor_data <- c(\"XLI.Adjusted\", \"inflation\")\nnumeric_XLI_factor_data <- final_XLI_factor_data[, numeric_vars_XLI_factor_data]\nnormalized_XLI_factor_data_numeric <- scale(numeric_XLI_factor_data)\nnormalized_XLI_factor_data_numeric_df <- data.frame(normalized_XLI_factor_data_numeric)\nnormalized_XLI_factor_data_ts <- ts(normalized_XLI_factor_data_numeric, start = c(2018, 7), frequency = 4)\n\nautoplot(normalized_XLI_factor_data_ts, facets=TRUE) +\n  xlab(\"Year\") + ylab(\"\") +\n  ggtitle(\"Industrial Sector Fund Stock Price, Inflation Rate and Unemployment Rate in USA 2018-2023\")\n\n\n\n\n\n\n\nCheck the stationarity\nBefore proceeding with further analysis, it’s important to check the stationarity of the time series data. If the series is not stationary, it needs to be differentiated before performing a ARIMAX model. The stationarity of a multivariate time series data can be checked using the Phillips-Perron test, which is specifically designed for multivariate data. By performing the Phillips-Perron test on the multivariate time series data, we can determine if the data is stationary or if it requires differencing to make it suitable for ARIMAX modeling.\n\n\nCode\n# Convert your multivariate time series data to a matrix\nfinal_XLI_factor_data_ts_multivariate <- as.matrix(normalized_XLI_factor_data_ts)\n\n# Check for stationarity using Phillips-Perron test\nphillips_perron_test <- ur.pp(final_XLI_factor_data_ts_multivariate)  \nsummary(phillips_perron_test)\n\n\n\n################################## \n# Phillips-Perron Unit Root Test # \n################################## \n\nTest regression with intercept \n\n\nCall:\nlm(formula = y ~ y.l1)\n\nResiduals:\n     Min       1Q   Median       3Q      Max \n-1.75546 -0.19705 -0.06804  0.37121  0.81303 \n\nCoefficients:\n            Estimate Std. Error t value Pr(>|t|)    \n(Intercept)  0.06906    0.09327   0.740    0.464    \ny.l1         0.86276    0.09899   8.716  4.5e-10 ***\n---\nSignif. codes:  0 '***' 0.001 '**' 0.01 '*' 0.05 '.' 0.1 ' ' 1\n\nResidual standard error: 0.5511 on 33 degrees of freedom\nMultiple R-squared:  0.6971,    Adjusted R-squared:  0.688 \nF-statistic: 75.96 on 1 and 33 DF,  p-value: 4.497e-10\n\n\nValue of test-statistic, type: Z-alpha  is: -6.4883 \n\n         aux. Z statistics\nZ-tau-mu            0.6574\n\n\nThe Phillips-Perron unit root test output suggests that the data is stationary. The test regression includes an intercept term and tests whether the coefficient on the lagged value is significantly different from one. The estimated coefficients indicate that the intercept is not significant (p-value > 0.05), while the coefficient on the lagged value is highly significant (p-value < 4.497e-10), suggesting that the data is stationary.\nAdditionally, the “Z-alpha” value is the test statistic, which is compared to critical values of the standard normal distribution to determine significance. In this case, the test statistic is -6.4883, indicating strong evidence against the null hypothesis of a unit root. The “Z-tau-mu” value is an auxiliary test statistic used to correct for autocorrelation in the residuals, and it is close to zero, suggesting that there is no evidence of residual autocorrelation.\n\n\nFitting a ARIMAX model\nFitting an ARIMAX model will be a useful approach to modeling and predicting time series data when external factors are believed to have an impact on the response variable. In this case, we will be using an ARIMAX model to predict the Industrial Sector Fund Stock Price, taking into account the effects of inflation on the sector. By including these exogenous variables in our model, we can improve the accuracy of our predictions and gain a better understanding of the underlying dynamics of the time series.\n\nAuto ARIMAResiduals\n\n\n\n\nCode\nxreg <- cbind(Inflation = normalized_XLI_factor_data_ts[, \"inflation\"])\n\nfit <- auto.arima(normalized_XLI_factor_data_ts[, \"XLI.Adjusted\"], xreg = xreg)\nsummary(fit)\n\n\nSeries: normalized_XLI_factor_data_ts[, \"XLI.Adjusted\"] \nRegression with ARIMA(1,0,0) errors \n\nCoefficients:\n         ar1    xreg\n      0.5686  0.7112\ns.e.  0.2008  0.2346\n\nsigma^2 = 0.3259:  log likelihood = -14.59\nAIC=35.17   AICc=36.89   BIC=37.84\n\nTraining set error measures:\n                      ME    RMSE       MAE       MPE     MAPE      MASE\nTraining set -0.02217365 0.53825 0.4345546 -23.76264 95.84547 0.4338544\n                    ACF1\nTraining set -0.01895771\n\n\n\n\n\n\nCode\ncheckresiduals(fit)\n\n\n\n\n\n\n    Ljung-Box test\n\ndata:  Residuals from Regression with ARIMA(1,0,0) errors\nQ* = 3.0475, df = 3, p-value = 0.3844\n\nModel df: 1.   Total lags used: 4\n\n\n\n\n\nThe auto.arima function reveals that the best-fit model is (1,0,0). Analysis of the residuals plot indicates that the fluctuates a lot and lies between -0.6 to 0.6. The ACF plot of the residuals shows significant lags, indicating that the model is performing well. The qq-plot also suggests normality in the residuals. The p-value greater than 0.05 actually suggests that there is no evidence of significant autocorrelation in the residuals, which means that the model is performing well in terms of capturing the temporal patterns in the data.\n\n\nFitting the model manually\n\nLinear ModelACF of ResidualsACF of ResidualsDifferentiated Residual\n\n\n\n\nCode\nnormalized_XLI_factor_data_numeric_df$XLI.Adjusted<-ts(normalized_XLI_factor_data_numeric_df$XLI.Adjusted,star=decimal_date(as.Date(\"2018-07-01\",format = \"%Y-%m-%d\")),frequency = 4)\nnormalized_XLI_factor_data_numeric_df$inflation<-ts(normalized_XLI_factor_data_numeric_df$inflation,star=decimal_date(as.Date(\"2018-07-01\",format = \"%Y-%m-%d\")),frequency = 4)\n\n\n############# First fit the linear model##########\nfit.reg <- lm(XLI.Adjusted ~ inflation, data=normalized_XLI_factor_data_numeric_df)\nsummary(fit.reg)\n\n\n\nCall:\nlm(formula = XLI.Adjusted ~ inflation, data = normalized_XLI_factor_data_numeric_df)\n\nResiduals:\n     Min       1Q   Median       3Q      Max \n-1.00573 -0.52568 -0.03409  0.36864  1.16237 \n\nCoefficients:\n             Estimate Std. Error t value Pr(>|t|)    \n(Intercept) 2.971e-16  1.617e-01   0.000 1.000000    \ninflation   7.464e-01  1.664e-01   4.487 0.000374 ***\n---\nSignif. codes:  0 '***' 0.001 '**' 0.01 '*' 0.05 '.' 0.1 ' ' 1\n\nResidual standard error: 0.686 on 16 degrees of freedom\nMultiple R-squared:  0.5571,    Adjusted R-squared:  0.5295 \nF-statistic: 20.13 on 1 and 16 DF,  p-value: 0.0003738\n\n\n\n\n\n\nCode\nres.fit<-ts(residuals(fit.reg),star=decimal_date(as.Date(\"2018-07-01\",format = \"%Y-%m-%d\")),frequency = 4)\n############## Then look at the residuals ############\nacf(res.fit)\n\n\n\n\n\n\n\n\n\nCode\nPacf(res.fit)\n\n\n\n\n\n\n\n\n\nCode\nres.fit %>% diff() %>% ggtsdisplay()\n\n\n\n\n\n\n\n\nThe output displays the results of a linear regression model using two predictor variables, inflation and unemployment, to explain the variation in the response variable, XLI.Adjusted The coefficients of inflation have p-values of less than 0.05, indicating that both variables significantly impact XLI.Adjusted The R-squared value of approximately 56% suggests that the model explains a moderate amount of the variation in the response variable. Examination of the ACF plot of residuals reveals some autocorrelation, and therefore, the series is differentiated to achieve stationarity. By analyzing the differentiated ACF and PACF plots, the parameters if the ARIMAX modle are that p=0, q=0, and d=1.\n\n\nFinding the model parameters.\n\nARIMAX ResultModel Plot 1Model 1Model Plot 2Model 2\n\n\n\n\nCode\nARIMA.c=function(p1,p2,q1,q2,data){\ntemp=c()\nd=1\ni=1\ntemp= data.frame()\nls=matrix(rep(NA,6*30),nrow=30)\n\n\nfor (p in p1:p2)#\n{\n  for(q in q1:q2)#\n  {\n    for(d in 0:1)\n    {\n      \n      if(p+d+q<=6)\n      {\n        \n        model<- Arima(data,order=c(p,d,q))\n        ls[i,]= c(p,d,q,model$aic,model$bic,model$aicc)\n        i=i+1\n        #print(i)\n        \n      }\n      \n    }\n  }\n}\n\n\ntemp= as.data.frame(ls)\nnames(temp)= c(\"p\",\"d\",\"q\",\"AIC\",\"BIC\",\"AICc\")\n\ntemp\n}\n\noutput <- ARIMA.c(0,1,0,1,data=residuals(fit.reg))\n\noutput[which.min(output$AIC),] \n\n\n  p d q      AIC      BIC     AICc\n2 0 1 0 33.00926 33.84248 33.27593\n\n\nCode\noutput[which.min(output$BIC),]\n\n\n  p d q      AIC      BIC     AICc\n2 0 1 0 33.00926 33.84248 33.27593\n\n\nCode\noutput[which.min(output$AICc),]\n\n\n  p d q      AIC      BIC     AICc\n2 0 1 0 33.00926 33.84248 33.27593\n\n\n\n\n\n\nCode\nset.seed(1234)\n\nmodel_output <- capture.output(sarima(res.fit, 0,1,0)) \n\n\n\n\n\n\n\n\n\nCode\ncat(model_output[9:38], model_output[length(model_output)], sep = \"\\n\")\n\n\n$fit\n\nCall:\narima(x = xdata, order = c(p, d, q), seasonal = list(order = c(P, D, Q), period = S), \n    xreg = constant, transform.pars = trans, fixed = fixed, optim.control = list(trace = trc, \n        REPORT = 1, reltol = tol))\n\nCoefficients:\n      constant\n       -0.0207\ns.e.    0.1460\n\nsigma^2 estimated as 0.3624:  log likelihood = -15.49,  aic = 34.99\n\n$degrees_of_freedom\n[1] 16\n\n$ttable\n         Estimate    SE t.value p.value\nconstant  -0.0207 0.146  -0.142  0.8889\n\n$AIC\n[1] 2.058184\n\n$AICc\n[1] 2.07387\n\n$BIC\n[1] 2.156209\n\n\n\n\n\n\nCode\nset.seed(1234)\n\nmodel_output <- capture.output(sarima(res.fit, 1,0,0)) \n\n\n\n\n\n\n\n\n\nCode\ncat(model_output[21:50], model_output[length(model_output)], sep = \"\\n\")\n\n\n$fit\n\nCall:\narima(x = xdata, order = c(p, d, q), seasonal = list(order = c(P, D, Q), period = S), \n    xreg = xmean, include.mean = FALSE, transform.pars = trans, fixed = fixed, \n    optim.control = list(trace = trc, REPORT = 1, reltol = tol))\n\nCoefficients:\n         ar1    xmean\n      0.5780  -0.1029\ns.e.  0.2013   0.2887\n\nsigma^2 estimated as 0.2877:  log likelihood = -14.53,  aic = 35.06\n\n$degrees_of_freedom\n[1] 16\n\n$ttable\n      Estimate     SE t.value p.value\nar1     0.5780 0.2013  2.8707  0.0111\nxmean  -0.1029 0.2887 -0.3565  0.7261\n\n$AIC\n[1] 1.947862\n\n$AICc\n[1] 1.992306\n\n$BIC\n[1] 2.096257\n\n\n\n\n\nFollowing the manual fitting process, we identified that the ARIMAX model with the minimum AIC,BIC and AICc has the parameters (0,1,0). We fitted both models using both the manual and auto.arima methods and observed that all the models have p-values greater than 0, indicating statistical significance. To determine the best fit model, we performed cross-validation.\n\n\nCross Validation\n\nRMSE PlotRMSE Results\n\n\n\n\nCode\nn=length(res.fit)\nk= 10\n \n \nrmse1 <- matrix(NA, (n-k),4)\nrmse2 <- matrix(NA, (n-k),4)\nrmse3 <- matrix(NA, (n-k),4)\n\nst <- tsp(res.fit)[1]+(k-5)/4 \n\nfor(i in 1:(n-k))\n{\n  xtrain <- window(res.fit, end=st + i/4)\n  xtest <- window(res.fit, start=st + (i+1)/4, end=st + (i+4)/4)\n\n  \n  fit <- Arima(xtrain, order=c(0,1,0),\n                include.drift=TRUE, method=\"ML\")\n  fcast <- forecast(fit, h=4)\n  \n  fit2 <- Arima(xtrain, order=c(1,0,0),\n                include.drift=TRUE, method=\"ML\")\n  fcast2 <- forecast(fit2, h=4)\n\n\n  rmse1[i,1:length(xtest)]   <- sqrt((fcast$mean-xtest)^2)\n  rmse2[i,1:length(xtest)] <- sqrt((fcast2$mean-xtest)^2)\n}\n\nplot(1:4,colMeans(rmse1,na.rm=TRUE), type=\"l\",col=2, xlab=\"horizon\", ylab=\"RMSE\")\nlines(1:4, colMeans(rmse2,na.rm=TRUE), type=\"l\",col=3)\nlegend(\"topleft\",legend=c(\"fit1\",\"fit2\"),col=2:4,lty=1)\n\n\n\n\n\n\n\n\n\nCode\ncat(\"RMSE values for Model 1\\n\", colMeans(rmse1))\n\n\nRMSE values for Model 1\n 0.566305 0.8614442 1.146143 1.314074\n\n\nCode\ncat(\"RMSE values for Model 2\\n\", colMeans(rmse2))\n\n\nRMSE values for Model 2\n 0.7080378 0.7809364 0.8256064 1.12462\n\n\n\n\n\nThe RMSE cross-validation plot shows that the RMSE of Model 2 (1,0,0) is lower than that of Model 1 (0,1,0). This suggests that Model 2 (1,0,0) is a better fit for the data when compared other model.\n\n\nForecast\n::: panel-tabset ### Forecast for Industrial Sector Fund with feature variable\n\n\nCode\n#fiting an ARIMA model to the Inflation variable\ninflation_fit<-auto.arima(normalized_XLI_factor_data_numeric_df$inflation) \nfinflation<-forecast(inflation_fit)\n\n\n# best model fit for forcasting\nxreg <- cbind(Inflation = normalized_XLI_factor_data_ts[, \"inflation\"])\n\nfit <- Arima(normalized_XLI_factor_data_ts[, \"XLI.Adjusted\"],order=c(1,0,0),xreg=xreg)\n\n# Forcast the stock price using feature variables\nfxreg <- cbind(Inflation = finflation$mean)\nfcast <- forecast(fit, xreg=fxreg) \nautoplot(fcast) + xlab(\"Year\") +\n  ylab(\"Normalised XLI.Adjusted\")\n\n\n\n\n\n\n\nARIMA Model for Inflation\n\n\nCode\n#fiting an ARIMA model to the Inflation variable\ninflation_fit<-auto.arima(normalized_XLI_factor_data_numeric_df$inflation) \nfinflation<-forecast(inflation_fit)\nsummary(inflation_fit)\n\n\nSeries: normalized_XLI_factor_data_numeric_df$inflation \nARIMA(0,1,0) \n\nsigma^2 = 0.1378:  log likelihood = -7.28\nAIC=16.55   AICc=16.82   BIC=17.38\n\nTraining set error measures:\n                    ME      RMSE       MAE       MPE     MAPE      MASE\nTraining set 0.1201871 0.3607553 0.2312026 -26.91442 61.15158 0.2676132\n                  ACF1\nTraining set 0.2431721\n\n\n\n\nARIMA Model for Industrial Sector Fund with feature variable\n\n\nCode\n# best model fit for forcasting\nxreg <- cbind(Inflation = normalized_XLI_factor_data_ts[, \"inflation\"])\n\nfit <- Arima(normalized_XLI_factor_data_ts[, \"XLI.Adjusted\"],order=c(1,0,0),xreg=xreg)\nsummary(fit)\n\n\nSeries: normalized_XLI_factor_data_ts[, \"XLI.Adjusted\"] \nRegression with ARIMA(1,0,0) errors \n\nCoefficients:\n         ar1  intercept    xreg\n      0.5764    -0.0999  0.7172\ns.e.  0.2002     0.2879  0.2366\n\nsigma^2 = 0.345:  log likelihood = -14.52\nAIC=37.05   AICc=40.12   BIC=40.61\n\nTraining set error measures:\n                     ME      RMSE       MAE       MPE     MAPE      MASE\nTraining set 0.02177766 0.5361627 0.4398962 -27.78407 100.0628 0.4391875\n                    ACF1\nTraining set -0.02341074\n\n\nThe ARIMAX model may have incorporated exogenous variables such as macroeconomic factors, which could indicate the potential impact of external factors on the forecasted values. The findings suggest that inflation rates significantly impact the Industrial Sector Fund stock price, underscoring the importance of including macroeconomic factors in stock market forecasting. This is further supported by the comparison of RMSE, MAE, AIC and BIC values for forecasts with and without the feature variable. It is found that including the feature variable leads to a lower values, indicating that the feature variable makes a significant impact and should be considered in stock market forecasting."
  },
  {
    "objectID": "arimax_sector_market_macroeconomic.html",
    "href": "arimax_sector_market_macroeconomic.html",
    "title": "ARIMAX/SARIMAX Model for Sector market and macroeconomic factors as exogenous variables",
    "section": "",
    "text": "The ARIMAX/SARIMA model is a powerful tool for analyzing the relationships between sector markets, macroeconomic factors, and other exogenous variables. By incorporating both endogenous and exogenous variables, the ARIMAX/SARIMA model can provide a more accurate and comprehensive analysis of the stock market.\nIn the case of sector markets, the ARIMAX/SARIMAX model can be used to identify important relationships between different sectors and their performance. For example, if the technology sector is performing well, we might expect to see a corresponding increase in the performance of other sectors that rely on technology.\nBy including macroeconomic factors as exogenous variables, the ARIMAX/SARIMAX model can also help to identify how changes in the broader economy might impact the performance of different sectors. For example, if interest rates are expected to rise, we might expect to see a decrease in the performance of sectors that are particularly sensitive to interest rates, such as real estate or financial sectors.\nAccording to the findings, the endogenous and exogenous variables in the time series data are not interdependent, then the ARIMAX model can be a good choice for predicting the stock market indices. If there is seasonality in the data, then the SARIMAX model can be used to account for this seasonal variation. If there is no seasonality, then the simpler ARIMA model can be used instead of SARIMAX.\n\n\n\n\n\n\nLet’s examine the relationship between endogenous and exogenous variables before proceeding with the ARIMAX/SARIMAX model.\n\nPlotNormalized Plot\n\n\n\n\nCode\nts_plot(sector_factor_data,\n        title = \"Sector Market and Macroeconomic Variables\",\n        Ytitle = \"Values\",\n        Xtitle = \"Year\")\n\n\n\n\n\n\n\n\n\n\nCode\nnumeric_vars_sector_factor_data <- c(\"XLB.Adjusted\", \"XLC.Adjusted\", \"XLE.Adjusted\",\"XLF.Adjusted\",\"XLI.Adjusted\",\"XLP.Adjusted\",\"XLK.Adjusted\",\"XLRE.Adjusted\",\"XLU.Adjusted\",\"XLV.Adjusted\",\"XLY.Adjusted\",\"gdp\", \"interest\", \"inflation\", \"unemployment\")\nnumeric_sector_factor_data <- sector_factor_data[, numeric_vars_sector_factor_data]\nnormalized_sector_factor_data_numeric <- scale(numeric_sector_factor_data)\nnormalized_sector_factor_data <- ts(normalized_sector_factor_data_numeric, start = c(2010, 1), end = c(2021,10),frequency = 4)\nts_plot(normalized_sector_factor_data,\n        title = \"Normalized Time Series Data for Sector Market and Macroeconomic Variables\",\n        Ytitle = \"Normalized Values\",\n        Xtitle = \"Year\")\n\n\n\n\n\n\n\n\n\nPlotting the raw time series data for these variables can be challenging due to differences in scale and units of measurement. Therefore, normalizing the data can provide a clearer picture of the relationships and patterns in the data.\nIn the Normalized Time Series Data for Sector Market and Macroeconomic Factors plot, the variables have been scaled using a common scaling technique such as z-scores or percentage changes. This process allows for a fair comparison between variables and eliminates the impact of differing scales or units of measurement.\nNormalizing the data is essential when analyzing the relationship between sector market data and macroeconomic factors. It can help remove any bias or distortion introduced by variables with different units or magnitudes and can stabilize the estimation of models. Additionally, normalizing the data can improve the interpretability of model coefficients, making it easier to assess their relative importance. By normalizing the time series data, it is possible to gain insights into the relationships between sector market data and macroeconomic factors and to make informed investment decisions based on these insights.\n\nCross-Correlation for the Variables and Selection of Feature Variables\nCross-correlation is a statistical technique used to measure the relationship between two or more variables in a time series. In the context of ARIMAX modeling, cross-correlation is often used for feature selection. For selecting feature variables in our analysis, we will first examine the correlation through a heatmap among all the variables, and then analyze the autocorrelation function (ACF) plots between the response variable and the exogenous variables.\n\n\nCorrelation Heatmap\n\n\nCode\n# Get upper triangle of the correlation matrix\nget_upper_tri <- function(cormat){\n    cormat[lower.tri(cormat)]<- NA\n    return(cormat)\n}\ncormat <- round(cor(normalized_sector_factor_data_numeric),2)\n\nupper_tri <- get_upper_tri(cormat)\n\nmelted_cormat <- melt(upper_tri, na.rm = TRUE)\n# Create a ggheatmap\nggheatmap <- ggplot(melted_cormat, aes(Var2, Var1, fill = value))+\n geom_tile(color = \"white\")+\n scale_fill_gradient2(low = \"blue\", high = \"red\", mid = \"white\", \n   midpoint = 0, limit = c(-1,1), space = \"Lab\", \n    name=\"Pearson\\nCorrelation\") +\n  theme_minimal()+ # minimal theme\n theme(axis.text.x = element_text(angle = 45, vjust = 1, \n    size = 12, hjust = 1))+\n coord_fixed()\n\nggheatmap + \ngeom_text(aes(Var2, Var1, label = value), color = \"black\", size = 4) +\ntheme(\n  axis.title.x = element_blank(),\n  axis.title.y = element_blank(),\n  panel.grid.major = element_blank(),\n  panel.border = element_blank(),\n  panel.background = element_blank(),\n  axis.ticks = element_blank(),\n  legend.justification = c(1, 0),\n  legend.position = c(0.6, 0.7),\n  legend.direction = \"horizontal\")+\n  guides(fill = guide_colorbar(barwidth = 7, barheight = 1,\n                title.position = \"top\", title.hjust = 0.5))\n\n\n\n\n\nThe heatmap reveal important insights into the relationships between the sector market and various economic indicators. The strong positive correlations between the sector market and inflation, along with the negative correlations with unemployment rate, and gdp growth rate and sector market have positive correlation suggest that these variables may play a significant role in influencing sector market movements. In contrast, the weaker correlations between the sector market interest rates indicate that these variables may have less impact on sector market fluctuations except for the enegry sector. These findings provide valuable guidance for selecting relevant variables in the VAR model to better understand and forecast stock market dynamics.\nClick to view ARIMAX/SARIMAX Page for Consumer Staples Sector Fund and macroeconomic factors as exogenous variables\nClick to view ARIMAX/SARIMAX Page for Utilities Sector Fund and macroeconomic factors as exogenous variables\nClick to view ARIMAX/SARIMAX Page for Health Care Sector Fund and macroeconomic factors as exogenous variables\nClick to view ARIMAX/SARIMAX Page for Industrial Sector Fund and macroeconomic factors as exogenous variables\nClick to view ARIMAX/SARIMAX Page for Financial Sector Fund and macroeconomic factors as exogenous variables\nClick to view ARIMAX/SARIMAX Page for Consumer Discretionary Sector Fund and macroeconomic factors as exogenous variables\nClick to view ARIMAX/SARIMAX Page for Communication Services Sector Fund and macroeconomic factors as exogenous variables\nClick to view ARIMAX/SARIMAX Page for Real Estate Sector Fund and macroeconomic factors as exogenous variables\nClick to view ARIMAX/SARIMAX Page for Materials Sector Fund and macroeconomic factors as exogenous variables\nClick to view ARIMAX/SARIMAX Page for Technology Sector Fund and macroeconomic factors as exogenous variables\nClick to view ARIMAX/SARIMAX Page for Energy Sector Fund and macroeconomic factors as exogenous variables"
  },
  {
    "objectID": "arimax_sector_materials_sector_macroeconomic.html",
    "href": "arimax_sector_materials_sector_macroeconomic.html",
    "title": "ARIMAX/SARIMAX Page for Materials Sector Fund and macroeconomic factors",
    "section": "",
    "text": "Endogenous variable: Materials Sector Fund Exogenous variables: Macroeconomic indicators such as GDP, inflation rate, unemployment rate and interest rates\nBefore proceeding with model selection, it can be useful to examine the feature variables and select the appropriate ones. This can be done by creating cross-correlation function plots, which can serve as a verification of the correlation heatmap for the variables.\n\nCCF Plot for Materials Sector Fund and Exogenous Variables\n\nGDPInterest RateInflation RateUnemployment Rate\n\n\n\n\nCode\npar(mfrow=c(1,1))\nccf_result <- ccf(normalized_sector_factor_data[, c(\"XLB.Adjusted\")], normalized_sector_factor_data[, c(\"gdp\")], \n    lag.max = 300,\n    main = \"Cros-Correlation Plot for Materials Sector Fund and GDP Growth Rate \",\n    ylab = \"CCF\")\n\n\n\n\n\nCode\ncat(\"The sum of cross correlation function is\", sum(abs(ccf_result$acf)))\n\n\nThe sum of cross correlation function is 4.874086\n\n\n\n\n\n\nCode\npar(mfrow=c(1,1))\nccf_result <- ccf(normalized_sector_factor_data[, c(\"XLB.Adjusted\")], normalized_sector_factor_data[, c(\"interest\")], \n    lag.max = 300,\n    main = \"Cros-Correlation Plot for Materials Sector Fund and Interest Rate\",\n    ylab = \"CCF\")\n\n\n\n\n\nCode\ncat(\"The sum of cross correlation function is\", sum(abs(ccf_result$acf)))\n\n\nThe sum of cross correlation function is 9.572994\n\n\n\n\n\n\nCode\npar(mfrow=c(1,1))\nccf_result <- ccf(normalized_sector_factor_data[, c(\"XLB.Adjusted\")], normalized_sector_factor_data[, c(\"inflation\")], \n    lag.max = 300,\n    main = \"Cros-Correlation Plot for Materials Sector Fund and Inflation Rate\",\n    ylab = \"CCF\")\n\n\n\n\n\nCode\ncat(\"The sum of cross correlation function is\", sum(abs(ccf_result$acf)))\n\n\nThe sum of cross correlation function is 10.64938\n\n\n\n\n\n\nCode\npar(mfrow=c(1,1))\nccf_result <- ccf(normalized_sector_factor_data[, c(\"XLB.Adjusted\")], normalized_sector_factor_data[, c(\"unemployment\")], \n    lag.max = 300,\n    main = \"Cros-Correlation Plot for Materials Sector Fund and Unemployment Rate\",\n    ylab = \"CCF\")\n\n\n\n\n\nCode\ncat(\"The sum of cross correlation function is\", sum(abs(ccf_result$acf)))\n\n\nThe sum of cross correlation function is 6.967941\n\n\n\n\n\nThe cross-correlation feature plots reaffirm the findings from the heatmap analysis, indicating that inflation rate exhibit stronger correlations with the index compared to GDP, interest rate and unemployment rate. The cross-correlation plots for GDP, unemployment and interest rates show weaker and more scattered patterns, with strikes between the blue lines indicating lower correlation coefficients. This suggests that inflation rate are more suitable feature variables for the ARIMAX model when predicting Materials Sector Fund movements.\nFinal Exogenous variables: Macroeconomic indicators: Inflation rate\n\n\nEndogenous and Exogenous Variables Plot\n\n\nCode\nfinal_XLB_factor_data <- sector_factor_data %>%dplyr::select( Date,XLB.Adjusted, inflation)\nnumeric_vars_XLB_factor_data <- c(\"XLB.Adjusted\", \"inflation\")\nnumeric_XLB_factor_data <- final_XLB_factor_data[, numeric_vars_XLB_factor_data]\nnormalized_XLB_factor_data_numeric <- scale(numeric_XLB_factor_data)\nnormalized_XLB_factor_data_numeric_df <- data.frame(normalized_XLB_factor_data_numeric)\nnormalized_XLB_factor_data_ts <- ts(normalized_XLB_factor_data_numeric, start = c(2018, 7), frequency = 4)\n\nautoplot(normalized_XLB_factor_data_ts, facets=TRUE) +\n  xlab(\"Year\") + ylab(\"\") +\n  ggtitle(\"Materials Sector Fund Stock Price, Inflation Rate and Unemployment Rate in USA 2018-2023\")\n\n\n\n\n\n\n\nCheck the stationarity\nBefore proceeding with further analysis, it’s important to check the stationarity of the time series data. If the series is not stationary, it needs to be differentiated before performing a ARIMAX model. The stationarity of a multivariate time series data can be checked using the Phillips-Perron test, which is specifically designed for multivariate data. By performing the Phillips-Perron test on the multivariate time series data, we can determine if the data is stationary or if it requires differencing to make it suitable for ARIMAX modeling.\n\n\nCode\n# Convert your multivariate time series data to a matrix\nfinal_XLB_factor_data_ts_multivariate <- as.matrix(normalized_XLB_factor_data_ts)\n\n# Check for stationarity using Phillips-Perron test\nphillips_perron_test <- ur.pp(final_XLB_factor_data_ts_multivariate)  \nsummary(phillips_perron_test)\n\n\n\n################################## \n# Phillips-Perron Unit Root Test # \n################################## \n\nTest regression with intercept \n\n\nCall:\nlm(formula = y ~ y.l1)\n\nResiduals:\n     Min       1Q   Median       3Q      Max \n-1.25487 -0.20793 -0.06239  0.27586  0.77975 \n\nCoefficients:\n            Estimate Std. Error t value Pr(>|t|)    \n(Intercept)  0.06667    0.08207   0.812    0.422    \ny.l1         0.90923    0.08710  10.439 5.46e-12 ***\n---\nSignif. codes:  0 '***' 0.001 '**' 0.01 '*' 0.05 '.' 0.1 ' ' 1\n\nResidual standard error: 0.4849 on 33 degrees of freedom\nMultiple R-squared:  0.7676,    Adjusted R-squared:  0.7605 \nF-statistic:   109 on 1 and 33 DF,  p-value: 5.459e-12\n\n\nValue of test-statistic, type: Z-alpha  is: -5.164 \n\n         aux. Z statistics\nZ-tau-mu            0.6788\n\n\nThe Phillips-Perron unit root test output suggests that the data is stationary. The test regression includes an intercept term and tests whether the coefficient on the lagged value is significantly different from one. The estimated coefficients indicate that the intercept is not significant (p-value > 0.05), while the coefficient on the lagged value is highly significant (p-value < 5.459e-12), suggesting that the data is stationary.\nAdditionally, the “Z-alpha” value is the test statistic, which is compared to critical values of the standard normal distribution to determine significance. In this case, the test statistic is -5.164, indicating strong evidence against the null hypothesis of a unit root. The “Z-tau-mu” value is an auxiliary test statistic used to correct for autocorrelation in the residuals, and it is close to zero, suggesting that there is no evidence of residual autocorrelation.\n\n\nFitting a ARIMAX model\nFitting an ARIMAX model will be a useful approach to modeling and predicting time series data when external factors are believed to have an impact on the response variable. In this case, we will be using an ARIMAX model to predict the Materials Sector Fund Stock Price, taking into account the effects of inflation on the sector. By including these exogenous variables in our model, we can improve the accuracy of our predictions and gain a better understanding of the underlying dynamics of the time series.\n\nAuto ARIMAResiduals\n\n\n\n\nCode\nxreg <- cbind(Inflation = normalized_XLB_factor_data_ts[, \"inflation\"])\n\nfit <- auto.arima(normalized_XLB_factor_data_ts[, \"XLB.Adjusted\"], xreg = xreg)\nsummary(fit)\n\n\nSeries: normalized_XLB_factor_data_ts[, \"XLB.Adjusted\"] \nRegression with ARIMA(1,0,0) errors \n\nCoefficients:\n         ar1    xreg\n      0.6997  0.7067\ns.e.  0.1671  0.2202\n\nsigma^2 = 0.2083:  log likelihood = -10.7\nAIC=27.4   AICc=29.11   BIC=30.07\n\nTraining set error measures:\n                      ME      RMSE       MAE       MPE     MAPE      MASE\nTraining set -0.02304632 0.4303271 0.3380746 -5.294387 55.01567 0.3985993\n                  ACF1\nTraining set 0.0983079\n\n\n\n\n\n\nCode\ncheckresiduals(fit)\n\n\n\n\n\n\n    Ljung-Box test\n\ndata:  Residuals from Regression with ARIMA(1,0,0) errors\nQ* = 1.3297, df = 3, p-value = 0.7221\n\nModel df: 1.   Total lags used: 4\n\n\n\n\n\nThe auto.arima function reveals that the best-fit model is (1,0,0). Analysis of the residuals plot indicates that the fluctuates a lot and lies between -0.6 to 0.6. The ACF plot of the residuals shows significant lags, indicating that the model is performing well. The qq-plot also suggests normality in the residuals. The p-value greater than 0.05 actually suggests that there is no evidence of significant autocorrelation in the residuals, which means that the model is performing well in terms of capturing the temporal patterns in the data.\n\n\nFitting the model manually\n\nLinear ModelACF of ResidualsACF of ResidualsDifferentiated Residual\n\n\n\n\nCode\nnormalized_XLB_factor_data_numeric_df$XLB.Adjusted<-ts(normalized_XLB_factor_data_numeric_df$XLB.Adjusted,star=decimal_date(as.Date(\"2018-07-01\",format = \"%Y-%m-%d\")),frequency = 4)\nnormalized_XLB_factor_data_numeric_df$inflation<-ts(normalized_XLB_factor_data_numeric_df$inflation,star=decimal_date(as.Date(\"2018-07-01\",format = \"%Y-%m-%d\")),frequency = 4)\n\n\n############# First fit the linear model##########\nfit.reg <- lm(XLB.Adjusted ~ inflation, data=normalized_XLB_factor_data_numeric_df)\nsummary(fit.reg)\n\n\n\nCall:\nlm(formula = XLB.Adjusted ~ inflation, data = normalized_XLB_factor_data_numeric_df)\n\nResiduals:\n    Min      1Q  Median      3Q     Max \n-0.9269 -0.3982 -0.1172  0.4342  1.1631 \n\nCoefficients:\n              Estimate Std. Error t value Pr(>|t|)    \n(Intercept) -6.601e-16  1.481e-01   0.000        1    \ninflation    7.927e-01  1.524e-01   5.202 8.74e-05 ***\n---\nSignif. codes:  0 '***' 0.001 '**' 0.01 '*' 0.05 '.' 0.1 ' ' 1\n\nResidual standard error: 0.6284 on 16 degrees of freedom\nMultiple R-squared:  0.6284,    Adjusted R-squared:  0.6052 \nF-statistic: 27.06 on 1 and 16 DF,  p-value: 8.736e-05\n\n\n\n\n\n\nCode\nres.fit<-ts(residuals(fit.reg),star=decimal_date(as.Date(\"2018-07-01\",format = \"%Y-%m-%d\")),frequency = 4)\n############## Then look at the residuals ############\nacf(res.fit)\n\n\n\n\n\n\n\n\n\nCode\nPacf(res.fit)\n\n\n\n\n\n\n\n\n\nCode\nres.fit %>% diff() %>% ggtsdisplay()\n\n\n\n\n\n\n\n\nThe output displays the results of a linear regression model using two predictor variables, inflation and unemployment, to explain the variation in the response variable, XLB.Adjusted The coefficients of inflation have p-values of less than 0.05, indicating that both variables significantly impact XLB.Adjusted The R-squared value of approximately 63% suggests that the model explains a moderate amount of the variation in the response variable. Examination of the ACF plot of residuals reveals some autocorrelation, and therefore, the series is differentiated to achieve stationarity. By analyzing the differentiated ACF and PACF plots, the parameters if the ARIMAX modle are that p=0, q=0, and d=1.\nAs both the model fitted manually and the auto.arima parameters are the same, i.e. p = 0, q = 0 and d = 1. We proceed with fitting the model for the parameters.\n\n\nFitting Model.\n\nModel Plot 1Model 1Model Plot 2Model 2\n\n\n\n\nCode\nset.seed(1234)\n\nmodel_output <- capture.output(sarima(res.fit, 0,1,0)) \n\n\n\n\n\n\n\n\n\nCode\ncat(model_output[9:38], model_output[length(model_output)], sep = \"\\n\")\n\n\n$fit\n\nCall:\narima(x = xdata, order = c(p, d, q), seasonal = list(order = c(P, D, Q), period = S), \n    xreg = constant, transform.pars = trans, fixed = fixed, optim.control = list(trace = trc, \n        REPORT = 1, reltol = tol))\n\nCoefficients:\n      constant\n       -0.0299\ns.e.    0.1126\n\nsigma^2 estimated as 0.2156:  log likelihood = -11.08,  aic = 26.16\n\n$degrees_of_freedom\n[1] 16\n\n$ttable\n         Estimate     SE t.value p.value\nconstant  -0.0299 0.1126 -0.2654  0.7941\n\n$AIC\n[1] 1.538614\n\n$AICc\n[1] 1.5543\n\n$BIC\n[1] 1.636639\n\n\n\n\n\n\nCode\nset.seed(1234)\n\nmodel_output <- capture.output(sarima(res.fit, 1,0,0)) \n\n\n\n\n\n\n\n\n\nCode\ncat(model_output[22:51], model_output[length(model_output)], sep = \"\\n\")\n\n\n$fit\n\nCall:\narima(x = xdata, order = c(p, d, q), seasonal = list(order = c(P, D, Q), period = S), \n    xreg = xmean, include.mean = FALSE, transform.pars = trans, fixed = fixed, \n    optim.control = list(trace = trc, REPORT = 1, reltol = tol))\n\nCoefficients:\n         ar1    xmean\n      0.7071  -0.1423\ns.e.  0.1686   0.3201\n\nsigma^2 estimated as 0.1844:  log likelihood = -10.67,  aic = 27.34\n\n$degrees_of_freedom\n[1] 16\n\n$ttable\n      Estimate     SE t.value p.value\nar1     0.7071 0.1686  4.1951  0.0007\nxmean  -0.1423 0.3201 -0.4445  0.6626\n\n$AIC\n[1] 1.518822\n\n$AICc\n[1] 1.563266\n\n$BIC\n[1] 1.667217\n\n\n\n\n\nWe fitted both models using both the manual and auto.arima methods and observed that all the models have p-values greater than 0, indicating statistical significance. To determine the best fit model, we performed cross-validation.\n\n\nCross Validation\n\nRMSE PlotRMSE Results\n\n\n\n\nCode\nn=length(res.fit)\nk= 10\n \n \nrmse1 <- matrix(NA, (n-k),4)\nrmse2 <- matrix(NA, (n-k),4)\nrmse3 <- matrix(NA, (n-k),4)\n\nst <- tsp(res.fit)[1]+(k-5)/4 \n\nfor(i in 1:(n-k))\n{\n  xtrain <- window(res.fit, end=st + i/4)\n  xtest <- window(res.fit, start=st + (i+1)/4, end=st + (i+4)/4)\n\n  \n  fit <- Arima(xtrain, order=c(0,1,0),\n                include.drift=TRUE, method=\"ML\")\n  fcast <- forecast(fit, h=4)\n  \n  fit2 <- Arima(xtrain, order=c(1,0,0),\n                include.drift=TRUE, method=\"ML\")\n  fcast2 <- forecast(fit2, h=4)\n\n\n  rmse1[i,1:length(xtest)]   <- sqrt((fcast$mean-xtest)^2)\n  rmse2[i,1:length(xtest)] <- sqrt((fcast2$mean-xtest)^2)\n}\n\nplot(1:4,colMeans(rmse1,na.rm=TRUE), type=\"l\",col=2, xlab=\"horizon\", ylab=\"RMSE\")\nlines(1:4, colMeans(rmse2,na.rm=TRUE), type=\"l\",col=3)\nlegend(\"topleft\",legend=c(\"fit1\",\"fit2\"),col=2:4,lty=1)\n\n\n\n\n\n\n\n\n\nCode\ncat(\"RMSE values for Model 1\\n\", colMeans(rmse1))\n\n\nRMSE values for Model 1\n 0.4816911 0.6352956 0.9074147 1.197127\n\n\nCode\ncat(\"RMSE values for Model 2\\n\", colMeans(rmse2))\n\n\nRMSE values for Model 2\n 0.5090903 0.6158319 0.8876596 1.092305\n\n\n\n\n\nThe RMSE cross-validation plot shows that the RMSE of Model 2 (1,2,0) is lower than that of Model 1 (0,1,0). This suggests that Model 2 (1,2,0) is a better fit for the data when compared other model. ##### Forecast\n::: panel-tabset ### Forecast for Materials Sector Fund with feature variable\n\n\nCode\n#fiting an ARIMA model to the Inflation variable\ninflation_fit<-auto.arima(normalized_XLB_factor_data_numeric_df$inflation) \nfinflation<-forecast(inflation_fit)\n\n\n# best model fit for forcasting\nxreg <- cbind(Inflation = normalized_XLB_factor_data_ts[, \"inflation\"])\n\nfit <- Arima(normalized_XLB_factor_data_ts[, \"XLB.Adjusted\"],order=c(1,0,0),xreg=xreg)\n\n# Forcast the stock price using feature variables\nfxreg <- cbind(Inflation = finflation$mean)\nfcast <- forecast(fit, xreg=fxreg) \nautoplot(fcast) + xlab(\"Year\") +\n  ylab(\"Normalised XLB.Adjusted\")\n\n\n\n\n\n\n\nARIMA Model for Inflation\n\n\nCode\n#fiting an ARIMA model to the Inflation variable\ninflation_fit<-auto.arima(normalized_XLB_factor_data_numeric_df$inflation) \nfinflation<-forecast(inflation_fit)\nsummary(inflation_fit)\n\n\nSeries: normalized_XLB_factor_data_numeric_df$inflation \nARIMA(0,1,0) \n\nsigma^2 = 0.1378:  log likelihood = -7.28\nAIC=16.55   AICc=16.82   BIC=17.38\n\nTraining set error measures:\n                    ME      RMSE       MAE       MPE     MAPE      MASE\nTraining set 0.1201871 0.3607553 0.2312026 -26.91442 61.15158 0.2676132\n                  ACF1\nTraining set 0.2431721\n\n\n\n\nARIMA Model for Materials Sector Fund with feature variable\n\n\nCode\n# best model fit for forcasting\nxreg <- cbind(Inflation = normalized_XLB_factor_data_ts[, \"inflation\"])\n\nfit <- Arima(normalized_XLB_factor_data_ts[, \"XLB.Adjusted\"],order=c(1,0,0),xreg=xreg)\nsummary(fit)\n\n\nSeries: normalized_XLB_factor_data_ts[, \"XLB.Adjusted\"] \nRegression with ARIMA(1,0,0) errors \n\nCoefficients:\n         ar1  intercept    xreg\n      0.7045    -0.1307  0.7154\ns.e.  0.1649     0.3156  0.2210\n\nsigma^2 = 0.2198:  log likelihood = -10.61\nAIC=29.22   AICc=32.29   BIC=32.78\n\nTraining set error measures:\n                     ME      RMSE      MAE       MPE     MAPE      MASE\nTraining set 0.01785613 0.4279991 0.342922 -7.378369 56.62805 0.4043146\n                   ACF1\nTraining set 0.09753204\n\n\nThe ARIMAX model may have incorporated exogenous variables such as macroeconomic factors, which could indicate the potential impact of external factors on the forecasted values. The findings suggest that inflation rates significantly impact the Materials Sector Fund stock price, underscoring the importance of including macroeconomic factors in stock market forecasting. This is further supported by the comparison of RMSE, MAE, AIC and BIC values for forecasts with and without the feature variable. It is found that including the feature variable leads to a lower values, indicating that the feature variable makes a significant impact and should be considered in stock market forecasting."
  },
  {
    "objectID": "arimax_sector_real_estate_macroeconomic.html",
    "href": "arimax_sector_real_estate_macroeconomic.html",
    "title": "ARIMAX/SARIMAX Page for Real Estate Sector Fund and macroeconomic factors",
    "section": "",
    "text": "Endogenous variable: Real Estate Sector Fund Exogenous variables: Macroeconomic indicators such as GDP, inflation rate, unemployment rate and interest rates\nBefore proceeding with model selection, it can be useful to examine the feature variables and select the appropriate ones. This can be done by creating cross-correlation function plots, which can serve as a verification of the correlation heatmap for the variables.\n\nCCF Plot for Real Estate Sector Fund and Exogenous Variables\n\nGDPInterest RateInflation RateUnemployment Rate\n\n\n\n\nCode\npar(mfrow=c(1,1))\nccf_result <- ccf(normalized_sector_factor_data[, c(\"XLRE.Adjusted\")], normalized_sector_factor_data[, c(\"gdp\")], \n    lag.max = 300,\n    main = \"Cros-Correlation Plot for Real Estate Sector Fund and GDP Growth Rate \",\n    ylab = \"CCF\")\n\n\n\n\n\nCode\ncat(\"The sum of cross correlation function is\", sum(abs(ccf_result$acf)))\n\n\nThe sum of cross correlation function is 5.423388\n\n\n\n\n\n\nCode\npar(mfrow=c(1,1))\nccf_result <- ccf(normalized_sector_factor_data[, c(\"XLRE.Adjusted\")], normalized_sector_factor_data[, c(\"interest\")], \n    lag.max = 300,\n    main = \"Cros-Correlation Plot for Real Estate Sector Fund and Interest Rate\",\n    ylab = \"CCF\")\n\n\n\n\n\nCode\ncat(\"The sum of cross correlation function is\", sum(abs(ccf_result$acf)))\n\n\nThe sum of cross correlation function is 8.679655\n\n\n\n\n\n\nCode\npar(mfrow=c(1,1))\nccf_result <- ccf(normalized_sector_factor_data[, c(\"XLRE.Adjusted\")], normalized_sector_factor_data[, c(\"inflation\")], \n    lag.max = 300,\n    main = \"Cros-Correlation Plot for Real Estate Sector Fund and Inflation Rate\",\n    ylab = \"CCF\")\n\n\n\n\n\nCode\ncat(\"The sum of cross correlation function is\", sum(abs(ccf_result$acf)))\n\n\nThe sum of cross correlation function is 9.490159\n\n\n\n\n\n\nCode\npar(mfrow=c(1,1))\nccf_result <- ccf(normalized_sector_factor_data[, c(\"XLRE.Adjusted\")], normalized_sector_factor_data[, c(\"unemployment\")], \n    lag.max = 300,\n    main = \"Cros-Correlation Plot for Real Estate Sector Fund and Unemployment Rate\",\n    ylab = \"CCF\")\n\n\n\n\n\nCode\ncat(\"The sum of cross correlation function is\", sum(abs(ccf_result$acf)))\n\n\nThe sum of cross correlation function is 6.085289\n\n\n\n\n\nThe cross-correlation feature plots reaffirm the findings from the heatmap analysis, indicating that inflation rate exhibit stronger correlations with the index compared to GDP, interest rate and unemployment rate. The cross-correlation plots for GDP, unemployment and interest rates show weaker and more scattered patterns, with strikes between the blue lines indicating lower correlation coefficients. This suggests that inflation rate are more suitable feature variables for the ARIMAX model when predicting Real Estate Sector Fund movements.\nFinal Exogenous variables: Macroeconomic indicators: Inflation rate\n\n\nEndogenous and Exogenous Variables Plot\n\n\nCode\nfinal_XLRE_factor_data <- sector_factor_data %>%dplyr::select( Date,XLRE.Adjusted, inflation)\nnumeric_vars_XLRE_factor_data <- c(\"XLRE.Adjusted\", \"inflation\")\nnumeric_XLRE_factor_data <- final_XLRE_factor_data[, numeric_vars_XLRE_factor_data]\nnormalized_XLRE_factor_data_numeric <- scale(numeric_XLRE_factor_data)\nnormalized_XLRE_factor_data_numeric_df <- data.frame(normalized_XLRE_factor_data_numeric)\nnormalized_XLRE_factor_data_ts <- ts(normalized_XLRE_factor_data_numeric, start = c(2018, 7), frequency = 4)\n\nautoplot(normalized_XLRE_factor_data_ts, facets=TRUE) +\n  xlab(\"Year\") + ylab(\"\") +\n  ggtitle(\"Real Estate Sector Fund Stock Price, Inflation Rate and Unemployment Rate in USA 2018-2023\")\n\n\n\n\n\n\n\nCheck the stationarity\nBefore proceeding with further analysis, it’s important to check the stationarity of the time series data. If the series is not stationary, it needs to be differentiated before performing a ARIMAX model. The stationarity of a multivariate time series data can be checked using the Phillips-Perron test, which is specifically designed for multivariate data. By performing the Phillips-Perron test on the multivariate time series data, we can determine if the data is stationary or if it requires differencing to make it suitable for ARIMAX modeling.\n\n\nCode\n# Convert your multivariate time series data to a matrix\nfinal_XLRE_factor_data_ts_multivariate <- as.matrix(normalized_XLRE_factor_data_ts)\n\n# Check for stationarity using Phillips-Perron test\nphillips_perron_test <- ur.pp(final_XLRE_factor_data_ts_multivariate)  \nsummary(phillips_perron_test)\n\n\n\n################################## \n# Phillips-Perron Unit Root Test # \n################################## \n\nTest regression with intercept \n\n\nCall:\nlm(formula = y ~ y.l1)\n\nResiduals:\n     Min       1Q   Median       3Q      Max \n-1.26257 -0.18719 -0.08398  0.26427  1.00292 \n\nCoefficients:\n            Estimate Std. Error t value Pr(>|t|)    \n(Intercept)  0.07513    0.08539   0.880    0.385    \ny.l1         0.88596    0.09063   9.776 2.85e-11 ***\n---\nSignif. codes:  0 '***' 0.001 '**' 0.01 '*' 0.05 '.' 0.1 ' ' 1\n\nResidual standard error: 0.5045 on 33 degrees of freedom\nMultiple R-squared:  0.7433,    Adjusted R-squared:  0.7355 \nF-statistic: 95.57 on 1 and 33 DF,  p-value: 2.847e-11\n\n\nValue of test-statistic, type: Z-alpha  is: -6.2131 \n\n         aux. Z statistics\nZ-tau-mu            0.7396\n\n\nThe Phillips-Perron unit root test output suggests that the data is stationary. The test regression includes an intercept term and tests whether the coefficient on the lagged value is significantly different from one. The estimated coefficients indicate that the intercept is not significant (p-value > 0.05), while the coefficient on the lagged value is highly significant (p-value < 2.847e-11), suggesting that the data is stationary.\nAdditionally, the “Z-alpha” value is the test statistic, which is compared to critical values of the standard normal distribution to determine significance. In this case, the test statistic is -6.2131, indicating strong evidence against the null hypothesis of a unit root. The “Z-tau-mu” value is an auxiliary test statistic used to correct for autocorrelation in the residuals, and it is close to zero, suggesting that there is no evidence of residual autocorrelation.\n\n\nFitting a ARIMAX model\nFitting an ARIMAX model will be a useful approach to modeling and predicting time series data when external factors are believed to have an impact on the response variable. In this case, we will be using an ARIMAX model to predict the Real Estate Sector Fund Stock Price, taking into account the effects of inflation on the sector. By including these exogenous variables in our model, we can improve the accuracy of our predictions and gain a better understanding of the underlying dynamics of the time series.\n\nAuto ARIMAResiduals\n\n\n\n\nCode\nxreg <- cbind(Inflation = normalized_XLRE_factor_data_ts[, \"inflation\"])\n\nfit <- auto.arima(normalized_XLRE_factor_data_ts[, \"XLRE.Adjusted\"], xreg = xreg)\nsummary(fit)\n\n\nSeries: normalized_XLRE_factor_data_ts[, \"XLRE.Adjusted\"] \nRegression with ARIMA(1,0,0) errors \n\nCoefficients:\n         ar1    xreg\n      0.6998  0.8171\ns.e.  0.2199  0.2459\n\nsigma^2 = 0.2316:  log likelihood = -11.65\nAIC=29.3   AICc=31.02   BIC=31.97\n\nTraining set error measures:\n                      ME     RMSE       MAE       MPE     MAPE      MASE\nTraining set -0.03870243 0.453705 0.3575106 -90.06175 114.9634 0.3657111\n                  ACF1\nTraining set 0.1202663\n\n\n\n\n\n\nCode\ncheckresiduals(fit)\n\n\n\n\n\n\n    Ljung-Box test\n\ndata:  Residuals from Regression with ARIMA(1,0,0) errors\nQ* = 1.4709, df = 3, p-value = 0.689\n\nModel df: 1.   Total lags used: 4\n\n\n\n\n\nThe auto.arima function reveals that the best-fit model is (1,0,0). Analysis of the residuals plot indicates that the fluctuates a lot and lies between -1 to 0.5. The ACF plot of the residuals shows significant lags, indicating that the model is performing well. The qq-plot also suggests normality in the residuals. The p-value greater than 0.05 actually suggests that there is no evidence of significant autocorrelation in the residuals, which means that the model is performing well in terms of capturing the temporal patterns in the data.\n\n\nFitting the model manually\n\nLinear ModelACF of ResidualsACF of ResidualsDifferentiated Residual\n\n\n\n\nCode\nnormalized_XLRE_factor_data_numeric_df$XLRE.Adjusted<-ts(normalized_XLRE_factor_data_numeric_df$XLRE.Adjusted,star=decimal_date(as.Date(\"2018-07-01\",format = \"%Y-%m-%d\")),frequency = 4)\nnormalized_XLRE_factor_data_numeric_df$inflation<-ts(normalized_XLRE_factor_data_numeric_df$inflation,star=decimal_date(as.Date(\"2018-07-01\",format = \"%Y-%m-%d\")),frequency = 4)\n\n\n############# First fit the linear model##########\nfit.reg <- lm(XLRE.Adjusted ~ inflation, data=normalized_XLRE_factor_data_numeric_df)\nsummary(fit.reg)\n\n\n\nCall:\nlm(formula = XLRE.Adjusted ~ inflation, data = normalized_XLRE_factor_data_numeric_df)\n\nResiduals:\n    Min      1Q  Median      3Q     Max \n-1.2846 -0.4851  0.2955  0.4518  0.7921 \n\nCoefficients:\n             Estimate Std. Error t value Pr(>|t|)    \n(Intercept) 3.085e-16  1.429e-01   0.000        1    \ninflation   8.086e-01  1.471e-01   5.498 4.86e-05 ***\n---\nSignif. codes:  0 '***' 0.001 '**' 0.01 '*' 0.05 '.' 0.1 ' ' 1\n\nResidual standard error: 0.6064 on 16 degrees of freedom\nMultiple R-squared:  0.6539,    Adjusted R-squared:  0.6323 \nF-statistic: 30.23 on 1 and 16 DF,  p-value: 4.864e-05\n\n\n\n\n\n\nCode\nres.fit<-ts(residuals(fit.reg),star=decimal_date(as.Date(\"2018-07-01\",format = \"%Y-%m-%d\")),frequency = 4)\n############## Then look at the residuals ############\nacf(res.fit)\n\n\n\n\n\n\n\n\n\nCode\nPacf(res.fit)\n\n\n\n\n\n\n\n\n\nCode\nres.fit %>% diff() %>% ggtsdisplay()\n\n\n\n\n\n\n\n\nThe output displays the results of a linear regression model using two predictor variables, inflation and unemployment, to explain the variation in the response variable, XLRE.Adjusted The coefficients of inflation have p-values of less than 0.05, indicating that both variables significantly impact XLRE.Adjusted The R-squared value of approximately 65% suggests that the model explains a moderate amount of the variation in the response variable. Examination of the ACF plot of residuals reveals some autocorrelation, and therefore, the series is differentiated to achieve stationarity. By analyzing the differentiated ACF and PACF plots, the parameters if the ARIMAX modle are that p=0, q=0, and d=1.\n\n\nFitting Model.\n\nModel Plot 1Model 1Model Plot 2Model 2\n\n\n\n\nCode\nset.seed(1234)\n\nmodel_output <- capture.output(sarima(res.fit, 0,1,0)) \n\n\n\n\n\n\n\n\n\nCode\ncat(model_output[9:38], model_output[length(model_output)], sep = \"\\n\")\n\n\n$fit\n\nCall:\narima(x = xdata, order = c(p, d, q), seasonal = list(order = c(P, D, Q), period = S), \n    xreg = constant, transform.pars = trans, fixed = fixed, optim.control = list(trace = trc, \n        REPORT = 1, reltol = tol))\n\nCoefficients:\n      constant\n       -0.0316\ns.e.    0.1155\n\nsigma^2 estimated as 0.2269:  log likelihood = -11.51,  aic = 27.03\n\n$degrees_of_freedom\n[1] 16\n\n$ttable\n         Estimate     SE t.value p.value\nconstant  -0.0316 0.1155 -0.2735   0.788\n\n$AIC\n[1] 1.589926\n\n$AICc\n[1] 1.605612\n\n$BIC\n[1] 1.687951\n\n\n\n\n\n\nCode\nset.seed(1234)\n\nmodel_output <- capture.output(sarima(res.fit, 1,0,0)) \n\n\n\n\n\n\n\n\n\nCode\ncat(model_output[25:54], model_output[length(model_output)], sep = \"\\n\")\n\n\n$fit\n\nCall:\narima(x = xdata, order = c(p, d, q), seasonal = list(order = c(P, D, Q), period = S), \n    xreg = xmean, include.mean = FALSE, transform.pars = trans, fixed = fixed, \n    optim.control = list(trace = trc, REPORT = 1, reltol = tol))\n\nCoefficients:\n         ar1    xmean\n      0.7152  -0.2216\ns.e.  0.1956   0.3674\n\nsigma^2 estimated as 0.2004:  log likelihood = -11.43,  aic = 28.86\n\n$degrees_of_freedom\n[1] 16\n\n$ttable\n      Estimate     SE t.value p.value\nar1     0.7152 0.1956  3.6568  0.0021\nxmean  -0.2216 0.3674 -0.6032  0.5548\n\n$AIC\n[1] 1.603332\n\n$AICc\n[1] 1.647777\n\n$BIC\n[1] 1.751728\n\n\n\n\n\nWe fitted both models using both the manual and auto.arima methods and observed that all the models have p-values greater than 0, indicating statistical significance. To determine the best fit model, we performed cross-validation.\n\n\nCross Validation\n\nRMSE PlotRMSE Results\n\n\n\n\nCode\nn=length(res.fit)\nk= 10\n \n \nrmse1 <- matrix(NA, (n-k),4)\nrmse2 <- matrix(NA, (n-k),4)\nrmse3 <- matrix(NA, (n-k),4)\n\nst <- tsp(res.fit)[1]+(k-5)/4 \n\nfor(i in 1:(n-k))\n{\n  xtrain <- window(res.fit, end=st + i/4)\n  xtest <- window(res.fit, start=st + (i+1)/4, end=st + (i+4)/4)\n\n  \n  fit <- Arima(xtrain, order=c(0,1,0),\n                include.drift=TRUE, method=\"ML\")\n  fcast <- forecast(fit, h=4)\n  \n  fit2 <- Arima(xtrain, order=c(1,0,0),\n                include.drift=TRUE, method=\"ML\")\n  fcast2 <- forecast(fit2, h=4)\n\n\n  rmse1[i,1:length(xtest)]   <- sqrt((fcast$mean-xtest)^2)\n  rmse2[i,1:length(xtest)] <- sqrt((fcast2$mean-xtest)^2)\n}\n\nplot(1:4,colMeans(rmse1,na.rm=TRUE), type=\"l\",col=2, xlab=\"horizon\", ylab=\"RMSE\")\nlines(1:4, colMeans(rmse2,na.rm=TRUE), type=\"l\",col=3)\nlegend(\"topleft\",legend=c(\"fit1\",\"fit2\"),col=2:4,lty=1)\n\n\n\n\n\n\n\n\n\nCode\ncat(\"RMSE values for Model 1\\n\", colMeans(rmse1))\n\n\nRMSE values for Model 1\n 0.3030661 0.2579258 0.4671162 0.7678888\n\n\nCode\ncat(\"RMSE values for Model 2\\n\", colMeans(rmse2))\n\n\nRMSE values for Model 2\n 0.2511232 0.2178128 0.4407134 0.7708769\n\n\n\n\n\nThe RMSE cross-validation plot shows that the RMSE of Model 2 (1,0,0) is lower than that of Model 1 (0,1,0). This suggests that Model 2 (1,0,0) is a better fit for the data when compared other model. ##### Forecast\n::: panel-tabset ### Forecast for Real Estate Sector Fund with feature variable\n\n\nCode\n#fiting an ARIMA model to the Inflation variable\ninflation_fit<-auto.arima(normalized_XLRE_factor_data_numeric_df$inflation) \nfinflation<-forecast(inflation_fit)\n\n\n# best model fit for forcasting\nxreg <- cbind(Inflation = normalized_XLRE_factor_data_ts[, \"inflation\"])\n\nfit <- Arima(normalized_XLRE_factor_data_ts[, \"XLRE.Adjusted\"],order=c(1,0,0),xreg=xreg)\n\n# Forcast the stock price using feature variables\nfxreg <- cbind(Inflation = finflation$mean)\nfcast <- forecast(fit, xreg=fxreg) \nautoplot(fcast) + xlab(\"Year\") +\n  ylab(\"Normalised XLRE.Adjusted\")\n\n\n\n\n\n\n\nARIMA Model for Inflation\n\n\nCode\n#fiting an ARIMA model to the Inflation variable\ninflation_fit<-auto.arima(normalized_XLRE_factor_data_numeric_df$inflation) \nfinflation<-forecast(inflation_fit)\nsummary(inflation_fit)\n\n\nSeries: normalized_XLRE_factor_data_numeric_df$inflation \nARIMA(0,1,0) \n\nsigma^2 = 0.1378:  log likelihood = -7.28\nAIC=16.55   AICc=16.82   BIC=17.38\n\nTraining set error measures:\n                    ME      RMSE       MAE       MPE     MAPE      MASE\nTraining set 0.1201871 0.3607553 0.2312026 -26.91442 61.15158 0.2676132\n                  ACF1\nTraining set 0.2431721\n\n\n\n\nARIMA Model for Real Estate Sector Fund with feature variable\n\n\nCode\n# best model fit for forcasting\nxreg <- cbind(Inflation = normalized_XLRE_factor_data_ts[, \"inflation\"])\n\nfit <- Arima(normalized_XLRE_factor_data_ts[, \"XLRE.Adjusted\"],order=c(1,0,0),xreg=xreg)\nsummary(fit)\n\n\nSeries: normalized_XLRE_factor_data_ts[, \"XLRE.Adjusted\"] \nRegression with ARIMA(1,0,0) errors \n\nCoefficients:\n         ar1  intercept    xreg\n      0.7267    -0.2368  0.8445\ns.e.  0.2147     0.4018  0.2617\n\nsigma^2 = 0.2397:  log likelihood = -11.42\nAIC=30.84   AICc=33.92   BIC=34.4\n\nTraining set error measures:\n                     ME      RMSE      MAE       MPE     MAPE      MASE\nTraining set 0.02855094 0.4469379 0.363796 -98.75629 121.8159 0.3721406\n                  ACF1\nTraining set 0.1157745\n\n\nThe ARIMAX model may have incorporated exogenous variables such as macroeconomic factors, which could indicate the potential impact of external factors on the forecasted values. The findings suggest that inflation rates significantly impact the Real Estate Sector Fund stock price, underscoring the importance of including macroeconomic factors in stock market forecasting. This is further supported by the comparison of RMSE, MAE, AIC and BIC values for forecasts with and without the feature variable. It is found that including the feature variable leads to a lower values but the MAE value is slightly higher with factor varaibles, indicating that the feature variable makes a significant impact and should be considered in stock market forecasting."
  },
  {
    "objectID": "arimax_sector_technology_macroeconomic.html",
    "href": "arimax_sector_technology_macroeconomic.html",
    "title": "ARIMAX/SARIMAX Page for Technology Sector Fund and macroeconomic factors",
    "section": "",
    "text": "Endogenous variable: Technology Sector Fund Exogenous variables: Macroeconomic indicators such as GDP, inflation rate, unemployment rate and interest rates\nBefore proceeding with model selection, it can be useful to examine the feature variables and select the appropriate ones. This can be done by creating cross-correlation function plots, which can serve as a verification of the correlation heatmap for the variables.\n\nCCF Plot for Technology Sector Fund and Exogenous Variables\n\nGDPInterest RateInflation RateUnemployment Rate\n\n\n\n\nCode\npar(mfrow=c(1,1))\nccf_result <- ccf(normalized_sector_factor_data[, c(\"XLK.Adjusted\")], normalized_sector_factor_data[, c(\"gdp\")], \n    lag.max = 300,\n    main = \"Cros-Correlation Plot for Technology Sector Fund and GDP Growth Rate \",\n    ylab = \"CCF\")\n\n\n\n\n\nCode\ncat(\"The sum of cross correlation function is\", sum(abs(ccf_result$acf)))\n\n\nThe sum of cross correlation function is 5.131569\n\n\n\n\n\n\nCode\npar(mfrow=c(1,1))\nccf_result <- ccf(normalized_sector_factor_data[, c(\"XLK.Adjusted\")], normalized_sector_factor_data[, c(\"interest\")], \n    lag.max = 300,\n    main = \"Cros-Correlation Plot for Technology Sector Fund and Interest Rate\",\n    ylab = \"CCF\")\n\n\n\n\n\nCode\ncat(\"The sum of cross correlation function is\", sum(abs(ccf_result$acf)))\n\n\nThe sum of cross correlation function is 9.940572\n\n\n\n\n\n\nCode\npar(mfrow=c(1,1))\nccf_result <- ccf(normalized_sector_factor_data[, c(\"XLK.Adjusted\")], normalized_sector_factor_data[, c(\"inflation\")], \n    lag.max = 300,\n    main = \"Cros-Correlation Plot for Technology Sector Fund and Inflation Rate\",\n    ylab = \"CCF\")\n\n\n\n\n\nCode\ncat(\"The sum of cross correlation function is\", sum(abs(ccf_result$acf)))\n\n\nThe sum of cross correlation function is 11.25703\n\n\n\n\n\n\nCode\npar(mfrow=c(1,1))\nccf_result <- ccf(normalized_sector_factor_data[, c(\"XLK.Adjusted\")], normalized_sector_factor_data[, c(\"unemployment\")], \n    lag.max = 300,\n    main = \"Cros-Correlation Plot for Technology Sector Fund and Unemployment Rate\",\n    ylab = \"CCF\")\n\n\n\n\n\nCode\ncat(\"The sum of cross correlation function is\", sum(abs(ccf_result$acf)))\n\n\nThe sum of cross correlation function is 6.990579\n\n\n\n\n\nThe cross-correlation feature plots reaffirm the findings from the heatmap analysis, indicating that inflation rate exhibit stronger correlations with the index compared to GDP, interest rate and unemployment rate. The cross-correlation plots for GDP, unemployment and interest rates show weaker and more scattered patterns, with strikes between the blue lines indicating lower correlation coefficients. This suggests that inflation rate are more suitable feature variables for the ARIMAX model when predicting Technology Sector Fund movements.\nFinal Exogenous variables: Macroeconomic indicators: Inflation rate\n\n\nEndogenous and Exogenous Variables Plot\n\n\nCode\nfinal_XLK_factor_data <- sector_factor_data %>%dplyr::select( Date,XLK.Adjusted, inflation)\nnumeric_vars_XLK_factor_data <- c(\"XLK.Adjusted\", \"inflation\")\nnumeric_XLK_factor_data <- final_XLK_factor_data[, numeric_vars_XLK_factor_data]\nnormalized_XLK_factor_data_numeric <- scale(numeric_XLK_factor_data)\nnormalized_XLK_factor_data_numeric_df <- data.frame(normalized_XLK_factor_data_numeric)\nnormalized_XLK_factor_data_ts <- ts(normalized_XLK_factor_data_numeric, start = c(2018, 7), frequency = 4)\n\nautoplot(normalized_XLK_factor_data_ts, facets=TRUE) +\n  xlab(\"Year\") + ylab(\"\") +\n  ggtitle(\"Technology Sector Fund Stock Price, Inflation Rate and Unemployment Rate in USA 2018-2023\")\n\n\n\n\n\n\n\nCheck the stationarity\nBefore proceeding with further analysis, it’s important to check the stationarity of the time series data. If the series is not stationary, it needs to be differentiated before performing a ARIMAX model. The stationarity of a multivariate time series data can be checked using the Phillips-Perron test, which is specifically designed for multivariate data. By performing the Phillips-Perron test on the multivariate time series data, we can determine if the data is stationary or if it requires differencing to make it suitable for ARIMAX modeling.\n\n\nCode\n# Convert your multivariate time series data to a matrix\nfinal_XLK_factor_data_ts_multivariate <- as.matrix(normalized_XLK_factor_data_ts)\n\n# Check for stationarity using Phillips-Perron test\nphillips_perron_test <- ur.pp(final_XLK_factor_data_ts_multivariate)  \nsummary(phillips_perron_test)\n\n\n\n################################## \n# Phillips-Perron Unit Root Test # \n################################## \n\nTest regression with intercept \n\n\nCall:\nlm(formula = y ~ y.l1)\n\nResiduals:\n     Min       1Q   Median       3Q      Max \n-0.90362 -0.17250 -0.03719  0.22961  0.75107 \n\nCoefficients:\n            Estimate Std. Error t value Pr(>|t|)    \n(Intercept)  0.07863    0.07069   1.112    0.274    \ny.l1         0.93102    0.07502  12.410  5.6e-14 ***\n---\nSignif. codes:  0 '***' 0.001 '**' 0.01 '*' 0.05 '.' 0.1 ' ' 1\n\nResidual standard error: 0.4176 on 33 degrees of freedom\nMultiple R-squared:  0.8235,    Adjusted R-squared:  0.8182 \nF-statistic:   154 on 1 and 33 DF,  p-value: 5.604e-14\n\n\nValue of test-statistic, type: Z-alpha  is: -4.5045 \n\n         aux. Z statistics\nZ-tau-mu            0.8813\n\n\nThe Phillips-Perron unit root test output suggests that the data is stationary. The test regression includes an intercept term and tests whether the coefficient on the lagged value is significantly different from one. The estimated coefficients indicate that the intercept is not significant (p-value > 0.05), while the coefficient on the lagged value is highly significant (p-value < 5.604e-14), suggesting that the data is stationary.\nAdditionally, the “Z-alpha” value is the test statistic, which is compared to critical values of the standard normal distribution to determine significance. In this case, the test statistic is -4.5045, indicating strong evidence against the null hypothesis of a unit root. The “Z-tau-mu” value is an auxiliary test statistic used to correct for autocorrelation in the residuals, and it is close to zero, suggesting that there is no evidence of residual autocorrelation.\n\n\nFitting a ARIMAX model\nFitting an ARIMAX model will be a useful approach to modeling and predicting time series data when external factors are believed to have an impact on the response variable. In this case, we will be using an ARIMAX model to predict the Technology Sector Fund Stock Price, taking into account the effects of inflation on the sector. By including these exogenous variables in our model, we can improve the accuracy of our predictions and gain a better understanding of the underlying dynamics of the time series.\n\nAuto ARIMAResiduals\n\n\n\n\nCode\nxreg <- cbind(Inflation = normalized_XLK_factor_data_ts[, \"inflation\"])\n\nfit <- auto.arima(normalized_XLK_factor_data_ts[, \"XLK.Adjusted\"], xreg = xreg)\nsummary(fit)\n\n\nSeries: normalized_XLK_factor_data_ts[, \"XLK.Adjusted\"] \nRegression with ARIMA(0,2,1) errors \n\nCoefficients:\n          ma1    xreg\n      -0.6612  0.6823\ns.e.   0.1973  0.2670\n\nsigma^2 = 0.1721:  log likelihood = -7.85\nAIC=21.69   AICc=23.69   BIC=24.01\n\nTraining set error measures:\n                     ME      RMSE       MAE       MPE     MAPE      MASE\nTraining set -0.0773098 0.3658797 0.2700827 -35.34724 52.98199 0.3336819\n                   ACF1\nTraining set -0.1131779\n\n\n\n\n\n\nCode\ncheckresiduals(fit)\n\n\n\n\n\n\n    Ljung-Box test\n\ndata:  Residuals from Regression with ARIMA(0,2,1) errors\nQ* = 0.93391, df = 3, p-value = 0.8172\n\nModel df: 1.   Total lags used: 4\n\n\n\n\n\nThe auto.arima function reveals that the best-fit model is (0,2,1). Analysis of the residuals plot indicates that the fluctuates a lot and lies between -0.6 to 0.6. The ACF plot of the residuals shows significant lags, indicating that the model is performing well. The qq-plot also suggests normality in the residuals. The p-value greater than 0.05 actually suggests that there is no evidence of significant autocorrelation in the residuals, which means that the model is performing well in terms of capturing the temporal patterns in the data.\n\n\nFitting the model manually\n\nLinear ModelACF of ResidualsACF of ResidualsDifferentiated Residual\n\n\n\n\nCode\nnormalized_XLK_factor_data_numeric_df$XLK.Adjusted<-ts(normalized_XLK_factor_data_numeric_df$XLK.Adjusted,star=decimal_date(as.Date(\"2018-07-01\",format = \"%Y-%m-%d\")),frequency = 4)\nnormalized_XLK_factor_data_numeric_df$inflation<-ts(normalized_XLK_factor_data_numeric_df$inflation,star=decimal_date(as.Date(\"2018-07-01\",format = \"%Y-%m-%d\")),frequency = 4)\n\n\n############# First fit the linear model##########\nfit.reg <- lm(XLK.Adjusted ~ inflation, data=normalized_XLK_factor_data_numeric_df)\nsummary(fit.reg)\n\n\n\nCall:\nlm(formula = XLK.Adjusted ~ inflation, data = normalized_XLK_factor_data_numeric_df)\n\nResiduals:\n    Min      1Q  Median      3Q     Max \n-0.9576 -0.5535 -0.1653  0.6354  1.2228 \n\nCoefficients:\n             Estimate Std. Error t value Pr(>|t|)    \n(Intercept) 4.627e-16  1.694e-01   0.000 1.000000    \ninflation   7.168e-01  1.743e-01   4.112 0.000816 ***\n---\nSignif. codes:  0 '***' 0.001 '**' 0.01 '*' 0.05 '.' 0.1 ' ' 1\n\nResidual standard error: 0.7187 on 16 degrees of freedom\nMultiple R-squared:  0.5138,    Adjusted R-squared:  0.4834 \nF-statistic: 16.91 on 1 and 16 DF,  p-value: 0.0008158\n\n\n\n\n\n\nCode\nres.fit<-ts(residuals(fit.reg),star=decimal_date(as.Date(\"2018-07-01\",format = \"%Y-%m-%d\")),frequency = 4)\n############## Then look at the residuals ############\nacf(res.fit)\n\n\n\n\n\n\n\n\n\nCode\nPacf(res.fit)\n\n\n\n\n\n\n\n\n\nCode\nres.fit %>% diff() %>% ggtsdisplay()\n\n\n\n\n\n\n\n\nThe output displays the results of a linear regression model using two predictor variables, inflation and unemployment, to explain the variation in the response variable, XLK.Adjusted The coefficients of inflation have p-values of less than 0.05, indicating that both variables significantly impact XLK.Adjusted The R-squared value of approximately 51% suggests that the model explains a moderate amount of the variation in the response variable. Examination of the ACF plot of residuals reveals some autocorrelation, and therefore, the series is differentiated to achieve stationarity. By analyzing the differentiated ACF and PACF plots, the parameters if the ARIMAX modle are that p=0, q=0, and d=1.\n\n\nFitting Model.\n\nModel Plot 1Model 1Model Plot 2Model 2\n\n\n\n\nCode\nset.seed(1234)\n\nmodel_output <- capture.output(sarima(res.fit, 0,1,0)) \n\n\n\n\n\n\n\n\n\nCode\ncat(model_output[9:38], model_output[length(model_output)], sep = \"\\n\")\n\n\n$fit\n\nCall:\narima(x = xdata, order = c(p, d, q), seasonal = list(order = c(P, D, Q), period = S), \n    xreg = constant, transform.pars = trans, fixed = fixed, optim.control = list(trace = trc, \n        REPORT = 1, reltol = tol))\n\nCoefficients:\n      constant\n        0.0005\ns.e.    0.0921\n\nsigma^2 estimated as 0.1441:  log likelihood = -7.66,  aic = 19.31\n\n$degrees_of_freedom\n[1] 16\n\n$ttable\n         Estimate     SE t.value p.value\nconstant    5e-04 0.0921  0.0049  0.9962\n\n$AIC\n[1] 1.13613\n\n$AICc\n[1] 1.151816\n\n$BIC\n[1] 1.234155\n\n\n\n\n\n\nCode\nset.seed(1234)\n\nmodel_output <- capture.output(sarima(res.fit, 0,2,1)) \n\n\n\n\n\n\n\n\n\nCode\ncat(model_output[21:49], model_output[length(model_output)], sep = \"\\n\")\n\n\n$fit\n\nCall:\narima(x = xdata, order = c(p, d, q), seasonal = list(order = c(P, D, Q), period = S), \n    include.mean = !no.constant, transform.pars = trans, fixed = fixed, optim.control = list(trace = trc, \n        REPORT = 1, reltol = tol))\n\nCoefficients:\n          ma1\n      -0.6563\ns.e.   0.1932\n\nsigma^2 estimated as 0.1509:  log likelihood = -7.85,  aic = 19.71\n\n$degrees_of_freedom\n[1] 15\n\n$ttable\n    Estimate     SE t.value p.value\nma1  -0.6563 0.1932 -3.3973   0.004\n\n$AIC\n[1] 1.231717\n\n$AICc\n[1] 1.249574\n\n$BIC\n[1] 1.32829\n\n\n\n\n\nWe fitted both models using both the manual and auto.arima methods and observed that all the models have p-values greater than 0, indicating statistical significance. To determine the best fit model, we performed cross-validation.\n\n\nCross Validation\n\nRMSE PlotRMSE Results\n\n\n\n\nCode\nn=length(res.fit)\nk= 10\n \n \nrmse1 <- matrix(NA, (n-k),4)\nrmse2 <- matrix(NA, (n-k),4)\nrmse3 <- matrix(NA, (n-k),4)\n\nst <- tsp(res.fit)[1]+(k-5)/4 \n\nfor(i in 1:(n-k))\n{\n  xtrain <- window(res.fit, end=st + i/4)\n  xtest <- window(res.fit, start=st + (i+1)/4, end=st + (i+4)/4)\n\n  \n  fit <- Arima(xtrain, order=c(0,1,0),\n                include.drift=TRUE, method=\"ML\")\n  fcast <- forecast(fit, h=4)\n  \n  fit2 <- Arima(xtrain, order=c(0,2,1),\n                include.drift=TRUE, method=\"ML\")\n  fcast2 <- forecast(fit2, h=4)\n\n\n  rmse1[i,1:length(xtest)]   <- sqrt((fcast$mean-xtest)^2)\n  rmse2[i,1:length(xtest)] <- sqrt((fcast2$mean-xtest)^2)\n}\n\nplot(1:4,colMeans(rmse1,na.rm=TRUE), type=\"l\",col=2, xlab=\"horizon\", ylab=\"RMSE\")\nlines(1:4, colMeans(rmse2,na.rm=TRUE), type=\"l\",col=3)\nlegend(\"topleft\",legend=c(\"fit1\",\"fit2\"),col=2:4,lty=1)\n\n\n\n\n\n\n\n\n\nCode\ncat(\"RMSE values for Model 1\\n\", colMeans(rmse1))\n\n\nRMSE values for Model 1\n 0.2847241 0.5452483 0.8481385 1.136754\n\n\nCode\ncat(\"RMSE values for Model 2\\n\", colMeans(rmse2))\n\n\nRMSE values for Model 2\n 0.2893202 0.5651061 0.8779249 1.176469\n\n\n\n\n\nThe RMSE cross-validation plot shows that the RMSE of Model 1 (0,1,0) is lower than that of Model 1 (0,2,1). This suggests that Model 1 (0,1,0) is a better fit for the data when compared other model. ##### Forecast\n::: panel-tabset ### Forecast for Technology Sector Fund with feature variable\n\n\nCode\n#fiting an ARIMA model to the Inflation variable\ninflation_fit<-auto.arima(normalized_XLK_factor_data_numeric_df$inflation) \nfinflation<-forecast(inflation_fit)\n\n\n# best model fit for forcasting\nxreg <- cbind(Inflation = normalized_XLK_factor_data_ts[, \"inflation\"])\n\nfit <- Arima(normalized_XLK_factor_data_ts[, \"XLK.Adjusted\"],order=c(0,1,0),xreg=xreg)\n\n# Forcast the stock price using feature variables\nfxreg <- cbind(Inflation = finflation$mean)\nfcast <- forecast(fit, xreg=fxreg) \nautoplot(fcast) + xlab(\"Year\") +\n  ylab(\"Normalised XLK.Adjusted\")\n\n\n\n\n\n\n\nARIMA Model for Inflation\n\n\nCode\n#fiting an ARIMA model to the Inflation variable\ninflation_fit<-auto.arima(normalized_XLK_factor_data_numeric_df$inflation) \nfinflation<-forecast(inflation_fit)\nsummary(inflation_fit)\n\n\nSeries: normalized_XLK_factor_data_numeric_df$inflation \nARIMA(0,1,0) \n\nsigma^2 = 0.1378:  log likelihood = -7.28\nAIC=16.55   AICc=16.82   BIC=17.38\n\nTraining set error measures:\n                    ME      RMSE       MAE       MPE     MAPE      MASE\nTraining set 0.1201871 0.3607553 0.2312026 -26.91442 61.15158 0.2676132\n                  ACF1\nTraining set 0.2431721\n\n\n\n\nARIMA Model for Technology Sector Fund with feature variable\n\n\nCode\n# best model fit for forcasting\nxreg <- cbind(Inflation = normalized_XLK_factor_data_ts[, \"inflation\"])\n\nfit <- Arima(normalized_XLK_factor_data_ts[, \"XLK.Adjusted\"],order=c(0,1,0),xreg=xreg)\nsummary(fit)\n\n\nSeries: normalized_XLK_factor_data_ts[, \"XLK.Adjusted\"] \nRegression with ARIMA(0,1,0) errors \n\nCoefficients:\n        xreg\n      0.5427\ns.e.  0.2444\n\nsigma^2 = 0.1487:  log likelihood = -7.41\nAIC=18.81   AICc=19.67   BIC=20.48\n\nTraining set error measures:\n                     ME      RMSE       MAE       MPE     MAPE      MASE\nTraining set 0.02130958 0.3635602 0.2907369 -33.27718 67.01639 0.3591998\n                  ACF1\nTraining set 0.1503141\n\n\nThe ARIMAX model may have incorporated exogenous variables such as macroeconomic factors, which could indicate the potential impact of external factors on the forecasted values. The findings suggest that inflation rates significantly impact the Technology Sector Fund stock price, underscoring the importance of including macroeconomic factors in stock market forecasting. This is further supported by the comparison of RMSE, MAE, AIC and BIC values for forecasts with and without the feature variable. It is found that including the feature variable leads to a lower values, indicating that the feature variable makes a significant impact and should be considered in stock market forecasting."
  },
  {
    "objectID": "arimax_sector_utilites_macroeconomic.html",
    "href": "arimax_sector_utilites_macroeconomic.html",
    "title": "ARIMAX/SARIMAX Page for Utilities Sector Fund and macroeconomic factors",
    "section": "",
    "text": "Endogenous variable: Utilities Sector Fund Exogenous variables: Macroeconomic indicators such as GDP, inflation rate, unemployment rate and interest rates\nBefore proceeding with model selection, it can be useful to examine the feature variables and select the appropriate ones. This can be done by creating cross-correlation function plots, which can serve as a verification of the correlation heatmap for the variables.\n\nCCF Plot for Utilities Sector Fund and Exogenous Variables\n\nGDPInterest RateInflation RateUnemployment Rate\n\n\n\n\nCode\npar(mfrow=c(1,1))\nccf_result <- ccf(normalized_sector_factor_data[, c(\"XLU.Adjusted\")], normalized_sector_factor_data[, c(\"gdp\")], \n    lag.max = 300,\n    main = \"Cros-Correlation Plot for Utilities Sector Fund and GDP Growth Rate \",\n    ylab = \"CCF\")\n\n\n\n\n\nCode\ncat(\"The sum of cross correlation function is\", sum(abs(ccf_result$acf)))\n\n\nThe sum of cross correlation function is 5.255006\n\n\n\n\n\n\nCode\npar(mfrow=c(1,1))\nccf_result <- ccf(normalized_sector_factor_data[, c(\"XLU.Adjusted\")], normalized_sector_factor_data[, c(\"interest\")], \n    lag.max = 300,\n    main = \"Cros-Correlation Plot for Utilities Sector Fund and Interest Rate\",\n    ylab = \"CCF\")\n\n\n\n\n\nCode\ncat(\"The sum of cross correlation function is\", sum(abs(ccf_result$acf)))\n\n\nThe sum of cross correlation function is 8.616293\n\n\n\n\n\n\nCode\npar(mfrow=c(1,1))\nccf_result <- ccf(normalized_sector_factor_data[, c(\"XLU.Adjusted\")], normalized_sector_factor_data[, c(\"inflation\")], \n    lag.max = 300,\n    main = \"Cros-Correlation Plot for Utilities Sector Fund and Inflation Rate\",\n    ylab = \"CCF\")\n\n\n\n\n\nCode\ncat(\"The sum of cross correlation function is\", sum(abs(ccf_result$acf)))\n\n\nThe sum of cross correlation function is 10.01093\n\n\n\n\n\n\nCode\npar(mfrow=c(1,1))\nccf_result <- ccf(normalized_sector_factor_data[, c(\"XLU.Adjusted\")], normalized_sector_factor_data[, c(\"unemployment\")], \n    lag.max = 300,\n    main = \"Cros-Correlation Plot for Utilities Sector Fund and Unemployment Rate\",\n    ylab = \"CCF\")\n\n\n\n\n\nCode\ncat(\"The sum of cross correlation function is\", sum(abs(ccf_result$acf)))\n\n\nThe sum of cross correlation function is 5.849381\n\n\n\n\n\nThe cross-correlation feature plots reaffirm the findings from the heatmap analysis, indicating that inflation rate exhibit stronger correlations with the index compared to GDP, interest rate and unemployment rate. The cross-correlation plots for GDP, unemployment and interest rates show weaker and more scattered patterns, with strikes between the blue lines indicating lower correlation coefficients. This suggests that inflation rate are more suitable feature variables for the ARIMAX model when predicting Utilities Sector Fund movements.\nFinal Exogenous variables: Macroeconomic indicators: Inflation rate\n\n\nEndogenous and Exogenous Variables Plot\n\n\nCode\nfinal_XLU_factor_data <- sector_factor_data %>%dplyr::select( Date,XLU.Adjusted, inflation)\nnumeric_vars_XLU_factor_data <- c(\"XLU.Adjusted\", \"inflation\")\nnumeric_XLU_factor_data <- final_XLU_factor_data[, numeric_vars_XLU_factor_data]\nnormalized_XLU_factor_data_numeric <- scale(numeric_XLU_factor_data)\nnormalized_XLU_factor_data_numeric_df <- data.frame(normalized_XLU_factor_data_numeric)\nnormalized_XLU_factor_data_ts <- ts(normalized_XLU_factor_data_numeric, start = c(2018, 7), frequency = 4)\n\nautoplot(normalized_XLU_factor_data_ts, facets=TRUE) +\n  xlab(\"Year\") + ylab(\"\") +\n  ggtitle(\"Utilities Sector Fund Stock Price, Inflation Rate and Unemployment Rate in USA 2018-2023\")\n\n\n\n\n\n\n\nCheck the stationarity\nBefore proceeding with further analysis, it’s important to check the stationarity of the time series data. If the series is not stationary, it needs to be differentiated before performing a ARIMAX model. The stationarity of a multivariate time series data can be checked using the Phillips-Perron test, which is specifically designed for multivariate data. By performing the Phillips-Perron test on the multivariate time series data, we can determine if the data is stationary or if it requires differencing to make it suitable for ARIMAX modeling.\n\n\nCode\n# Convert your multivariate time series data to a matrix\nfinal_XLU_factor_data_ts_multivariate <- as.matrix(normalized_XLU_factor_data_ts)\n\n# Check for stationarity using Phillips-Perron test\nphillips_perron_test <- ur.pp(final_XLU_factor_data_ts_multivariate)  \nsummary(phillips_perron_test)\n\n\n\n################################## \n# Phillips-Perron Unit Root Test # \n################################## \n\nTest regression with intercept \n\n\nCall:\nlm(formula = y ~ y.l1)\n\nResiduals:\n     Min       1Q   Median       3Q      Max \n-1.47539 -0.19741 -0.04694  0.29952  0.80845 \n\nCoefficients:\n            Estimate Std. Error t value Pr(>|t|)    \n(Intercept)  0.08310    0.08383   0.991    0.329    \ny.l1         0.87760    0.08897   9.864 2.28e-11 ***\n---\nSignif. codes:  0 '***' 0.001 '**' 0.01 '*' 0.05 '.' 0.1 ' ' 1\n\nResidual standard error: 0.4953 on 33 degrees of freedom\nMultiple R-squared:  0.7467,    Adjusted R-squared:  0.7391 \nF-statistic: 97.29 on 1 and 33 DF,  p-value: 2.28e-11\n\n\nValue of test-statistic, type: Z-alpha  is: -6.1735 \n\n         aux. Z statistics\nZ-tau-mu            0.8524\n\n\nThe Phillips-Perron unit root test output suggests that the data is stationary. The test regression includes an intercept term and tests whether the coefficient on the lagged value is significantly different from one. The estimated coefficients indicate that the intercept is not significant (p-value > 0.05), while the coefficient on the lagged value is highly significant (p-value < 2.28e-11), suggesting that the data is stationary.\nAdditionally, the “Z-alpha” value is the test statistic, which is compared to critical values of the standard normal distribution to determine significance. In this case, the test statistic is -6.1735, indicating strong evidence against the null hypothesis of a unit root. The “Z-tau-mu” value is an auxiliary test statistic used to correct for autocorrelation in the residuals, and it is close to zero, suggesting that there is no evidence of residual autocorrelation.\n\n\nFitting a ARIMAX model\nFitting an ARIMAX model will be a useful approach to modeling and predicting time series data when external factors are believed to have an impact on the response variable. In this case, we will be using an ARIMAX model to predict the Utilities Sector Fund Stock Price, taking into account the effects of inflation on the sector. By including these exogenous variables in our model, we can improve the accuracy of our predictions and gain a better understanding of the underlying dynamics of the time series.\n\nAuto ARIMAResiduals\n\n\n\n\nCode\nxreg <- cbind(Inflation = normalized_XLU_factor_data_ts[, \"inflation\"])\n\nfit <- auto.arima(normalized_XLU_factor_data_ts[, \"XLU.Adjusted\"], xreg = xreg)\nsummary(fit)\n\n\nSeries: normalized_XLU_factor_data_ts[, \"XLU.Adjusted\"] \nRegression with ARIMA(0,1,0) errors \n\nCoefficients:\n        xreg\n      0.7131\ns.e.  0.2737\n\nsigma^2 = 0.1865:  log likelihood = -9.33\nAIC=22.66   AICc=23.52   BIC=24.33\n\nTraining set error measures:\n                     ME      RMSE       MAE       MPE     MAPE      MASE\nTraining set 0.05185477 0.4071088 0.3328825 -716.5274 805.8123 0.3886691\n                   ACF1\nTraining set 0.04186762\n\n\n\n\n\n\nCode\ncheckresiduals(fit)\n\n\n\n\n\n\n    Ljung-Box test\n\ndata:  Residuals from Regression with ARIMA(0,1,0) errors\nQ* = 4.4695, df = 4, p-value = 0.3462\n\nModel df: 0.   Total lags used: 4\n\n\n\n\n\nThe auto.arima function reveals that the best-fit model is (0,1,0). Analysis of the residuals plot indicates that the fluctuates a lot and lies between -0.8 to 0.6. The ACF plot of the residuals shows significant lags, indicating that the model is performing well. The qq-plot also suggests normality in the residuals. The p-value greater than 0.05 actually suggests that there is no evidence of significant autocorrelation in the residuals, which means that the model is performing well in terms of capturing the temporal patterns in the data.\n\n\nFitting the model manually\n\nLinear ModelACF of ResidualsACF of ResidualsDifferentiated Residual\n\n\n\n\nCode\nnormalized_XLU_factor_data_numeric_df$XLU.Adjusted<-ts(normalized_XLU_factor_data_numeric_df$XLU.Adjusted,star=decimal_date(as.Date(\"2018-07-01\",format = \"%Y-%m-%d\")),frequency = 4)\nnormalized_XLU_factor_data_numeric_df$inflation<-ts(normalized_XLU_factor_data_numeric_df$inflation,star=decimal_date(as.Date(\"2018-07-01\",format = \"%Y-%m-%d\")),frequency = 4)\n\n\n############# First fit the linear model##########\nfit.reg <- lm(XLU.Adjusted ~ inflation, data=normalized_XLU_factor_data_numeric_df)\nsummary(fit.reg)\n\n\n\nCall:\nlm(formula = XLU.Adjusted ~ inflation, data = normalized_XLU_factor_data_numeric_df)\n\nResiduals:\n     Min       1Q   Median       3Q      Max \n-1.02519 -0.25326 -0.01658  0.46674  0.84797 \n\nCoefficients:\n              Estimate Std. Error t value Pr(>|t|)    \n(Intercept) -4.079e-16  1.306e-01   0.000        1    \ninflation    8.433e-01  1.344e-01   6.275 1.11e-05 ***\n---\nSignif. codes:  0 '***' 0.001 '**' 0.01 '*' 0.05 '.' 0.1 ' ' 1\n\nResidual standard error: 0.554 on 16 degrees of freedom\nMultiple R-squared:  0.7111,    Adjusted R-squared:  0.693 \nF-statistic: 39.38 on 1 and 16 DF,  p-value: 1.106e-05\n\n\n\n\n\n\nCode\nres.fit<-ts(residuals(fit.reg),star=decimal_date(as.Date(\"2018-01-01\",format = \"%Y-%m-%d\")),frequency = 4)\n############## Then look at the residuals ############\nacf(res.fit)\n\n\n\n\n\n\n\n\n\nCode\nPacf(res.fit)\n\n\n\n\n\n\n\n\n\nCode\nres.fit %>% diff() %>% ggtsdisplay()\n\n\n\n\n\n\n\n\nThe output displays the results of a linear regression model using two predictor variables, inflation and unemployment, to explain the variation in the response variable, XLU.Adjusted The coefficients of inflation have p-values of less than 0.05, indicating that both variables significantly impact XLU.Adjusted The R-squared value of approximately 71% suggests that the model explains a moderate amount of the variation in the response variable. Examination of the ACF plot of residuals reveals some autocorrelation, and therefore, the series is differentiated to achieve stationarity. By analyzing the differentiated ACF and PACF plots, the parameters if the ARIMAX modle are that p=5, q=0, and d=1.\n\n\nFinding the model parameters.\n\nARIMAX ResultModel PlotModel\n\n\n\n\nCode\nARIMA.c=function(p1,p2,q1,data){\ntemp=c()\nd=1\ni=1\ntemp= data.frame()\nls=matrix(rep(NA,6*30),nrow=30)\n\n\nfor (p in p1:p2)#\n{\n  for(q in q1)#\n  {\n    for(d in 0:1)\n    {\n      \n      if(p+d+q<=6)\n      {\n        \n        model<- Arima(data,order=c(p,d,q))\n        ls[i,]= c(p,d,q,model$aic,model$bic,model$aicc)\n        i=i+1\n        #print(i)\n        \n      }\n      \n    }\n  }\n}\n\n\ntemp= as.data.frame(ls)\nnames(temp)= c(\"p\",\"d\",\"q\",\"AIC\",\"BIC\",\"AICc\")\n\ntemp\n}\n\noutput <- ARIMA.c(0,5,0,data=residuals(fit.reg))\n\noutput[which.min(output$AIC),] \n\n\n  p d q      AIC      BIC     AICc\n2 0 1 0 20.88538 21.71859 21.15204\n\n\nCode\noutput[which.min(output$BIC),]\n\n\n  p d q      AIC      BIC     AICc\n2 0 1 0 20.88538 21.71859 21.15204\n\n\nCode\noutput[which.min(output$AICc),]\n\n\n  p d q      AIC      BIC     AICc\n2 0 1 0 20.88538 21.71859 21.15204\n\n\n\n\n\n\nCode\nset.seed(1234)\n\nmodel_output <- capture.output(sarima(res.fit, 0,1,0)) \n\n\n\n\n\n\n\n\n\nCode\ncat(model_output[9:38], model_output[length(model_output)], sep = \"\\n\")\n\n\n$fit\n\nCall:\narima(x = xdata, order = c(p, d, q), seasonal = list(order = c(P, D, Q), period = S), \n    xreg = constant, transform.pars = trans, fixed = fixed, optim.control = list(trace = trc, \n        REPORT = 1, reltol = tol))\n\nCoefficients:\n      constant\n        0.0384\ns.e.    0.1019\n\nsigma^2 estimated as 0.1763:  log likelihood = -9.37,  aic = 22.74\n\n$degrees_of_freedom\n[1] 16\n\n$ttable\n         Estimate     SE t.value p.value\nconstant   0.0384 0.1019   0.377  0.7111\n\n$AIC\n[1] 1.337872\n\n$AICc\n[1] 1.353558\n\n$BIC\n[1] 1.435897\n\n\n\n\n\nFollowing the manual fitting process, we identified that the ARIMAX model with the minimum AIC,BIC and AICc has the parameters (0,1,0), which is the same as the auto.arima parameters.\n\n\nForecast\n::: panel-tabset ### Forecast for Utilities Sector Fund with feature variable\n\n\nCode\n#fiting an ARIMA model to the Inflation variable\ninflation_fit<-auto.arima(normalized_XLU_factor_data_numeric_df$inflation) \nfinflation<-forecast(inflation_fit)\n\n\n# best model fit for forcasting\nxreg <- cbind(Inflation = normalized_XLU_factor_data_ts[, \"inflation\"])\n\nfit <- Arima(normalized_XLU_factor_data_ts[, \"XLU.Adjusted\"],order=c(0,1,0),xreg=xreg)\n\n# Forcast the stock price using feature variables\nfxreg <- cbind(Inflation = finflation$mean)\nfcast <- forecast(fit, xreg=fxreg) \nautoplot(fcast) + xlab(\"Year\") +\n  ylab(\"Normalised XLU.Adjusted\")\n\n\n\n\n\n\n\nARIMA Model for Inflation\n\n\nCode\n#fiting an ARIMA model to the Inflation variable\ninflation_fit<-auto.arima(normalized_XLU_factor_data_numeric_df$inflation) \nfinflation<-forecast(inflation_fit)\nsummary(inflation_fit)\n\n\nSeries: normalized_XLU_factor_data_numeric_df$inflation \nARIMA(0,1,0) \n\nsigma^2 = 0.1378:  log likelihood = -7.28\nAIC=16.55   AICc=16.82   BIC=17.38\n\nTraining set error measures:\n                    ME      RMSE       MAE       MPE     MAPE      MASE\nTraining set 0.1201871 0.3607553 0.2312026 -26.91442 61.15158 0.2676132\n                  ACF1\nTraining set 0.2431721\n\n\n\n\nARIMA Model for Utilities Sector Fund with feature variable\n\n\nCode\n# best model fit for forcasting\nxreg <- cbind(Inflation = normalized_XLU_factor_data_ts[, \"inflation\"])\n\nfit <- Arima(normalized_XLU_factor_data_ts[, \"XLU.Adjusted\"],order=c(0,1,0),xreg=xreg)\nsummary(fit)\n\n\nSeries: normalized_XLU_factor_data_ts[, \"XLU.Adjusted\"] \nRegression with ARIMA(0,1,0) errors \n\nCoefficients:\n        xreg\n      0.7131\ns.e.  0.2737\n\nsigma^2 = 0.1865:  log likelihood = -9.33\nAIC=22.66   AICc=23.52   BIC=24.33\n\nTraining set error measures:\n                     ME      RMSE       MAE       MPE     MAPE      MASE\nTraining set 0.05185477 0.4071088 0.3328825 -716.5274 805.8123 0.3886691\n                   ACF1\nTraining set 0.04186762\n\n\nThe ARIMAX model may have incorporated exogenous variables such as macroeconomic factors, which could indicate the potential impact of external factors on the forecasted values. The findings suggest that inflation rates significantly impact the Utilities Sector Fund stock price, underscoring the importance of including macroeconomic factors in stock market forecasting. This is further supported by the comparison of RMSE, MAE, AIC and BIC values for forecasts with and without the feature variable. It is found that including the feature variable leads to a lower values but MAE is slightly high i.e with factor it is 0.33 and without it is 0.28, indicating that the feature variable makes a significant impact and should be considered in stock market forecasting.\n```"
  },
  {
    "objectID": "arimax_sp500_macroeconomic.html",
    "href": "arimax_sp500_macroeconomic.html",
    "title": "ARIMAX/SARIMAX Model for S&P 500 index and macroeconomic factors",
    "section": "",
    "text": "Endogenous variable: S&P 500 Index Exogenous variables: Macroeconomic indicators such as GDP, inflation rate, unemployment rate and interest rates\nBefore proceeding with model selection, it can be useful to examine the feature variables and select the appropriate ones. This can be done by creating cross-correlation function plots, which can serve as a verification of the correlation heatmap for the variables.\n\nCCF Plot for S&P 500 Index and Exogenous Variables\n\nGDPInterest RateInflation RateUnemployment Rate\n\n\n\n\nCode\npar(mfrow=c(1,1))\nccf_result <- ccf(normalized_index_factor_data[, c(\"GSPC.Adjusted\")], normalized_index_factor_data[, c(\"gdp\")], \n    lag.max = 300,\n    main = \"Cros-Correlation Plot for S&P 500 Index and GDP Growth Rate \",\n    ylab = \"CCF\")\n\n\n\n\n\nCode\ncat(\"The sum of cross correlation function is\", sum(abs(ccf_result$acf)))\n\n\nThe sum of cross correlation function is 4.829354\n\n\n\n\n\n\nCode\npar(mfrow=c(1,1))\nccf_result <- ccf(normalized_index_factor_data[, c(\"GSPC.Adjusted\")], normalized_index_factor_data[, c(\"interest\")], \n    lag.max = 300,\n    main = \"Cros-Correlation Plot for S&P 500 Index and Interest Rate\",\n    ylab = \"CCF\")\n\n\n\n\n\nCode\ncat(\"The sum of cross correlation function is\", sum(abs(ccf_result$acf)))\n\n\nThe sum of cross correlation function is 10.66717\n\n\n\n\n\n\nCode\npar(mfrow=c(1,1))\nccf_result <- ccf(normalized_index_factor_data[, c(\"GSPC.Adjusted\")], normalized_index_factor_data[, c(\"inflation\")], \n    lag.max = 300,\n    main = \"Cros-Correlation Plot for S&P 500 Index and Inflation Rate\",\n    ylab = \"CCF\")\n\n\n\n\n\nCode\ncat(\"The sum of cross correlation function is\", sum(abs(ccf_result$acf)))\n\n\nThe sum of cross correlation function is 19.21394\n\n\n\n\n\n\nCode\npar(mfrow=c(1,1))\nccf_result <- ccf(normalized_index_factor_data[, c(\"GSPC.Adjusted\")], normalized_index_factor_data[, c(\"unemployment\")], \n    lag.max = 300,\n    main = \"Cros-Correlation Plot for S&P 500 Index and Unemployment Rate\",\n    ylab = \"CCF\")\n\n\n\n\n\nCode\ncat(\"The sum of cross correlation function is\", sum(abs(ccf_result$acf)))\n\n\nThe sum of cross correlation function is 22.33667\n\n\n\n\n\nThe cross-correlation feature plots reaffirm the findings from the heatmap analysis, indicating that inflation and unemployment rate exhibit stronger correlations with the index compared to GDP and interest rates. The cross-correlation plots for GDP and interest rates show weaker and more scattered patterns, with strikes between the blue lines indicating lower correlation coefficients. This suggests that inflation and unemployment rate are more suitable feature variables for the ARIMAX model when predicting S&P 500 movements.\nFinal Exogenous variables: Macroeconomic indicators: Inflation rate and unemployment rate.\n\n\nEndogenous and Exogenous Variables Plot\n\n\nCode\nfinal_sp500_factor_data <- index_factor_data %>%dplyr::select( Date,GSPC.Adjusted, inflation,unemployment)\nnumeric_vars_sp500_factor_data <- c(\"GSPC.Adjusted\", \"inflation\", \"unemployment\")\nnumeric_sp500_factor_data <- final_sp500_factor_data[, numeric_vars_sp500_factor_data]\nnormalized_sp500_factor_data_numeric <- scale(numeric_sp500_factor_data)\nnormalized_sp500_factor_data_numeric_df <- data.frame(normalized_sp500_factor_data_numeric)\nnormalized_sp500_factor_data_ts <- ts(normalized_sp500_factor_data_numeric, start = c(2010, 1), frequency = 4)\n\nautoplot(normalized_sp500_factor_data_ts, facets=TRUE) +\n  xlab(\"Year\") + ylab(\"\") +\n  ggtitle(\"NASDAQ  Index Stock Price, Inflation Rate and Unemployment Rate in USA 2010-2023\")\n\n\n\n\n\n\n\nCheck the stationarity\nBefore proceeding with further analysis, it’s important to check the stationarity of the time series data. If the series is not stationary, it needs to be differentiated before performing a ARIMAX model. The stationarity of a multivariate time series data can be checked using the Phillips-Perron test, which is specifically designed for multivariate data. By performing the Phillips-Perron test on the multivariate time series data, we can determine if the data is stationary or if it requires differencing to make it suitable for ARIMAX modeling.\n\n\nCode\n# Convert your multivariate time series data to a matrix\nfinal_sp500_factor_data_ts_multivariate <- as.matrix(normalized_sp500_factor_data_ts)\n\n# Check for stationarity using Phillips-Perron test\nphillips_perron_test <- ur.pp(final_sp500_factor_data_ts_multivariate)  \nsummary(phillips_perron_test)\n\n\n\n################################## \n# Phillips-Perron Unit Root Test # \n################################## \n\nTest regression with intercept \n\n\nCall:\nlm(formula = y ~ y.l1)\n\nResiduals:\n    Min      1Q  Median      3Q     Max \n-1.6643 -0.1555 -0.0444  0.0823  4.4931 \n\nCoefficients:\n            Estimate Std. Error t value Pr(>|t|)    \n(Intercept) 0.002211   0.040927   0.054    0.957    \ny.l1        0.857650   0.041342  20.745   <2e-16 ***\n---\nSignif. codes:  0 '***' 0.001 '**' 0.01 '*' 0.05 '.' 0.1 ' ' 1\n\nResidual standard error: 0.5095 on 153 degrees of freedom\nMultiple R-squared:  0.7377,    Adjusted R-squared:  0.736 \nF-statistic: 430.4 on 1 and 153 DF,  p-value: < 2.2e-16\n\n\nValue of test-statistic, type: Z-alpha  is: -23.4208 \n\n         aux. Z statistics\nZ-tau-mu             0.054\n\n\nThe Phillips-Perron unit root test output suggests that the data is stationary. The test regression includes an intercept term and tests whether the coefficient on the lagged value is significantly different from one. The estimated coefficients indicate that the intercept is not significant (p-value > 0.05), while the coefficient on the lagged value is highly significant (p-value < 2.2e-16), suggesting that the data is stationary.\nAdditionally, the “Z-alpha” value is the test statistic, which is compared to critical values of the standard normal distribution to determine significance. In this case, the test statistic is -23.4208, indicating strong evidence against the null hypothesis of a unit root. The “Z-tau-mu” value is an auxiliary test statistic used to correct for autocorrelation in the residuals, and it is close to zero, suggesting that there is no evidence of residual autocorrelation.\n\n\nFitting a ARIMAX model\nFitting an ARIMAX model will be a useful approach to modeling and predicting time series data when external factors are believed to have an impact on the response variable. In this case, we will be using an ARIMAX model to predict the S&P 500 Index, taking into account the effects of inflation and unemployment on the Index. By including these exogenous variables in our model, we can improve the accuracy of our predictions and gain a better understanding of the underlying dynamics of the time series.\n\nAuto ARIMAResiduals\n\n\n\n\nCode\nxreg <- cbind(Inflation = normalized_sp500_factor_data_ts[, \"inflation\"],\n              Unemployment = normalized_sp500_factor_data_ts[, \"unemployment\"])\n\nfit <- auto.arima(normalized_sp500_factor_data_ts[, \"GSPC.Adjusted\"], xreg = xreg)\nsummary(fit)\n\n\nSeries: normalized_sp500_factor_data_ts[, \"GSPC.Adjusted\"] \nRegression with ARIMA(0,1,0) errors \n\nCoefficients:\n      Inflation  Unemployment\n         0.1629       -0.1812\ns.e.     0.0824        0.0374\n\nsigma^2 = 0.03594:  log likelihood = 13.47\nAIC=-20.94   AICc=-20.42   BIC=-15.14\n\nTraining set error measures:\n                    ME      RMSE       MAE       MPE    MAPE      MASE\nTraining set 0.0292091 0.1840189 0.1345003 -186.6889 215.484 0.3969253\n                   ACF1\nTraining set 0.05410462\n\n\n\n\n\n\nCode\ncheckresiduals(fit)\n\n\n\n\n\n\n    Ljung-Box test\n\ndata:  Residuals from Regression with ARIMA(0,1,0) errors\nQ* = 5.1399, df = 8, p-value = 0.7425\n\nModel df: 0.   Total lags used: 8\n\n\n\n\n\nThe auto.arima function reveals that the best-fit model is (0,1,0). Analysis of the residuals plot indicates that the fluctuates a lot and lies between -0.5 to 0.5.The ACF plot of the residuals shows significant lags, indicating that the model is performing well. The qq-plot also suggests normality in the residuals. The p-value greater than 0.05 actually suggests that there is no evidence of significant autocorrelation in the residuals, which means that the model is performing well in terms of capturing the temporal patterns in the data.\n\n\nFitting the model manually\n\nLinear ModelACF of ResidualsACF of ResidualsDifferentiated Residual\n\n\n\n\nCode\nnormalized_sp500_factor_data_numeric_df$GSPC.Adjusted<-ts(normalized_sp500_factor_data_numeric_df$GSPC.Adjusted,star=decimal_date(as.Date(\"2010-01-01\",format = \"%Y-%m-%d\")),frequency = 4)\nnormalized_sp500_factor_data_numeric_df$inflation<-ts(normalized_sp500_factor_data_numeric_df$inflation,star=decimal_date(as.Date(\"2010-01-01\",format = \"%Y-%m-%d\")),frequency = 4)\nnormalized_sp500_factor_data_numeric_df$unemployment<-ts(normalized_sp500_factor_data_numeric_df$unemployment,star=decimal_date(as.Date(\"2010-01-01\",format = \"%Y-%m-%d\")),frequency = 4)\n\n############# First fit the linear model##########\nfit.reg <- lm(GSPC.Adjusted ~ inflation+unemployment, data=normalized_sp500_factor_data_numeric_df)\nsummary(fit.reg)\n\n\n\nCall:\nlm(formula = GSPC.Adjusted ~ inflation + unemployment, data = normalized_sp500_factor_data_numeric_df)\n\nResiduals:\n    Min      1Q  Median      3Q     Max \n-0.9069 -0.3819 -0.1741  0.2899  1.7751 \n\nCoefficients:\n               Estimate Std. Error t value Pr(>|t|)    \n(Intercept)   1.679e-16  8.960e-02   0.000   1.0000    \ninflation     6.104e-01  1.032e-01   5.914 3.17e-07 ***\nunemployment -2.653e-01  1.032e-01  -2.571   0.0132 *  \n---\nSignif. codes:  0 '***' 0.001 '**' 0.01 '*' 0.05 '.' 0.1 ' ' 1\n\nResidual standard error: 0.6461 on 49 degrees of freedom\nMultiple R-squared:  0.5989,    Adjusted R-squared:  0.5825 \nF-statistic: 36.58 on 2 and 49 DF,  p-value: 1.905e-10\n\n\n\n\n\n\nCode\nres.fit<-ts(residuals(fit.reg),star=decimal_date(as.Date(\"2010-01-01\",format = \"%Y-%m-%d\")),frequency = 4)\n############## Then look at the residuals ############\nacf(res.fit)\n\n\n\n\n\n\n\n\n\nCode\nPacf(res.fit)\n\n\n\n\n\n\n\n\n\nCode\nres.fit %>% diff() %>% ggtsdisplay()\n\n\n\n\n\n\n\n\nThe output displays the results of a linear regression model using two predictor variables, inflation and unemployment, to explain the variation in the response variable, IXIC.Adjusted. The coefficients of inflation and unemployment have p-values of less than 0.05, indicating that both variables significantly impact IXIC.Adjusted. The R-squared value of approximately 59% suggests that the model explains a moderate amount of the variation in the response variable. Examination of the ACF plot of residuals reveals some autocorrelation, and therefore, the series is differentiated to achieve stationarity. By analyzing the differentiated ACF and PACF plots, the parameters if the ARIMAX model are that p=0, q=3, and d=1.\n\n\nFinding the model parameters.\n\nARIMAX ResultModel 1 PlotModel 1Model 2 PlotModel 2Model 3 PlotModel 3\n\n\n\n\nCode\nARIMA.c=function(p1,q1,q2,data){\ntemp=c()\nd=1\ni=1\ntemp= data.frame()\nls=matrix(rep(NA,6*30),nrow=30)\n\n\nfor (p in p1)#\n{\n  for(q in q1:q2)#\n  {\n    for(d in 0:1)\n    {\n      \n      if(p+d+q<=6)\n      {\n        \n        model<- Arima(data,order=c(p,d,q))\n        ls[i,]= c(p,d,q,model$aic,model$bic,model$aicc)\n        i=i+1\n        #print(i)\n        \n      }\n      \n    }\n  }\n}\n\n\ntemp= as.data.frame(ls)\nnames(temp)= c(\"p\",\"d\",\"q\",\"AIC\",\"BIC\",\"AICc\")\n\ntemp\n}\n\noutput <- ARIMA.c(0,1,3,data=residuals(fit.reg))\n\noutput[which.min(output$AIC),] \n\n\n  p d q     AIC      BIC     AICc\n6 0 1 3 6.14935 13.87665 7.018916\n\n\nCode\noutput[which.min(output$BIC),]\n\n\n  p d q      AIC      BIC     AICc\n2 0 1 1 9.834173 13.69782 10.08417\n\n\nCode\noutput[which.min(output$AICc),]\n\n\n  p d q     AIC      BIC     AICc\n6 0 1 3 6.14935 13.87665 7.018916\n\n\n\n\n\n\nCode\nset.seed(1234)\n\nmodel_output_1 <- capture.output(sarima(res.fit, 0,1,3)) \n\n\n\n\n\n\n\n\n\nCode\ncat(model_output_1[23:54], model_output_1[length(model_output_1)], sep = \"\\n\")\n\n\n$fit\n\nCall:\narima(x = xdata, order = c(p, d, q), seasonal = list(order = c(P, D, Q), period = S), \n    xreg = constant, transform.pars = trans, fixed = fixed, optim.control = list(trace = trc, \n        REPORT = 1, reltol = tol))\n\nCoefficients:\n         ma1     ma2     ma3  constant\n      0.1540  0.2014  0.3736   -0.0118\ns.e.  0.1427  0.1720  0.1263    0.0562\n\nsigma^2 estimated as 0.05585:  log likelihood = 0.95,  aic = 8.11\n\n$degrees_of_freedom\n[1] 47\n\n$ttable\n         Estimate     SE t.value p.value\nma1        0.1540 0.1427  1.0791  0.2860\nma2        0.2014 0.1720  1.1709  0.2476\nma3        0.3736 0.1263  2.9583  0.0048\nconstant  -0.0118 0.0562 -0.2105  0.8342\n\n$AIC\n[1] 0.1589218\n\n$AICc\n[1] 0.1759721\n\n$BIC\n[1] 0.3483165\n\n\n\n\n\n\nCode\nmodel_output_2 <- capture.output(sarima(res.fit,0,1,1)) \n\n\n\n\n\n\n\n\n\nCode\ncat(model_output_2[18:48], model_output_2[length(model_output_2)], sep = \"\\n\")\n\n\n$fit\n\nCall:\narima(x = xdata, order = c(p, d, q), seasonal = list(order = c(P, D, Q), period = S), \n    xreg = constant, transform.pars = trans, fixed = fixed, optim.control = list(trace = trc, \n        REPORT = 1, reltol = tol))\n\nCoefficients:\n         ma1  constant\n      0.1506   -0.0057\ns.e.  0.1364    0.0412\n\nsigma^2 estimated as 0.06559:  log likelihood = -2.91,  aic = 11.82\n\n$degrees_of_freedom\n[1] 49\n\n$ttable\n         Estimate     SE t.value p.value\nma1        0.1506 0.1364  1.1046  0.2747\nconstant  -0.0057 0.0412 -0.1373  0.8914\n\n$AIC\n[1] 0.2316735\n\n$AICc\n[1] 0.2365755\n\n$BIC\n[1] 0.3453103\n\n\n\n\n\n\nCode\nmodel_output_3 <- capture.output(sarima(res.fit,0,1,0)) \n\n\n\n\n\n\n\n\n\nCode\ncat(model_output_3[9:38], model_output_3[length(model_output_3)], sep = \"\\n\")\n\n\n$fit\n\nCall:\narima(x = xdata, order = c(p, d, q), seasonal = list(order = c(P, D, Q), period = S), \n    xreg = constant, transform.pars = trans, fixed = fixed, optim.control = list(trace = trc, \n        REPORT = 1, reltol = tol))\n\nCoefficients:\n      constant\n       -0.0062\ns.e.    0.0363\n\nsigma^2 estimated as 0.06729:  log likelihood = -3.55,  aic = 11.1\n\n$degrees_of_freedom\n[1] 50\n\n$ttable\n         Estimate     SE t.value p.value\nconstant  -0.0062 0.0363 -0.1719  0.8642\n\n$AIC\n[1] 0.2175886\n\n$AICc\n[1] 0.2191893\n\n$BIC\n[1] 0.2933465\n\n\n\n\n\nFollowing the manual fitting process, we identified that the ARIMAX model with the minimum AIC and AICc has the parameters (0,1,3), while the model with the minimum BIC has the parameters (0,1,1). We fitted both models using both the manual and auto.arima methods and observed that all the models have p-values greater than 0, indicating statistical significance. To determine the best fit model, we performed cross-validation.\n\n\nCross Validation\n\nRMSE PlotRMSE Results\n\n\n\n\nCode\nn=length(res.fit)\nk= 51\n \n \nrmse1 <- matrix(NA, (n-k),4)\nrmse2 <- matrix(NA, (n-k),4)\nrmse3 <- matrix(NA, (n-k),4)\n\nst <- tsp(res.fit)[1]+(k-5)/4 \n\nfor(i in 1:(n-k))\n{\n  xtrain <- window(res.fit, end=st + i/4)\n  xtest <- window(res.fit, start=st + (i+1)/4, end=st + (i+4)/4)\n\n  \n  fit <- Arima(xtrain, order=c(0,1,3),\n                include.drift=TRUE, method=\"ML\")\n  fcast <- forecast(fit, h=4)\n  \n  fit2 <- Arima(xtrain, order=c(0,1,1),\n                include.drift=TRUE, method=\"ML\")\n  fcast2 <- forecast(fit2, h=4)\n\n  fit3 <- Arima(xtrain, order=c(0,1,0),\n                include.drift=TRUE, method=\"ML\")\n  fcast3 <- forecast(fit3, h=4)\n\n  rmse1[i,1:length(xtest)]   <- sqrt((fcast$mean-xtest)^2)\n  rmse2[i,1:length(xtest)] <- sqrt((fcast2$mean-xtest)^2)\n  rmse3[i,1:length(xtest)] <- sqrt((fcast3$mean-xtest)^2)\n}\n\nplot(1:4,colMeans(rmse1,na.rm=TRUE), type=\"l\",col=2, xlab=\"horizon\", ylab=\"RMSE\")\nlines(1:4, colMeans(rmse2,na.rm=TRUE), type=\"l\",col=3)\nlines(1:4, colMeans(rmse3,na.rm=TRUE), type=\"l\",col=4)\nlegend(\"topleft\",legend=c(\"fit1\",\"fit2\",\"fit3\"),col=2:4,lty=1)\n\n\n\n\n\n\n\n\n\nCode\ncat(\"RMSE values for Model 1\\n\", colMeans(rmse1))\n\n\nRMSE values for Model 1\n 0.09801778 0.3890418 0.9544379 1.291413\n\n\nCode\ncat(\"RMSE values for Model 2\\n\", colMeans(rmse2))\n\n\nRMSE values for Model 2\n 0.3002188 0.718625 1.342683 1.684389\n\n\nCode\ncat(\"RMSE values for Model 3\\n\", colMeans(rmse3))\n\n\nRMSE values for Model 3\n 0.2982561 0.7166822 1.34076 1.682486\n\n\n\n\n\nThe RMSE cross-validation plot shows that the RMSE of Model 1 (0,1,3) is lower than that of Model 2 (0,1,1) and Model 3 (0,1,0). This suggests that Model 3 (0,1,3) is a better fit for the data when compared other models.\n\n\nForecast\n\nForecast for S&P 500 Index with feature variablesARIMA Model for InflationARIMA Model for EmploymentARIMA Model for S&P 500 Index with feature variables\n\n\n\n\nCode\n#fiting an ARIMA model to the Inflation variable\ninflation_fit<-auto.arima(normalized_sp500_factor_data_numeric_df$inflation) \nfinflation<-forecast(inflation_fit)\n\n#fitting an ARIMA model to the Unemployment variable\nunemployment_fit<-auto.arima(normalized_sp500_factor_data_numeric_df$unemployment) \nfunemployment<-forecast(unemployment_fit)\n\n# best model fit for forcasting\nxreg <- cbind(Inflation = normalized_sp500_factor_data_ts[, \"inflation\"],\n              Unemployment = normalized_sp500_factor_data_ts[, \"unemployment\"])\n\nfit <- Arima(normalized_sp500_factor_data_ts[, \"GSPC.Adjusted\"],order=c(0,1,3),xreg=xreg)\n\n# Forcast the stock price using feature variables\nfxreg <- cbind(Inflation = finflation$mean,\n              Unemployment = funemployment$mean)\nfcast <- forecast(fit, xreg=fxreg) \nautoplot(fcast) + xlab(\"Year\") +\n  ylab(\"Normalised GSPC.Adjusted\")\n\n\n\n\n\n\n\n\n\nCode\n#fiting an ARIMA model to the Inflation variable\ninflation_fit<-auto.arima(normalized_sp500_factor_data_numeric_df$inflation) \nfinflation<-forecast(inflation_fit)\nsummary(inflation_fit)\n\n\nSeries: normalized_sp500_factor_data_numeric_df$inflation \nARIMA(0,1,3)(0,0,1)[4] with drift \n\nCoefficients:\n         ma1     ma2     ma3     sma1   drift\n      0.2153  0.5354  0.4353  -0.3707  0.0567\ns.e.  0.1719  0.1915  0.1455   0.1942  0.0499\n\nsigma^2 = 0.0708:  log likelihood = -3.48\nAIC=18.96   AICc=20.87   BIC=30.55\n\nTraining set error measures:\n                      ME     RMSE       MAE       MPE    MAPE      MASE\nTraining set 0.002515126 0.250258 0.1802161 -17.11236 114.163 0.3180481\n                    ACF1\nTraining set -0.06141191\n\n\n\n\n\n\nCode\n#fitting an ARIMA model to the Unemployment variable\nunemployment_fit<-auto.arima(normalized_sp500_factor_data_numeric_df$unemployment) \nfunemployment<-forecast(unemployment_fit)\nsummary(unemployment_fit)\n\n\nSeries: normalized_sp500_factor_data_numeric_df$unemployment \nARIMA(0,1,2) \n\nCoefficients:\n          ma1      ma2\n      -0.3027  -0.2410\ns.e.   0.1376   0.1472\n\nsigma^2 = 0.4982:  log likelihood = -53.72\nAIC=113.45   AICc=113.96   BIC=119.24\n\nTraining set error measures:\n                     ME      RMSE      MAE      MPE     MAPE      MASE\nTraining set -0.1001611 0.6851533 0.281119 140.1191 219.8632 0.4565275\n                    ACF1\nTraining set -0.02079058\n\n\n\n\n\n\nCode\n# best model fit for forcasting\nxreg <- cbind(Inflation = normalized_sp500_factor_data_ts[, \"inflation\"],\n              Unemployment = normalized_sp500_factor_data_ts[, \"unemployment\"])\n\nfit <- Arima(normalized_sp500_factor_data_ts[, \"GSPC.Adjusted\"],order=c(0,1,3),xreg=xreg)\nsummary(fit)\n\n\nSeries: normalized_sp500_factor_data_ts[, \"GSPC.Adjusted\"] \nRegression with ARIMA(0,1,3) errors \n\nCoefficients:\n         ma1     ma2     ma3  Inflation  Unemployment\n      0.1048  0.0781  0.4723     0.1549       -0.1928\ns.e.  0.1705  0.1723  0.1810     0.0866        0.0326\n\nsigma^2 = 0.0333:  log likelihood = 16.64\nAIC=-21.27   AICc=-19.36   BIC=-9.68\n\nTraining set error measures:\n                     ME     RMSE       MAE       MPE     MAPE      MASE\nTraining set 0.01014764 0.171641 0.1307816 -111.0777 138.5378 0.3859508\n                    ACF1\nTraining set -0.01615945\n\n\n\n\n\nThe ARIMAX model may have incorporated exogenous variables such as macroeconomic factors, which could indicate the potential impact of external factors on the forecasted values. The findings suggest that inflation and unemployment rates significantly impact the S&P 500 Index stock price, underscoring the importance of including macroeconomic factors in stock market forecasting.This is further supported by the comparison of RMSE, MAE, AIC and BIC values for forecasts with and without the feature variable. It is found that including the feature variable leads to a lower values, indicating that the feature variable makes a significant impact and should be considered in stock market forecasting."
  },
  {
    "objectID": "arimax_stock_index_macroeconomic.html",
    "href": "arimax_stock_index_macroeconomic.html",
    "title": "ARIMAX/SARIMAX Model for US stock indices and macroeconomic factors as exogenous variables",
    "section": "",
    "text": "In this case, we are interested in predicting the performance of three stock indices, such as the S&P 500, NASDAQ, and Dow Jones Industrial Average. We will use macroeconomic factors as exogenous variables to improve the accuracy of our predictions. Some examples of macroeconomic factors we might consider include GDP growth, inflation rates, and interest rates.\nIncluding these exogenous variables can help us to better understand how the stock market might be impacted by changes in the broader economy. For example, if GDP growth is predicted to increase, we might expect to see a corresponding increase in the stock market indices as well.\nAccording to the findings, the endogenous and exogenous variables in the time series data are not interdependent, then the ARIMAX model can be a good choice for predicting the stock market indices. If there is seasonality in the data, then the SARIMAX model can be used to account for this seasonal variation. If there is no seasonality, then the simpler ARIMA model can be used instead of SARIMAX.\n\n\n\n\n\n\nLet’s examine the relationship between endogenous and exogenous variables before proceeding with the ARIMAX/SARIMAX model.\n\nPlotNormalized Plot\n\n\n\n\nCode\nts_plot(index_factor_data,\n        title = \"Stock Prices and Macroeconomic Variables\",\n        Ytitle = \"Values\",\n        Xtitle = \"Year\")\n\n\n\n\n\n\n\n\n\n\nCode\nnumeric_vars_index_factor_data <- c(\"DJI.Adjusted\", \"IXIC.Adjusted\", \"GSPC.Adjusted\", \"gdp\", \"interest\", \"inflation\", \"unemployment\")\nnumeric_index_factor_data <- index_factor_data[, numeric_vars_index_factor_data]\nnormalized_index_factor_data_numeric <- scale(numeric_index_factor_data)\nnormalized_index_factor_data <- ts(normalized_index_factor_data_numeric, start = c(2010, 1), frequency = 4)\nts_plot(normalized_index_factor_data,\n        title = \"Normalized Time Series Data for Stock Prices and Macroeconomic Variables\",\n        Ytitle = \"Normalized Values\",\n        Xtitle = \"Year\")\n\n\n\n\n\n\n\n\n\nThe Stock Prices and Macroeconomic Variables plot, displays the time series data of various stock prices and macroeconomic variables from 2010 to 2022. Since the variables in the time series data have different scales or units, it can make the plot difficult to interpret, as the differences between variables may be obscured by the varying magnitudes. Normalizing the data by scaling it to a common scale, such as z-scores or percentage changes, can help to eliminate this issue and provide a clearer view of the relationships and patterns in the data.\nThe Normalized Time Series Data for Stock Prices and Macroeconomic Variables plot, shows the same variables as the first plot, but the data has been normalized. Normalization is the process of scaling data to a common range, usually between 0 and 1, to eliminate the impact of different scales or units of measurement. In this case, the data has been scaled using the scale() function in R, which standardizes the variables to have a mean of 0 and a standard deviation of 1.\nNormalizing the time series data is beneficial for several reasons. First, it helps to remove any bias or distortion that may be introduced by variables with different units or magnitudes, allowing for a fair comparison between variables. Second, normalizing the data can help to stabilize the VAR model estimation, as variables with large values or extreme fluctuations may disproportionately influence the results. Lastly, normalizing the data can also improve the interpretability of the model coefficients, as the coefficients will be in the same scale and can be directly compared to assess their relative importance.\n\nCross-Correlation for the Variables and Selection of Feature Variables\nCross-correlation is a statistical technique used to measure the relationship between two or more variables in a time series. In the context of ARIMAX modeling, cross-correlation is often used for feature selection. For selecting feature variables in our analysis, we will first examine the correlation through a heatmap among all the variables, and then analyze the autocorrelation function (ACF) plots between the response variable and the exogenous variables.\n\n\nCorrelation Heatmap\n\n\nCode\n# Get upper triangle of the correlation matrix\nget_upper_tri <- function(cormat){\n    cormat[lower.tri(cormat)]<- NA\n    return(cormat)\n}\ncormat <- round(cor(normalized_index_factor_data_numeric),2)\n\nupper_tri <- get_upper_tri(cormat)\n\nmelted_cormat <- melt(upper_tri, na.rm = TRUE)\n# Create a ggheatmap\nggheatmap <- ggplot(melted_cormat, aes(Var2, Var1, fill = value))+\n geom_tile(color = \"white\")+\n scale_fill_gradient2(low = \"blue\", high = \"red\", mid = \"white\", \n   midpoint = 0, limit = c(-1,1), space = \"Lab\", \n    name=\"Pearson\\nCorrelation\") +\n  theme_minimal()+ # minimal theme\n theme(axis.text.x = element_text(angle = 45, vjust = 1, \n    size = 12, hjust = 1))+\n coord_fixed()\n\nggheatmap + \ngeom_text(aes(Var2, Var1, label = value), color = \"black\", size = 4) +\ntheme(\n  axis.title.x = element_blank(),\n  axis.title.y = element_blank(),\n  panel.grid.major = element_blank(),\n  panel.border = element_blank(),\n  panel.background = element_blank(),\n  axis.ticks = element_blank(),\n  legend.justification = c(1, 0),\n  legend.position = c(0.6, 0.7),\n  legend.direction = \"horizontal\")+\n  guides(fill = guide_colorbar(barwidth = 7, barheight = 1,\n                title.position = \"top\", title.hjust = 0.5))\n\n\n\n\n\nThe heatmap reveal important insights into the relationships between the stock market indices and various economic indicators. The strong positive correlations between the stock market indices and inflation, along with the negative correlations with unemployment rate, suggest that these variables may play a significant role in influencing stock market movements. In contrast, the weaker correlations between the stock market indices and GDP and interest rates indicate that these variables may have less impact on stock market fluctuations. These findings provide valuable guidance for selecting relevant variables in the VAR model to better understand and forecast stock market dynamics.\nClick to view ARIMAX/SARIMAX Model for Dow Jones index and macroeconomic factors as exogenous variables\nClick to view ARIMAX/SARIMAX Model for NASDAQ Composite index and macroeconomic factors as exogenous variables\nClick to view ARIMAX/SARIMAX Model for S&P 500 index and macroeconomic factors as exogenous variables"
  },
  {
    "objectID": "arma.html",
    "href": "arma.html",
    "title": "",
    "section": "",
    "text": "ARMA/ARIMA/SARIMA Model\nWhen working with time series data, it is common to start with autoregressive (AR), moving average (MA), or autoregressive moving average (ARMA) models. Additionally, autoregressive integrated moving average (ARIMA) and seasonal autoregressive integrated moving average (SARIMA) models are widely used for better understanding the data and forecasting future values based on historical data. However, before using these models, it is crucial to check for stationarity in the time series data, which means that the mean and variance should not change over time. Non-stationarity is often caused by trends, where the values slowly increase or decrease over time.\nTo test for stationarity, an autocorrelation function (ACF) plot can be used to check for correlations between the time series and its lagged version. If there is a significant correlation, then the data is likely non-stationary. In such cases, taking the first or second differences of the data can help remove any trends or seasonality and make the data stationary.\nIn the EDA tab of our project, we have executed an ACF plot to test for stationarity and made the necessary data transformations to make the data stationary. The ACF and partial autocorrelation (PACF) plots for the stationary data can help identify the appropriate parameters for our ARIMA or SARIMA models.\n\n\nARIMA Model for US Stock Indicies\nThe ARIMA model is a popular choice for modeling and forecasting US stock indices despite the fact that they do not exhibit stationarity in their raw form. This is because the ARIMA model can handle non-stationary time series data by first differencing the series to make it stationary.\nThe first step in using the ARIMA model for US stock indices is to identify the appropriate order of differencing required to make the series stationary. This can be done using statistical tests, such as the Augmented Dickey-Fuller (ADF) test, which test for the presence of unit roots in the series. Once the series has been differenced, the ARIMA model can be fitted to the data to identify the appropriate autoregressive (AR), integrated (I), and moving average (MA) parameters. The model can then be used to make forecasts of future values based on the identified patterns and trends in the data.\nOverall, the ARIMA model is a powerful tool for modeling and forecasting US stock indices, despite their non-stationarity. It can account for the complex patterns and trends present in the data and provide valuable insights into the behavior of the stock market.\nClick to view the ARIMA Page for US Stock Indices\n\n\nARIMA Model for Sector Market\nFrom previous exploratory data analysis (EDA), it was found that sector markets, such as the technology or energy sectors, exhibit unique patterns and trends over time. These patterns may be driven by specific economic and industry-related factors that can impact stock prices.\nTo model and forecast sector markets, the ARIMA model is commonly used. The appropriate order of differencing can be identified using statistical tests, such as the ADF test. Once identified, the ARIMA model can be fitted to the data to identify the appropriate AR, I, and MA parameters.\nOverall, the ARIMA model is a powerful tool for modeling and forecasting sector markets, allowing analysts to identify unique patterns and trends and make more accurate predictions about future values. However, as with any forecasting model, it is important to carefully consider the underlying assumptions and limitations of the model and to incorporate relevant exogenous variables where appropriate.\nClick to view the ARIMA Page for Sector Market\n\n\nARIMA/SARIMA Model for Macroeconomic factors\nFrom previous exploratory data analysis (EDA), it was found that macroeconomic factors such as GDP, inflation, and unemployment rates exhibit complex patterns and trends over time. These patterns may be driven by long-term economic cycles, seasonal effects, and the impact of specific economic policies.\nTo model and forecast these factors, the ARIMA/SARIMA model is often used due to its ability to capture and model these complex patterns. The appropriate order of differencing and seasonality can be identified using statistical tests, such as the ADF and seasonal ADF tests. Once identified, the ARIMA/SARIMA model can be fitted to the data to identify the appropriate AR, I, and MA parameters.\nOverall, the ARIMA/SARIMA model is a powerful tool for modeling and forecasting macroeconomic factors, allowing analysts to identify and model the complex trends and patterns that are often present in economic data. However, it is important to carefully consider the underlying assumptions and limitations of the model and to incorporate relevant exogenous variables where appropriate.\nClick to view the ARIMA Page for Macroeconomic Factors"
  },
  {
    "objectID": "asv.html",
    "href": "asv.html",
    "title": "",
    "section": "",
    "text": "ARIMAX Model\nARIMAX (Autoregressive Integrated Moving Average with Exogenous Variables) and SARIMAX (Seasonal Autoregressive Integrated Moving Average with Exogenous Variables) are extensions of the ARIMA and SARIMA models, respectively, that allow for the inclusion of exogenous variables in the modeling process. Exogenous variables are independent variables that may affect the dependent variable (i.e., the time series of interest) but are not affected by it.\nVAR (Vector Autoregression) is another model commonly used for time series analysis, which allows for the inclusion of multiple time series as both dependent and independent variables in the modeling process.\nThese models are useful when there are external factors that may influence the time series of interest, and we want to account for these factors in our forecasting. For example in the project, when predicting stock prices, we may want to include economic indicators such as interest rates, inflation rates, unemployment rate, and GDP growth as exogenous variables, as these factors can impact the stock price of indices and sector market.\nChoosing the appropriate model depends on the characteristics of the data and the research question at hand. - If there is no clear seasonal pattern in the data and only a few exogenous variables, ARIMAX may be appropriate. - If there is a clear seasonal pattern in the data and/or multiple exogenous variables, SARIMAX may be more appropriate. - If there are multiple time series that are potentially interacting with each other, VAR may be more appropriate.\nHowever, the selection of the appropriate model should be based on a thorough analysis of the data and model diagnostics, such as checking for stationarity, autocorrelation, and heteroscedasticity. It is also important to consider the interpretability and ease of implementation of the chosen model. By using ARIMAX or VAR models to analyze the relationship between macroeconomic factors and the stock market, we can better understand how changes in the broader economy may impact the stock prices of specific indices or sectors. These models take into account both endogenous and exogenous variables, which allows us to examine how different factors interact with one another and make more accurate predictions of future stock prices.\n\n\n\nARIMAX/SARIMAX Model for US stock indices and macroeconomic factors as exogenous variables\nARIMAX and SARIMAX models can be useful in analyzing the relationship between US stock indices and macroeconomic factors as exogenous variables. In ARIMAX models, exogenous variables are included to help explain the behavior of the endogenous variable, which is the stock price of a specific US stock index or sector. SARIMAX models, on the other hand, are designed to capture the complex patterns and seasonality in the data.\nChoosing between ARIMAX and SARIMAX models depends on the nature of the data and the research question. If the focus is on the impact of macroeconomic factors on a specific US stock index or sector, then ARIMAX may be more appropriate. However, if the data has clear seasonal patterns, such as monthly or quarterly data, then SARIMAX may be more effective in capturing these patterns and providing more accurate forecasts. Incorporating exogenous variables in ARIMAX or SARIMAX models can help identify patterns and relationships between the US stock market and the broader economy.\nClick to view ARIMAX/SARIMAX Model Page for US stock indices and macroeconomic factors as exogenous variables\n\n\nARIMAX/SARIMAX Model for Sector market and macroeconomic factors as exogenous variables\nARIMAX and SARIMA models can also be useful in analyzing the relationship between sector market data and macroeconomic factors as exogenous variables. In this case, the endogenous variable is the stock price of a particular sector, and the exogenous variables may include factors such as interest rates, inflation, and GDP growth.\nARIMAX models can help to identify the relationship between the sector market and macroeconomic factors while controlling for the impact of other endogenous variables. SARIMA models, on the other hand, are effective in capturing the complex seasonal patterns in the data.\nWhen deciding between ARIMAX and SARIMA models, it is important to consider the nature of the data and the research question. If the data has clear seasonal patterns, such as quarterly or monthly data, then SARIMA may be more effective in capturing these patterns and providing accurate forecasts. On the other hand, if the focus is on understanding the impact of macroeconomic factors on a specific sector market, then ARIMAX may be more appropriate.\nOverall, incorporating exogenous variables in ARIMAX or SARIMA models can provide valuable insights into the relationship between sector market data and macroeconomic factors.\nClick to view ARIMAX/SARIMAX Model Page for Sector market and macroeconomic factors as exogenous variables"
  },
  {
    "objectID": "conclusion.html",
    "href": "conclusion.html",
    "title": "",
    "section": "",
    "text": "Conclusion\nThe stock market and sector market are vital components of the global economy, and various macroeconomic factors influence their performance. As investors seek opportunities to grow their wealth, they look to the stock market to invest in publicly traded companies or the sector market to focus on specific industries or segments of the economy. However, the performance of these markets is separate from the broader economic environment. Changes in macroeconomic factors such as inflation, interest rates, and economic growth can significantly impact the stock and sector markets.\nOur project aimed to investigate how macroeconomic factors affect the performance of the US stock market indices and sector markets. By analyzing the data and exploring the relationships between these factors and the stock market, we aimed to gain a deeper understanding of the complex interactions between macroeconomic indicators and financial markets. Ultimately, this project aimed to provide insights into how investors, businesses, and policymakers can navigate the dynamic landscape of the stock market and sector market in response to macroeconomic changes.\n\nThe COVID-19 pandemic has profoundly impacted the stock market indices and sector market, causing significant fluctuations in stock prices and investor sentiment. In the early stages of the pandemic, stock markets worldwide experienced a sharp decline as investors feared the economic impact of the virus. As the pandemic continued to spread, some sectors of the economy, such as travel and hospitality, were hit particularly hard, leading to a further decline in the sector market. However, other sectors, such as technology and healthcare, saw significant growth as companies adapted to the new normal of remote work and increased demand for medical equipment and services.\n\nDespite the initial shock, the stock market has shown resilience and since partially recovered, thanks in part to government stimulus measures and vaccine development. However, the impact of the pandemic on the stock market and sector market is far from over, as uncertainties and risks still loom large, including new variants of the virus and inflation concerns. As a result, investors and businesses must remain vigilant and adapt to the evolving market conditions to navigate the post-COVID landscape successfully.\n\nThe analysis conducted in our project confirms that macroeconomic factors play a significant role in the performance of the stock market indices and sector markets. Our results show that including macroeconomic variables in the models led to better predictive accuracy than models that only considered historical prices. Among the macroeconomic factors, inflation and unemployment rates significantly impacted the stock and sector prices.\nAs a future direction, our project could be expanded to include a more in-depth analysis of the top companies in each sector market and how macroeconomic factors have impacted them. By quantifying the performance of these companies and analyzing their responses to changes in macroeconomic indicators, we can better understand the specific drivers of sector market performance.\nIn addition to traditional time series models, we also explored the potential of deep learning models in predicting stock and sector prices. Our results indicate that deep learning models can offer superior predictive accuracy compared to traditional time series models, but they often require more computational resources and data. While the benefits of deep learning models should be considered, it is essential to consider the advantages of time series models, such as their simplicity, interpretability, and computational efficiency. Time series models can provide valuable insights into the underlying drivers of stock and sector prices, and they can be a practical choice for applications with limited resources. Moreover, time series models can generate long-term forecasts, making them useful for planning and decision-making.\nDespite the robust findings of our analysis, there were some limitations to our research. One notable limitation is that our analysis did not find a significant relationship between GDP growth rate and interest rate, and stock price, contrary to prior research in this area. While our research is consistent with some recent studies, this finding highlights the need for further research to investigate the complex relationship between macroeconomic factors and stock market performance.\nOne possible explanation for the lack of a significant relationship between GDP growth rate, interest rate, and stock price is that our analysis needed to account for all the potential factors that may impact the stock market. For example, other factors such as political events, company-specific news, and investor sentiment changes may also significantly impact the stock market and sector market.\n\nMoreover, our analysis only focused on the US macroeconomic factors, which may limit the generalizability of our findings. The performance of the US stock market and sector market is also influenced by factors outside the United States, such as international trade policies and geopolitical events. Thus, future research could expand on our analysis by examining the impact of global macroeconomic factors on the US stock market and sector market.\nDespite these limitations, our research provides valuable insights into the relationship between macroeconomic factors and the US stock market and sector market. Our findings can assist investors, businesses, and policymakers make informed decisions based on macroeconomic conditions and provide a framework for further research."
  },
  {
    "objectID": "dv_macroeconomic_factor.html",
    "href": "dv_macroeconomic_factor.html",
    "title": "",
    "section": "",
    "text": "Code#import the data\ngdp <- read.csv(\"DATA/RAW DATA/gdp-growth.csv\")\n\n#change date format\ngdp$DATE <- as.Date(gdp$DATE , \"%m/%d/%Y\")\n\n#plot gdp growth rate \nfig <- plot_ly(gdp, x = ~DATE, y = ~value, type = 'scatter', mode = 'lines',line = list(color = 'rgb(179, 210, 165)'))\nfig <- fig %>% layout(title = \"U.S GPD Growth Rate: 2010 - 2022\",xaxis = list(title = \"Time\"),yaxis = list(title =\"GDP Growth Rate\"))\nfig\n\n\n\n\n\nThe period between 2010 and 2022 has been characterized by moderate fluctuations in economic growth. After a period of slow growth following the 2008 financial crisis, the US economy experienced a notable uptick in GDP growth from 2012 to 2015, with rates reaching a peak of 2.9% in 2015. However, growth rates began to decline again from 2016 to 2019, falling to a low of 2.2% in 2019. The COVID-19 pandemic caused a sharp contraction in the economy in 2020, with GDP growth falling by 3.5%, the largest annual decline since the 1940s. However, there was a partial recovery in 2021, with growth rates projected to reach 6.3% by the end of the year, reflecting a combination of fiscal stimulus measures and the easing of pandemic-related restrictions. Overall, the trend in GDP growth rate in the United States from 2010 to 2022 has been characterized by moderate fluctuations, with notable shifts in response to both domestic and global economic conditions.\n\n\nCode#import the data\ninterest_data <- read.csv(\"DATA/RAW DATA/interest-rate.csv\")\n\n#change date format\ninterest_data$Date <- as.Date(interest_data$Date , \"%m/%d/%Y\")\n\n#plot interest rate \nfig <- plot_ly(interest_data, x = ~Date, y = ~value, type = 'scatter', mode = 'lines',line = list(color = 'rgb(59, 14, 37)'))\nfig <- fig %>% layout(title = \"U.S Interest Rate: January 2010 - March 2023\",xaxis = list(title = \"Time\"),yaxis = list(title =\"Interest Rate\"))\nfig\n\n\n\n\n\nThe graph of the real interest rate in the US from 2010 to March 2023 shows a general trend of volatility and fluctuation. The real interest rate is a measure of the cost of borrowing for the US government and is adjusted for inflation.\nFrom 2010 to mid-2012, the real interest rate remained relatively low and stable, with only minor fluctuations.This increase in the real interest rate was likely due to concerns about inflation and the impact of the US government’s monetary policy measures. From mid-2013 to mid-2016, the real interest rate remained relatively low, with only minor fluctuations. However, from mid-2016 to mid-2018, there was another sharp increase in the real interest rate. This increase was driven by a combination of factors, including the improving US economy, rising inflation expectations, and the Federal Reserve’s decision to raise interest rates.\nFrom mid-2018 to mid-2019, the real interest rate declined sharply, and then remained relatively stable at lower levels until early 2021. This decline was largely due to concerns about a slowing global economy, trade tensions, and the impact of the COVID-19 pandemic. Since early 2021, the real interest rate has been increasing again, and it remains at a relatively high level as of March 2023. This increase may be due to concerns about inflation, the impact of government stimulus measures, and the possibility of an economic recovery.\nOverall, the trend of the real interest rate in the US from 2010 to March 2023 has been characterized by periods of volatility and fluctuation, driven by a range of economic and policy factors.\n\n\nCode#import the data\ninflation_rate <- read.csv(\"DATA/RAW DATA/inflation-rate.csv\")\n\n\n#cleaning the data\n#remove unwanted columns\ninflation_rate_clean <- subset(inflation_rate, select = -c(1,HALF1,HALF2))\n\n#convert the data to time series data\ninflation_data_ts <- ts(as.vector(t(as.matrix(inflation_rate_clean))), start=c(2010,1), end=c(2023,2), frequency=12)\n\n#plot inflation rate \nfig <- autoplot(inflation_data_ts, ylab = \"Inflation Rate\", color=\"#8B8695\")+ggtitle(\"U.S Inflation Rate: January 2010 - February 2023\")+theme_bw()\nggplotly(fig)\n\n\n\n\n\nThe U.S inflation rate from January 2010 to February 2023 has been a topic of concern for many individuals and businesses. From 2010 to 2012, the inflation rate remained relatively low. However, from 2013 to 2023, the inflation rate fluctuated significantly. The COVID-19 pandemic had a significant impact on the inflation rate, causing it to rise rapidly in 2021 due to supply chain disruptions and other factors. The Federal Reserve has implemented various policies to try and manage the inflation rate, including adjusting interest rates and reducing bond purchases. The U.S inflation rate remains a closely watched indicator of economic health, and its fluctuations can have significant impacts on individuals, businesses, and the broader economy.\n\n\nCode#import the data\nunemployment_rate <- read.csv(\"DATA/RAW DATA/unemployment-rate.csv\")\n\n#change date format\nunemployment_rate$Date <- as.Date(unemployment_rate$Date , \"%m/%d/%Y\")\n\n#plot unemployment rate \n#plot interest rate \nfig <- plot_ly(unemployment_rate, x = ~Date, y = ~Value, type = 'scatter', mode = 'lines',line = list(color = 'rgb(235, 231, 115)'))\nfig <- fig %>% layout(title = \"U.S Unemployment Rate: January 2010 - March 2023\",xaxis = list(title = \"Time\"),yaxis = list(title =\"Unemployment Rate\"))\nfig\n\n\n\n\n\nThe U.S unemployment rate has experienced significant fluctuations between January 2010 and February 2023. Following the Great Recession of 2008, the unemployment rate peaked in October 2009, but gradually decreased to by January 2010. Throughout the years, the rate has continued to fluctuate, before increasing in April 2020 due to the COVID-19 pandemic. However, as the pandemic-related restrictions were lifted and the economy started to recover, the unemployment rate began to decline. Despite the progress made in recent years, the unemployment rate remains a significant economic indicator and a source of concern for policymakers, as persistent unemployment can have long-term effects on the economy and the well-being of individuals and families."
  },
  {
    "objectID": "dv_sector_market.html",
    "href": "dv_sector_market.html",
    "title": "",
    "section": "",
    "text": "On the stock market, a sector is a group of stocks that are all in the same industry and are very similar to each other. The Global Industrial Classification Standard, which is the most common way to group things, says that there are 11 different stock market sectors (GICS).\n\n\n\n\nName\nSymbol\n\n\n\nConsumer Staples Sector Fund\nXLP\n\n\nUtilities Sector Fund\nXLU\n\n\nHealth Care Sector Fund\nXLV\n\n\nIndustrial Sector Fund\nXLI\n\n\nFinancial Sector Fund\nXLF\n\n\nConsumer Discretionary Sector Fund\nXLY\n\n\nCommunication Services Sector Fund\nXLC\n\n\nReal Estate Sector Fund\nXLRE\n\n\nMaterials Sector Fund\nXLB\n\n\nTechnology Sector Fund\nXLK\n\n\nEnergy Sector Fund\nXLE\n\n\n\n\n\n\n\nView the visualization\n\nThere are 11 parts to the sector market, and each has companies with good stock prices. When we look at the graph, we can see that the Consumer Discretionary sector has higher stock prices than other sectors. This is because there is a chance for high returns, especially when the economy is doing well and consumers are spending a lot. Because they are near the middle of the risk spectrum, stocks in the Financial Sector are worth the least. They can be prone to recessions and are sensitive to changes in interest rates, to name just two major risks. But like most other kinds of businesses, the risk of bank stocks can vary a lot from one company to the next. We can also guess that most sector prices have been going up since 2021, which could be because of post covid."
  },
  {
    "objectID": "dv_stock_indices.html",
    "href": "dv_stock_indices.html",
    "title": "",
    "section": "",
    "text": "Stock market indexes all over the world are good measures of both the world economy and the economies of individual countries. In the United States, the S&P 500, the Dow Jones Industrial Average, and the Nasdaq Composite get the most attention from investors and the media. There are more than just these three indexes that make up the U.S. stock market. About 5,000 more are there.\nWith so many indexes, the U.S. market has many ways to classify things and methods that can be used for many different things. Most of the time, the news tells us several times a day how the top three indexes are going, using important news stories to show how they are going up or down. Investment managers use indexes to measure how well an investment is doing.\nIndexes are used by all types of investors as proxies for performance and guides for how to put their money to work. Indexes are also the basis for passive index investing, which is usually done through exchange-traded funds that track indexes. Overall, knowing how market indexes are made and how they are used can make many different types of investing easier to understand.\n\nCodeoptions(\"getSymbols.warning4.0\"=FALSE)\noptions(\"getSymbols.yahoo.warning\"=FALSE)\n\n#America's top stock market index\ntickers = c(\"^GSPC\",\"^DJI\",\"^IXIC\")\n\nfor (i in tickers){\n  getSymbols(i,\n             from = \"2000-01-01\",\n             to = \"2023-01-01\")}\n\nx <- list(\n  title = \"date\"\n)\ny <- list(\n  title = \"value\"\n)\n\n#create dataframe\ndv_america_stock_index_data = cbind(GSPC,DJI,IXIC)\ndv_america_stock_index_data = as.data.frame(dv_america_stock_index_data)\n#export it to csv file\nwrite_csv(dv_america_stock_index_data, \"DATA/RAW DATA/dv_america_stock_index_data.csv\")\n\nstock <- data.frame(GSPC$GSPC.Adjusted,\n                    DJI$DJI.Adjusted,\n                    IXIC$IXIC.Adjusted)\n\n\n\nstock <- data.frame(stock,rownames(stock))\ncolnames(stock) <- append(tickers,'Dates')\n\nstock$date<-as.Date(stock$Dates,\"%Y-%m-%d\")\ncolnames(stock)=c(\"GSPC\",\"DJI\",\"IXIC\",\"Dates\",\"date\")\n\n\n#remove columns\nstock <- stock[,-c(4)]\n\ng1<- ggplot(stock, aes(x=date)) +\n  geom_line(aes(y=GSPC, colour=\"GSPC\"))+\n  geom_line(aes(y=DJI, colour=\"DJI\"))+\n  geom_line(aes(y=IXIC, colour=\"IXIC\"))+\n  scale_color_brewer(palette=\"Greens\")+\n  theme_bw()+\n   labs(\n    title = \"America's Top 3 Stock Market Index History\",\n    subtitle = \"From Jan 2000-Jan 2023\",\n    x = \"Date\",\n    y = \"Adjusted Closing Prices\")+\n    guides(colour=guide_legend(title=\"Indices\")) \n\nplot = ggplotly(g1)%>%\n  layout(title = list(text = paste0(\"America's Top 3 Stock Market Index History\",\n                                    '<br>',\n                                    '<sup>',\n                                    'From Jan 2000-Jan 2023',\n                                    '</sup>')))\nggplotly(plot)%>%layout(hovermode = \"x\")\n\n\n\n\n\nEach of the indices’ stock prices tend to have a upward trend. This is because the companies in each of the indices are growing. Compared to S&P 500 and NASDAQ, the Dow Jones index has the largest share. This could be because of the companies in each index and how many companies are in each. The effect of covud on all businesses causes stock prices to drop at the beginning of 2020.\n\nThe Dow Jones Industrial Average (DJIA) is one of the oldest, best-known, and most-used indexes in the world. It has the shares of 30 of the biggest and most powerful companies in the US.The DJIA is an index based on prices. At first, it was made by adding up the price per share of each company’s stock in the index and dividing by the number of companies. The index is no longer this easy to figure out, though. Over time, stock splits, spin-offs, and other things have changed the divisor, which is a number that Dow Jones uses to figure out the level of the DJIA. This has made the divisor a very small number.\nAbout a quarter of the value of the whole U.S. stock market is represented by the DJIA, but a percent change in the Dow is not a sure sign that the whole market has dropped by the same percent. When the Dow goes up or down, it shows how investors feel about the earnings and risks of the big companies in the index. Because the way people feel about large-cap stocks is often different from how they feel about small-cap stocks, international stocks, or technology stocks, the Dow shouldn’t be used to show how people feel about other types of stocks in the market.\nIn general, the Dow is known for having a list of the best blue-chip companies on the U.S. market that pay regular dividends. So, it doesn’t have to be a reflection of the whole market, but it can be a reflection of the market for blue-chip, dividend-value stocks.\n\n\n\n\n\nCompany\nSymbol\nWeight\n\n\n\nUnitedHealth Group Incorporated\nUNH\n9.531581\n\n\nGoldman Sachs Group, Inc.\nGS\n7.240363\n\n\nHome Depot Inc.\nHD\n6.282804\n\n\nMcDonald’s Corporation\nMCD\n5.199097\n\n\nMicrosoft Corporation\nMSFT\n5.127123\n\n\nCaterpillar Inc.\nCAT\n4.821433\n\n\nAmgen Inc.\nAMGN\n4.580870\n\n\nVisa Inc. Class A\nV\n4.416778\n\n\nBoeing Company\nBA\n4.150398\n\n\nHoneywell International Inc.\nHON\n3.899078\n\n\n\n\n\n\nCodeoptions(\"getSymbols.warning4.0\"=FALSE)\noptions(\"getSymbols.yahoo.warning\"=FALSE)\n\ntickers = c(\"UNH\",\"GS\",\"HD\", \"MCD\",\"MSFT\",\"CAT\",\"AMGN\",\"V\",\"BA\",\"HON\")\nfor (i in tickers){\n  getSymbols(i,\n             from = \"2012-01-01\",\n             to = \"2023-01-01\")}\n\nx <- list(\n  title = \"date\"\n)\ny <- list(\n  title = \"value\"\n)\n\nstock <- data.frame(UNH$UNH.Adjusted,\n                    GS$GS.Adjusted,\n                    HD$HD.Adjusted,\n                    MCD$MCD.Adjusted,\n                    MSFT$MSFT.Adjusted,\n                    CAT$CAT.Adjusted,\n                    AMGN$AMGN.Adjusted,\n                    V$V.Adjusted,\n                    HON$HON.Adjusted,\n                    BA$BA.Adjusted)\n\nstock <- data.frame(stock,rownames(stock))\ncolnames(stock) <- append(tickers,'Dates')\n\nstock$date<-as.Date(stock$Dates,\"%Y-%m-%d\")\n\n\ng1<- ggplot(stock, aes(x=date)) +\n  geom_line(aes(y=UNH, colour=\"UNH\"))+\n  geom_line(aes(y=GS, colour=\"GS\"))+\n  geom_line(aes(y=HD, colour=\"HD\"))+\n  geom_line(aes(y=MCD, colour=\"MCD\"))+\n  geom_line(aes(y=MSFT, colour=\"MSFT\"))+\n  geom_line(aes(y=CAT, colour=\"CAT\"))+\n  geom_line(aes(y=AMGN, colour=\"AMGN\"))+\n  geom_line(aes(y=V, colour=\"V\"))+\n  geom_line(aes(y=HON, colour=\"HON\"))+\n  geom_line(aes(y=BA, colour=\"BA\"))+\n  scale_color_brewer(palette=\"OrRd\")+\n  theme_bw()+\n   labs(\n    title = \"Stock Prices for the Top 10 Dow Jones Companies\",\n    subtitle = \"From Jan 2012-Jan 2023\",\n    x = \"Date\",\n    y = \"Adjusted Closing Prices\")+\n    guides(colour=guide_legend(title=\"Companies\")) \n\nplot = ggplotly(g1)%>%\n  layout(title = list(text = paste0(\"Stock Prices for the Top 10 Dow Jones Companies\",\n                                    '<br>',\n                                    '<sup>',\n                                    'From Jan 2012-Jan 2023',\n                                    '</sup>')))\nggplotly(plot)%>%layout(hovermode = \"x\")\n\n\n\n\n\nHere, the top 10 companies in the Dow Jones Index are shown as a time series. The companies are sorted by how much they make up the index. As it is clear that all the companies tend to go up, we can see that UNH has the biggest share of stock prices compared to the other companies. There has been a drop in the price of Home Depot Inc.’s stock, which was one of the best-performing stocks from 2018 to 2020. However, the drop may have been caused more by macro conditions and negative sentiment than by problems with the company itself or investors’ worries about a slowdown in the home improvement market. Because of the effect of covid, the price of UNH stock is going up. People started investing in health insurance, but the price has gone up and down because of this. Goldman Sachs Group, Inc. has a good stock price, but the price has gone up and down because of the pandemic in the fourth quarter. This was caused by weakness in investment banking and asset management, as well as a large loss in the unit that includes its consumer banking business.\n\nMost investors know that the Nasdaq is where tech stocks trade. The Nasdaq Composite Index is a list of all the stocks that are traded on the Nasdaq stock exchange. It is based on how much each stock is worth on the market. Some of the companies in this index are not from the U.S. People know that this index has a lot of tech companies in it. It has things from the tech market like software, biotech, semiconductors, and more.\nThere are a lot of technology stocks in this index, but there are also stocks from other industries. Investors can also buy securities from a wide range of industries, such as financials, industrials, insurance, transportation, and others.\nThere are both big and small companies in the Nasdaq Composite. However, unlike the Dow and the S&P 500, it also has a lot of small, risky companies. So, its movement is usually a good sign of how well the technology industry is doing and how investors feel about riskier stocks.\n\n\n\n\n\nCompany\nSymbol\nWeight\n\n\n\nApple Inc\nAAPL\n12.230\n\n\nMicrosoft Corp\nMSFT\n12.101\n\n\nAmazon.com Inc\nAMZN\n6.226\n\n\nNVIDIA Corp\nNVDA\n4.366\n\n\nTesla Inc\nTSLA\n3.967\n\n\nAlphabet Inc\nGOOG\n3.625\n\n\nAlphabet Inc\nGOOGL\n3.616\n\n\nMeta Platforms Inc\nMETA\n3.128\n\n\nBroadcom Inc\nAVGO\n1.962\n\n\nPepsiCo Inc\nPEP\n1.951\n\n\n\n\n\n\nCodeoptions(\"getSymbols.warning4.0\"=FALSE)\noptions(\"getSymbols.yahoo.warning\"=FALSE)\n\ntickers = c(\"AAPL\",\"MSFT\",\"AMZN\", \"NVDA\",\"TSLA\",\"GOOG\",\"PEP\",\"GOOGL\",\"META\",\"AVGO\")\nfor (i in tickers){\n  getSymbols(i,\n             from = \"2015-01-01\",\n             to = \"2023-01-01\")}\n\nx <- list(\n  title = \"date\"\n)\ny <- list(\n  title = \"value\"\n)\n\nstock <- data.frame(AAPL$AAPL.Adjusted,\n                    MSFT$MSFT.Adjusted,\n                    AMZN$AMZN.Adjusted,\n                    NVDA$NVDA.Adjusted,\n                    TSLA$TSLA.Adjusted,\n                    GOOG$GOOG.Adjusted,\n                    GOOGL$GOOGL.Adjusted,\n                    META$META.Adjusted,\n                    PEP$PEP.Adjusted,\n                    AVGO$AVGO.Adjusted)\n\nstock <- data.frame(stock,rownames(stock))\ncolnames(stock) <- append(tickers,'Dates')\n\nstock$date<-as.Date(stock$Dates,\"%Y-%m-%d\")\n\n\ng2<- ggplot(stock, aes(x=date)) +\n  geom_line(aes(y=AAPL, colour=\"AAPL\"))+\n  geom_line(aes(y=MSFT, colour=\"MSFT\"))+\n  geom_line(aes(y=AMZN, colour=\"AMZN\"))+\n  geom_line(aes(y=NVDA, colour=\"NVDA\"))+\n  geom_line(aes(y=TSLA, colour=\"TSLA\"))+\n  geom_line(aes(y=GOOG, colour=\"GOOG\"))+\n  geom_line(aes(y=GOOGL, colour=\"GOOGL\"))+\n  geom_line(aes(y=META, colour=\"META\"))+\n  geom_line(aes(y=PEP, colour=\"PEP\"))+\n  geom_line(aes(y=AVGO, colour=\"AVGO\"))+\n  scale_color_brewer(palette=\"PuRd\")+\n  theme_bw()+\n   labs(\n    title = \"Stock Prices for the Top 10 NASDAQ Companies\",\n    subtitle = \"From Jan 2015-Jan 2023\",\n    x = \"Date\",\n    y = \"Adjusted Closing Prices\")+\n    guides(colour=guide_legend(title=\"Companies\")) \n\nplot = ggplotly(g2)%>%\n  layout(title = list(text = paste0(\"Stock Prices for the Top 10 NASDAQ Companies\",\n                                    '<br>',\n                                    '<sup>',\n                                    'From Jan 2015-Jan 2023',\n                                    '</sup>')))\nggplotly(plot)%>%layout(hovermode = \"x\")\n\n\n\n\n\nThe weights of the top 10 companies in the NASDAQ Index are used to filter the time series shown here. Broadcom Inc.’s stock price is high compared to others, but it goes up and down a lot. The price drop is mostly due to the fact that the company’s earnings growth will slow in fiscal 2019, while the price rise is due to its designs for data centers and networking. When you look at other companies, you can see that they all follow the same pattern. Most of them see a drop in their stock prices at the beginning of 2020, which is because of the covid. Google’s stock price has gone down recently, which is because its AI chatbot, Bard, gave a wrong answer.\n\nThe Standard & Poor’s 500 Index, or S&P 500, is a list of the 500 best companies in the United States. Stocks are chosen for the index based on their market capitalization, but the constituent committee also looks at their liquidity, public float, sector classification, financial stability, and trading history.\nThe S&P 500 Index contains about 80% of the total value of the U.S. stock market. The S&P 500 Index is a good way to get a general idea of how the whole U.S. market is doing. Most indexes are based on what something is worth on the market. The S&P 500 Index is the market-weighted index (also referred to as capitalization-weighted).\nSo, the weight of each stock in the index is the same as its total market capitalization. In other words, the value of the index falls by 10% if the market value of all 500 companies in the S&P 500 falls by 10%.\n\n\n\n\n\nCompany\nSymbol\nWeight\n\n\n\nApple Inc.\nAAPL\n6.711304\n\n\nMicrosoft Corporation\nMSFT\n5.705910\n\n\nAmazon.com Inc.\nAMZN\n2.543539\n\n\nAlphabet Inc. Class A\nGOOGL\n1.665711\n\n\nBerkshire Hathaway Inc. Class B\nBRK.B\n1.621257\n\n\nNVIDIA Corporation\nNVDA\n1.601427\n\n\nTesla Inc\nTSLA\n1.583414\n\n\nAlphabet Inc. Class C\nGOOG\n1.480755\n\n\nExxon Mobil Corporation\nXOM\n1.391616\n\n\nUnitedHealth Group Incorporated\nUNH\n1.329559\n\n\n\n\n\n\nCodeoptions(\"getSymbols.warning4.0\"=FALSE)\noptions(\"getSymbols.yahoo.warning\"=FALSE)\n\ntickers = c(\"AAPL\",\"MSFT\",\"AMZN\", \"NVDA\",\"TSLA\",\"GOOGL\",\"GOOG\",\"BRK\",\"UNH\",\"XOM\")\nfor (i in tickers){\n  getSymbols(i,\n             from = \"2015-01-01\",\n             to = \"2023-01-01\")}\n\nx <- list(\n  title = \"date\"\n)\ny <- list(\n  title = \"value\"\n)\n\nstock <- data.frame(AAPL$AAPL.Adjusted,\n                    MSFT$MSFT.Adjusted,\n                    AMZN$AMZN.Adjusted,\n                    NVDA$NVDA.Adjusted,\n                    TSLA$TSLA.Adjusted,\n                    GOOG$GOOG.Adjusted,\n                    GOOGL$GOOGL.Adjusted,\n                    BRK$BRK.Adjusted,\n                    UNH$UNH.Adjusted,\n                    XOM$XOM.Adjusted)\n\nstock <- data.frame(stock,rownames(stock))\ncolnames(stock) <- append(tickers,'Dates')\n\nstock$date<-as.Date(stock$Dates,\"%Y-%m-%d\")\n\n\ng3<- ggplot(stock, aes(x=date)) +\n  geom_line(aes(y=AAPL, colour=\"AAPL\"))+\n  geom_line(aes(y=MSFT, colour=\"MSFT\"))+\n  geom_line(aes(y=AMZN, colour=\"AMZN\"))+\n  geom_line(aes(y=NVDA, colour=\"NVDA\"))+\n  geom_line(aes(y=TSLA, colour=\"TSLA\"))+\n  geom_line(aes(y=GOOG, colour=\"GOOG\"))+\n  geom_line(aes(y=GOOGL, colour=\"GOOGL\"))+\n  geom_line(aes(y=BRK, colour=\"BRK\"))+\n  geom_line(aes(y=UNH, colour=\"UNH\"))+\n  geom_line(aes(y=XOM, colour=\"XOM\"))+\n  scale_color_brewer(palette=\"GnBu\")+\n  theme_bw()+\n   labs(\n    title = \"Stock Prices for the Top 10 S&P 500 Companies\",\n    subtitle = \"From Jan 2015-Jan 2023\",\n    x = \"Date\",\n    y = \"Adjusted Closing Prices\")+\n    guides(colour=guide_legend(title=\"Companies\")) \n\nplot = ggplotly(g3)%>%\n  layout(title = list(text = paste0(\"Stock Prices for the Top 10 S&P 500 Companies\",\n                                    '<br>',\n                                    '<sup>',\n                                    'From Jan 2015-Jan 2023',\n                                    '</sup>')))\nggplotly(plot)%>%layout(hovermode = \"x\")\n\n\n\n\n\nThe time series shown here is filtered by the weights of the top 10 companies in the S&P 500 Index. How much each company makes up the index is used to sort the companies. Since it’s clear that all the companies’ stock prices tend to go up, we can see that UNH has the most stock prices compared to the other companies. Most of their stock prices will go down at the start of 2020 because of the covid. Recently, Google’s stock price went down because its artificial intelligence chatbot, Bard, gave the wrong answer. The price of TESLA stock has been going down since early 2022. This is because investors worry that CEO Elon Musk is too busy with his plan to take over Twitter."
  },
  {
    "objectID": "dv.html",
    "href": "dv.html",
    "title": "",
    "section": "",
    "text": "Data visualization plays a crucial role in understanding and analyzing complex financial data, such as stock market indices. In the United States, the stock market is represented by various indices, including the widely known S&P 500, Dow Jones Industrial Average (DJIA), and Nasdaq Composite. These indices are used as indicators of the overall health and performance of the U.S. economy and are closely monitored by investors, financial analysts, and the media.\nThis data visualization focuses on three prominent U.S. stock market indices(DJI, S&P 500 and Nasdaq Composite) and provides insights into the top companies within these indices based on their weightage. The DJI, consisting of 30 large and established companies, is known for its historical significance and representation of blue-chip stocks. S&P 500 is known for its diverse representation of the U.S. stock market, encompassing 500 of the largest publicly traded companies across various industries. As a market-capitalization-weighted index, the S&P 500 reflects changes in the stock prices of these companies, making it a widely recognized benchmark for measuring the performance of the U.S. stock market. On the other hand, the Nasdaq Composite is known for its focus on technology companies, but also includes stocks from other industries.\n\nBy visualizing the stock prices of the top companies within these indices over time, we can gain valuable insights into their trends, performance, and the impact of external factors such as the COVID-19 pandemic. This analysis can be useful for investors, financial professionals, and anyone interested in understanding the dynamics of the U.S. stock market and the performance of top companies within these indices.\nClick to view the data visualizations\n\n\nThe stock market is comprised of various sectors, which are groups of companies operating in the same industry and sharing similar characteristics. The Global Industry Classification Standard (GICS) is a widely used framework that classifies stocks into 11 different sectors, including Consumer Discretionary, Consumer Staples, Energy, Financials, Health Care, Industrials, Information Technology, Materials, Real Estate, Telecommunication Services, and Utilities.\n\nData visualization of the sector market stocks provides a valuable tool for investors, financial analysts, and decision-makers to gain insights into the performance of different sectors within the stock market. By visually representing stock prices, trends, and performance of sectors over time, data visualization can help identify investment opportunities, assess risks, and make informed decisions.\nClick to view the data visualizations\n\n\n\nMacroeconomic factors play a crucial role in shaping the overall health and performance of an economy. These factors, including gross domestic product (GDP) growth rate, interest rate, inflation rate, and others, have a significant impact on businesses, consumers, and policymakers. Monitoring and understanding these macroeconomic factors is essential for making informed decisions, formulating economic policies, and assessing the overall health of an economy.\n\nVisualizing macroeconomic factors can provide valuable insights and help stakeholders better understand the trends, patterns, and relationships between these factors. Through visualizations, complex data can be presented in a clear and concise manner, allowing for easier interpretation and analysis. Visualizations can highlight historical trends, identify potential correlations, and provide a visual context for understanding the impact of policy changes or external events on macroeconomic factors.\nIn this analysis, we will use visualizations to explore and analyze key macroeconomic factors, including GDP growth rate, interest rate, and inflation rate in the United States from 2010 to present. Through these visualizations, we aim to provide a comprehensive overview of the trends and dynamics of these macroeconomic factors over time, highlighting important events or shifts that may have influenced their trajectories. By visualizing these macroeconomic factors, we can gain a deeper understanding of their impact on the U.S. economy and make informed assessments about their future implications.\nClick to view the data visualizations"
  },
  {
    "objectID": "eda_index_dow_jones.html",
    "href": "eda_index_dow_jones.html",
    "title": "EDA for Dow Jones Index",
    "section": "",
    "text": "Exploratory Data Analysis (EDA) for the Dow Jones Index involves analyzing time series data to identify the underlying patterns and characteristics of the index. The Dow Jones is a stock market index that measures the performance of 30 large companies listed on the US stock exchanges. EDA for the Dow Jones typically involves analyzing the daily closing prices of the index and examining key aspects such as autocorrelation, seasonality, trend, and stationarity. This information can be used to identify potential patterns and trends in the data, inform our modeling approach, and potentially improve our investment strategies.\n\nTime Series Plot\n\n\nCode\n# get data\noptions(\"getSymbols.warning4.0\"=FALSE)\noptions(\"getSymbols.yahoo.warning\"=FALSE)\n\n\ndata = getSymbols(\"^DJI\",src='yahoo', from = '2010-01-01',to = \"2023-03-01\")\n\ndf <- data.frame(Date=index(DJI),coredata(DJI))\n\n# create Bollinger Bands\nbbands <- BBands(DJI[,c(\"DJI.High\",\"DJI.Low\",\"DJI.Close\")])\n\n# join and subset data\ndf <- subset(cbind(df, data.frame(bbands[,1:3])), Date >= \"2010-01-01\")\n\n#export the data \ndji_data <- df\nwrite.csv(dji_data, \"DATA/CLEANED DATA/dji_raw_data.csv\", row.names=FALSE)\n\n# colors column for increasing and decreasing\nfor (i in 1:length(df[,1])) {\n  if (df$DJI.Close[i] >= df$DJI.Open[i]) {\n      df$direction[i] = 'Increasing'\n  } else {\n      df$direction[i] = 'Decreasing'\n  }\n}\n\ni <- list(line = list(color = '#6F9860'))\nd <- list(line = list(color = '#7F7F7F'))\n\n# plot candlestick chart\n\nfig <- df %>% plot_ly(x = ~Date, type=\"candlestick\",\n          open = ~DJI.Open, close = ~DJI.Close,\n          high = ~DJI.High, low = ~DJI.Low, name = \"DJI\",\n          increasing = i, decreasing = d) \nfig <- fig %>% add_lines(x = ~Date, y = ~up , name = \"B Bands\",\n            line = list(color = '#ccc', width = 0.5),\n            legendgroup = \"Bollinger Bands\",\n            hoverinfo = \"none\", inherit = F) \nfig <- fig %>% add_lines(x = ~Date, y = ~dn, name = \"B Bands\",\n            line = list(color = '#ccc', width = 0.5),\n            legendgroup = \"Bollinger Bands\", inherit = F,\n            showlegend = FALSE, hoverinfo = \"none\") \nfig <- fig %>% add_lines(x = ~Date, y = ~mavg, name = \"Mv Avg\",\n            line = list(color = '#E377C2', width = 0.5),\n            hoverinfo = \"none\", inherit = F) \nfig <- fig %>% layout(yaxis = list(title = \"Price\"))\n\n# plot volume bar chart\nfig2 <- df \nfig2 <- fig2 %>% plot_ly(x=~Date, y=~DJI.Volume, type='bar', name = \"DJI Volume\",\n          color = ~direction, colors = c('#6F9860','#7F7F7F')) \nfig2 <- fig2 %>% layout(yaxis = list(title = \"Volume\"))\n\n# create rangeselector buttons\nrs <- list(visible = TRUE, x = 0.5, y = -0.055,\n           xanchor = 'center', yref = 'paper',\n           font = list(size = 9),\n           buttons = list(\n             list(count=1,\n                  label='RESET',\n                  step='all'),\n             list(count=3,\n                  label='3 YR',\n                  step='year',\n                  stepmode='backward'),\n             list(count=1,\n                  label='1 YR',\n                  step='year',\n                  stepmode='backward'),\n             list(count=1,\n                  label='1 MO',\n                  step='month',\n                  stepmode='backward')\n           ))\n\n# subplot with shared x axis\nfig <- subplot(fig, fig2, heights = c(0.7,0.2), nrows=2,\n             shareX = TRUE, titleY = TRUE)\nfig <- fig %>% layout(title = paste(\"DOW Jones Index Stock Price: January 2010 - March 2023\"),\n         xaxis = list(rangeselector = rs),\n         legend = list(orientation = 'h', x = 0.5, y = 1,\n                       xanchor = 'center', yref = 'paper',\n                       font = list(size = 10),\n                       bgcolor = 'transparent'))\n\nfig\n\n\n\n\n\n\nThe Dow Jones Industrial Average (DJIA) stock prices from 2010 to March 2023 have displayed a volatile trend, reflecting the changes in the US stock market during this period. The DJIA witnessed a steady growth in the early 2010s, achieving new all-time highs by 2013. However, it experienced a sharp correction in the second half of 2015 and early 2016, followed by a quick recovery. The DJIA hit new highs in 2018 and 2019, only to be impacted by the COVID-19 pandemic in 2020, resulting in a significant decline followed by a quick recovery aided by government stimulus measures. The DJIA reached a new all-time high in May 2021.\nInflation has played a crucial role in shaping the DJIA stock prices during this period. The US experienced moderate inflation rates during the early 2010s, and inflation remained subdued until the COVID-19 pandemic hit in 2020, causing significant disruptions to supply chains, resulting in higher prices for goods and services. Inflation rates surged in 2021, leading to concerns about its impact on the economy and the stock market. However, the Federal Reserve has indicated that the current inflationary pressures are transitory.\nFor stock prices, a multiplicative decomposition is typically preferred because the percentage changes in stock prices tend to be more important than the absolute changes. For example, a $1 increase in a $10 stock price is more significant than a $1 increase in a $100 stock price. Therefore, the relative changes in stock prices are more relevant than the absolute changes. Additionally, stock prices tend to exhibit non-constant variance, meaning that the variance of the series changes over time. A multiplicative decomposition can handle this non-constant variance more effectively than an additive decomposition.\n\n\nDecomposed Time Series\n\nDecomposition PlotAdjusted Decomposition Plot\n\n\n\n\nCode\n#time series data\nmyts<-ts(df$DJI.Adjusted,frequency=252,start=c(2010,1,1)) \n#original plot for time series data\norginial_plot <- autoplot(myts,xlab =\"Year\", ylab = \"Adjusted Closing Price\", main = \"DOW Jones Index Stock price: Jan 2010 - March 2023\")\n#decompose the data\ndecompose = decompose(myts, \"multiplicative\")\n#decomposition plot\nautoplot(decompose)\n\n\n\n\n\n\n\n\n\nCode\n#adjusted plot\ntrendadj <- myts/decompose$trend\ndecompose_adjtrend_plot <- autoplot(trendadj,ylab='trend') +ggtitle('Adjusted trend component in the multiplicative time series model')\nseasonaladj <- myts/decompose$seasonal\ndecompose_adjseasonal_plot <- autoplot(seasonaladj,ylab='seasonal') +ggtitle('Adjusted seasonal component in the multiplicative time series model')\ngrid.arrange(orginial_plot, decompose_adjtrend_plot,decompose_adjseasonal_plot, nrow=3)\n\n\n\n\n\n\n\n\nThe adjusted seasonal component tend to have upward trend and there is more variability in the model when compared to the original plot where the variation during the years but the adjusted trend then to have more fluctuation showing no trend when compared to the original plot.\n\n\nLag Plots\n\nDaily Time LagsMonthly Time Lags\n\n\n\n\nCode\n#Lag plots \ngglagplot(myts, do.lines=FALSE, lags=1)+xlab(\"Lag 1\")+ylab(\"Yi\")+ggtitle(\"Lag Plot for DOW Jones Index Stock Jan 2010 - March 2023\")\n\n\n\n\n\n\n\n\n\nCode\n#montly data\nmean_data <- df %>% \n  mutate(month = month(Date), year = year(Date)) %>% \n  group_by(year, month) %>% \n  summarize(mean_value = mean(DJI.Adjusted))\nmonth<-ts(mean_data$mean_value,star=decimal_date(as.Date(\"2010-01-01\",format = \"%Y-%m-%d\")),frequency = 12)\n#Lag plot\nts_lags(month)\n\n\n\n\n\n\n\n\n\nLag Plot for DOW Jones Index Stock Jan 2010 - March 2023, there should be a strong link between the series and the related lag as there are positive correlation and inclined to 45 degree.This is the lag plot signature of a process with strong positive autocorrelation. Such processes are highly non-random, there is strong association between an observation and a succeeding observation. Additionally, seasonality can be examined by plotting observations for a larger number of time periods i.e. the lags. Using the mean function, the time series data is aggregated to monthly data for better understanding of the series and for the clearer plots. Observing the last graph closely reveals that more dots are on to the diagonal line at 45 degrees.the second graph indicates the monthly of the variable on the vertical axis. The lines connect points in chronological order. This suggest that there is strong association between an observation and a succeeding observation.\n\n\nSeasonality\n\nSeasonal HeatmapSeasonal Line plot\n\n\n\n\nCode\n# Create seasonal plot\nts_heatmap(month, color = \"PuRd\", title = 'Seasonality Heatmap of DOW Jones Index Stock Jan 2010 - March 2023')\n\n\n\n\n\n\n\n\n\n\nCode\n# Create a line graph for each year with months on the x-axis\nggseasonplot(month, datecol = \"date\", valuecol = \"value\")+ggtitle(\"Seasonal Yearly Plot for DOW Jones Index Stock Jan 2010 - March 2023\")\n\n\n\n\n\n\n\n\nThe Seasonality Heatmap for the DOW Jones Index Stock Jan 2010 - March 2023 does not reveal any clear seasonality in the data. The heatmap shows the mean value of the time series for each month and year combination, with the darker colors indicating higher values. The lack of clear patterns or darker colors in specific months or years suggests that there is no consistent seasonal pattern in the data. However, the yearly line graph shows a slight upward trend in the stock price from 2010 to 2023, but does not show any clear seasonality. Each year’s data is represented by a line, and the months are plotted on the x-axis. Overall, the lack of clear seasonality in both the heatmap and yearly line graph suggests that other factors beyond seasonality are driving the stock price fluctuations.\n\n\nMoving Average\n\n4 Month MA1 Year MA3 Year MA5 Year MA\n\n\n\n\nCode\n#SMA Smoothing \nma <- autoplot(month, series=\"Data\") +\n  autolayer(ma(month,5), series=\"4 Month MA\") +\n  xlab(\"Year\") + ylab(\"GWh\") +\n  ggtitle(\"DOW Jones Index Stock Jan 2010 - March 2023(4 Month Moving Average)\") +\n  scale_colour_manual(values=c(\"Data\"=\"grey50\",\"4 Month MA\"=\"red\"),\n                      breaks=c(\"Data\",\"4 Month MA\"))\nma\n\n\n\n\n\n\n\n\n\nCode\n#SMA Smoothing \nma <- autoplot(month, series=\"Data\") +\n  autolayer(ma(month,13), series=\"1 Year MA\") +\n  xlab(\"Year\") + ylab(\"GWh\") +\n  ggtitle(\"DOW Jones Index Stock Jan 2010 - March 2023(1 Year Moving Average)\") +\n  scale_colour_manual(values=c(\"Data\"=\"grey50\",\"1 Year MA\"=\"red\"),\n                      breaks=c(\"Data\",\"1 Year MA\"))\nma\n\n\n\n\n\n\n\n\n\nCode\n#SMA Smoothing \nma <- autoplot(month, series=\"Data\") +\n  autolayer(ma(month,37), series=\"3 Year MA\") +\n  xlab(\"Year\") + ylab(\"GWh\") +\n  ggtitle(\"DOW Jones Index Stock Jan 2010 - March 2023(3 Year Moving Average)\") +\n  scale_colour_manual(values=c(\"Data\"=\"grey50\",\"3 Year MA\"=\"red\"),\n                      breaks=c(\"Data\",\"3 Year MA\"))\nma\n\n\n\n\n\n\n\n\n\nCode\n#SMA Smoothing \nma <- autoplot(month, series=\"Data\") +\n  autolayer(ma(month,61), series=\"5 Year MA\") +\n  xlab(\"Year\") + ylab(\"GWh\") +\n  ggtitle(\"DOW Jones Index Stock Jan 2010 - March 2023(5 Year Moving Average)\") +\n  scale_colour_manual(values=c(\"Data\"=\"grey50\",\"5 Year MA\"=\"red\"),\n                      breaks=c(\"Data\",\"5 Year MA\"))\nma\n\n\n\n\n\n\n\n\nThe four plots show the DOW Jones Index stock prices from January 2010 to March 2023, along with the moving averages for 4 months, 1 year, 3 years and 5 years. As the window of the moving average increases, the smoother the trend line becomes, reducing the impact of noise and fluctuations in the original time series.\nThe 4-month moving average plot shows frequent fluctuations in the stock price, with the trend line following the general direction of the time series. The 1-year moving average plot shows a smoother trend, following the overall upward trend of the stock price.\nThe 3-year moving average plot shows a similar trend to the 1-year plot but is even smoother, with fewer fluctuations. Finally, the 5-year moving average plot shows the smoothest trend, with an almost constant upward slope.As the moving average window increases, the smoother trend allows for a clearer identification of the general trend of the DOW Jones Index stock prices over time. From the moving average obtained above we can see that there is upward tend in the stock price of DOW Jones Index.\n\n\nAutocorrelation Time Series\n\nACFPACFADF Test\n\n\n\n\nCode\n#ACF for  data\nggAcf(month)+ggtitle(\"ACF Plot for DOW Jones Index Stock Jan 2010 - March 2023\")\n\n\n\n\n\n\n\n\n\nCode\n#PACF for data\nggPacf(month)+ggtitle(\"PACF Plot for DOW Jones Index Stock Jan 2010 - March 2023\")\n\n\n\n\n\n\n\n\n\nCode\n#check the stationarity\ntseries::adf.test(month)\n\n\n\n    Augmented Dickey-Fuller Test\n\ndata:  month\nDickey-Fuller = -2.8227, Lag order = 5, p-value = 0.2333\nalternative hypothesis: stationary\n\n\n\n\n\nIn the plot of autocorrelation function, which is the acf graph for monthly data, there are clear autocorrelation in lag. The above lag plots and autocorrelation plot indicates seasonality in the series, which means the series is not stationary.. It was also verified using Augmented Dickey-Fuller Test which tells us that as the p value is greater than 0.05, the series is not stationary.\n\n\nDetrend and Differenced Time Series\n\nLinear Fitting ModelACF Plot\n\n\n\n\nCode\nfit = lm(myts~time(myts), na.action=NULL) \nsummary(fit) \n\n\n\nCall:\nlm(formula = myts ~ time(myts), na.action = NULL)\n\nResiduals:\n    Min      1Q  Median      3Q     Max \n-9403.6  -926.2     6.2   952.0  5318.6 \n\nCoefficients:\n              Estimate Std. Error t value Pr(>|t|)    \n(Intercept) -3.907e+06  1.716e+04  -227.6   <2e-16 ***\ntime(myts)   1.948e+03  8.511e+00   228.8   <2e-16 ***\n---\nSignif. codes:  0 '***' 0.001 '**' 0.01 '*' 0.05 '.' 0.1 ' ' 1\n\nResidual standard error: 1858 on 3309 degrees of freedom\nMultiple R-squared:  0.9406,    Adjusted R-squared:  0.9405 \nF-statistic: 5.236e+04 on 1 and 3309 DF,  p-value: < 2.2e-16\n\n\n\n\n\n\nCode\n# plot ACFs\nplot1 <- ggAcf(myts, 48, main=\"Original Data: DOW Jones Index Stock Stock Price\")\nplot2 <- ggAcf(resid(fit), 48, main=\"Detrended data\")\nplot3 <- ggAcf(diff(myts), 48, main=\"First differenced data\")\ngrid.arrange(plot1, plot2, plot3, nrow=3)\n\n\n\n\n\n\n\n\nThe estimated slope coefficient β1, 2.821e+03. With a standard error of 1.233e+01, yielding a significant estimated increase of stock price is very less yearly. Equation of the fit for stationary process: \\[\\hat{y}_{t} = x_{t}+(5.662e+06)-(2.821e+03)t\\]\nFrom the above graph we can say that there is high correlation in the original plot, but in the detrended plot the correlation is reduced but there is still high correlation in the detrended data.But when the first order difference is applied the high correlation is removed but there is no seasonal correlation.\nAs depicted in the above figure, the series is now stationary and ready for future study."
  },
  {
    "objectID": "eda_index_sp500.html",
    "href": "eda_index_sp500.html",
    "title": "EDA for S&P 500 Index",
    "section": "",
    "text": "Exploratory Data Analysis (EDA) for the S&P 500 Index involves analyzing time series data to identify the underlying patterns and characteristics of the index. The S&P 500 is a stock market index that measures the performance of 500 large companies listed on the US stock exchanges. EDA for the S&P 500 typically involves analyzing the daily closing prices of the index and examining key aspects such as autocorrelation, seasonality, trend, and stationarity. This information can be used to identify potential patterns and trends in the data, inform our modeling approach, and potentially improve our investment strategies.\n\nTime Series Plot\n\n\nCode\n# get data\noptions(\"getSymbols.warning4.0\"=FALSE)\noptions(\"getSymbols.yahoo.warning\"=FALSE)\n\n\ndata = getSymbols(\"^GSPC\",src='yahoo',from = '2010-01-01',to = \"2023-03-01\")\n\ndf <- data.frame(Date=index(GSPC),coredata(GSPC))\n\n# create Bollinger Bands\nbbands <- BBands(GSPC[,c(\"GSPC.High\",\"GSPC.Low\",\"GSPC.Close\")])\n\n# join and subset data\ndf <- subset(cbind(df, data.frame(bbands[,1:3])), Date >= \"2010-01-01\")\n\n#export data\nsp_raw_data <- df\nwrite.csv(sp_raw_data, \"DATA/CLEANED DATA/sp500_raw_data.csv\", row.names=FALSE)\n\n# colors column for increasing and decreasing\nfor (i in 1:length(df[,1])) {\n  if (df$GSPC.Close[i] >= df$GSPC.Open[i]) {\n      df$direction[i] = 'Increasing'\n  } else {\n      df$direction[i] = 'Decreasing'\n  }\n}\n\ni <- list(line = list(color = '#EBD168'))\nd <- list(line = list(color = '#7F7F7F'))\n\n# plot candlestick chart\n\nfig <- df %>% plot_ly(x = ~Date, type=\"candlestick\",\n          open = ~GSPC.Open, close = ~GSPC.Close,\n          high = ~GSPC.High, low = ~GSPC.Low, name = \"GSPC\",\n          increasing = i, decreasing = d) \nfig <- fig %>% add_lines(x = ~Date, y = ~up , name = \"B Bands\",\n            line = list(color = '#ccc', width = 0.5),\n            legendgroup = \"Bollinger Bands\",\n            hoverinfo = \"none\", inherit = F) \nfig <- fig %>% add_lines(x = ~Date, y = ~dn, name = \"B Bands\",\n            line = list(color = '#ccc', width = 0.5),\n            legendgroup = \"Bollinger Bands\", inherit = F,\n            showlegend = FALSE, hoverinfo = \"none\") \nfig <- fig %>% add_lines(x = ~Date, y = ~mavg, name = \"Mv Avg\",\n            line = list(color = '#E377C2', width = 0.5),\n            hoverinfo = \"none\", inherit = F) \nfig <- fig %>% layout(yaxis = list(title = \"Price\"))\n\n# plot volume bar chart\nfig2 <- df \nfig2 <- fig2 %>% plot_ly(x=~Date, y=~GSPC.Volume, type='bar', name = \"GSPC Volume\",\n          color = ~direction, colors = c('#EBD168','#7F7F7F')) \nfig2 <- fig2 %>% layout(yaxis = list(title = \"Volume\"))\n\n# create rangeselector buttons\nrs <- list(visible = TRUE, x = 0.5, y = -0.055,\n           xanchor = 'center', yref = 'paper',\n           font = list(size = 9),\n           buttons = list(\n             list(count=1,\n                  label='RESET',\n                  step='all'),\n             list(count=1,\n                  label='1 YR',\n                  step='year',\n                  stepmode='backward'),\n             list(count=3,\n                  label='3 MO',\n                  step='month',\n                  stepmode='backward'),\n             list(count=1,\n                  label='1 MO',\n                  step='month',\n                  stepmode='backward')\n           ))\n\n# subplot with shared x axis\nfig <- subplot(fig, fig2, heights = c(0.7,0.2), nrows=2,\n             shareX = TRUE, titleY = TRUE)\nfig <- fig %>% layout(title = paste(\"S&P 500 Index Stock Price: January 2010 - March 2023\"),\n         xaxis = list(rangeselector = rs),\n         legend = list(orientation = 'h', x = 0.5, y = 1,\n                       xanchor = 'center', yref = 'paper',\n                       font = list(size = 10),\n                       bgcolor = 'transparent'))\n\nfig\n\n\n\n\n\n\nOver the period from January 2010 to March 2023, the S&P 500 index exhibited both upward and downward trends in its stock price. The index started off on a positive note, rising steadily from early 2010 to early 2011. However, it then experienced a significant decline in value, dropping by over 15% by October 2011. From there, the index began a slow but steady climb, reaching new all-time highs by mid-2015.\nThe S&P 500 continued to rise throughout 2016 and into 2017, with a few minor dips along the way. However, it then experienced a sharp drop in value in early 2018, losing over 10% of its value in just a few days. The index then regained some of its losses but continued to fluctuate throughout the remainder of 2018.\nIn 2019, the S&P 500 once again resumed its upward trend, with a few minor dips along the way. However, the COVID-19 pandemic in 2020 caused a significant drop in the index’s value, with the index losing nearly 34% of its value in just a few weeks. However, the index quickly rebounded, aided by government stimulus measures and low-interest rates, and continued its upward trend throughout 2020 and 2021.\nOverall, the S&P 500 index’s stock price exhibited significant fluctuations over the period from January 2010 to March 2023. However, despite the fluctuations, the index exhibited an overall upward trend, reaching new all-time highs multiple times over the period.\nFor stock prices, a multiplicative decomposition is typically preferred because the percentage changes in stock prices tend to be more important than the absolute changes. Additionally, stock prices tend to exhibit non-constant variance, meaning that the variance of the series changes over time. A multiplicative decomposition can handle this non-constant variance more effectively than an additive decomposition.\n\n\nDecomposed Time Series\n\nDecomposition PlotAdjusted Decomposition Plot\n\n\n\n\nCode\n#convert data to ts data\nmyts<-ts(df$GSPC.Adjusted,frequency=252,start=c(2010,1,1)) \norginial_plot <- autoplot(myts,xlab =\"Year\", ylab = \"Adjusted Closing Price\", main = \"S&P 500 Index Stock price: Jan 2010 - March 2023\")\ndecompose = decompose(myts, \"multiplicative\")\nautoplot(decompose)\n\n\n\n\n\n\n\n\n\nCode\n#adjusted decomposition\ntrendadj <- myts/decompose$trend\ndecompose_adjtrend_plot <- autoplot(trendadj,ylab='trend') +ggtitle('Adjusted trend component in the multiplicative time series model')\nseasonaladj <- myts/decompose$seasonal\ndecompose_adjseasonal_plot <- autoplot(seasonaladj,ylab='seasonal') +ggtitle('Adjusted seasonal component in the multiplicative time series model')\ngrid.arrange(orginial_plot, decompose_adjtrend_plot,decompose_adjseasonal_plot, nrow=3)\n\n\n\n\n\n\n\n\nWhen compared to the original figure, the corrected seasonal component has an upward trend and greater variability in the model, but the adjusted trend component has a stable trend through time.\n\n\nLag Plots\n\nDaily Time LagsMonthly Time Lags\n\n\n\n\nCode\n#Lag plots \ngglagplot(myts, do.lines=FALSE, lags=1)+xlab(\"Lag 1\")+ylab(\"Yi\")+ggtitle(\"Lag Plot for S&P 500 Index Stock Jan 2010 - March 2023\")\n\n\n\n\n\n\n\n\n\nCode\n#montly data\nmean_data <- df %>% \n  mutate(month = month(Date), year = year(Date)) %>% \n  group_by(year, month) %>% \n  summarize(mean_value = mean(GSPC.Adjusted))\n\n#ts for month data\nmonth<-ts(mean_data$mean_value,star=decimal_date(as.Date(\"2010-01-01\",format = \"%Y-%m-%d\")),frequency = 12)\n\n#Lag plot for month\nts_lags(month)\n\n\n\n\n\n\n\n\n\nS&P 500 Index Stock Lag Plot As there is a positive correlation and an inclination angle of 45 degrees, there should be a significant link between the series and the relevant lag from January 2010 to March 2023. This is the lag plot hallmark of a process that has a high degree of positive autocorrelation. Such processes are highly non-random, there is a substantial relationship between one observation and the next. Seasonality can also be investigated by plotting observations for a wider number of time periods, i.e. the lags. The time series data is aggregated to monthly data using the mean function for a better comprehension of the series and crisper plots. Further inspection of the last graph reveals that more dots are on the diagonal line at 45 degrees. The second graph shows the monthly variation of the variable on the vertical axis. In chronological order, the lines connect the points. This suggest that there is strong association between an observation and a succeeding observation.\n\n\nSeasonality\n\nSeasonal HeatmapSeasonal Line plot\n\n\n\n\nCode\n# Create seasonal plot\nts_heatmap(month, color = \"YlOrBr\", title = 'Seasonality Heatmap of S&P 500 Index Stock Jan 2010 - March 2023')\n\n\n\n\n\n\n\n\n\n\nCode\n# Create a line graph for each year with months on the x-axis\nggseasonplot(month, datecol = \"date\", valuecol = \"value\")+ggtitle(\"Seasonal Yearly Plot for S&P 500 Index Stock Jan 2010 - March 2023\")\n\n\n\n\n\n\n\n\nThe Seasonality Heatmap for the S&P 500 Index stock from January 2010 to March 2023 shows some evidence of seasonality in the data, although the patterns are not consistent across all years. The heatmap displays the mean value of the time series for each month and year combination, with darker colors indicating higher values. The heatmap reveals that the S&P 500 Index tends to exhibit higher values during the months of December and January in many of the years studied. However, this pattern is not consistently observed across all years. Similarly, the yearly line graph also shows some evidence of seasonality, with a general trend of higher values during the latter part of the year. However, this trend is not present in all years, and the magnitude of the seasonal effect varies between years. Overall, the presence of some evidence of seasonality in the S&P 500 Index suggests that seasonality may be one of the factors contributing to the fluctuations in the stock price. However, other factors beyond seasonality, such as economic and political events, also play a significant role in determining the stock price.\n\n\nMoving Average\n\n4 Month MA1 Year MA3 Year MA3 Year MA\n\n\n\n\nCode\n#SMA Smoothing - 4 month\nma <- autoplot(month, series=\"Data\") +\n  autolayer(ma(month,5), series=\"4 month MA\") +\n  xlab(\"Year\") + ylab(\"GWh\") +\n  ggtitle(\"S&P 500 Index Stock Jan 2010 - March 2023 (4 Month Moving Average\") +\n  scale_colour_manual(values=c(\"Data\"=\"grey50\",\"4 month MA\"=\"red\"),\n                      breaks=c(\"Data\",\"4 month MA\"))\nma\n\n\n\n\n\n\n\n\n\nCode\n#SMA Smoothing - 1 Year\nma <- autoplot(month, series=\"Data\") +\n  autolayer(ma(month,13), series=\"1 Year MA\") +\n  xlab(\"Year\") + ylab(\"GWh\") +\n  ggtitle(\"S&P 500 Index Stock Jan 2010 - March 2023 (1 Year Moving Average\") +\n  scale_colour_manual(values=c(\"Data\"=\"grey50\",\"1 Year MA\"=\"red\"),\n                      breaks=c(\"Data\",\"1 Year MA\"))\nma\n\n\n\n\n\n\n\n\n\nCode\n#SMA Smoothing - 3 Year\nma <- autoplot(month, series=\"Data\") +\n  autolayer(ma(month,37), series=\"3 Year MA\") +\n  xlab(\"Year\") + ylab(\"GWh\") +\n  ggtitle(\"S&P 500 Index Stock Jan 2010 - March 2023 (3 Year Moving Average)\") +\n  scale_colour_manual(values=c(\"Data\"=\"grey50\",\"3 Year MA\"=\"red\"),\n                      breaks=c(\"Data\",\"3 Year MA\"))\nma\n\n\n\n\n\n\n\n\n\nCode\n#SMA Smoothing - 5 Year\nma <- autoplot(month, series=\"Data\") +\n  autolayer(ma(month,61), series=\"5 Year MA\") +\n  xlab(\"Year\") + ylab(\"GWh\") +\n  ggtitle(\"S&P 500 Index Stock Jan 2010 - March 2023 (5 Year Moving Average)\") +\n  scale_colour_manual(values=c(\"Data\"=\"grey50\",\"5 Year MA\"=\"red\"),\n                      breaks=c(\"Data\",\"5 Year MA\"))\nma\n\n\n\n\n\n\n\n\nThe four plots above show the S&P 500 Index stock prices for the period between Jan 2010 and March 2023, smoothed using 4-month, 1-year, 3-year and 5-year moving averages.\nLooking at the plots, we can see that the moving average values are increasing over time. The 4-month MA plot shows a lot of fluctuations, which is expected because it captures the short-term variations in the stock prices. On the other hand, the 1-year and 3-year and 5-year MA plots smooth out the fluctuations and show the overall trend of the stock prices.\nWe can observe that the 5-year MA plot provides a smoother trend as it takes into account a longer time period compared to the other plots. The 1-year MA plot also provides a relatively smooth trend, but it captures shorter-term variations compared to the 5-year MA plot. The 4-month MA plot is even more sensitive to shorter-term variations in the stock prices. From the moving average obtained above we can see that there is upward tend in the stock price of S&P 500 Index.\n\n\nAutocorrelation Time Series\n\nACFPACFADF Test\n\n\n\n\nCode\n#ACF plots for month data\nggAcf(month)+ggtitle(\"ACF Plot for S&P 500 Index Stock Jan 2010 - March 2023\")\n\n\n\n\n\n\n\n\n\nCode\n#PACF plots for month data\nggPacf(month)+ggtitle(\"PACF Plot for S&P 500 Index Stock Jan 2010 - March 2023\")\n\n\n\n\n\n\n\n\n\nCode\n# ADF Test\ntseries::adf.test(month)\n\n\n\n    Augmented Dickey-Fuller Test\n\ndata:  month\nDickey-Fuller = -2.5431, Lag order = 5, p-value = 0.3499\nalternative hypothesis: stationary\n\n\n\n\n\nThere is clear autocorrelation in lag in the plot of autocorrelation function, which is the acf graph for monthly data. The lag plots and autocorrelation plots shown above suggest seasonality in the series, indicating that it is not stationary. It was also validated using the Augmented Dickey-Fuller Test, which indicates that the series is not stationary as the p value is more than 0.05.\n\n\nDetrend and Differenced Time Series\n\nLinear Fitting ModelACF Plot\n\n\n\n\nCode\nfit = lm(myts~time(myts), na.action=NULL) \nsummary(fit) \n\n\n\nCall:\nlm(formula = myts ~ time(myts), na.action = NULL)\n\nResiduals:\n     Min       1Q   Median       3Q      Max \n-1100.29  -200.78   -24.77    78.55  1008.30 \n\nCoefficients:\n              Estimate Std. Error t value Pr(>|t|)    \n(Intercept) -5.064e+05  2.628e+03  -192.7   <2e-16 ***\ntime(myts)   2.523e+02  1.303e+00   193.6   <2e-16 ***\n---\nSignif. codes:  0 '***' 0.001 '**' 0.01 '*' 0.05 '.' 0.1 ' ' 1\n\nResidual standard error: 284.5 on 3309 degrees of freedom\nMultiple R-squared:  0.9189,    Adjusted R-squared:  0.9188 \nF-statistic: 3.748e+04 on 1 and 3309 DF,  p-value: < 2.2e-16\n\n\n\n\n\n\nCode\n# plot ACFs\nplot1 <- ggAcf(myts, 48, main=\"Original Data: S&P 500 Index Stock Price\")\nplot2 <- ggAcf(resid(fit), 48, main=\"detrended data\") \nplot3 <- ggAcf(diff(myts), 48, main=\"first differenced data\")\ngrid.arrange(plot1, plot2, plot3,nrow=3)\n\n\n\n\n\n\n\n\nThe estimated slope coefficient β1, 3.655e+02. With a standard error of 1.888e+00, yielding a significant estimated increase of stock price is very less yearly. Equation of the fit for stationary process: \\[\\hat{y}_{t} = x_{t}+(7.338e+05)-(3.655e+02)t\\]\nFrom the above graph we can say that there is high correlation in the original plot, but in the detrended plot the correlation is reduced but there is still high correlation in the detrended data.But when the first order difference is applied the high correlation is removed but there is seasonal correlation.\nAs depicted in the above figure, the series is now stationary and ready for future study."
  },
  {
    "objectID": "eda_index_nadsaq.html",
    "href": "eda_index_nadsaq.html",
    "title": "EDA for NADSAQ Composite Index",
    "section": "",
    "text": "Time Series Plot\n\n\nCode\n# get data\noptions(\"getSymbols.warning4.0\"=FALSE)\noptions(\"getSymbols.yahoo.warning\"=FALSE)\n\n\ndata = getSymbols(\"^IXIC\",src='yahoo',from = '2010-01-01',to = \"2023-03-01\")\n\ndf <- data.frame(Date=index(IXIC),coredata(IXIC))\n\n# create Bollinger Bands\nbbands <- BBands(IXIC[,c(\"IXIC.High\",\"IXIC.Low\",\"IXIC.Close\")])\n\n# join and subset data\ndf <- subset(cbind(df, data.frame(bbands[,1:3])), Date >= \"2010-01-01\")\n\n#export data\nnasdaq_raw_data <- df\nwrite.csv(nasdaq_raw_data, \"DATA/CLEANED DATA/nasdaq_raw_data.csv\", row.names=FALSE)\n\n# colors column for increasing and decreasing\nfor (i in 1:length(df[,1])) {\n  if (df$IXIC.Close[i] >= df$IXIC.Open[i]) {\n      df$direction[i] = 'Increasing'\n  } else {\n      df$direction[i] = 'Decreasing'\n  }\n}\n\ni <- list(line = list(color = '#CCCCFF'))\nd <- list(line = list(color = '#7F7F7F'))\n\n# plot candlestick chart\n\nfig <- df %>% plot_ly(x = ~Date, type=\"candlestick\",\n          open = ~IXIC.Open, close = ~IXIC.Close,\n          high = ~IXIC.High, low = ~IXIC.Low, name = \"IXIC\",\n          increasing = i, decreasing = d) \nfig <- fig %>% add_lines(x = ~Date, y = ~up , name = \"B Bands\",\n            line = list(color = '#ccc', width = 0.5),\n            legendgroup = \"Bollinger Bands\",\n            hoverinfo = \"none\", inherit = F) \nfig <- fig %>% add_lines(x = ~Date, y = ~dn, name = \"B Bands\",\n            line = list(color = '#ccc', width = 0.5),\n            legendgroup = \"Bollinger Bands\", inherit = F,\n            showlegend = FALSE, hoverinfo = \"none\") \nfig <- fig %>% add_lines(x = ~Date, y = ~mavg, name = \"Mv Avg\",\n            line = list(color = '#E377C2', width = 0.5),\n            hoverinfo = \"none\", inherit = F) \nfig <- fig %>% layout(yaxis = list(title = \"Price\"))\n\n# plot volume bar chart\nfig2 <- df \nfig2 <- fig2 %>% plot_ly(x=~Date, y=~IXIC.Volume, type='bar', name = \"IXIC Volume\",\n          color = ~direction, colors = c('#CCCCFF','#7F7F7F')) \nfig2 <- fig2 %>% layout(yaxis = list(title = \"Volume\"))\n\n# create rangeselector buttons\nrs <- list(visible = TRUE, x = 0.5, y = -0.055,\n           xanchor = 'center', yref = 'paper',\n           font = list(size = 9),\n           buttons = list(\n             list(count=1,\n                  label='RESET',\n                  step='all'),\n             list(count=1,\n                  label='1 YR',\n                  step='year',\n                  stepmode='backward'),\n             list(count=3,\n                  label='3 MO',\n                  step='month',\n                  stepmode='backward'),\n             list(count=1,\n                  label='1 MO',\n                  step='month',\n                  stepmode='backward')\n           ))\n\n# subplot with shared x axis\nfig <- subplot(fig, fig2, heights = c(0.7,0.2), nrows=2,\n             shareX = TRUE, titleY = TRUE)\nfig <- fig %>% layout(title = paste(\"NASDAQ Composite Index Stock Price: January 2010 - March 2023\" ),\n         xaxis = list(rangeselector = rs),\n         legend = list(orientation = 'h', x = 0.5, y = 1,\n                       xanchor = 'center', yref = 'paper',\n                       font = list(size = 10),\n                       bgcolor = 'transparent'))\n\nfig\n\n\n\n\n\n\nOver the past decade, the NASDAQ Composite Index has shown a strong upward trend, with periods of volatility and corrections along the way. One of the key drivers of this growth has been the rapid expansion of the technology industry, which has fueled investor optimism and driven up the prices of technology stocks. As a result, the NASDAQ Composite Index has become closely associated with the technology sector, and investors often view it as a barometer of the industry’s health.\nHowever, inflation has also played a role in the movement of the NASDAQ Composite Index stock price. Inflation erodes the value of money, making it more expensive to purchase goods and services. This can lead to higher interest rates, which can negatively impact the stock market. Inflation concerns have been a major factor in market volatility, and recent increases in inflation have caused some investors to become cautious. The Federal Reserve has responded to these concerns by raising interest rates and scaling back its bond-buying program.\nDespite these challenges, the NASDAQ Composite Index has continued to rise, reflecting the underlying strength of the technology industry and the broader economy. As technology continues to transform the way we live and work, the NASDAQ Composite Index is likely to remain an important indicator of trends in the sector and a key benchmark for investors.\nFor stock prices, a multiplicative decomposition is typically preferred because the percentage changes in stock prices tend to be more important than the absolute changes. Additionally, stock prices tend to exhibit non-constant variance, meaning that the variance of the series changes over time. A multiplicative decomposition can handle this non-constant variance more effectively than an additive decomposition.\n\n\nDecomposed Time Series\n\nDecomposition PlotAdjusted Decomposition Plot\n\n\n\n\nCode\n#convert to ts data\nmyts<-ts(df$IXIC.Adjusted,frequency=252,start=c(2010,1,1)) \norginial_plot <- autoplot(myts,xlab =\"Year\", ylab = \"Adjusted Closing Price\", main = \"NASDAQ Composite Index Stock price: Jan 2010 - March 2023\")\ndecompose = decompose(myts, \"multiplicative\")\nautoplot(decompose)\n\n\n\n\n\n\n\n\n\nCode\n#adjusted decomposition\ntrendadj <- myts/decompose$trend\ndecompose_adjtrend_plot <- autoplot(trendadj,ylab='trend') +ggtitle('Adjusted trend component in the multiplicative time series model')\nseasonaladj <- myts/decompose$seasonal\ndecompose_adjseasonal_plot <- autoplot(seasonaladj,ylab='seasonal') +ggtitle('Adjusted seasonal component in the multiplicative time series model')\ngrid.arrange(orginial_plot, decompose_adjtrend_plot,decompose_adjseasonal_plot, nrow=3)\n\n\n\n\n\n\n\n\nWhen compared to the original plot, the adjusted seasonal component tends to have an upward trend, and the model is more variable than the original plot, where the plot changes over time but the trend stays the same.\n\n\nLag Plots\n\nDaily Time LagsMonthly Time Lags\n\n\n\n\nCode\n#Lag plots \ngglagplot(myts, do.lines=FALSE, lags=1)+xlab(\"Lag 1\")+ylab(\"Yi\")+ggtitle(\"Lag Plot for NASDAQ Composite Index Stock Jan 2010 - March 2023\")\n\n\n\n\n\n\n\n\n\nCode\n#montly data\nmean_data <- df %>% \n  mutate(month = month(Date), year = year(Date)) %>% \n  group_by(year, month) %>% \n  summarize(mean_value = mean(IXIC.Adjusted))\n#ts of montly data\nmonth<-ts(mean_data$mean_value,star=decimal_date(as.Date(\"2010-01-01\",format = \"%Y-%m-%d\")),frequency = 12)\n\n#Lag plot for month\nts_lags(month)\n\n\n\n\n\n\n\n\n\nNASDAQ Composite Index Stock’s Lag Plot From January 2010 to March 2023, there should be a strong connection between the series and the related lag, since there is a positive correlation and a 45-degree slope. This is how a process that has strong positive autocorrelation shows up in a lag plot. Such processes are not very random, because there is a strong link between one observation and the next. Another way to look at seasonality is to plot observations for a larger number of time periods, called “lags.” Using the mean function, the time series data is turned into monthly data so that the series can be better understood and the plots can be more clear. If you look closely at the last graph, you can see that there are more dots on the diagonal line at 45 degrees. On the vertical axis of the second graph, the month of the variable is shown. The lines link the points in order of time. This suggest that there is strong association between an observation and a succeeding observation.\n\n\nSeasonality\n\nSeasonal HeatmapSeasonal Line plot\n\n\n\n\nCode\n# Create seasonal plot\nts_heatmap(month, color = \"YlGn\", title = 'Seasonality Heatmap of NASDAQ Composite Index Stock Jan 2010 - March 2023')\n\n\n\n\n\n\n\n\n\n\nCode\n# Create a line graph for each year with months on the x-axis\nggseasonplot(month, datecol = \"date\", valuecol = \"value\")+ggtitle(\"Seasonal Yearly Plot for NASDAQ Composite Index Stock Jan 2010 - March 2023\")\n\n\n\n\n\n\n\n\nThe Seasonality Heatmap for the NASDAQ Composite Index Stock from January 2010 to March 2023 does not reveal any significant seasonality in the data. The heatmap displays the mean value of the time series for each month and year combination, with darker colors indicating higher values. The absence of any consistent patterns or darker colors in specific months or years indicates that there is no clear seasonal trend in the data. Similarly, the yearly line graph also does not show any discernible seasonality. Each year’s data is represented by a line, and the months are plotted on the x-axis. However, the graph does display a strong upward trend in the stock price from 2010 to 2023. Overall, the lack of clear seasonality in both the heatmap and yearly line graph suggests that other factors beyond seasonality are the primary drivers of the fluctuations in the NASDAQ Composite Index.\n\n\nMoving Average\n\n4 Month MA1 Year MA3 Year MA5 Year MA\n\n\n\n\nCode\n#SMA Smoothing \nma <- autoplot(month, series=\"Data\") +\n  autolayer(ma(month,5), series=\"4 Month MA\") +\n  xlab(\"Year\") + ylab(\"GWh\") +\n  ggtitle(\"NASDAQ Composite Index Stock Jan 2010 - March 2023(4 Month Moving Average)\") +\n  scale_colour_manual(values=c(\"Data\"=\"grey50\",\"4 Month MA\"=\"red\"),\n                      breaks=c(\"Data\",\"4 Month MA\"))\nma\n\n\n\n\n\n\n\n\n\nCode\n#SMA Smoothing \nma <- autoplot(month, series=\"Data\") +\n  autolayer(ma(month,13), series=\"1 Year MA\") +\n  xlab(\"Year\") + ylab(\"GWh\") +\n  ggtitle(\"NASDAQ Composite Index Stock Jan 2010 - March 2023 (1 Year Moving Average)\") +\n  scale_colour_manual(values=c(\"Data\"=\"grey50\",\"1 Year MA\"=\"red\"),\n                      breaks=c(\"Data\",\"1 Year MA\"))\nma\n\n\n\n\n\n\n\n\n\nCode\n#SMA Smoothing \nma <- autoplot(month, series=\"Data\") +\n  autolayer(ma(month,37), series=\"3 Year MA\") +\n  xlab(\"Year\") + ylab(\"GWh\") +\n  ggtitle(\"NASDAQ Composite Index Stock Jan 2010 - March 2023(3 Year Moving Average)\") +\n  scale_colour_manual(values=c(\"Data\"=\"grey50\",\"3 Year MA\"=\"red\"),\n                      breaks=c(\"Data\",\"3 Year MA\"))\nma\n\n\n\n\n\n\n\n\n\nCode\n#SMA Smoothing \nma <- autoplot(month, series=\"Data\") +\n  autolayer(ma(month,61), series=\"5 Year MA\") +\n  xlab(\"Year\") + ylab(\"GWh\") +\n  ggtitle(\"NASDAQ Composite Index Stock Jan 2010 - March 2023(5 Year Moving Average)\") +\n  scale_colour_manual(values=c(\"Data\"=\"grey50\",\"5 Year MA\"=\"red\"),\n                      breaks=c(\"Data\",\"5 Year MA\"))\nma\n\n\n\n\n\n\n\n\nThe four plots above show the NASDAQ Composite Index stock price from January 2010 to March 2023 with different timeframes of moving averages (MA) overlaid. The moving average is a common smoothing technique used to reduce the noise in the time series data and highlight underlying trends. As the length of the MA window increases, the smoother the plot becomes and the trend is more visible. The first plot shows a 4-month moving average, the second plot shows a 1-year moving average, the third plot shows a 3-year moving average and the fourth plot shows a 5-year moving average\nComparing the plots, we can see that as the length of the moving average window increases, the plot becomes smoother, and the trend becomes clearer. In the 4-month moving average plot, the stock price fluctuates significantly, making it challenging to identify the trend. However, in the 5-year moving average plot, we can observe a clear upward trend in the stock price, and it is easier to identify the long-term trend. From the moving average obtained above we can see that there is upward tend in the stock price of NASDAQ Composite Index.\n\n\nAutocorrelation Time Series\n\nACFPACFADF Test\n\n\n\n\nCode\n#ACF plots for month data\nggAcf(month, 120)+ggtitle(\"ACF Plot for NASDAQ Composite Index Stock Jan 2010 - March 2023\")\n\n\n\n\n\n\n\n\n\nCode\n#PACF plots for month data\nggPacf(month, 120)+ggtitle(\"PACF Plot for NASDAQ Composite Index Stock Jan 2010 - March 2023\")\n\n\n\n\n\n\n\n\n\nCode\n# ADF Test\ntseries::adf.test(month)\n\n\n\n    Augmented Dickey-Fuller Test\n\ndata:  month\nDickey-Fuller = -2.514, Lag order = 5, p-value = 0.362\nalternative hypothesis: stationary\n\n\n\n\n\nThere is clear autocorrelation in lag in the plot of autocorrelation function, which is the acf graph for monthly data. The above lag plots and autocorrelation plots show that the series has seasonality, which means that the series doesn’t stay the same over time. It was also checked with the Augmented Dickey-Fuller Test. This test tells us that the series is not stationary because the p value is greater than 0.05.\n\n\nDetrend and Differenced Time Series\n\nLinear Fitting ModelACF Plot\n\n\n\n\nCode\nfit = lm(myts~time(myts), na.action=NULL) \nsummary(fit) \n\n\n\nCall:\nlm(formula = myts ~ time(myts), na.action = NULL)\n\nResiduals:\n    Min      1Q  Median      3Q     Max \n-2998.7 -1052.2  -276.2   735.6  4672.4 \n\nCoefficients:\n              Estimate Std. Error t value Pr(>|t|)    \n(Intercept) -1.835e+06  1.280e+04  -143.3   <2e-16 ***\ntime(myts)   9.132e+02  6.350e+00   143.8   <2e-16 ***\n---\nSignif. codes:  0 '***' 0.001 '**' 0.01 '*' 0.05 '.' 0.1 ' ' 1\n\nResidual standard error: 1386 on 3309 degrees of freedom\nMultiple R-squared:  0.8621,    Adjusted R-squared:  0.8621 \nF-statistic: 2.069e+04 on 1 and 3309 DF,  p-value: < 2.2e-16\n\n\n\n\n\n\nCode\n# plot ACFs\nplot1 <- ggAcf(myts, 48, main=\"Original Data: NASDAQ Composite Index Stock Price\")\nplot2 <- ggAcf(resid(fit), 48, main=\"Detrended data\") \nplot3 <- ggAcf(diff(myts), 48, main=\"First differenced data\")\ngrid.arrange(plot1, plot2, plot3,nrow=3)\n\n\n\n\n\n\n\n\nThe estimated slope coefficient β1, 1.323e+03. With a standard error of 9.197e+00, yielding a significant estimated increase of stock price is very less yearly. Equation of the fit for stationary process: \\[\\hat{y}_{t} = x_{t}+(2.658e+06)-(1.323e+03)t\\] From the above graph, we can see that the original plot has a high correlation, while the detrended plot has a lower correlation but still a high correlation. But when the first order difference is used, the high correlation goes away, but there is still a correlation between the time of year and the data.\nAs depicted in the above figure, the series is now stationary and ready for future study."
  },
  {
    "objectID": "eda_macroeconomic_factors.html",
    "href": "eda_macroeconomic_factors.html",
    "title": "EDA for Macroeconomic Factors",
    "section": "",
    "text": "Exploratory Data Analysis (EDA) is a crucial step in understanding the behavior of macroeconomic factors such as GDP growth rate, inflation, interest rates, and unemployment rates. These factors play a critical role in shaping the overall performance of an economy. EDA involves various statistical techniques and tools that help in analyzing and visualizing data to identify patterns, trends, and anomalies.\nOne of the key aspects of EDA is time series analysis, which is used to analyze data collected over time. Time series data such as GDP growth rate, inflation, interest rates, and unemployment rates are characterized by autocorrelation, which means that the value of a variable at a particular time is dependent on its past values. Autocorrelation can be quantified using statistical measures such as the autocorrelation function (ACF) and partial autocorrelation function (PACF). These measures are used to identify the strength and direction of the relationship between variables.\nAnother important concept in time series analysis is stationarity, which refers to the stability of the statistical properties of a time series over time. Stationarity is a critical assumption in many time series models, and it can be tested using techniques such as the Augmented Dickey-Fuller (ADF) test. Seasonality is another aspect of time series data that can be identified using EDA. Seasonality refers to the presence of regular and predictable patterns in the data that recur over a fixed time period.\nMoving averages are another statistical technique commonly used in EDA for time series data. Moving averages smooth out fluctuations in the data and help identify trends and patterns. Autocorrelation is also a key concept in EDA for time series data. Autocorrelation refers to the degree of correlation between a variable and its past values.\nFinally, detrend check is a technique used to identify trends in time series data. Detrending involves removing the trend component from the data to isolate the cyclical and irregular components. EDA for macroeconomic factors such as GDP growth rate, inflation, interest rates, and unemployment rates involves a careful consideration of all these concepts and techniques to identify patterns and trends that can provide valuable insights into the behavior of these critical economic indicators.\nClick to view EDA Page for GDP Growth Rate\nClick to view EDA Page for Interest Rate\nClick to view EDA Page for Inflation Rate\nClick to view EDA Page for Unemployment Rate"
  },
  {
    "objectID": "eda_macroeconomic_gdp.html",
    "href": "eda_macroeconomic_gdp.html",
    "title": "EDA for GDP Growth Rate",
    "section": "",
    "text": "GDP growth rate is one of the most important indicators of a country’s economic performance, and analyzing its behavior is critical to understanding the underlying factors driving economic growth. Exploratory Data Analysis (EDA) is a powerful tool for gaining insights into GDP growth rate data and identifying patterns and trends that can inform economic policy decisions. In this page, we will explore various EDA techniques that can be applied to GDP growth rate data, such as time series analysis, autocorrelation analysis, seasonality analysis, moving averages, and detrending. By the end of this page, you will have a better understanding of how to apply EDA techniques to GDP growth rate data and draw valuable insights from it.\n\nTime Series Plot\n\n\nCode\n#import the data\ngdp <- read.csv(\"DATA/RAW DATA/gdp-growth.csv\")\n\n#change date format\ngdp$Date <- as.Date(gdp$DATE , \"%m/%d/%Y\")\n\n#drop DATE column\ngdp <- subset(gdp, select = -c(1))\n\n#export the cleaned data\ngdp_clean <- gdp\nwrite.csv(gdp_clean, \"DATA/CLEANED DATA/gdp_clean_data.csv\", row.names=FALSE)\n\n#plot gdp growth rate \nfig <- plot_ly(gdp, x = ~Date, y = ~value, type = 'scatter', mode = 'lines',line = list(color = 'rgb(220,20,60)'))\nfig <- fig %>% layout(title = \"U.S GPD Growth Rate: 2010 - 2022\",xaxis = list(title = \"Time\"),yaxis = list(title =\"GDP Growth Rate\"))\nfig\n\n\n\n\n\n\nThe trend in GDP growth rate in the United States from 2010 to 2022 has been characterized by moderate fluctuations, reflecting a range of economic conditions and policy responses. Between 2010 and 2022, the United States experienced a range of GDP growth rates, reflecting various economic conditions and policy responses. In the early years of this period, the economy was still recovering from the 2008 financial crisis, which had led to a prolonged period of slow growth. In 2012, GDP growth began to pick up, reaching 2.8% that year, followed by 1.8% in 2013 and 2.5% in 2014. The peak in this period came in 2018, when the GDP growth rate reached 2.9%. However, the momentum of growth slowed in the years that followed. In 2016, GDP growth declined to 1.6%, followed by 2.2% in 2018, and 2.9% in 2018. By 2019, growth had slowed again to 2.2%. This period of slower growth was attributed to a range of factors, including the tightening of monetary policy by the Federal Reserve, global economic headwinds, and ongoing concerns about political instability and trade tensions. The COVID-19 pandemic in 2020 led to a sharp contraction in economic activity, with GDP growth declining by 3.5%, the largest annual decline since the 1940s. The pandemic resulted in widespread shutdowns of businesses, schools, and public spaces, as well as disruptions to global supply chains and trade. However, the US government and the Federal Reserve responded with a range of fiscal and monetary policies, including direct payments to households, increased unemployment benefits, and massive injections of liquidity into financial markets. These measures helped to mitigate the impact of the pandemic on the economy. In 2021, the US economy began to recover, with GDP growth projected to reach 6.3% by the end of the year. This rebound was due to a combination of factors, including the easing of pandemic-related restrictions, increased vaccination rates, and the continuation of government stimulus measures.\nThe GDP growth rate in the United States from 2010 to 2021, there appears to be some seasonality in the data. he seasonality appears to be relatively consistent over time, with spikes in GDP growth rate occurring in the second quarter of each year, followed by a dip in the third quarter. This pattern is likely due to various factors, such as changes in consumer spending and production schedules. Multiplicative decomposition model may be more appropriate, as it accounts for changes in both the level and the variability of the data.\n\n\nDecomposed Time Series\n\nDecomposition PlotAdjusted Decomposition Plot\n\n\n\n\nCode\n#convert to ts data\nmyts<-ts(gdp$value,frequency=4,start=c(2010/1/1))\n#decomposition\norginial_plot <- autoplot(myts,xlab =\"Year\", ylab = \"GDP Growth Rate\", main = \"U.S GDP Growth Rate: 2010 - 2021\")\ndecompose = decompose(myts,\"multiplicative\")\nautoplot(decompose)\n\n\n\n\n\n\n\n\n\nCode\n#adjusted decomposition\ntrendadj <- myts/decompose$trend\ndecompose_adjtrend_plot <- autoplot(trendadj,ylab='seasonal') +ggtitle('Adjusted trend component in the multiplicative time series model')\nseasonaladj <- myts/decompose$seasonal\ndecompose_adjseasonal_plot <- autoplot(seasonaladj,ylab='seasonal') +ggtitle('Adjusted seasonal component in the multiplicative time series model')\ngrid.arrange(orginial_plot, decompose_adjtrend_plot,decompose_adjseasonal_plot, nrow=3)\n\n\n\n\n\n\n\n\nWhen compared to the original plot, the adjusted seasonal component tends to have more fluctuation, and the model is more variable than the original plot, where the plot changes over time but the trend stays the same.\n\n\nLag Plots\n\nDaily Time LagsMonthly Time Lags\n\n\n\n\nCode\n#Lag plots \ngglagplot(myts, do.lines=FALSE, lags=1)+xlab(\"Lag 1\")+ylab(\"Yi\")+ggtitle(\"Lag Plot for U.S GDP Growth Rate: 2010 - 2021\")\n\n\n\n\n\n\n\n\n\nCode\n#montly data\nmean_data <- gdp %>% \n  mutate(month = month(Date), year = year(Date)) %>% \n  group_by(year, month) %>% \n  summarize(mean_value = mean(value))\n\nmonth<-ts(mean_data$mean_value,star=decimal_date(as.Date(\"2010-01-01\",format = \"%Y-%m-%d\")),frequency = 4)\n\n#Lag plot for month\nts_lags(month,lags = c(1, 4, 7, 10) )\n\n\n\n\n\n\n\n\n\nThe lag plot shows that there is a cluster in the middle, and the monthly lag plot shows that there is no autocorrelation.\n\n\nSeasonality\n\nSeasonal HeatmapSeasonal Line plot\n\n\n\n\nCode\n# Create seasonal plot\nts_heatmap(month, color = \"Purples\",title = 'Seasonality U.S GDP Growth Rate: 2010 - 2021')\n\n\n\n\n\n\n\n\n\n\nCode\n# Create a line graph for each year with months on the x-axis\nggseasonplot(month, datecol = \"date\", valuecol = \"value\")+ggtitle(\"Seasonal Yearly Plot forU.S GDP Growth Rate: 2010 - 2021\")\n\n\n\n\n\n\n\n\nThe seasonality plots shows that series follow seasonality, as the heat map shows no much difference with each year and the line graph lie on similar values for most of the year.\n\n\nMoving Average\n\n4 Month MA1 Year MA3 Year MA\n\n\n\n\nCode\n#SMA Smoothing \nma <- autoplot(month, series=\"gdp\") +\n  autolayer(ma(month,5), series=\"4 Month MA\") +\n  xlab(\"Year\") + ylab(\"GWh\") +\n  ggtitle(\"GDP Growth Rate 2010 - 2022 (4 Month Moving Average)\") +\n  scale_colour_manual(values=c(\"Data\"=\"grey50\",\"4 Month MA\"=\"red\"),\n                      breaks=c(\"Data\",\"4 Month MA\"))\nma\n\n\n\n\n\n\n\n\n\nCode\n#SMA Smoothing \nma <- autoplot(month, series=\"gdp\") +\n  autolayer(ma(month,13), series=\"1 Year MA\") +\n  xlab(\"Year\") + ylab(\"GWh\") +\n  ggtitle(\"GDP Growth Rate 2010 - 2022 (1 Year Moving Average)\") +\n  scale_colour_manual(values=c(\"Data\"=\"grey50\",\"1 Year MA\"=\"red\"),\n                      breaks=c(\"Data\",\"1 Year MA\"))\nma\n\n\n\n\n\n\n\n\n\nCode\n#SMA Smoothing \nma <- autoplot(month, series=\"gdp\") +\n  autolayer(ma(month,37), series=\"3 Year MA\") +\n  xlab(\"Year\") + ylab(\"GWh\") +\n  ggtitle(\"GDP Growth Rate 2010 - 2022 (3 Year Moving Average)\") +\n  scale_colour_manual(values=c(\"Data\"=\"grey50\",\"3 Year MA\"=\"red\"),\n                      breaks=c(\"Data\",\"3 Year MA\"))\nma\n\n\n\n\n\n\n\n\nThe three plots show the same data series of GDP growth rate from 2010 to 2022, but each plot has a different moving average smoothing applied to it. The first plot shows a 4-month moving average, the second plot shows a 1-year moving average, and the third plot shows a 3-year moving average.\nLooking at the three plots, we can see that the 4-month moving average plot has a lot of fluctuations, and it follows the ups and downs of the original data series more closely. The 1-year moving average plot has less fluctuations compared to the 4-month moving average plot, and it provides a smoother trend of the data series. The 3-year moving average plot has even less fluctuations and a much smoother trend than the previous two plots.\nThe choice of moving average window size depends on the analyst’s preference and the objective of the analysis. Shorter window sizes like the 4-month moving average can provide more detailed insights into the data series, but they may also be more susceptible to noise and fluctuations. Longer window sizes like the 3-year moving average can provide a more stable and robust trend but may smooth out important details in the data series. As the moving average increases, GDP Growth Rtae tend to have no trend, it seem to be stable.\n\n\nAutocorrelation Time Series\n\nACFPACFADF Test\n\n\n\n\nCode\n#ACF plots for month data\nggAcf(myts)+ggtitle(\"ACF Plot for GDP Growth Rate: 2010 - 2022\")\n\n\n\n\n\n\n\n\n\nCode\n#PACF plots for month data\nggPacf(myts)+ggtitle(\"PACF Plot for GDP Growth Rate: 2010 - 2022\")\n\n\n\n\n\n\n\n\n\nCode\n# ADF Test\ntseries::adf.test(myts)\n\n\n\n    Augmented Dickey-Fuller Test\n\ndata:  myts\nDickey-Fuller = -5.768, Lag order = 3, p-value = 0.01\nalternative hypothesis: stationary\n\n\n\n\n\nThe above autocorrelation plots show that the series doesn’t change with the seasons, which indicates that there series is stationary. This is verified was checked using the Augmented Dickey-Fuller Test and the result of the test says that series is stationary because the p value is less than 0.05."
  },
  {
    "objectID": "eda_macroeconomic_inflation.html",
    "href": "eda_macroeconomic_inflation.html",
    "title": "EDA for Inflation Rate",
    "section": "",
    "text": "Inflation is a crucial macroeconomic indicator that measures the rate at which the prices of goods and services in an economy are rising over time. Exploratory Data Analysis (EDA) is a powerful technique for analyzing inflation rate data and gaining insights into the factors driving inflation. In this page, we will explore various EDA techniques that can be applied to inflation rate data, such as time series analysis, autocorrelation analysis, seasonality analysis, moving averages, and detrending. By the end of this page, you will have a better understanding of how to apply EDA techniques to inflation rate data and draw valuable insights from it.\n\nTime Series Plot\n\n\nCode\n#import the data\ninflation_rate <- read.csv(\"DATA/RAW DATA/inflation-rate.csv\")\n\n#cleaning the data\n#remove unwanted columns\ninflation_rate_clean <- subset(inflation_rate, select = -c(1,HALF1,HALF2))\n\n#convert the data to time series data\ninflation_data_ts <- ts(as.vector(t(as.matrix(inflation_rate_clean))), start=c(2010,1), end=c(2023,2), frequency=12)\n\n#export the data\nwrite.csv(inflation_rate_clean, \"DATA/CLEANED DATA/inflation_rate_clean_data.csv\", row.names=FALSE)\n\n\n#plot inflation rate \nfig <- autoplot(inflation_data_ts, ylab = \"Inflation Rate\", color=\"#DB7093\")+ggtitle(\"U.S Inflation Rate: January 2010 - February 2023\")+theme_bw()\nggplotly(fig)\n\n\n\n\n\n\nThe inflation rate in the United States has varied from year to year since 2010. From 2010 to 2018, the inflation rate generally remained below 2% per year, with some slight fluctuations. In 2016, it started to rise gradually and continued to increase until it reached a peak of 6.6% in June 2022. Since then, it has slightly decreased and as of February 2023. The COVID-19 pandemic has played a significant role in driving up inflation in the United States, as supply chain disruptions and increased demand have led to higher prices for goods and services. The Federal Reserve has taken steps to address inflation, including raising interest rates and reducing asset purchases, in order to keep it under control.\nBased on the plot of the inflation rate in the USA from 2010 to Feb 2023, it appears that there is a clear upward trend, and some level of seasonality as well. Therefore, it would be appropriate to use a multiplicative decomposition method for this time series data. A multiplicative model will allow us to separate the overall trend from the seasonal variations in a way that is appropriate for this type of data.\n\n\nDecomposed Time Series\n\nDecomposition PlotAdjusted Decomposition Plot\n\n\n\n\nCode\n#decomposition\norginial_plot <- autoplot(inflation_data_ts,xlab =\"Year\", ylab = \"Interest Rate\", main = \"U.S Inflation Rate: January 2010 - Feb 2023\")\ndecompose = decompose(inflation_data_ts,\"multiplicative\")\nautoplot(decompose)\n\n\n\n\n\n\n\n\n\nCode\ntrendadj <- inflation_data_ts/decompose$trend\ndecompose_adjtrend_plot <- autoplot(trendadj,ylab='seasonal') +ggtitle('Adjusted trend component in the additive time series model')\nseasonaladj <- inflation_data_ts/decompose$seasonal\ndecompose_adjseasonal_plot <- autoplot(seasonaladj,ylab='seasonal') +ggtitle('Adjusted seasonal component in the additive time series model')\ngrid.arrange(orginial_plot, decompose_adjtrend_plot,decompose_adjseasonal_plot, nrow=3)\n\n\n\n\n\n\n\n\nWhen compared to the original figure, the corrected seasonal component shows tend to show upward trend in the model, but the adjusted trend component has a stable trend through time with some fluctuation.\n\n\nLag Plots\n\nDaily Time LagsMonthly Time Lags\n\n\n\n\nCode\n#Lag plots \ngglagplot(inflation_data_ts, do.lines=FALSE, lags=1)+xlab(\"Lag 1\")+ylab(\"Yi\")+ggtitle(\"Lag Plot for U.S Inflation Rate: January 2010 - Feb 2023\")\n\n\n\n\n\n\n\n\n\nCode\n#Lag plot for monthly data\nts_lags(inflation_data_ts)\n\n\n\n\n\n\n\n\n\nThere should be a strong relationship between the series and the pertinent lag from January 2010 to Feb 2023 because there is a positive correlation and an inclination angle of 45 degrees in the lag plot. This is the characteristic lag plot of a process with positive autocorrelation. One observation and the next have a significant link, making such processes remarkably random. Investigating seasonality also involves graphing observations over a larger range of time intervals, or the lags. To make the time series data easier to understand and create graphs with more clarity, the time series data is combined with monthly data using the mean function. The second graph displays the variable’s monthly variation along the vertical axis. The lines link the points in the order of time. There is a correlation and it is significantly positive, supporting the seasonality of the data.\n\n\nSeasonality\n\nSeasonal HeatmapSeasonal Line plot\n\n\n\n\nCode\n# Create seasonal plot\nts_heatmap(inflation_data_ts,color = \"Reds\", title = 'Seasonality U.S Inflation Rate: 2010 - 2021')\n\n\n\n\n\n\n\n\n\n\nCode\n# Create a line graph for each year with months on the x-axis\nggseasonplot(inflation_data_ts, datecol = \"date\", valuecol = \"value\")+ggtitle(\"Seasonal Yearly Plot forU.S Inflation Rate: 2010 - 2021\")\n\n\n\n\n\n\n\n\nThe Seasonality Heatmap for the Inflation Rate data from JAN 2010 - March 2023 does indicate some significant seasonality for few years in the data, but there seem no to be seasonality in the line graph. To confirm the seasonality we can check on the acf plot for the series.\n\n\nMoving Average\n\n4 Month MA1 Year MA3 Year MA5 Year MA\n\n\n\n\nCode\n#SMA Smoothing \nma <- autoplot(inflation_data_ts, series=\"inflation_rate_clean\") +\n  autolayer(ma(inflation_data_ts,5), series=\"4 Month MA\") +\n  xlab(\"Year\") + ylab(\"GWh\") +\n  ggtitle(\"Inflation Rate January 2010 - Feb 2023 (4 Month Moving Average)\") +\n  scale_colour_manual(values=c(\"Data\"=\"grey50\",\"4 Month MA\"=\"red\"),\n                      breaks=c(\"Data\",\"4 Month MA\"))\nma\n\n\n\n\n\n\n\n\n\nCode\n#SMA Smoothing \nma <- autoplot(inflation_data_ts, series=\"inflation_rate_clean\") +\n  autolayer(ma(inflation_data_ts,13), series=\"1 Year MA\") +\n  xlab(\"Year\") + ylab(\"GWh\") +\n  ggtitle(\"Inflation Rate January 2010 - Feb 20233 (1 Year Moving Average)\") +\n  scale_colour_manual(values=c(\"Data\"=\"grey50\",\"1 Year MA\"=\"red\"),\n                      breaks=c(\"Data\",\"1 Year MA\"))\nma\n\n\n\n\n\n\n\n\n\nCode\n#SMA Smoothing \nma <- autoplot(inflation_data_ts, series=\"inflation_rate_clean\") +\n  autolayer(ma(inflation_data_ts,37), series=\"3 Year MA\") +\n  xlab(\"Year\") + ylab(\"GWh\") +\n  ggtitle(\"Inflation Rate January 2010 - Feb 2023 (3 Year Moving Average)\") +\n  scale_colour_manual(values=c(\"Data\"=\"grey50\",\"3 Year MA\"=\"red\"),\n                      breaks=c(\"Data\",\"3 Year MA\"))\nma\n\n\n\n\n\n\n\n\n\nCode\n#SMA Smoothing \nma <- autoplot(inflation_data_ts, series=\"inflation_rate_clean\") +\n  autolayer(ma(inflation_data_ts,61), series=\"5 Year MA\") +\n  xlab(\"Year\") + ylab(\"GWh\") +\n  ggtitle(\"Inflation Rate January 2010 - Feb 2023 (5 Year Moving Average)\") +\n  scale_colour_manual(values=c(\"Data\"=\"grey50\",\"5 Year MA\"=\"red\"),\n                      breaks=c(\"Data\",\"5 Year MA\"))\nma\n\n\n\n\n\n\n\n\nThe Inflation Rate is displayed for the time period between January 2010 and Feb 2023 in the four plots above, which have been smoothed using 4-month, 1-year, 3-year, and 5-year moving averages. We can observe from the graphs that the moving average values have been rising over time. Given that it captures the short-term differences in stock prices, the 4-month MA plot exhibits a great deal of volatility, which is to be expected. The 1-year, 3-year, and 5-year MA plots, on the other hand, tame the oscillations and reveal the general direction of stock prices. We can see that the 5-year MA plot, which considers a longer time period than the other plots, shows a trend that is smoother. The 3-year MA plot similarly shows a fairly smooth trend, but unlike the 5-year MA plot, it also shows shorter-term variability. Even more susceptible to short-term changes in stock prices is the 1-year MA plot. As the moving average increases we can notive that the trend for Inflation Rate isn’t stable is towards upward.\n\n\nAutocorrelation Time Series\n\nACFPACFADF Test\n\n\n\n\nCode\n#ACF plots for monthly data\nggAcf(inflation_data_ts)+ggtitle(\"ACF Plot for Inflation Rate: January 2010 - Feb 2023\")\n\n\n\n\n\n\n\n\n\nCode\n#PACF plots for monthly data\nggPacf(inflation_data_ts)+ggtitle(\"PACF Plot for Inflation Rate: January 2010 - Feb 2023\")\n\n\n\n\n\n\n\n\n\nCode\n# ADF Test\ntseries::adf.test(inflation_data_ts)\n\n\n\n    Augmented Dickey-Fuller Test\n\ndata:  inflation_data_ts\nDickey-Fuller = -1.7207, Lag order = 5, p-value = 0.6928\nalternative hypothesis: stationary\n\n\n\n\n\nThe autocorrelation function plot, which is the acf graph for monthly data, clearly shows autocorrelation in lag and seasonality. The series appears to be seasonal, according to the lag plots and autocorrelation plots displayed above, proving that it is not stable. The Augmented Dickey-Fuller Test, which reveals that the series is not stationary if the p value is more than 0.05, was also used to validate it. The p value obtained from ADF test is greater than 0.05, which indicates taht the series is not stationary.\n\n\nDetrend and Differenced Time Series\n\nLinear Fitting ModelACF Plot\n\n\n\n\nCode\nfit = lm(inflation_data_ts~time(inflation_data_ts), na.action=NULL) \nsummary(fit) \n\n\n\nCall:\nlm(formula = inflation_data_ts ~ time(inflation_data_ts), na.action = NULL)\n\nResiduals:\n    Min      1Q  Median      3Q     Max \n-2.1136 -0.6549 -0.1224  0.4632  2.8301 \n\nCoefficients:\n                          Estimate Std. Error t value Pr(>|t|)    \n(Intercept)             -474.68145   43.32414  -10.96   <2e-16 ***\ntime(inflation_data_ts)    0.23655    0.02148   11.01   <2e-16 ***\n---\nSignif. codes:  0 '***' 0.001 '**' 0.01 '*' 0.05 '.' 0.1 ' ' 1\n\nResidual standard error: 1.026 on 156 degrees of freedom\nMultiple R-squared:  0.4373,    Adjusted R-squared:  0.4337 \nF-statistic: 121.2 on 1 and 156 DF,  p-value: < 2.2e-16\n\n\n\n\n\n\nCode\n# plot ACFs\nplot1 <- ggAcf(inflation_data_ts, 48, main=\"Original Data: Inflation Rate\")\nplot2 <- ggAcf(resid(fit), 48, main=\"Detrended data\") \nplot3 <- ggAcf(diff(inflation_data_ts), 48, main=\"First differenced data\")\ngrid.arrange(plot1, plot2, plot3,nrow=3)\n\n\n\n\n\n\n\n\nThe estimated slope coefficient β1, 0.23655 With a standard error of 0.02148, yielding a significant estimated increase of stock price is very less yearly. Equation of the fit for stationary process: \\[\\hat{y}_{t} = x_{t}+(474.68145)-(0.23655)t\\]\nFrom the above graph we can say that there is correlation in the original plot, but in the detrended plot the correlation is reduced but there is still high correlation in the detrended data.But when the first order difference is applied the high correlation is removed but there is seasonal correlation.\nAs depicted in the above figure, the series is now stationary and ready for future study."
  },
  {
    "objectID": "eda_macroeconomic_interest.html",
    "href": "eda_macroeconomic_interest.html",
    "title": "EDA for Interest Rate",
    "section": "",
    "text": "Interest rate is a critical macroeconomic variable that plays a key role in shaping economic growth and financial stability. Exploratory Data Analysis (EDA) is a powerful technique for analyzing interest rate data and gaining insights into the factors driving interest rate movements. In this page, we will explore various EDA techniques that can be applied to interest rate data, such as time series analysis, autocorrelation analysis, seasonality analysis, moving averages, and detrending. By the end of this page, you will have a better understanding of how to apply EDA techniques to interest rate data and draw valuable insights from it.\n\nTime Series Plot\n\n\nCode\n#import the data\ninterest_data <- read.csv(\"DATA/RAW DATA/interest-rate.csv\")\n\n#change date format\ninterest_data$Date <- as.Date(interest_data$Date , \"%m/%d/%Y\")\n\n#export the cleaned data\ninterest_clean_data <- interest_data\nwrite.csv(interest_clean_data, \"DATA/CLEANED DATA/interest_rate_clean_data.csv\", row.names=FALSE)\n\n#plot interest rate \nfig <- plot_ly(interest_data, x = ~Date, y = ~value, type = 'scatter', mode = 'lines',line = list(color='rgb(176,224,230)'))\nfig <- fig %>% layout(title = \"U.S Interest Rate: January 2010 - March 2023\",xaxis = list(title = \"Time\"),yaxis = list(title =\"Interest Rate\"))\nfig\n\n\n\n\n\n\nThe interest rate in the United States has exhibited fluctuations from 2010 to 2022. In the years following the 2008 global financial crisis, the interest rate was very low. The interest rate in the US was very low and stable from 2010 to 2018. During this period, the Federal Reserve implemented several monetary policy measures, such as quantitative easing and forward guidance, in order to stimulate the economy and support economic recovery after the global financial crisis.\nStarting in 2018, the Federal Reserve began a gradual process of raising interest rates as the US economy continued to improve. This process of increasing interest rates was driven by a combination of factors such as low unemployment rates, a steady increase in GDP, and the need to prevent inflation from rising too quickly.\nHowever, as global economic conditions became more uncertain, the Federal Reserve began to pause its process of increasing interest rates. The US-China trade war and concerns about the potential impact of Brexit led to a more cautious approach from the Federal Reserve. In 2019, the Federal Reserve lowered interest rates three times in response to these external factors.\nIn 2020, the COVID-19 pandemic caused a major shock to the global economy, leading the Federal Reserve to take unprecedented measures to support the US economy. The Federal Reserve lowered interest rates to near-zero levels, implemented quantitative easing, and established several lending facilities to support businesses and households.\nThere is no clear evidence of a relationship between the variability of the series and its level, which suggests that an additive model might be more appropriate. Additionally, an additive model can be useful when the trend is relatively stable and the amplitude of seasonal fluctuations remains constant over time.\n\n\nDecomposed Time Series\n\nDecomposition PlotAdjusted Decomposition Plot\n\n\n\n\nCode\n#convert to ts data\nmyts<-ts(interest_data$value,frequency=12,start=c(2010/1/1))\n#decomposition\norginial_plot <- autoplot(myts,xlab =\"Year\", ylab = \"Interest Rate\", main = \"U.S Interest Rate: January 2010 - March 2023\")\ndecompose = decompose(myts,\"additive\")\nautoplot(decompose)\n\n\n\n\n\n\n\n\n\nCode\ntrendadj <- myts/decompose$trend\ndecompose_adjtrend_plot <- autoplot(trendadj,ylab='seasonal') +ggtitle('Adjusted trend component in the additive time series model')\nseasonaladj <- myts/decompose$seasonal\ndecompose_adjseasonal_plot <- autoplot(seasonaladj,ylab='seasonal') +ggtitle('Adjusted seasonal component in the additive time series model')\ngrid.arrange(orginial_plot, decompose_adjtrend_plot,decompose_adjseasonal_plot, nrow=3)\n\n\n\n\n\n\n\n\nWhen compared to the original figure, the corrected seasonal component shows seasonality in the model, but the adjusted trend component has a stable trend through time.\n\n\nLag Plots\n\nDaily Time LagsMonthly Time Lags\n\n\n\n\nCode\n#Lag plots \ngglagplot(myts, do.lines=FALSE, lags=1)+xlab(\"Lag 1\")+ylab(\"Yi\")+ggtitle(\"Lag Plot for U.S Interest Rate: January 2010 - March 2023\")\n\n\n\n\n\n\n\n\n\nCode\n#monthly data\nmean_data <- interest_data %>% \n  mutate(month = month(Date), year = year(Date)) %>% \n  group_by(year, month) %>% \n  summarize(mean_value = mean(value))\n\nmonth<-ts(mean_data$mean_value,star=decimal_date(as.Date(\"2010-01-01\",format = \"%Y-%m-%d\")),frequency = 12)\n\n#Lag plot for monthly data\nts_lags(month)\n\n\n\n\n\n\n\n\n\nThere should be a strong relationship between the series and the pertinent lag from January 2010 to March 2023 because there is a positive correlation and an inclination angle of 45 degrees in the lag plot. This is the characteristic lag plot of a process with strong positive autocorrelation. One observation and the next have a significant link, making such processes remarkably non-random. Investigating seasonality also involves graphing observations over a larger range of time intervals, or the lags. To make the time series data easier to understand and create graphs with more clarity, the time series data is combined with monthly data using the mean function. A closer look at the previous graph indicates that there are more dots on the diagonal line at 45 degrees. The second graph displays the variable’s monthly variation along the vertical axis. The lines link the points in the order of time. There is a correlation and it is significantly positive, supporting the strong seasonality of the data.\n\n\nSeasonality\n\nSeasonal HeatmapSeasonal Line plot\n\n\n\n\nCode\n# Create seasonal plot\nts_heatmap(month,color = \"Greens\", title = 'Seasonality U.S Interest Rate: 2010 - 2021')\n\n\n\n\n\n\n\n\n\n\nCode\n# Create a line graph for each year with months on the x-axis\nggseasonplot(month, datecol = \"date\", valuecol = \"value\")+ggtitle(\"Seasonal Yearly Plot forU.S Interest Rate: 2010 - 2021\")\n\n\n\n\n\n\n\n\nThe seasonality plots shows that series follow seasonality, as the heat map shows no much difference with each year and the line graph shows some kind of seasonality as th is rise in the rate during september this can be confirmed using the acf plot.\n\n\nMoving Average\n\n4 Month MA1 Year MA3 Year MA5 Year MA\n\n\n\n\nCode\n#SMA Smoothing \nma <- autoplot(month, series=\"interest_data\") +\n  autolayer(ma(month,5), series=\"4 Month MA\") +\n  xlab(\"Year\") + ylab(\"GWh\") +\n  ggtitle(\"Interest Rate January 2010 - March 2023 (4 Month Moving Average)\") +\n  scale_colour_manual(values=c(\"Data\"=\"grey50\",\"4 Month MA\"=\"red\"),\n                      breaks=c(\"Data\",\"4 Month MA\"))\nma\n\n\n\n\n\n\n\n\n\nCode\n#SMA Smoothing \nma <- autoplot(month, series=\"interest_data\") +\n  autolayer(ma(month,13), series=\"1 Year MA\") +\n  xlab(\"Year\") + ylab(\"GWh\") +\n  ggtitle(\"Interest Rate January 2010 - March 2023 (1 Year Moving Average)\") +\n  scale_colour_manual(values=c(\"Data\"=\"grey50\",\"1 Year MA\"=\"red\"),\n                      breaks=c(\"Data\",\"1 Year MA\"))\nma\n\n\n\n\n\n\n\n\n\nCode\n#SMA Smoothing \nma <- autoplot(month, series=\"interest_data\") +\n  autolayer(ma(month,37), series=\"3 Year MA\") +\n  xlab(\"Year\") + ylab(\"GWh\") +\n  ggtitle(\"Interest Rate January 2010 - March 2023 (3 Year Moving Average)\") +\n  scale_colour_manual(values=c(\"Data\"=\"grey50\",\"3 Year MA\"=\"red\"),\n                      breaks=c(\"Data\",\"3 Year MA\"))\nma\n\n\n\n\n\n\n\n\n\nCode\n#SMA Smoothing \nma <- autoplot(month, series=\"interest_data\") +\n  autolayer(ma(month,61), series=\"5 Year MA\") +\n  xlab(\"Year\") + ylab(\"GWh\") +\n  ggtitle(\"Interest Rate January 2010 - March 2023 (5 Year Moving Average)\") +\n  scale_colour_manual(values=c(\"Data\"=\"grey50\",\"5 Year MA\"=\"red\"),\n                      breaks=c(\"Data\",\"5 Year MA\"))\nma\n\n\n\n\n\n\n\n\nThe Interest Rate is displayed for the time period between January 2010 and March 2023 in the four plots above, which have been smoothed using 4-month, 1-year, 3-year, and 5-year moving averages. We can observe from the graphs that the moving average values have been rising over time. Given that it captures the short-term differences in stock prices, the 4-month MA plot exhibits a great deal of volatility, which is to be expected. The 1-year, 3-year, and 5-year MA plots, on the other hand, tame the oscillations and reveal the general direction of stock prices. We can see that the 5-year MA plot, which considers a longer time period than the other plots, shows a trend that is smoother. The 3-year MA plot similarly shows a fairly smooth trend, but unlike the 5-year MA plot, it also shows shorter-term variability. Even more susceptible to short-term changes in stock prices is the 1-year MA plot. The trend for Interest Rate isn’t stable, there is fluctuation in the trend but it seems that there is upward trend from 2020.\n\n\nAutocorrelation Time Series\n\nACFPACFADF Test\n\n\n\n\nCode\n#ACF plots for monthly data\nggAcf(myts)+ggtitle(\"ACF Plot for Interest Rate: January 2010 - March 2023\")\n\n\n\n\n\n\n\n\n\nCode\n#PACF plots for monthly data\nggPacf(myts)+ggtitle(\"PACF Plot for Interest Rate: January 2010 - March 2023\")\n\n\n\n\n\n\n\n\n\nCode\n# ADF Test\ntseries::adf.test(myts)\n\n\n\n    Augmented Dickey-Fuller Test\n\ndata:  myts\nDickey-Fuller = -1.3832, Lag order = 5, p-value = 0.8336\nalternative hypothesis: stationary\n\n\n\n\n\nThe autocorrelation function plot, which is the acf graph for monthly data, clearly shows autocorrelation in lag and seasonality. The series appears to be seasonal, according to the lag plots and autocorrelation plots displayed above, proving that it is not stable. The Augmented Dickey-Fuller Test, which reveals that the series is not stationary if the p value is more than 0.05, was also used to validate it.\n\n\nDetrend and Differenced Time Series\n\nLinear Fitting ModelACF Plot\n\n\n\n\nCode\nfit = lm(myts~time(myts), na.action=NULL) \nsummary(fit) \n\n\n\nCall:\nlm(formula = myts ~ time(myts), na.action = NULL)\n\nResiduals:\n     Min       1Q   Median       3Q      Max \n-0.97697 -0.33564  0.00804  0.23469  1.46755 \n\nCoefficients:\n              Estimate Std. Error t value Pr(>|t|)\n(Intercept) -18.300672  20.233604  -0.904    0.367\ntime(myts)    0.009339   0.010034   0.931    0.353\n\nResidual standard error: 0.4839 on 157 degrees of freedom\nMultiple R-squared:  0.005488,  Adjusted R-squared:  -0.0008465 \nF-statistic: 0.8664 on 1 and 157 DF,  p-value: 0.3534\n\n\n\n\n\n\nCode\nplot1 <- ggAcf(myts, 48, main=\"Original Data: Interest Rate\")\nplot2 <- ggAcf(resid(fit), 48, main=\"Detrended data\") \nplot3 <- ggAcf(diff(myts), 48, main=\"First differenced data\")\ngrid.arrange(plot1, plot2, plot3,nrow=3)\n\n\n\n\n\n\n\n\nThe estimated slope coefficient β1, 0.00933. With a standard error of 0.010034, yielding a significant estimated increase of stock price is very less yearly. Equation of the fit for stationary process: \\[\\hat{y}_{t} = x_{t}+(18.300672)-(0.00933.)t\\]\nFrom the above graph we can say that there is high correlation in the original plot, but in the detrended plot the correlation is reduced but there is still high correlation in the detrended data.But when the first order difference is applied the high correlation is removed but there is seasonal correlation.\nAs depicted in the above figure, the series is now stationary and ready for future study."
  },
  {
    "objectID": "eda_macroeconomic_unemployment.html",
    "href": "eda_macroeconomic_unemployment.html",
    "title": "EDA for Unemployment Rate",
    "section": "",
    "text": "Unemployment is a critical macroeconomic indicator that measures the percentage of the labor force that is actively seeking employment but unable to find work. Exploratory Data Analysis (EDA) is a powerful technique for analyzing unemployment rate data and gaining insights into the factors driving unemployment. In this page, we will explore various EDA techniques that can be applied to unemployment rate data, such as time series analysis, autocorrelation analysis, seasonality analysis, moving averages, and detrending. By the end of this page, you will have a better understanding of how to apply EDA techniques to unemployment rate data and draw valuable insights from it.\n\nTime Series Plot\n\n\nCode\n#import the data\nunemployment_rate <- read.csv(\"DATA/RAW DATA/unemployment-rate.csv\")\n\n#change date format\nunemployment_rate$Date <- as.Date(unemployment_rate$Date , \"%m/%d/%Y\")\n\n# export the data\nwrite.csv(unemployment_rate, \"DATA/CLEANED DATA/unemployment_rate_clean_data.csv\", row.names=FALSE)\n\n#plot unemployment rate \nfig <- plot_ly(unemployment_rate, x = ~Date, y = ~Value, type = 'scatter', mode = 'lines',line = list(color = 'rgb(255,215,0)'))\nfig <- fig %>% layout(title = \"U.S Unemployment Rate: January 2010 - March 2023\",xaxis = list(title = \"Time\"),yaxis = list(title =\"Unemployment Rate\"))\nfig\n\n\n\n\n\n\nThe unemployment rate in the United States has seen significant fluctuations since 2010, with various economic factors contributing to changes in the rate over the years. The data from the Federal Reserve Economic Data (FRED) series shows that the unemployment rate peaked at 9.9% in 2010, following the 2008 financial crisis. However, it has steadily declined over the years and currently stands at 3.9% as of February 2023.\nThe first half of the 2010s saw a slow but steady decline in the unemployment rate, dropping from the 9.9% peak in 2010 to 5.3% by 2018. The latter half of the decade saw even further improvements, with the rate hitting a low of 3.5% in September 2019. However, the onset of the COVID-19 pandemic in early 2020 led to a sharp increase in unemployment, with the rate skyrocketing to 14.8% in April of that year.\nSince then, the unemployment rate has been slowly but steadily improving as the economy recovers from the pandemic-induced recession. By the end of 2021, the rate had fallen to 4.2% and has continued to decline into 2022 and 2023. However, it is worth noting that some industries and sectors are still struggling to recover from the pandemic, and some individuals have not yet returned to the labor force, which could impact the overall unemployment rate.\nOverall, the unemployment rate in the United States has undergone significant fluctuations over the past decade, with various economic and social factors contributing to the changes.\nFrom the graph, it appears that the magnitude of the seasonal fluctuations in the unemployment rate has remained relatively constant over time, while the overall trend has shown both increasing and decreasing phases. Therefore, an additive decomposition method could be appropriate for analyzing the time series data.\n\n\nDecomposed Time Series\n\nDecomposition PlotAdjusted Decomposition Plot\n\n\n\n\nCode\n#convert the data to ts data\nmyts<-ts(unemployment_rate$Value,frequency=12,start=c(2010/1/1))\n#decomposition\norginial_plot <- autoplot(myts,xlab =\"Year\", ylab = \"Interest Rate\", main = \"U.S Unemployment Rate: January 2010 - March 2023\")\ndecompose = decompose(myts,\"additive\")\nautoplot(decompose)\n\n\n\n\n\n\n\n\n\nCode\ntrendadj <- myts/decompose$trend\ndecompose_adjtrend_plot <- autoplot(trendadj,ylab='seasonal') +ggtitle('Adjusted trend component in the additive time series model')\nseasonaladj <- myts/decompose$seasonal\ndecompose_adjseasonal_plot <- autoplot(seasonaladj,ylab='seasonal') +ggtitle('Adjusted seasonal component in the additive time series model')\ngrid.arrange(orginial_plot, decompose_adjtrend_plot,decompose_adjseasonal_plot, nrow=3)\n\n\n\n\n\n\n\n\nWhen compared to the original figure, the corrected seasonal component shows some seasonality in the model, but the adjusted trend component has a stable trend through time with a high increase due to pandemic, but there the trend dropped after few months.\n\n\nLag Plots\n\nDaily Time LagsMonthly Time Lags\n\n\n\n\nCode\n#Lag plots \ngglagplot(myts, do.lines=FALSE, lags=1)+xlab(\"Lag 1\")+ylab(\"Yi\")+ggtitle(\"Lag Plot for U.S Unemployment Rate: January 2010 - March 2023\")\n\n\n\n\n\n\n\n\n\nCode\n#monthly data\nmean_data <- unemployment_rate %>% \n  mutate(month = month(Date), year = year(Date)) %>% \n  group_by(year, month) %>% \n  summarize(mean_value = mean(Value))\nmonth<-ts(mean_data$mean_value,star=decimal_date(as.Date(\"2010-01-01\",format = \"%Y-%m-%d\")),frequency = 12)\n#Lag plot for monthly data\nts_lags(month)\n\n\n\n\n\n\n\n\n\nThere should be a strong relationship between the series and the pertinent lag from January 2010 to March 2023 because there is a positive correlation and an inclination angle of 45 degrees in the lag plot. This is the characteristic lag plot of a process with strong positive autocorrelation. One observation and the next have a significant link, making such processes remarkably non-random. Investigating seasonality also involves graphing observations over a larger range of time intervals, or the lags. To make the time series data easier to understand and create graphs with more clarity, the time series data is combined with monthly data using the mean function. A closer look at the previous graph indicates that there are more dots on the diagonal line at 45 degrees. The second graph displays the variable’s monthly variation along the vertical axis. The lines link the points in the order of time. There is a correlation and it is significantly positive, supporting the strong seasonality of the data.\n\n\nSeasonality\n\nSeasonal HeatmapSeasonal Line plot\n\n\n\n\nCode\n# Create seasonal plot\nts_heatmap(myts, color = \"Oranges\", title = \"Seasonality plot for Unemployment Rate USA: 2010 - 2021\")\n\n\n\n\n\n\n\n\n\n\nCode\n# Create a line graph for each year with months on the x-axis\nggseasonplot(month, datecol = \"date\", valuecol = \"value\")+ggtitle(\"Seasonal Yearly Plot forU.S Interest Rate: 2010 - 2021\")\n\n\n\n\n\n\n\n\nThe Seasonality Heatmap for the Unemployment Rate data from JAN 2010 - March 2023 does not indicate any significant seasonality in the data. The heatmap displays the mean value of the time series for each month and year combination, with darker colors indicating higher values. The absence of any discernible patterns or darker colors in specific months or years suggests that there is no consistent seasonal trend in the data. However, the yearly line graph shows some variations in the interest rates over the years, with some years showing higher rates than others. Each year’s data is represented by a line, and the months are plotted on the x-axis. Overall, the lack of clear seasonality in both the heatmap and yearly line graph suggests that other factors beyond seasonality, such as economic conditions, government policies, and global events, are likely driving the fluctuations in unemployment rates. #### Moving Average\n\n4 Month MA1 Year MA3 Year MA5 Year MA\n\n\n\n\nCode\n#SMA Smoothing \nma <- autoplot(month, series=\"unemployment_rate\") +\n  autolayer(ma(month,5), series=\"4 Month MA\") +\n  xlab(\"Year\") + ylab(\"GWh\") +\n  ggtitle(\"Unemployment Rate January 2010 - March 2023 (4 Month Moving Average)\") +\n  scale_colour_manual(values=c(\"Data\"=\"grey50\",\"4 Month MA\"=\"red\"),\n                      breaks=c(\"Data\",\"4 Month MA\"))\nma\n\n\n\n\n\n\n\n\n\nCode\n#SMA Smoothing \nma <- autoplot(month, series=\"unemployment_rate\") +\n  autolayer(ma(month,13), series=\"1 Year MA\") +\n  xlab(\"Year\") + ylab(\"GWh\") +\n  ggtitle(\"Unemployment Rate January 2010 - March 2023 (1 Year Moving Average)\") +\n  scale_colour_manual(values=c(\"Data\"=\"grey50\",\"1 Year MA\"=\"red\"),\n                      breaks=c(\"Data\",\"1 Year MA\"))\nma\n\n\n\n\n\n\n\n\n\nCode\n#SMA Smoothing \nma <- autoplot(month, series=\"unemployment_rate\") +\n  autolayer(ma(month,37), series=\"3 Year MA\") +\n  xlab(\"Year\") + ylab(\"GWh\") +\n  ggtitle(\"Unemployment Rate January 2010 - March 2023 (3 Year Moving Average)\") +\n  scale_colour_manual(values=c(\"Data\"=\"grey50\",\"3 Year MA\"=\"red\"),\n                      breaks=c(\"Data\",\"3 Year MA\"))\nma\n\n\n\n\n\n\n\n\n\nCode\n#SMA Smoothing \nma <- autoplot(month, series=\"unemployment_rate\") +\n  autolayer(ma(month,61), series=\"5 Year MA\") +\n  xlab(\"Year\") + ylab(\"GWh\") +\n  ggtitle(\"Unemployment Rate January 2010 - March 2023 (5 Year Moving Average)\") +\n  scale_colour_manual(values=c(\"Data\"=\"grey50\",\"5 Year MA\"=\"red\"),\n                      breaks=c(\"Data\",\"5 Year MA\"))\nma\n\n\n\n\n\n\n\n\nThe Unemployment Rate is displayed for the time period between January 2010 and March 2023 in the four plots above, which have been smoothed using 4-month, 1-year, 3-year, and 5-year moving averages. We can observe from the graphs that the moving average values have been rising over time. Given that it captures the short-term differences in stock prices, the 4-month MA plot exhibits a great deal of volatility, which is to be expected. The 1-year, 3-year, and 5-year MA plots, on the other hand, tame the oscillations and reveal the general direction of stock prices. We can see that the 5-year MA plot, which considers a longer time period than the other plots, shows a trend that is smoother. The 3-year MA plot similarly shows a fairly smooth trend, but unlike the 5-year MA plot, it also shows shorter-term variability. Even more susceptible to short-term changes in stock prices is the 1-year MA plot. The trend for Unemployment Rate was downward from 2010 to 2019, but there is increase in the moving average due to the increase in unemployment rate in US during pandemic.\n\n\nAutocorrelation Time Series\n\nACFPACFADF Test\n\n\n\n\nCode\n#ACF plots \nggAcf(myts)+ggtitle(\"ACF Plot for Unemployment Rate: January 2010 - March 2023\")\n\n\n\n\n\n\n\n\n\nCode\n#PACF plots for monthly data\nggPacf(myts)+ggtitle(\"PACF Plot for Unemployment Rate: January 2010 - March 2023\")\n\n\n\n\n\n\n\n\n\nCode\n# ADF Test\ntseries::adf.test(myts)\n\n\n\n    Augmented Dickey-Fuller Test\n\ndata:  myts\nDickey-Fuller = -2.7517, Lag order = 5, p-value = 0.2629\nalternative hypothesis: stationary\n\n\n\n\n\nThe autocorrelation function plot, which is the acf graph for monthly data, clearly shows autocorrelation in lag. The series appears to be seasonal, according to the lag plots and autocorrelation plots displayed above, proving that it is not stable. The Augmented Dickey-Fuller Test, which reveals that the series is not stationary if the p value is more than 0.05, was also used to validate it. The p value obtained from ADF test is greater than 0.05, which indicates taht the series is not stationary.\n\n\nDetrend and Differenced Time Series\n\nLinear Fitting ModelACF Plot\n\n\n\n\nCode\nfit = lm(myts~time(myts), na.action=NULL) \nsummary(fit) \n\n\n\nCall:\nlm(formula = myts ~ time(myts), na.action = NULL)\n\nResiduals:\n    Min      1Q  Median      3Q     Max \n-1.5761 -1.2396 -0.2468  0.7124 10.0644 \n\nCoefficients:\n             Estimate Std. Error t value Pr(>|t|)    \n(Intercept) 785.15846   72.32427   10.86   <2e-16 ***\ntime(myts)   -0.38635    0.03587  -10.77   <2e-16 ***\n---\nSignif. codes:  0 '***' 0.001 '**' 0.01 '*' 0.05 '.' 0.1 ' ' 1\n\nResidual standard error: 1.713 on 156 degrees of freedom\nMultiple R-squared:  0.4266,    Adjusted R-squared:  0.4229 \nF-statistic:   116 on 1 and 156 DF,  p-value: < 2.2e-16\n\n\n\n\n\n\nCode\n# plot ACFs\nplot1 <- ggAcf(myts, 48, main=\"Original Data: Unemployment Rate\")\nplot2 <- ggAcf(resid(fit), 48, main=\"Detrended data\") \nplot3 <- ggAcf(diff(myts), 48, main=\"First differenced data\")\ngrid.arrange(plot1, plot2, plot3,nrow=3)\n\n\n\n\n\n\n\n\nThe estimated slope coefficient β1, -0.38635 With a standard error of 0.03587, yielding a significant estimated increase of stock price is very less yearly. Equation of the fit for stationary process: \\[\\hat{y}_{t} = x_{t}+(785.15846)-(0.38635)t\\]\nFrom the above graph we can say that there is high correlation in the original plot, but in the detrended plot the correlation is reduced but there is still correlation in the detrended data.But when the first order difference is applied the high correlation is removed but there is seasonal correlation.\nAs depicted in the above figure, the series is now stationary and ready for future study."
  },
  {
    "objectID": "eda_sector_communication_services.html",
    "href": "eda_sector_communication_services.html",
    "title": "EDA for Communication Services Sector Fund",
    "section": "",
    "text": "The Communication Services Sector Fund (XLC) is an exchange-traded fund (ETF) that seeks to track the performance of companies in the communication services sector of the S&P 500 index. The fund’s holdings include companies such as Alphabet Inc., Facebook Inc., and Verizon Communications Inc. The communication services sector is a relatively new sector, created in 2018, and encompasses companies that provide communication services such as telecommunication, media, and entertainment. The XLC ETF provides investors with exposure to the growth potential of these companies, which are often at the forefront of technological innovation and changing consumer habits. The fund is relatively diversified, with holdings across different sub-sectors within communication services. As with any ETF, the XLC fund provides investors with the ability to invest in a portfolio of stocks with a single trade, making it a convenient option for those seeking exposure to the communication services sector.\n\nTime Series Plot\n\n\nCode\n# get data\noptions(\"getSymbols.warning4.0\"=FALSE)\noptions(\"getSymbols.yahoo.warning\"=FALSE)\n\n\ndata = getSymbols(\"XLC\",src='yahoo', from = '2018-06-19',to = \"2023-03-01\")\n\ndf <- data.frame(Date=index(XLC),coredata(XLC))\n\n# create Bollinger Bands\nbbands <- BBands(XLC[,c(\"XLC.High\",\"XLC.Low\",\"XLC.Close\")])\n\n# join and subset data\ndf <- subset(cbind(df, data.frame(bbands[,1:3])), Date >= \"2018-06-19\")\n\n#export the data \nxlc_data <- df\nwrite.csv(xlc_data, \"DATA/CLEANED DATA/xlc_raw_data.csv\", row.names=FALSE)\n\n# colors column for increasing and decreasing\nfor (i in 1:length(df[,1])) {\n  if (df$XLC.Close[i] >= df$XLC.Open[i]) {\n      df$direction[i] = 'Increasing'\n  } else {\n      df$direction[i] = 'Decreasing'\n  }\n}\n\ni <- list(line = list(color = '#6F9860'))\nd <- list(line = list(color = '#7F7F7F'))\n\n# plot candlestick chart\n\nfig <- df %>% plot_ly(x = ~Date, type=\"candlestick\",\n          open = ~XLC.Open, close = ~XLC.Close,\n          high = ~XLC.High, low = ~XLC.Low, name = \"XLC\",\n          increasing = i, decreasing = d) \nfig <- fig %>% add_lines(x = ~Date, y = ~up , name = \"B Bands\",\n            line = list(color = '#ccc', width = 0.5),\n            legendgroup = \"Bollinger Bands\",\n            hoverinfo = \"none\", inherit = F) \nfig <- fig %>% add_lines(x = ~Date, y = ~dn, name = \"B Bands\",\n            line = list(color = '#ccc', width = 0.5),\n            legendgroup = \"Bollinger Bands\", inherit = F,\n            showlegend = FALSE, hoverinfo = \"none\") \nfig <- fig %>% add_lines(x = ~Date, y = ~mavg, name = \"Mv Avg\",\n            line = list(color = '#E377C2', width = 0.5),\n            hoverinfo = \"none\", inherit = F) \nfig <- fig %>% layout(yaxis = list(title = \"Price\"))\n\n# plot volume bar chart\nfig2 <- df \nfig2 <- fig2 %>% plot_ly(x=~Date, y=~XLC.Volume, type='bar', name = \"XLC Volume\",\n          color = ~direction, colors = c('#6F9860','#7F7F7F')) \nfig2 <- fig2 %>% layout(yaxis = list(title = \"Volume\"))\n\n# create rangeselector buttons\nrs <- list(visible = TRUE, x = 0.5, y = -0.055,\n           xanchor = 'center', yref = 'paper',\n           font = list(size = 9),\n           buttons = list(\n             list(count=1,\n                  label='RESET',\n                  step='all'),\n             list(count=3,\n                  label='3 YR',\n                  step='year',\n                  stepmode='backward'),\n             list(count=1,\n                  label='1 YR',\n                  step='year',\n                  stepmode='backward'),\n             list(count=1,\n                  label='1 MO',\n                  step='month',\n                  stepmode='backward')\n           ))\n\n# subplot with shared x axis\nfig <- subplot(fig, fig2, heights = c(0.7,0.2), nrows=2,\n             shareX = TRUE, titleY = TRUE)\nfig <- fig %>% layout(title = paste(\"Communication Services Sector Fund Stock Price: June 2018 - March 2023\"),\n         xaxis = list(rangeselector = rs),\n         legend = list(orientation = 'h', x = 0.5, y = 1,\n                       xanchor = 'center', yref = 'paper',\n                       font = list(size = 10),\n                       bgcolor = 'transparent'))\n\nfig\n\n\n\n\n\n\nSince its inception in June 2018, the Communication Services Sector Fund (XLC) has exhibited both upward and downward trends in its stock price. The fund started off on a positive note, rising steadily in 2018 and reaching new highs by the end of the year. However, like most other stocks and funds, it experienced a significant decline in value in early 2020 due to the COVID-19 pandemic, losing over 33% of its value in just a few weeks.\nThe XLC fund then regained some of its losses, aided by government stimulus measures and low-interest rates, and continued its upward trend throughout 2020 and into 2021. However, the fund experienced some volatility during the latter part of 2021 and early 2022, with a few significant dips in its stock price. This volatility can be attributed to various factors such as rising interest rates, concerns about inflation, and geopolitical tensions.\nOverall, the Communication Services Sector Fund’s stock price has exhibited significant fluctuations since its inception in 2018, with multiple peaks and troughs. However, despite the fluctuations, the fund has exhibited an overall upward trend, reaching new all-time highs multiple times over the period. The fund’s trend is reflective of the growth potential of the communication services sector, which continues to evolve rapidly as new technologies and changing consumer habits drive innovation and demand for new services.\nFor stock prices, a multiplicative decomposition is typically preferred because the percentage changes in stock prices tend to be more important than the absolute changes. Additionally, stock prices tend to exhibit non-constant variance, meaning that the variance of the series changes over time. A multiplicative decomposition can handle this non-constant variance more effectively than an additive decomposition.\n\n\nDecomposed Time Series\n\nDecomposition PlotAdjusted Decomposition Plot\n\n\n\n\nCode\n#time series data\nmyts<-ts(df$XLC.Adjusted,frequency=252,start=c(2018,6,19), end = c(2023,3,1)) \n#original plot for time series data\norginial_plot <- autoplot(myts,xlab =\"Year\", ylab = \"Adjusted Closing Price\", main = \"Communication Services Sector Fund Stock price: June 2018 - March 2023\")\n#decompose the data\ndecompose = decompose(myts, \"multiplicative\")\n#decomposition plot\nautoplot(decompose)\n\n\n\n\n\n\n\n\n\nCode\n#adjusted plot\ntrendadj <- myts/decompose$trend\ndecompose_adjtrend_plot <- autoplot(trendadj,ylab='trend') +ggtitle('Adjusted trend component in the multiplicative time series model')\nseasonaladj <- myts/decompose$seasonal\ndecompose_adjseasonal_plot <- autoplot(seasonaladj,ylab='seasonal') +ggtitle('Adjusted seasonal component in the multiplicative time series model')\ngrid.arrange(orginial_plot, decompose_adjtrend_plot,decompose_adjseasonal_plot, nrow=3)\n\n\n\n\n\n\n\n\nThe adjusted seasonal component tend to have upward trend and there is more variability in the model when compared to the original plot where the variation during the years but the adjusted trend then to have more fluctuation showing no trend when compared to the original plot.\n\n\nLag Plots\n\nDaily Time LagsMonthly Time Lags\n\n\n\n\nCode\n#Lag plots \ngglagplot(myts, do.lines=FALSE, lags=1)+xlab(\"Lag 1\")+ylab(\"Yi\")+ggtitle(\"Lag Plot for Communication Services Sector Fund Stock June 2018 - March 2023\")\n\n\n\n\n\n\n\n\n\nCode\n#montly data\nmean_data <- df %>% \n  mutate(month = month(Date), year = year(Date)) %>% \n  group_by(year, month) %>% \n  summarize(mean_value = mean(XLC.Adjusted))\nmonth<-ts(mean_data$mean_value,start = c(2018, 6),frequency = 12)\n#Lag plot\nts_lags(month)\n\n\n\n\n\n\n\n\n\nThe first lag plot shows the daily time lags of the Communication Services Sector Fund stock price from June 2018 to March 2023. The plot indicates that there is a strong positive correlation between the current value and the previous day’s value, as seen by the points clustering along the diagonal line. This suggests that the stock price has a positive autocorrelation at a lag of one day.\nThe second lag plot shows the monthly time lags of the mean value of the Communication Services Sector Fund stock price from June 2018 to March 2023. The plot indicates that there is a positive correlation between the current value and the value from the previous month. This suggests that the mean value of the stock price has a positive autocorrelation at a lag of one month.\nOverall, the lag plots indicate that there is a positive autocorrelation present in the Communication Services Sector Fund stock price data, with the strongest correlation observed in the daily time series.\n\n\nSeasonality\n\nSeasonal HeatmapSeasonal Line plot\n\n\n\n\nCode\n# Create seasonal plot\nts_heatmap(month, color = \"PuRd\", title = 'Seasonality Heatmap of Communication Services Sector Fund Stock Jan 2018 - March 2023')\n\n\n\n\n\n\n\n\n\n\nCode\n# Create a line graph for each year with months on the x-axis\nggseasonplot(month, datecol = \"date\", valuecol = \"value\")+ggtitle(\"Seasonal Yearly Plot for Communication Services Sector Fund Stock Jan 2018 - March 2023\")\n\n\n\n\n\n\n\n\nThe Seasonality Heatmap for the Communication Services Sector Fund Stock June 2018 - March 2023 does not reveal any clear seasonality in the data. The heatmap shows the mean value of the time series for each month and year combination, with the darker colors indicating higher values. The lack of clear patterns or darker colors in specific months or years suggests that there is no consistent seasonal pattern in the data. However, the yearly line graph shows a slight upward trend in the stock price from 2018 to 2023, but does not show any clear seasonality. Each year’s data is represented by a line, and the months are plotted on the x-axis. Overall, the lack of clear seasonality in both the heatmap and yearly line graph suggests that other factors beyond seasonality are driving the stock price fluctuations.\n\n\nMoving Average\n\n4 Month MA1 Year MA3 Year MA\n\n\n\n\nCode\n#SMA Smoothing \nma <- autoplot(month, series=\"Data\") +\n  autolayer(ma(month,5), series=\"4 Month MA\") +\n  xlab(\"Year\") + ylab(\"GWh\") +\n  ggtitle(\"Communication Services Sector Fund Stock June 2018 - March 2023(4 Month Moving Average)\") +\n  scale_colour_manual(values=c(\"Data\"=\"grey50\",\"4 Month MA\"=\"red\"),\n                      breaks=c(\"Data\",\"4 Month MA\"))\nma\n\n\n\n\n\n\n\n\n\nCode\n#SMA Smoothing \nma <- autoplot(month, series=\"Data\") +\n  autolayer(ma(month,13), series=\"1 Year MA\") +\n  xlab(\"Year\") + ylab(\"GWh\") +\n  ggtitle(\"Communication Services Sector Fund Stock June 2018 - March 2023(1 Year Moving Average)\") +\n  scale_colour_manual(values=c(\"Data\"=\"grey50\",\"1 Year MA\"=\"red\"),\n                      breaks=c(\"Data\",\"1 Year MA\"))\nma\n\n\n\n\n\n\n\n\n\nCode\n#SMA Smoothing \nma <- autoplot(month, series=\"Data\") +\n  autolayer(ma(month,37), series=\"3 Year MA\") +\n  xlab(\"Year\") + ylab(\"GWh\") +\n  ggtitle(\"Communication Services Sector Fund Stock June 2018 - March 2023(3 Year Moving Average)\") +\n  scale_colour_manual(values=c(\"Data\"=\"grey50\",\"3 Year MA\"=\"red\"),\n                      breaks=c(\"Data\",\"3 Year MA\"))\nma\n\n\n\n\n\n\n\n\nThe three plots show the Communication Services Sector Fund stock prices from June 2018 to March 2023, along with the moving averages for 4 months, 1 year and 3 years. As the window of the moving average increases, the smoother the trend line becomes, reducing the impact of noise and fluctuations in the original time series.\nThe 4-month moving average plot shows frequent fluctuations in the stock price, with the trend line following the general direction of the time series. The 1-year moving average plot shows a smoother trend, following the overall upward trend of the stock price.\nThe 1-year moving average plot shows a similar trend to the 4-month plot but is even smoother, with fewer fluctuations. Finally, the 3-year moving average plot shows the smoothest trend, with an almost constant upward slope.As the moving average window increases, the smoother trend allows for a clearer identification of the general trend of the Communication Services Sector Fund stock prices over time. From the moving average obtained above we can see that there is upward tend in the stock price of Communication Services Sector Fund.\n\n\nAutocorrelation Time Series\n\nACFPACFADF Test\n\n\n\n\nCode\n#ACF for  data\nggAcf(month)+ggtitle(\"ACF Plot for Communication Services Sector Fund Stock June 2018 - March 2023\")\n\n\n\n\n\n\n\n\n\nCode\n#PACF for data\nggPacf(month)+ggtitle(\"PACF Plot for Communication Services Sector Fund Stock June 2018 - March 2023\")\n\n\n\n\n\n\n\n\n\nCode\n#check the stationarity\ntseries::adf.test(month)\n\n\n\n    Augmented Dickey-Fuller Test\n\ndata:  month\nDickey-Fuller = -1.6946, Lag order = 3, p-value = 0.6979\nalternative hypothesis: stationary\n\n\n\n\n\nIn the plot of autocorrelation function, which is the acf graph for monthly data, there are clear autocorrelation in lag. The above lag plots and autocorrelation plot indicates seasonality in the series, which means the series is not stationary.. It was also verified using Augmented Dickey-Fuller Test which tells us that as the p value is greater than 0.05, the series is not stationary.\n\n\nDetrend and Differenced Time Series\n\nLinear Fitting ModelACF Plot\n\n\n\n\nCode\nfit = lm(myts~time(myts), na.action=NULL) \nsummary(fit) \n\n\n\nCall:\nlm(formula = myts ~ time(myts), na.action = NULL)\n\nResiduals:\n    Min      1Q  Median      3Q     Max \n-19.055  -6.857  -2.895   7.079  25.301 \n\nCoefficients:\n              Estimate Std. Error t value Pr(>|t|)    \n(Intercept) -6478.0498   435.9097  -14.86   <2e-16 ***\ntime(myts)      3.2341     0.2157   14.99   <2e-16 ***\n---\nSignif. codes:  0 '***' 0.001 '**' 0.01 '*' 0.05 '.' 0.1 ' ' 1\n\nResidual standard error: 11.03 on 1256 degrees of freedom\nMultiple R-squared:  0.1518,    Adjusted R-squared:  0.1511 \nF-statistic: 224.7 on 1 and 1256 DF,  p-value: < 2.2e-16\n\n\n\n\n\n\nCode\n# plot ACFs\nplot1 <- ggAcf(myts, 48, main=\"Original Data: Communication Services Sector Fund Stock Stock Price\")\nplot2 <- ggAcf(resid(fit), 48, main=\"Detrended data\")\nplot3 <- ggAcf(diff(myts), 48, main=\"First differenced data\")\ngrid.arrange(plot1, plot2, plot3, nrow=3)\n\n\n\n\n\n\n\n\nThe estimated slope coefficient β1, 0.2562. With a standard error of 0.1795, yielding a significant estimated increase of stock price is very less yearly. Equation of the fit for stationary process: \\[\\hat{y}_{t} = x_{t}+(462.6703)-(0.2562)t\\]\nFrom the above graph we can say that there is no change in detrended plot and the original data acf plot, it typically means that the data is stationary. But when the first order difference is applied the high correlation is removed but there is no seasonal correlation.\nAs depicted in the above figure, the series is now stationary and ready for future study."
  },
  {
    "objectID": "eda_sector_consumer_discretionary.html",
    "href": "eda_sector_consumer_discretionary.html",
    "title": "EDA for Consumer Discretionary Sector Fund",
    "section": "",
    "text": "The Consumer Discretionary Sector Fund (XLY) is an exchange-traded fund (ETF) that aims to track the performance of companies within the consumer discretionary sector. This sector includes businesses that offer non-essential goods and services such as apparel, leisure, media, and retail. The XLY fund invests in companies such as Amazon, Walt Disney, Nike, and Home Depot, among others. The consumer discretionary sector is known for being highly sensitive to economic conditions and consumer sentiment, and as such, it tends to be more volatile than other sectors. This makes the XLY fund a popular choice among investors looking to take on higher levels of risk in pursuit of potentially higher returns.\n\nTime Series Plot\n\n\nCode\n# get data\noptions(\"getSymbols.warning4.0\"=FALSE)\noptions(\"getSymbols.yahoo.warning\"=FALSE)\n\n\ndata = getSymbols(\"XLY\",src='yahoo', from = '2010-01-01',to = \"2023-03-01\")\n\ndf <- data.frame(Date=index(XLY),coredata(XLY))\n\n# create Bollinger Bands\nbbands <- BBands(XLY[,c(\"XLY.High\",\"XLY.Low\",\"XLY.Close\")])\n\n# join and subset data\ndf <- subset(cbind(df, data.frame(bbands[,1:3])), Date >= \"2010-01-01\")\n\n#export the data \nXLY_data <- df\nwrite.csv(XLY_data, \"DATA/CLEANED DATA/XLY_raw_data.csv\", row.names=FALSE)\n\n# colors column for increasing and decreasing\nfor (i in 1:length(df[,1])) {\n  if (df$XLY.Close[i] >= df$XLY.Open[i]) {\n      df$direction[i] = 'Increasing'\n  } else {\n      df$direction[i] = 'Decreasing'\n  }\n}\n\ni <- list(line = list(color = '#8B3A3A'))\nd <- list(line = list(color = '#7F7F7F'))\n\n# plot candlestick chart\n\nfig <- df %>% plot_ly(x = ~Date, type=\"candlestick\",\n          open = ~XLY.Open, close = ~XLY.Close,\n          high = ~XLY.High, low = ~XLY.Low, name = \"XLY\",\n          increasing = i, decreasing = d) \nfig <- fig %>% add_lines(x = ~Date, y = ~up , name = \"B Bands\",\n            line = list(color = '#ccc', width = 0.5),\n            legendgroup = \"Bollinger Bands\",\n            hoverinfo = \"none\", inherit = F) \nfig <- fig %>% add_lines(x = ~Date, y = ~dn, name = \"B Bands\",\n            line = list(color = '#ccc', width = 0.5),\n            legendgroup = \"Bollinger Bands\", inherit = F,\n            showlegend = FALSE, hoverinfo = \"none\") \nfig <- fig %>% add_lines(x = ~Date, y = ~mavg, name = \"Mv Avg\",\n            line = list(color = '#E377C2', width = 0.5),\n            hoverinfo = \"none\", inherit = F) \nfig <- fig %>% layout(yaxis = list(title = \"Price\"))\n\n# plot volume bar chart\nfig2 <- df \nfig2 <- fig2 %>% plot_ly(x=~Date, y=~XLY.Volume, type='bar', name = \"XLY Volume\",\n          color = ~direction, colors = c('#8B3A3A','#7F7F7F')) \nfig2 <- fig2 %>% layout(yaxis = list(title = \"Volume\"))\n\n# create rangeselector buttons\nrs <- list(visible = TRUE, x = 0.5, y = -0.055,\n           xanchor = 'center', yref = 'paper',\n           font = list(size = 9),\n           buttons = list(\n             list(count=1,\n                  label='RESET',\n                  step='all'),\n             list(count=3,\n                  label='3 YR',\n                  step='year',\n                  stepmode='backward'),\n             list(count=1,\n                  label='1 YR',\n                  step='year',\n                  stepmode='backward'),\n             list(count=1,\n                  label='1 MO',\n                  step='month',\n                  stepmode='backward')\n           ))\n\n# subplot with shared x axis\nfig <- subplot(fig, fig2, heights = c(0.7,0.2), nrows=2,\n             shareX = TRUE, titleY = TRUE)\nfig <- fig %>% layout(title = paste(\"Consumer Discretionary Sector Fund Stock Price: JAN 2010 - March 2023\"),\n         xaxis = list(rangeselector = rs),\n         legend = list(orientation = 'h', x = 0.5, y = 1,\n                       xanchor = 'center', yref = 'paper',\n                       font = list(size = 10),\n                       bgcolor = 'transparent'))\n\nfig\n\n\n\n\n\n\nThe Consumer Discretionary Sector Fund (XLY) has shown an overall upward trend from 2010 to March 2023, with some fluctuations along the way. One significant fluctuation occurred during the COVID-19 pandemic in early 2020, where the XLY experienced a significant decline as people spent less on discretionary items due to economic uncertainty and lockdowns. However, it has since recovered and reached new highs.\nOne reason for the overall upward trend could be attributed to the increasing consumer spending on discretionary items over the years, boosted by a growing economy and increasing disposable income. Additionally, the rise of e-commerce and online shopping has also contributed to the sector’s growth, with many consumers opting for the convenience and accessibility of shopping online. However, competition within the sector and changing consumer trends can also affect the fund’s fluctuations.\nFor stock prices, a multiplicative decomposition is typically preferred because the percentage changes in stock prices tend to be more important than the absolute changes. Additionally, stock prices tend to exhibit non-constant variance, meaning that the variance of the series changes over time. A multiplicative decomposition can handle this non-constant variance more effectively than an additive decomposition.\n\n\nDecomposed Time Series\n\nDecomposition PlotAdjusted Decomposition Plot\n\n\n\n\nCode\n#time series data\nmyts<-ts(df$XLY.Adjusted,frequency=252,start=c(2010,01,01), end = c(2023,3,1)) \n#original plot for time series data\norginial_plot <- autoplot(myts,xlab =\"Year\", ylab = \"Adjusted Closing Price\", main = \"Consumer Discretionary Sector Fund Stock price: JAN 2010 - March 2023\")\n#decompose the data\ndecompose = decompose(myts, \"multiplicative\")\n#decomposition plot\nautoplot(decompose)\n\n\n\n\n\n\n\n\n\nCode\n#adjusted plot\ntrendadj <- myts/decompose$trend\ndecompose_adjtrend_plot <- autoplot(trendadj,ylab='trend') +ggtitle('Adjusted trend component in the multiplicative time series model')\nseasonaladj <- myts/decompose$seasonal\ndecompose_adjseasonal_plot <- autoplot(seasonaladj,ylab='seasonal') +ggtitle('Adjusted seasonal component in the multiplicative time series model')\ngrid.arrange(orginial_plot, decompose_adjtrend_plot,decompose_adjseasonal_plot, nrow=3)\n\n\n\n\n\n\n\n\nThe adjusted seasonal component tend to have upward trend till 2019 and drops during the covid period and there is more variability in the model when compared to the original plot where the variation during the years but the adjusted trend then to have more fluctuation showing no trend when compared to the original plot.\n\n\nLag Plots\n\nDaily Time LagsMonthly Time Lags\n\n\n\n\nCode\n#Lag plots \ngglagplot(myts, do.lines=FALSE, lags=1)+xlab(\"Lag 1\")+ylab(\"Yi\")+ggtitle(\"Lag Plot for Consumer Discretionary Sector Fund Stock JAN 2010 - March 2023\")\n\n\n\n\n\n\n\n\n\nCode\n#montly data\nmean_data <- df %>% \n  mutate(month = month(Date), year = year(Date)) %>% \n  group_by(year, month) %>% \n  summarize(mean_value = mean(XLY.Adjusted))\nmonth<-ts(mean_data$mean_value,start = c(2010, 1),frequency = 12)\n#Lag plot\nts_lags(month)\n\n\n\n\n\n\n\n\n\nThe first lag plot shows the daily time lags of the Consumer Discretionary Sector Fund stock price from JAN 2010 to March 2023. The plot indicates that there is a strong positive correlation between the current value and the previous day’s value, as seen by the points clustering along the diagonal line. This suggests that the stock price has a positive autocorrelation at a lag of one day.\nThe second lag plot shows the monthly time lags of the mean value of the Consumer Discretionary Sector Fund stock price from JAN 2010 to March 2023. The plot indicates that there is a positive correlation between the current value and the value from the previous month. This suggests that the mean value of the stock price has a positive autocorrelation at a lag of one month.\nOverall, the lag plots indicate that there is a positive autocorrelation present in the Consumer Discretionary Sector Fund stock price data, with the strongest correlation observed in the daily time series.\n\n\nSeasonality\n\nSeasonal HeatmapSeasonal Line plot\n\n\n\n\nCode\n# Create seasonal plot\nts_heatmap(month, color = \"PuBu\", title = 'Seasonality Heatmap of Consumer Discretionary Sector Fund Stock Jan 2010 - March 2023')\n\n\n\n\n\n\n\n\n\n\nCode\n# Create a line graph for each year with months on the x-axis\nggseasonplot(month, datecol = \"date\", valuecol = \"value\")+ggtitle(\"Seasonal Yearly Plot for Consumer Discretionary Sector Fund Stock Jan 2010 - March 2023\")\n\n\n\n\n\n\n\n\nThe Seasonality Heatmap for the Consumer Discretionary Sector Fund Stock JAN 2010 - March 2023 does not reveal any clear seasonality in the data. The heatmap shows the mean value of the time series for each month and year combination, with the darker colors indicating higher values. The lack of clear patterns or darker colors in specific months or years suggests that there is no consistent seasonal pattern in the data. However, the yearly line graph shows a slight upward trend in the stock price from 2010 to 2023, but does not show any clear seasonality. Each year’s data is represented by a line, and the months are plotted on the x-axis. Overall, the lack of clear seasonality in both the heatmap and yearly line graph suggests that other factors beyond seasonality are driving the stock price fluctuations.\n\n\nMoving Average\n\n4 Month MA1 Year MA3 Year MA5 Year MA\n\n\n\n\nCode\n#SMA Smoothing \nma <- autoplot(month, series=\"Data\") +\n  autolayer(ma(month,5), series=\"4 Month MA\") +\n  xlab(\"Year\") + ylab(\"GWh\") +\n  ggtitle(\"Consumer Discretionary Sector Fund Stock JAN 2010 - March 2023(4 Month Moving Average)\") +\n  scale_colour_manual(values=c(\"Data\"=\"grey50\",\"4 Month MA\"=\"red\"),\n                      breaks=c(\"Data\",\"4 Month MA\"))\nma\n\n\n\n\n\n\n\n\n\nCode\n#SMA Smoothing \nma <- autoplot(month, series=\"Data\") +\n  autolayer(ma(month,13), series=\"1 Year MA\") +\n  xlab(\"Year\") + ylab(\"GWh\") +\n  ggtitle(\"Consumer Discretionary Sector Fund Stock JAN 2010 - March 2023(1 Year Moving Average)\") +\n  scale_colour_manual(values=c(\"Data\"=\"grey50\",\"1 Year MA\"=\"red\"),\n                      breaks=c(\"Data\",\"1 Year MA\"))\nma\n\n\n\n\n\n\n\n\n\nCode\n#SMA Smoothing \nma <- autoplot(month, series=\"Data\") +\n  autolayer(ma(month,37), series=\"3 Year MA\") +\n  xlab(\"Year\") + ylab(\"GWh\") +\n  ggtitle(\"Consumer Discretionary Sector Fund Stock JAN 2010 - March 2023(3 Year Moving Average)\") +\n  scale_colour_manual(values=c(\"Data\"=\"grey50\",\"3 Year MA\"=\"red\"),\n                      breaks=c(\"Data\",\"3 Year MA\"))\nma\n\n\n\n\n\n\n\n\n\nCode\n#SMA Smoothing \nma <- autoplot(month, series=\"Data\") +\n  autolayer(ma(month,61), series=\"5 Year MA\") +\n  xlab(\"Year\") + ylab(\"GWh\") +\n  ggtitle(\"Consumer Discretionary Sector Fund Stock JAN 2010 - March 2023(5 Year Moving Average)\") +\n  scale_colour_manual(values=c(\"Data\"=\"grey50\",\"5 Year MA\"=\"red\"),\n                      breaks=c(\"Data\",\"5 Year MA\"))\nma\n\n\n\n\n\n\n\n\nThe four plots show the Consumer Discretionary Sector Fund stock prices from JAN 2010 to March 2023, along with the moving averages for 4 months, 1 year 3 years and 4 years. As the window of the moving average increases, the smoother the trend line becomes, reducing the impact of noise and fluctuations in the original time series.\nThe 4-month moving average plot shows frequent fluctuations in the stock price, with the trend line following the general direction of the time series. The 1-year moving average plot shows a smoother trend, following the overall upward trend of the stock price.\nThe 1-year moving average plot shows a similar trend to the 4-month plot but is even smoother, with fewer fluctuations. Finally, the 5-year moving average plot shows the smoothest trend, with an almost constant upward slope.As the moving average window increases, the smoother trend allows for a clearer identification of the general trend of the Consumer Discretionary Sector Fund stock prices over time. From the moving average obtained above we can see that there is upward tend in the stock price of Consumer Discretionary Sector Fund.\n\n\nAutocorrelation Time Series\n\nACFPACFADF Test\n\n\n\n\nCode\n#ACF for  data\nggAcf(month)+ggtitle(\"ACF Plot for Consumer Discretionary Sector Fund Stock JAN 2010 - March 2023\")\n\n\n\n\n\n\n\n\n\nCode\n#PACF for data\nggPacf(month)+ggtitle(\"PACF Plot for Consumer Discretionary Sector Fund Stock JAN 2010 - March 2023\")\n\n\n\n\n\n\n\n\n\nCode\n#check the stationarity\ntseries::adf.test(month)\n\n\n\n    Augmented Dickey-Fuller Test\n\ndata:  month\nDickey-Fuller = -2.716, Lag order = 5, p-value = 0.2778\nalternative hypothesis: stationary\n\n\n\n\n\nIn the plot of autocorrelation function, which is the acf graph for monthly data, there are clear autocorrelation in lag. The above lag plots and autocorrelation plot indicates seasonality in the series, which means the series is not stationary. This can be verified by the Augmented Dickey-Fuller Test which tells us that as the p value is greater than 0.05.\n\n\nDetrend and Differenced Time Series\n\nLinear Fitting ModelACF Plot\n\n\n\n\nCode\nfit = lm(myts~time(myts), na.action=NULL) \nsummary(fit) \n\n\n\nCall:\nlm(formula = myts ~ time(myts), na.action = NULL)\n\nResiduals:\n    Min      1Q  Median      3Q     Max \n-44.408  -9.726  -2.247   6.546  59.145 \n\nCoefficients:\n              Estimate Std. Error t value Pr(>|t|)    \n(Intercept) -2.359e+04  1.343e+02  -175.7   <2e-16 ***\ntime(myts)   1.174e+01  6.661e-02   176.3   <2e-16 ***\n---\nSignif. codes:  0 '***' 0.001 '**' 0.01 '*' 0.05 '.' 0.1 ' ' 1\n\nResidual standard error: 14.33 on 3277 degrees of freedom\nMultiple R-squared:  0.9046,    Adjusted R-squared:  0.9046 \nF-statistic: 3.108e+04 on 1 and 3277 DF,  p-value: < 2.2e-16\n\n\n\n\n\n\nCode\n# plot ACFs\nplot1 <- ggAcf(myts, 48, main=\"Original Data: Consumer Discretionary Sector Fund Stock Stock Price\")\nplot2 <- ggAcf(resid(fit), 48, main=\"Detrended data\")\nplot3 <- ggAcf(diff(myts), 48, main=\"First differenced data\")\ngrid.arrange(plot1, plot2, plot3, nrow=3)\n\n\n\n\n\n\n\n\nThe estimated slope coefficient β1, 2.0846 With a standard error of 0.1679, yielding a significant estimated increase of stock price is very less yearly. Equation of the fit for stationary process: \\[\\hat{y}_{t} = x_{t}+(4129.4701)-(2.0846)t\\]\nFrom the above graph we can say that there is no change in detrended plot and the original data acf plot, it typically means that the data is stationary. But when the first order difference is applied the high correlation is removed but there is no seasonal correlation.\nAs depicted in the above figure, the series is now stationary and ready for future study."
  },
  {
    "objectID": "eda_sector_consumer_staples.html",
    "href": "eda_sector_consumer_staples.html",
    "title": "EDA for Consumer Staples Sector Fund",
    "section": "",
    "text": "The Consumer Staples Sector Fund (XLP) is an exchange-traded fund (ETF) that provides investors with exposure to the consumer staples sector of the US economy. The fund holds a diverse range of companies, including those involved in food and beverage production, personal and household products, and tobacco. XLP is a popular choice for investors looking for stability and consistent dividends, as the companies within the sector tend to have steady earnings and demand regardless of economic conditions. Overall, XLP can be a valuable addition to a well-diversified portfolio, providing exposure to a resilient sector of the US economy.\n\nTime Series Plot\n\n\nCode\n# get data\noptions(\"getSymbols.warning4.0\"=FALSE)\noptions(\"getSymbols.yahoo.warning\"=FALSE)\n\n\ndata = getSymbols(\"XLP\",src='yahoo', from = '2010-01-01',to = \"2023-03-01\")\n\ndf <- data.frame(Date=index(XLP),coredata(XLP))\n\n# create Bollinger Bands\nbbands <- BBands(XLP[,c(\"XLP.High\",\"XLP.Low\",\"XLP.Close\")])\n\n# join and subset data\ndf <- subset(cbind(df, data.frame(bbands[,1:3])), Date >= \"2010-01-01\")\n\n#export the data \nXLP_data <- df\nwrite.csv(XLP_data, \"DATA/CLEANED DATA/XLP_raw_data.csv\", row.names=FALSE)\n\n# colors column for increasing and decreasing\nfor (i in 1:length(df[,1])) {\n  if (df$XLP.Close[i] >= df$XLP.Open[i]) {\n      df$direction[i] = 'Increasing'\n  } else {\n      df$direction[i] = 'Decreasing'\n  }\n}\n\ni <- list(line = list(color = '#2297E6'))\nd <- list(line = list(color = '#7F7F7F'))\n\n# plot candlestick chart\n\nfig <- df %>% plot_ly(x = ~Date, type=\"candlestick\",\n          open = ~XLP.Open, close = ~XLP.Close,\n          high = ~XLP.High, low = ~XLP.Low, name = \"XLP\",\n          increasing = i, decreasing = d) \nfig <- fig %>% add_lines(x = ~Date, y = ~up , name = \"B Bands\",\n            line = list(color = '#ccc', width = 0.5),\n            legendgroup = \"Bollinger Bands\",\n            hoverinfo = \"none\", inherit = F) \nfig <- fig %>% add_lines(x = ~Date, y = ~dn, name = \"B Bands\",\n            line = list(color = '#ccc', width = 0.5),\n            legendgroup = \"Bollinger Bands\", inherit = F,\n            showlegend = FALSE, hoverinfo = \"none\") \nfig <- fig %>% add_lines(x = ~Date, y = ~mavg, name = \"Mv Avg\",\n            line = list(color = '#E377C2', width = 0.5),\n            hoverinfo = \"none\", inherit = F) \nfig <- fig %>% layout(yaxis = list(title = \"Price\"))\n\n# plot volume bar chart\nfig2 <- df \nfig2 <- fig2 %>% plot_ly(x=~Date, y=~XLP.Volume, type='bar', name = \"XLP Volume\",\n          color = ~direction, colors = c('#2297E6','#7F7F7F')) \nfig2 <- fig2 %>% layout(yaxis = list(title = \"Volume\"))\n\n# create rangeselector buttons\nrs <- list(visible = TRUE, x = 0.5, y = -0.055,\n           xanchor = 'center', yref = 'paper',\n           font = list(size = 9),\n           buttons = list(\n             list(count=1,\n                  label='RESET',\n                  step='all'),\n             list(count=3,\n                  label='3 YR',\n                  step='year',\n                  stepmode='backward'),\n             list(count=1,\n                  label='1 YR',\n                  step='year',\n                  stepmode='backward'),\n             list(count=1,\n                  label='1 MO',\n                  step='month',\n                  stepmode='backward')\n           ))\n\n# subplot with shared x axis\nfig <- subplot(fig, fig2, heights = c(0.7,0.2), nrows=2,\n             shareX = TRUE, titleY = TRUE)\nfig <- fig %>% layout(title = paste(\"Consumer Staples Sector Fund Stock Price: JAN 2010 - March 2023\"),\n         xaxis = list(rangeselector = rs),\n         legend = list(orientation = 'h', x = 0.5, y = 1,\n                       xanchor = 'center', yref = 'paper',\n                       font = list(size = 10),\n                       bgcolor = 'transparent'))\n\nfig\n\n\n\n\n\n\nSince its inception in 1998, the Consumer Staples Sector Fund (XLP) has experienced various trends and fluctuations over the years. In the period from 2010 to March 2023, XLP has seen several ups and downs due to various factors, including changes in the global economy, geopolitical events, and consumer behavior.\nIn the early part of the decade, XLP experienced a relatively stable period with consistent growth, as the economy began to recover from the financial crisis of 2008. However, in 2015 and 2016, XLP experienced a downturn, as the global economy slowed down, and there was increased volatility in the markets.\nThe fund saw a significant uptick in 2017, as investors sought safety in defensive stocks amid political and economic uncertainty. The fund’s performance remained relatively strong in 2018 and 2019, as consumer staples continued to outperform other sectors.\nHowever, the COVID-19 pandemic in 2020 caused significant disruptions to the global economy, leading to a sharp decline in XLP in the first quarter. Still, the fund bounced back quickly, as consumer staples became a preferred investment for investors seeking safety during the pandemic.\nIn 2021, XLP’s performance remained relatively stable, with minor fluctuations due to inflation concerns and rising interest rates. Overall, XLP has proven to be a reliable investment option for investors looking for stability and consistent returns over the years.\nFor stock prices, a multiplicative decomposition is typically preferred because the percentage changes in stock prices tend to be more important than the absolute changes. Additionally, stock prices tend to exhibit non-constant variance, meaning that the variance of the series changes over time. A multiplicative decomposition can handle this non-constant variance more effectively than an additive decomposition.\n\n\nDecomposed Time Series\n\nDecomposition PlotAdjusted Decomposition Plot\n\n\n\n\nCode\n#time series data\nmyts<-ts(df$XLP.Adjusted,frequency=252,start=c(2010,01,01), end = c(2023,3,1)) \n#original plot for time series data\norginial_plot <- autoplot(myts,xlab =\"Year\", ylab = \"Adjusted Closing Price\", main = \"Consumer Staples Sector Fund Stock price: JAN 2010 - March 2023\")\n#decompose the data\ndecompose = decompose(myts, \"multiplicative\")\n#decomposition plot\nautoplot(decompose)\n\n\n\n\n\n\n\n\n\nCode\n#adjusted plot\ntrendadj <- myts/decompose$trend\ndecompose_adjtrend_plot <- autoplot(trendadj,ylab='trend') +ggtitle('Adjusted trend component in the multiplicative time series model')\nseasonaladj <- myts/decompose$seasonal\ndecompose_adjseasonal_plot <- autoplot(seasonaladj,ylab='seasonal') +ggtitle('Adjusted seasonal component in the multiplicative time series model')\ngrid.arrange(orginial_plot, decompose_adjtrend_plot,decompose_adjseasonal_plot, nrow=3)\n\n\n\n\n\n\n\n\nThe adjusted seasonal component tend to have upward trend and there is more variability in the model when compared to the original plot where the variation during the years but the adjusted trend then to have more fluctuation showing no trend when compared to the original plot.\n\n\nLag Plots\n\nDaily Time LagsMonthly Time Lags\n\n\n\n\nCode\n#Lag plots \ngglagplot(myts, do.lines=FALSE, lags=1)+xlab(\"Lag 1\")+ylab(\"Yi\")+ggtitle(\"Lag Plot for Consumer Staples Sector Fund Stock JAN 2010 - March 2023\")\n\n\n\n\n\n\n\n\n\nCode\n#montly data\nmean_data <- df %>% \n  mutate(month = month(Date), year = year(Date)) %>% \n  group_by(year, month) %>% \n  summarize(mean_value = mean(XLP.Adjusted))\nmonth<-ts(mean_data$mean_value,start = c(2010, 1),frequency = 12)\n#Lag plot\nts_lags(month)\n\n\n\n\n\n\n\n\n\nThe first lag plot shows the daily time lags of the Consumer Staples Sector Fund stock price from JAN 2010 to March 2023. The plot indicates that there is a strong positive correlation between the current value and the previous day’s value, as seen by the points clustering along the diagonal line. This suggests that the stock price has a positive autocorrelation at a lag of one day.\nThe second lag plot shows the monthly time lags of the mean value of the Consumer Staples Sector Fund stock price from JAN 2010 to March 2023. The plot indicates that there is a positive correlation between the current value and the value from the previous month. This suggests that the mean value of the stock price has a positive autocorrelation at a lag of one month.\nOverall, the lag plots indicate that there is a positive autocorrelation present in the Consumer Staples Sector Fund stock price data, with the strongest correlation observed in the daily time series.\n\n\nSeasonality\n\nSeasonal HeatmapSeasonal Line plot\n\n\n\n\nCode\n# Create seasonal plot\nts_heatmap(month, color = \"BuGn\", title = 'Seasonality Heatmap of Consumer Staples Sector Fund Stock Jan 2010 - March 2023')\n\n\n\n\n\n\n\n\n\n\nCode\n# Create a line graph for each year with months on the x-axis\nggseasonplot(month, datecol = \"date\", valuecol = \"value\")+ggtitle(\"Seasonal Yearly Plot for Consumer Staples Sector Fund Stock Jan 2010 - March 2023\")\n\n\n\n\n\n\n\n\nThe Seasonality Heatmap for the Consumer Staples Sector Fund Stock JAN 2010 - March 2023 does not reveal any clear seasonality in the data. The heatmap shows the mean value of the time series for each month and year combination, with the darker colors indicating higher values. The lack of clear patterns or darker colors in specific months or years suggests that there is no consistent seasonal pattern in the data. However, the yearly line graph shows a slight upward trend in the stock price from 2010 to 2023, but does not show any clear seasonality. Each year’s data is represented by a line, and the months are plotted on the x-axis. Overall, the lack of clear seasonality in both the heatmap and yearly line graph suggests that other factors beyond seasonality are driving the stock price fluctuations.\n\n\nMoving Average\n\n4 Month MA1 Year MA3 Year MA5 Year MA\n\n\n\n\nCode\n#SMA Smoothing \nma <- autoplot(month, series=\"Data\") +\n  autolayer(ma(month,5), series=\"4 Month MA\") +\n  xlab(\"Year\") + ylab(\"GWh\") +\n  ggtitle(\"Consumer Staples Sector Fund Stock JAN 2010 - March 2023(4 Month Moving Average)\") +\n  scale_colour_manual(values=c(\"Data\"=\"grey50\",\"4 Month MA\"=\"red\"),\n                      breaks=c(\"Data\",\"4 Month MA\"))\nma\n\n\n\n\n\n\n\n\n\nCode\n#SMA Smoothing \nma <- autoplot(month, series=\"Data\") +\n  autolayer(ma(month,13), series=\"1 Year MA\") +\n  xlab(\"Year\") + ylab(\"GWh\") +\n  ggtitle(\"Consumer Staples Sector Fund Stock JAN 2010 - March 2023(1 Year Moving Average)\") +\n  scale_colour_manual(values=c(\"Data\"=\"grey50\",\"1 Year MA\"=\"red\"),\n                      breaks=c(\"Data\",\"1 Year MA\"))\nma\n\n\n\n\n\n\n\n\n\nCode\n#SMA Smoothing \nma <- autoplot(month, series=\"Data\") +\n  autolayer(ma(month,37), series=\"3 Year MA\") +\n  xlab(\"Year\") + ylab(\"GWh\") +\n  ggtitle(\"Consumer Staples Sector Fund Stock JAN 2010 - March 2023(3 Year Moving Average)\") +\n  scale_colour_manual(values=c(\"Data\"=\"grey50\",\"3 Year MA\"=\"red\"),\n                      breaks=c(\"Data\",\"3 Year MA\"))\nma\n\n\n\n\n\n\n\n\n\nCode\n#SMA Smoothing \nma <- autoplot(month, series=\"Data\") +\n  autolayer(ma(month,61), series=\"5 Year MA\") +\n  xlab(\"Year\") + ylab(\"GWh\") +\n  ggtitle(\"Consumer Staples Sector Fund Stock JAN 2010 - March 2023(5 Year Moving Average)\") +\n  scale_colour_manual(values=c(\"Data\"=\"grey50\",\"5 Year MA\"=\"red\"),\n                      breaks=c(\"Data\",\"5 Year MA\"))\nma\n\n\n\n\n\n\n\n\nThe four plots show the Consumer Staples Sector Fund stock prices from JAN 2010 to March 2023, along with the moving averages for 4 months, 1 year 3 years and 4 years. As the window of the moving average increases, the smoother the trend line becomes, reducing the impact of noise and fluctuations in the original time series.\nThe 4-month moving average plot shows frequent fluctuations in the stock price, with the trend line following the general direction of the time series. The 1-year moving average plot shows a smoother trend, following the overall upward trend of the stock price.\nThe 1-year moving average plot shows a similar trend to the 4-month plot but is even smoother, with fewer fluctuations. Finally, the 5-year moving average plot shows the smoothest trend, with an almost constant upward slope.As the moving average window increases, the smoother trend allows for a clearer identification of the general trend of the Consumer Staples Sector Fund stock prices over time. From the moving average obtained above we can see that there is upward tend in the stock price of Consumer Staples Sector Fund.\n\n\nAutocorrelation Time Series\n\nACFPACFADF Test\n\n\n\n\nCode\n#ACF for  data\nggAcf(month)+ggtitle(\"ACF Plot for Consumer Staples Sector Fund Stock JAN 2010 - March 2023\")\n\n\n\n\n\n\n\n\n\nCode\n#PACF for data\nggPacf(month)+ggtitle(\"PACF Plot for Consumer Staples Sector Fund Stock JAN 2010 - March 2023\")\n\n\n\n\n\n\n\n\n\nCode\n#check the stationarity\ntseries::adf.test(month)\n\n\n\n    Augmented Dickey-Fuller Test\n\ndata:  month\nDickey-Fuller = -2.0836, Lag order = 5, p-value = 0.5415\nalternative hypothesis: stationary\n\n\n\n\n\nIn the plot of autocorrelation function, which is the acf graph for monthly data, there are clear autocorrelation in lag. The above lag plots and autocorrelation plot indicates seasonality in the series, which means the series is not stationary. It was also verified using Augmented Dickey-Fuller Test which tells us that as the p value is greater than 0.05, the series is not stationary.\n\n\nDetrend and Differenced Time Series\n\nLinear Fitting ModelACF Plot\n\n\n\n\nCode\nfit = lm(myts~time(myts), na.action=NULL) \nsummary(fit) \n\n\n\nCall:\nlm(formula = myts ~ time(myts), na.action = NULL)\n\nResiduals:\n     Min       1Q   Median       3Q      Max \n-13.5256  -1.0564   0.2343   1.3352  10.7104 \n\nCoefficients:\n              Estimate Std. Error t value Pr(>|t|)    \n(Intercept) -8.255e+03  2.630e+01  -313.9   <2e-16 ***\ntime(myts)   4.115e+00  1.304e-02   315.5   <2e-16 ***\n---\nSignif. codes:  0 '***' 0.001 '**' 0.01 '*' 0.05 '.' 0.1 ' ' 1\n\nResidual standard error: 2.805 on 3277 degrees of freedom\nMultiple R-squared:  0.9681,    Adjusted R-squared:  0.9681 \nF-statistic: 9.953e+04 on 1 and 3277 DF,  p-value: < 2.2e-16\n\n\n\n\n\n\nCode\n# plot ACFs\nplot1 <- ggAcf(myts, 48, main=\"Original Data: Consumer Staples Sector Fund Stock Stock Price\")\nplot2 <- ggAcf(resid(fit), 48, main=\"Detrended data\")\nplot3 <- ggAcf(diff(myts), 48, main=\"First differenced data\")\ngrid.arrange(plot1, plot2, plot3, nrow=3)\n\n\n\n\n\n\n\n\nThe estimated slope coefficient β1, 0.7645 With a standard error of 0.0597, yielding a significant estimated increase of stock price is very less yearly. Equation of the fit for stationary process: \\[\\hat{y}_{t} = x_{t}+(1502.6506)-(0.7645)t\\]\nFrom the above graph we can say that there is no change in detrended plot and the original data acf plot, it typically means that the data is stationary. But when the first order difference is applied the high correlation is removed but there is no seasonal correlation.\nAs depicted in the above figure, the series is now stationary and ready for future study."
  },
  {
    "objectID": "eda_sector_energy.html",
    "href": "eda_sector_energy.html",
    "title": "EDA for Energy Sector Fund",
    "section": "",
    "text": "The Energy Sector Fund (XLE) tracks the performance of companies involved in the exploration, production, and distribution of energy. From 2010 to March 2023, the fund has shown a lot of volatility due to the fluctuating prices of oil and natural gas. During this period, the fund experienced a significant decline in 2014-2015 as oil prices dropped sharply, but it rebounded in 2016-2017 as prices stabilized. However, the fund experienced another sharp decline in 2020 due to the COVID-19 pandemic and a subsequent drop in demand for energy. The trend for this sector is closely tied to geopolitical events, global economic growth, and changes in regulations. As the world transitions towards renewable energy sources, it remains to be seen how the energy sector will perform in the future.\n\nTime Series Plot\n\n\nCode\n# get data\noptions(\"getSymbols.warning4.0\"=FALSE)\noptions(\"getSymbols.yahoo.warning\"=FALSE)\n\n\ndata = getSymbols(\"XLE\",src='yahoo', from = '2010-01-01',to = \"2023-03-01\")\n\ndf <- data.frame(Date=index(XLE),coredata(XLE))\n\n# create Bollinger Bands\nbbands <- BBands(XLE[,c(\"XLE.High\",\"XLE.Low\",\"XLE.Close\")])\n\n# join and subset data\ndf <- subset(cbind(df, data.frame(bbands[,1:3])), Date >= \"2010-01-01\")\n\n#export the data \nXLE_data <- df\nwrite.csv(XLE_data, \"DATA/CLEANED DATA/XLE_raw_data.csv\", row.names=FALSE)\n\n# colors column for increasing and decreasing\nfor (i in 1:length(df[,1])) {\n  if (df$XLE.Close[i] >= df$XLE.Open[i]) {\n      df$direction[i] = 'Increasing'\n  } else {\n      df$direction[i] = 'Decreasing'\n  }\n}\n\ni <- list(line = list(color = '#FFE4E1'))\nd <- list(line = list(color = '#7F7F7F'))\n\n# plot candlestick chart\n\nfig <- df %>% plot_ly(x = ~Date, type=\"candlestick\",\n          open = ~XLE.Open, close = ~XLE.Close,\n          high = ~XLE.High, low = ~XLE.Low, name = \"XLE\",\n          increasing = i, decreasing = d) \nfig <- fig %>% add_lines(x = ~Date, y = ~up , name = \"B Bands\",\n            line = list(color = '#ccc', width = 0.5),\n            legendgroup = \"Bollinger Bands\",\n            hoverinfo = \"none\", inherit = F) \nfig <- fig %>% add_lines(x = ~Date, y = ~dn, name = \"B Bands\",\n            line = list(color = '#ccc', width = 0.5),\n            legendgroup = \"Bollinger Bands\", inherit = F,\n            showlegend = FALSE, hoverinfo = \"none\") \nfig <- fig %>% add_lines(x = ~Date, y = ~mavg, name = \"Mv Avg\",\n            line = list(color = '#E377C2', width = 0.5),\n            hoverinfo = \"none\", inherit = F) \nfig <- fig %>% layout(yaxis = list(title = \"Price\"))\n\n# plot volume bar chart\nfig2 <- df \nfig2 <- fig2 %>% plot_ly(x=~Date, y=~XLE.Volume, type='bar', name = \"XLE Volume\",\n          color = ~direction, colors = c('#FFE4E1','#7F7F7F')) \nfig2 <- fig2 %>% layout(yaxis = list(title = \"Volume\"))\n\n# create rangeselector buttons\nrs <- list(visible = TRUE, x = 0.5, y = -0.055,\n           xanchor = 'center', yref = 'paper',\n           font = list(size = 9),\n           buttons = list(\n             list(count=1,\n                  label='RESET',\n                  step='all'),\n             list(count=3,\n                  label='3 YR',\n                  step='year',\n                  stepmode='backward'),\n             list(count=1,\n                  label='1 YR',\n                  step='year',\n                  stepmode='backward'),\n             list(count=1,\n                  label='1 MO',\n                  step='month',\n                  stepmode='backward')\n           ))\n\n# subplot with shared x axis\nfig <- subplot(fig, fig2, heights = c(0.7,0.2), nrows=2,\n             shareX = TRUE, titleY = TRUE)\nfig <- fig %>% layout(title = paste(\"Energy Sector Fund Stock Price: JAN 2010 - March 2023\"),\n         xaxis = list(rangeselector = rs),\n         legend = list(orientation = 'h', x = 0.5, y = 1,\n                       xanchor = 'center', yref = 'paper',\n                       font = list(size = 10),\n                       bgcolor = 'transparent'))\n\nfig\n\n\n\n\n\n\nThe Energy Sector Fund (XLE) is an exchange-traded fund that tracks the performance of the energy sector in the S&P 500 index. The fund includes companies involved in exploration, production, refining, and distribution of energy. From 2010 to March 2023, the XLE experienced significant fluctuations due to several factors.\nIn 2010-2014, the XLE saw steady growth due to rising demand for energy, increased production from shale, and higher oil prices. However, in mid-2014, the global oil market entered a period of oversupply, and oil prices crashed. This led to a sharp decline in XLE’s value, which lasted until early 2016.\nBetween 2016 and early 2020, XLE recovered and experienced moderate growth, fueled by global economic growth and higher demand for oil. However, in early 2020, the COVID-19 pandemic caused a sharp decline in energy demand, resulting in another crash in oil prices and a significant drop in XLE’s value.\nIn the second half of 2020 and early 2021, XLE began to recover as oil prices stabilized and energy demand gradually picked up. However, concerns over inflation and rising interest rates caused a drop in XLE’s value in the first quarter of 2022.\nOverall, the Energy Sector Fund has been highly sensitive to fluctuations in oil prices and global energy demand, which can be affected by various factors such as geopolitical events, technological advancements, and economic conditions.\nFor stock prices, a multiplicative decomposition is typically preferred because the percentage changes in stock prices tend to be more important than the absolute changes. Additionally, stock prices tend to exhibit non-constant variance, meaning that the variance of the series changes over time. A multiplicative decomposition can handle this non-constant variance more effectively than an additive decomposition.\n\n\nDecomposed Time Series\n\nDecomposition PlotAdjusted Decomposition Plot\n\n\n\n\nCode\n#time series data\nmyts<-ts(df$XLE.Adjusted,frequency=252,start=c(2010,01,01), end = c(2023,3,1)) \n#original plot for time series data\norginial_plot <- autoplot(myts,xlab =\"Year\", ylab = \"Adjusted Closing Price\", main = \"Energy Sector Fund Stock price: JAN 2010 - March 2023\")\n#decompose the data\ndecompose = decompose(myts, \"multiplicative\")\n#decomposition plot\nautoplot(decompose)\n\n\n\n\n\n\n\n\n\nCode\n#adjusted plot\ntrendadj <- myts/decompose$trend\ndecompose_adjtrend_plot <- autoplot(trendadj,ylab='trend') +ggtitle('Adjusted trend component in the multiplicative time series model')\nseasonaladj <- myts/decompose$seasonal\ndecompose_adjseasonal_plot <- autoplot(seasonaladj,ylab='seasonal') +ggtitle('Adjusted seasonal component in the multiplicative time series model')\ngrid.arrange(orginial_plot, decompose_adjtrend_plot,decompose_adjseasonal_plot, nrow=3)\n\n\n\n\n\n\n\n\nThe adjusted seasonal component tend to have upward trend till 2019 and drops during the covid period and there is more variability in the model when compared to the original plot where the variation during the years but the adjusted trend then to have more fluctuation showing no trend when compared to the original plot.\n\n\nLag Plots\n\nDaily Time LagsMonthly Time Lags\n\n\n\n\nCode\n#Lag plots \ngglagplot(myts, do.lines=FALSE, lags=1)+xlab(\"Lag 1\")+ylab(\"Yi\")+ggtitle(\"Lag Plot for Energy Sector Fund Stock JAN 2010 - March 2023\")\n\n\n\n\n\n\n\n\n\nCode\n#montly data\nmean_data <- df %>% \n  mutate(month = month(Date), year = year(Date)) %>% \n  group_by(year, month) %>% \n  summarize(mean_value = mean(XLE.Adjusted))\nmonth<-ts(mean_data$mean_value,start = c(2010, 1),frequency = 12)\n#Lag plot\nts_lags(month)\n\n\n\n\n\n\n\n\n\nThe first lag plot shows a scatter plot of the Energy Sector Fund stock prices against their lagged values by one time period. The plot shows a strong linear pattern, indicating a high correlation between the stock prices and their lagged values. This suggests that there is a high level of persistence in the Energy Sector Fund stock prices, meaning that the past prices are a good predictor of the future prices.\nThe second lag plot shows a similar pattern for the mean monthly values of the Energy Sector Fund stock prices. The plot shows a strong linear pattern, indicating a positive correlation between the mean monthly values and their lagged values by one time period. This suggests that there is a high level of persistence in the mean monthly values of the Energy Sector Fund stock prices, meaning that the past monthly means are a good predictor of the future monthly means. Overall, the lag plot outputs suggest that the Energy Sector Fund stock prices have a high level of autocorrelation, which is consistent with the trend and fluctuations in the sector.\n\n\nSeasonality\n\nSeasonal HeatmapSeasonal Line plot\n\n\n\n\nCode\n# Create seasonal plot\nts_heatmap(month, color = \"YlOrRd\", title = 'Seasonality Heatmap of Energy Sector Fund Stock Jan 2010 - March 2023')\n\n\n\n\n\n\n\n\n\n\nCode\n# Create a line graph for each year with months on the x-axis\nggseasonplot(month, datecol = \"date\", valuecol = \"value\")+ggtitle(\"Seasonal Yearly Plot for Energy Sector Fund Stock Jan 2010 - March 2023\")\n\n\n\n\n\n\n\n\nThe Seasonality Heatmap for the Energy Sector Fund Stock JAN 2010 - March 2023 does not reveal any clear seasonality in the data. The heatmap shows the mean value of the time series for each month and year combination, with the darker colors indicating higher values. The lack of clear patterns or darker colors in specific months or years suggests that there is no consistent seasonal pattern in the data. However, the yearly line graph shows a slight upward trend in the stock price from 2010 to 2023, but does not show any clear seasonality. Each year’s data is represented by a line, and the months are plotted on the x-axis. Overall, the lack of clear seasonality in both the heatmap and yearly line graph suggests that other factors beyond seasonality are driving the stock price fluctuations.\n\n\nMoving Average\n\n4 Month MA1 Year MA3 Year MA5 Year MA\n\n\n\n\nCode\n#SMA Smoothing \nma <- autoplot(month, series=\"Data\") +\n  autolayer(ma(month,5), series=\"4 Month MA\") +\n  xlab(\"Year\") + ylab(\"GWh\") +\n  ggtitle(\"Energy Sector Fund Stock JAN 2010 - March 2023(4 Month Moving Average)\") +\n  scale_colour_manual(values=c(\"Data\"=\"grey50\",\"4 Month MA\"=\"red\"),\n                      breaks=c(\"Data\",\"4 Month MA\"))\nma\n\n\n\n\n\n\n\n\n\nCode\n#SMA Smoothing \nma <- autoplot(month, series=\"Data\") +\n  autolayer(ma(month,13), series=\"1 Year MA\") +\n  xlab(\"Year\") + ylab(\"GWh\") +\n  ggtitle(\"Energy Sector Fund Stock JAN 2010 - March 2023(1 Year Moving Average)\") +\n  scale_colour_manual(values=c(\"Data\"=\"grey50\",\"1 Year MA\"=\"red\"),\n                      breaks=c(\"Data\",\"1 Year MA\"))\nma\n\n\n\n\n\n\n\n\n\nCode\n#SMA Smoothing \nma <- autoplot(month, series=\"Data\") +\n  autolayer(ma(month,37), series=\"3 Year MA\") +\n  xlab(\"Year\") + ylab(\"GWh\") +\n  ggtitle(\"Energy Sector Fund Stock JAN 2010 - March 2023(3 Year Moving Average)\") +\n  scale_colour_manual(values=c(\"Data\"=\"grey50\",\"3 Year MA\"=\"red\"),\n                      breaks=c(\"Data\",\"3 Year MA\"))\nma\n\n\n\n\n\n\n\n\n\nCode\n#SMA Smoothing \nma <- autoplot(month, series=\"Data\") +\n  autolayer(ma(month,61), series=\"5 Year MA\") +\n  xlab(\"Year\") + ylab(\"GWh\") +\n  ggtitle(\"Energy Sector Fund Stock JAN 2010 - March 2023(5 Year Moving Average)\") +\n  scale_colour_manual(values=c(\"Data\"=\"grey50\",\"5 Year MA\"=\"red\"),\n                      breaks=c(\"Data\",\"5 Year MA\"))\nma\n\n\n\n\n\n\n\n\nThe four plots show the Energy Sector Fund stock prices from JAN 2010 to March 2023, along with the moving averages for 4 months, 1 year 3 years and 4 years. As the window of the moving average increases, the smoother the trend line becomes, reducing the impact of noise and fluctuations in the original time series.\nThe 4-month moving average plot shows frequent fluctuations in the stock price, with the trend line following the general direction of the time series. The 1-year moving average plot shows a smoother trend, following the overall upward trend of the stock price.\nThe 1-year moving average plot shows a similar trend to the 4-month plot but is even smoother, with fewer fluctuations. Finally, the 5-year moving average plot shows the smoothest trend, with downward and upward slope.As the moving average window increases, the smoother trend allows for a clearer identification of the general trend of the Energy Sector Fund stock prices over time. From the moving average obtained above we can see that there is upward tend in the stock price of Energy Sector Fund.\n\n\nAutocorrelation Time Series\n\nACFPACFADF Test\n\n\n\n\nCode\n#ACF for  data\nggAcf(month)+ggtitle(\"ACF Plot for Energy Sector Fund Stock JAN 2010 - March 2023\")\n\n\n\n\n\n\n\n\n\nCode\n#PACF for data\nggPacf(month)+ggtitle(\"PACF Plot for Energy Sector Fund Stock JAN 2010 - March 2023\")\n\n\n\n\n\n\n\n\n\nCode\n#check the stationarity\ntseries::adf.test(month)\n\n\n\n    Augmented Dickey-Fuller Test\n\ndata:  month\nDickey-Fuller = -1.5196, Lag order = 5, p-value = 0.7767\nalternative hypothesis: stationary\n\n\n\n\n\nIn the plot of autocorrelation function, which is the acf graph for monthly data, there are clear autocorrelation in lag. The above lag plots and autocorrelation plot indicates seasonality in the series, which means the series is not stationary. This can be verified by the Augmented Dickey-Fuller Test which tells us that as the p value is greater than 0.05.\n\n\nDetrend and Differenced Time Series\n\nLinear Fitting ModelACF Plot\n\n\n\n\nCode\nfit = lm(myts~time(myts), na.action=NULL) \nsummary(fit) \n\n\n\nCall:\nlm(formula = myts ~ time(myts), na.action = NULL)\n\nResiduals:\n    Min      1Q  Median      3Q     Max \n-33.032  -5.253  -0.391   5.337  34.840 \n\nCoefficients:\n              Estimate Std. Error t value Pr(>|t|)    \n(Intercept) -1.522e+03  9.807e+01  -15.52   <2e-16 ***\ntime(myts)   7.798e-01  4.863e-02   16.03   <2e-16 ***\n---\nSignif. codes:  0 '***' 0.001 '**' 0.01 '*' 0.05 '.' 0.1 ' ' 1\n\nResidual standard error: 10.46 on 3277 degrees of freedom\nMultiple R-squared:  0.07274,   Adjusted R-squared:  0.07245 \nF-statistic: 257.1 on 1 and 3277 DF,  p-value: < 2.2e-16\n\n\n\n\n\n\nCode\n# plot ACFs\nplot1 <- ggAcf(myts, 48, main=\"Original Data: Energy Sector Fund Stock Stock Price\")\nplot2 <- ggAcf(resid(fit), 48, main=\"Detrended data\")\nplot3 <- ggAcf(diff(myts), 48, main=\"First differenced data\")\ngrid.arrange(plot1, plot2, plot3, nrow=3)\n\n\n\n\n\n\n\n\nThe estimated slope coefficient β1, 5.359e-01 With a standard error of 4.131e-02, yielding a significant estimated increase of stock price is very less yearly. Equation of the fit for stationary process: \\[\\hat{y}_{t} = x_{t}+(1.029e+03)-(5.359e-01)t\\]\nFrom the above graph we can say that there is no change in detrended plot and the original data acf plot, it typically means that the data is stationary. But when the first order difference is applied the high correlation is removed but there is no seasonal correlation.\nAs depicted in the above figure, the series is now stationary and ready for future study."
  },
  {
    "objectID": "eda_sector_financial.html",
    "href": "eda_sector_financial.html",
    "title": "EDA for Financial Sector Fund",
    "section": "",
    "text": "The Financial Sector Fund (XLF) is an exchange-traded fund that tracks the performance of companies in the financial sector, including banks, insurance companies, and other financial services firms. The fund provides investors with exposure to a diversified portfolio of financial stocks and offers a convenient way to gain access to this sector of the economy. As the financial sector is highly sensitive to changes in interest rates and economic conditions, the XLF can experience significant fluctuations in response to macroeconomic events and shifts in investor sentiment. The performance of the XLF is closely tied to the overall health of the economy, making it an important indicator of economic growth and stability.\n\nTime Series Plot\n\n\nCode\n# get data\noptions(\"getSymbols.warning4.0\"=FALSE)\noptions(\"getSymbols.yahoo.warning\"=FALSE)\n\n\ndata = getSymbols(\"XLF\",src='yahoo', from = '2010-01-01',to = \"2023-03-01\")\n\ndf <- data.frame(Date=index(XLF),coredata(XLF))\n\n# create Bollinger Bands\nbbands <- BBands(XLF[,c(\"XLF.High\",\"XLF.Low\",\"XLF.Close\")])\n\n# join and subset data\ndf <- subset(cbind(df, data.frame(bbands[,1:3])), Date >= \"2010-01-01\")\n\n#export the data \nXLF_data <- df\nwrite.csv(XLF_data, \"DATA/CLEANED DATA/XLF_raw_data.csv\", row.names=FALSE)\n\n# colors column for increasing and decreasing\nfor (i in 1:length(df[,1])) {\n  if (df$XLF.Close[i] >= df$XLF.Open[i]) {\n      df$direction[i] = 'Increasing'\n  } else {\n      df$direction[i] = 'Decreasing'\n  }\n}\n\ni <- list(line = list(color = '#8B3A62'))\nd <- list(line = list(color = '#7F7F7F'))\n\n# plot candlestick chart\n\nfig <- df %>% plot_ly(x = ~Date, type=\"candlestick\",\n          open = ~XLF.Open, close = ~XLF.Close,\n          high = ~XLF.High, low = ~XLF.Low, name = \"XLF\",\n          increasing = i, decreasing = d) \nfig <- fig %>% add_lines(x = ~Date, y = ~up , name = \"B Bands\",\n            line = list(color = '#ccc', width = 0.5),\n            legendgroup = \"Bollinger Bands\",\n            hoverinfo = \"none\", inherit = F) \nfig <- fig %>% add_lines(x = ~Date, y = ~dn, name = \"B Bands\",\n            line = list(color = '#ccc', width = 0.5),\n            legendgroup = \"Bollinger Bands\", inherit = F,\n            showlegend = FALSE, hoverinfo = \"none\") \nfig <- fig %>% add_lines(x = ~Date, y = ~mavg, name = \"Mv Avg\",\n            line = list(color = '#E377C2', width = 0.5),\n            hoverinfo = \"none\", inherit = F) \nfig <- fig %>% layout(yaxis = list(title = \"Price\"))\n\n# plot volume bar chart\nfig2 <- df \nfig2 <- fig2 %>% plot_ly(x=~Date, y=~XLF.Volume, type='bar', name = \"XLF Volume\",\n          color = ~direction, colors = c('#8B3A62','#7F7F7F')) \nfig2 <- fig2 %>% layout(yaxis = list(title = \"Volume\"))\n\n# create rangeselector buttons\nrs <- list(visible = TRUE, x = 0.5, y = -0.055,\n           xanchor = 'center', yref = 'paper',\n           font = list(size = 9),\n           buttons = list(\n             list(count=1,\n                  label='RESET',\n                  step='all'),\n             list(count=3,\n                  label='3 YR',\n                  step='year',\n                  stepmode='backward'),\n             list(count=1,\n                  label='1 YR',\n                  step='year',\n                  stepmode='backward'),\n             list(count=1,\n                  label='1 MO',\n                  step='month',\n                  stepmode='backward')\n           ))\n\n# subplot with shared x axis\nfig <- subplot(fig, fig2, heights = c(0.7,0.2), nrows=2,\n             shareX = TRUE, titleY = TRUE)\nfig <- fig %>% layout(title = paste(\"The Financial Sector Fund Stock Price: JAN 2010 - March 2023\"),\n         xaxis = list(rangeselector = rs),\n         legend = list(orientation = 'h', x = 0.5, y = 1,\n                       xanchor = 'center', yref = 'paper',\n                       font = list(size = 10),\n                       bgcolor = 'transparent'))\n\nfig\n\n\n\n\n\n\nFrom 2010 to early 2020, XLF had an overall upward trend, with occasional fluctuations due to various economic and political events. However, in March 2020, the COVID-19 pandemic caused a sharp drop in the value of the fund, as the financial sector was heavily impacted by the economic downturn. The Federal Reserve’s actions to lower interest rates and provide monetary stimulus helped to stabilize the market, and XLF began to recover in the second half of 2020.\nIn 2021, XLF experienced further growth as the economy continued to recover and interest rates remained low. However, there have been concerns about inflation and the potential for interest rate hikes, which could impact the financial sector. Additionally, the regulatory environment for banks and financial institutions could also affect the performance of XLF. Overall, XLF has shown a mix of stability and volatility in response to economic and political factors over the past decade.\nFor stock prices, a multiplicative decomposition is typically preferred because the percentage changes in stock prices tend to be more important than the absolute changes. Additionally, stock prices tend to exhibit non-constant variance, meaning that the variance of the series changes over time. A multiplicative decomposition can handle this non-constant variance more effectively than an additive decomposition.\n\n\nDecomposed Time Series\n\nDecomposition PlotAdjusted Decomposition Plot\n\n\n\n\nCode\n#time series data\nmyts<-ts(df$XLF.Adjusted,frequency=252,start=c(2010,01,01), end = c(2023,3,1)) \n#original plot for time series data\norginial_plot <- autoplot(myts,xlab =\"Year\", ylab = \"Adjusted Closing Price\", main = \"The Financial Sector Fund Stock price: JAN 2010 - March 2023\")\n#decompose the data\ndecompose = decompose(myts, \"multiplicative\")\n#decomposition plot\nautoplot(decompose)\n\n\n\n\n\n\n\n\n\nCode\n#adjusted plot\ntrendadj <- myts/decompose$trend\ndecompose_adjtrend_plot <- autoplot(trendadj,ylab='trend') +ggtitle('Adjusted trend component in the multiplicative time series model')\nseasonaladj <- myts/decompose$seasonal\ndecompose_adjseasonal_plot <- autoplot(seasonaladj,ylab='seasonal') +ggtitle('Adjusted seasonal component in the multiplicative time series model')\ngrid.arrange(orginial_plot, decompose_adjtrend_plot,decompose_adjseasonal_plot, nrow=3)\n\n\n\n\n\n\n\n\nThe adjusted seasonal component tend to have upward trend till 2019 and drops during the covid period and there is more variability in the model when compared to the original plot where the variation during the years but the adjusted trend then to have more fluctuation showing no trend when compared to the original plot.\n\n\nLag Plots\n\nDaily Time LagsMonthly Time Lags\n\n\n\n\nCode\n#Lag plots \ngglagplot(myts, do.lines=FALSE, lags=1)+xlab(\"Lag 1\")+ylab(\"Yi\")+ggtitle(\"Lag Plot for The Financial Sector Fund Stock JAN 2010 - March 2023\")\n\n\n\n\n\n\n\n\n\nCode\n#montly data\nmean_data <- df %>% \n  mutate(month = month(Date), year = year(Date)) %>% \n  group_by(year, month) %>% \n  summarize(mean_value = mean(XLF.Adjusted))\nmonth<-ts(mean_data$mean_value,start = c(2010, 1),frequency = 12)\n#Lag plot\nts_lags(month)\n\n\n\n\n\n\n\n\n\nThe first lag plot shows the daily time lags of the The Financial Sector Fund stock price from JAN 2010 to March 2023. The plot indicates that there is a strong positive correlation between the current value and the previous day’s value, as seen by the points clustering along the diagonal line. This suggests that the stock price has a positive autocorrelation at a lag of one day.\nThe second lag plot shows the monthly time lags of the mean value of the The Financial Sector Fund stock price from JAN 2010 to March 2023. The plot indicates that there is a positive correlation between the current value and the value from the previous month. This suggests that the mean value of the stock price has a positive autocorrelation at a lag of one month.\nOverall, the lag plots indicate that there is a positive autocorrelation present in the The Financial Sector Fund stock price data, with the strongest correlation observed in the daily time series.\n\n\nSeasonality\n\nSeasonal HeatmapSeasonal Line plot\n\n\n\n\nCode\n# Create seasonal plot\nts_heatmap(month,color = \"OrRd\", title = 'Seasonality Heatmap of The Financial Sector Fund Stock Jan 2010 - March 2023')\n\n\n\n\n\n\n\n\n\n\nCode\n# Create a line graph for each year with months on the x-axis\nggseasonplot(month, datecol = \"date\", valuecol = \"value\")+ggtitle(\"Seasonal Yearly Plot for The Financial Sector Fund Stock Jan 2010 - March 2023\")\n\n\n\n\n\n\n\n\nThe Seasonality Heatmap for the The Financial Sector Fund Stock JAN 2010 - March 2023 does not reveal any clear seasonality in the data. The heatmap shows the mean value of the time series for each month and year combination, with the darker colors indicating higher values. The lack of clear patterns or darker colors in specific months or years suggests that there is no consistent seasonal pattern in the data. However, the yearly line graph shows a slight upward trend in the stock price from 2010 to 2023, but does not show any clear seasonality. Each year’s data is represented by a line, and the months are plotted on the x-axis. Overall, the lack of clear seasonality in both the heatmap and yearly line graph suggests that other factors beyond seasonality are driving the stock price fluctuations.\n\n\nMoving Average\n\n4 Month MA1 Year MA3 Year MA5 Year MA\n\n\n\n\nCode\n#SMA Smoothing \nma <- autoplot(month, series=\"Data\") +\n  autolayer(ma(month,5), series=\"4 Month MA\") +\n  xlab(\"Year\") + ylab(\"GWh\") +\n  ggtitle(\"The Financial Sector Fund Stock JAN 2010 - March 2023(4 Month Moving Average)\") +\n  scale_colour_manual(values=c(\"Data\"=\"grey50\",\"4 Month MA\"=\"red\"),\n                      breaks=c(\"Data\",\"4 Month MA\"))\nma\n\n\n\n\n\n\n\n\n\nCode\n#SMA Smoothing \nma <- autoplot(month, series=\"Data\") +\n  autolayer(ma(month,13), series=\"1 Year MA\") +\n  xlab(\"Year\") + ylab(\"GWh\") +\n  ggtitle(\"The Financial Sector Fund Stock JAN 2010 - March 2023(1 Year Moving Average)\") +\n  scale_colour_manual(values=c(\"Data\"=\"grey50\",\"1 Year MA\"=\"red\"),\n                      breaks=c(\"Data\",\"1 Year MA\"))\nma\n\n\n\n\n\n\n\n\n\nCode\n#SMA Smoothing \nma <- autoplot(month, series=\"Data\") +\n  autolayer(ma(month,37), series=\"3 Year MA\") +\n  xlab(\"Year\") + ylab(\"GWh\") +\n  ggtitle(\"The Financial Sector Fund Stock JAN 2010 - March 2023(3 Year Moving Average)\") +\n  scale_colour_manual(values=c(\"Data\"=\"grey50\",\"3 Year MA\"=\"red\"),\n                      breaks=c(\"Data\",\"3 Year MA\"))\nma\n\n\n\n\n\n\n\n\n\nCode\n#SMA Smoothing \nma <- autoplot(month, series=\"Data\") +\n  autolayer(ma(month,61), series=\"5 Year MA\") +\n  xlab(\"Year\") + ylab(\"GWh\") +\n  ggtitle(\"The Financial Sector Fund Stock JAN 2010 - March 2023(5 Year Moving Average)\") +\n  scale_colour_manual(values=c(\"Data\"=\"grey50\",\"5 Year MA\"=\"red\"),\n                      breaks=c(\"Data\",\"5 Year MA\"))\nma\n\n\n\n\n\n\n\n\nThe four plots show the The Financial Sector Fund stock prices from JAN 2010 to March 2023, along with the moving averages for 4 months, 1 year 3 years and 4 years. As the window of the moving average increases, the smoother the trend line becomes, reducing the impact of noise and fluctuations in the original time series.\nThe 4-month moving average plot shows frequent fluctuations in the stock price, with the trend line following the general direction of the time series. The 1-year moving average plot shows a smoother trend, following the overall upward trend of the stock price.\nThe 1-year moving average plot shows a similar trend to the 4-month plot but is even smoother, with fewer fluctuations. Finally, the 5-year moving average plot shows the smoothest trend, with an almost constant upward slope.As the moving average window increases, the smoother trend allows for a clearer identification of the general trend of the The Financial Sector Fund stock prices over time. From the moving average obtained above we can see that there is upward tend in the stock price of The Financial Sector Fund.\n\n\nAutocorrelation Time Series\n\nACFPACFADF Test\n\n\n\n\nCode\n#ACF for  data\nggAcf(month)+ggtitle(\"ACF Plot for The Financial Sector Fund Stock JAN 2010 - March 2023\")\n\n\n\n\n\n\n\n\n\nCode\n#PACF for data\nggPacf(month)+ggtitle(\"PACF Plot for The Financial Sector Fund Stock JAN 2010 - March 2023\")\n\n\n\n\n\n\n\n\n\nCode\n#check the stationarity\ntseries::adf.test(month)\n\n\n\n    Augmented Dickey-Fuller Test\n\ndata:  month\nDickey-Fuller = -3.5093, Lag order = 5, p-value = 0.04386\nalternative hypothesis: stationary\n\n\n\n\n\nIn the plot of autocorrelation function, which is the acf graph for monthly data, there are clear autocorrelation in lag. The above lag plots and autocorrelation plot indicates seasonality in the series, which means the series is not stationary. Even though the Augmented Dickey-Fuller Test which tells us that as the p value is lesser than 0.05, it can’t conclude that the series is stationary. So, its better to proceed with differenitation and make the series stationary.\n\n\nDetrend and Differenced Time Series\n\nLinear Fitting ModelACF Plot\n\n\n\n\nCode\nfit = lm(myts~time(myts), na.action=NULL) \nsummary(fit) \n\n\n\nCall:\nlm(formula = myts ~ time(myts), na.action = NULL)\n\nResiduals:\n     Min       1Q   Median       3Q      Max \n-11.0485  -1.3444  -0.1532   1.3051   8.4971 \n\nCoefficients:\n              Estimate Std. Error t value Pr(>|t|)    \n(Intercept) -4.285e+03  2.538e+01  -168.8   <2e-16 ***\ntime(myts)   2.135e+00  1.259e-02   169.6   <2e-16 ***\n---\nSignif. codes:  0 '***' 0.001 '**' 0.01 '*' 0.05 '.' 0.1 ' ' 1\n\nResidual standard error: 2.708 on 3277 degrees of freedom\nMultiple R-squared:  0.8977,    Adjusted R-squared:  0.8977 \nF-statistic: 2.876e+04 on 1 and 3277 DF,  p-value: < 2.2e-16\n\n\n\n\n\n\nCode\n# plot ACFs\nplot1 <- ggAcf(myts, 48, main=\"Original Data: The Financial Sector Fund Stock Stock Price\")\nplot2 <- ggAcf(resid(fit), 48, main=\"Detrended data\")\nplot3 <- ggAcf(diff(myts), 48, main=\"First differenced data\")\ngrid.arrange(plot1, plot2, plot3, nrow=3)\n\n\n\n\n\n\n\n\nThe estimated slope coefficient β1, 0.37602 With a standard error of 0.03141, yielding a significant estimated increase of stock price is very less yearly. Equation of the fit for stationary process: \\[\\hat{y}_{t} = x_{t}+(740.61660)-(0.37602)t\\]\nFrom the above graph we can say that there is no change in detrended plot and the original data acf plot, it typically means that the data is stationary. But when the first order difference is applied the high correlation is removed but there is no seasonal correlation.\nAs depicted in the above figure, the series is now stationary and ready for future study."
  },
  {
    "objectID": "eda_sector_health_care.html",
    "href": "eda_sector_health_care.html",
    "title": "EDA for Health Care Sector Fund",
    "section": "",
    "text": "The Health Care Sector Fund (XLV) is an exchange-traded fund (ETF) that tracks the performance of companies in the healthcare sector, including pharmaceuticals, healthcare equipment and supplies, healthcare providers and services, biotechnology, and life sciences tools and services. This fund provides investors with a convenient way to invest in a diverse range of healthcare companies, enabling them to gain exposure to this sector without having to buy individual stocks. The XLV has been a popular choice among investors looking for a defensive play, given the healthcare sector’s reputation for being relatively resilient during market downturns.\n\nTime Series Plot\n\n\nCode\n# get data\noptions(\"getSymbols.warning4.0\"=FALSE)\noptions(\"getSymbols.yahoo.warning\"=FALSE)\n\n\ndata = getSymbols(\"XLV\",src='yahoo', from = '2010-01-01',to = \"2023-03-01\")\n\ndf <- data.frame(Date=index(XLV),coredata(XLV))\n\n# create Bollinger Bands\nbbands <- BBands(XLV[,c(\"XLV.High\",\"XLV.Low\",\"XLV.Close\")])\n\n# join and subset data\ndf <- subset(cbind(df, data.frame(bbands[,1:3])), Date >= \"2010-01-01\")\n\n#export the data \nXLV_data <- df\nwrite.csv(XLV_data, \"DATA/CLEANED DATA/XLV_raw_data.csv\", row.names=FALSE)\n\n# colors column for increasing and decreasing\nfor (i in 1:length(df[,1])) {\n  if (df$XLV.Close[i] >= df$XLV.Open[i]) {\n      df$direction[i] = 'Increasing'\n  } else {\n      df$direction[i] = 'Decreasing'\n  }\n}\n\ni <- list(line = list(color = '#FF7F50'))\nd <- list(line = list(color = '#7F7F7F'))\n\n# plot candlestick chart\n\nfig <- df %>% plot_ly(x = ~Date, type=\"candlestick\",\n          open = ~XLV.Open, close = ~XLV.Close,\n          high = ~XLV.High, low = ~XLV.Low, name = \"XLV\",\n          increasing = i, decreasing = d) \nfig <- fig %>% add_lines(x = ~Date, y = ~up , name = \"B Bands\",\n            line = list(color = '#ccc', width = 0.5),\n            legendgroup = \"Bollinger Bands\",\n            hoverinfo = \"none\", inherit = F) \nfig <- fig %>% add_lines(x = ~Date, y = ~dn, name = \"B Bands\",\n            line = list(color = '#ccc', width = 0.5),\n            legendgroup = \"Bollinger Bands\", inherit = F,\n            showlegend = FALSE, hoverinfo = \"none\") \nfig <- fig %>% add_lines(x = ~Date, y = ~mavg, name = \"Mv Avg\",\n            line = list(color = '#E377C2', width = 0.5),\n            hoverinfo = \"none\", inherit = F) \nfig <- fig %>% layout(yaxis = list(title = \"Price\"))\n\n# plot volume bar chart\nfig2 <- df \nfig2 <- fig2 %>% plot_ly(x=~Date, y=~XLV.Volume, type='bar', name = \"XLV Volume\",\n          color = ~direction, colors = c('#FF7F50','#7F7F7F')) \nfig2 <- fig2 %>% layout(yaxis = list(title = \"Volume\"))\n\n# create rangeselector buttons\nrs <- list(visible = TRUE, x = 0.5, y = -0.055,\n           xanchor = 'center', yref = 'paper',\n           font = list(size = 9),\n           buttons = list(\n             list(count=1,\n                  label='RESET',\n                  step='all'),\n             list(count=3,\n                  label='3 YR',\n                  step='year',\n                  stepmode='backward'),\n             list(count=1,\n                  label='1 YR',\n                  step='year',\n                  stepmode='backward'),\n             list(count=1,\n                  label='1 MO',\n                  step='month',\n                  stepmode='backward')\n           ))\n\n# subplot with shared x axis\nfig <- subplot(fig, fig2, heights = c(0.7,0.2), nrows=2,\n             shareX = TRUE, titleY = TRUE)\nfig <- fig %>% layout(title = paste(\"Health Care Sector Fund Stock Price: JAN 2010 - March 2023\"),\n         xaxis = list(rangeselector = rs),\n         legend = list(orientation = 'h', x = 0.5, y = 1,\n                       xanchor = 'center', yref = 'paper',\n                       font = list(size = 10),\n                       bgcolor = 'transparent'))\n\nfig\n\n\n\n\n\n\nThe Health Care Sector Fund (XLV) has experienced significant fluctuations and trends from 2010 to March 2023. The fund has shown an upward trend with several dips in between due to various reasons such as political uncertainty, global pandemic, and regulatory changes. The fund experienced a sharp drop in 2011 due to the debt ceiling crisis, but quickly recovered by the end of the year. It showed a steady increase until the end of 2015, followed by a sharp dip in 2016 due to the U.S. presidential election.\nIn 2020, the COVID-19 pandemic caused a significant impact on the healthcare sector, leading to a sharp dip in the XLV fund’s value. However, it quickly bounced back and reached an all-time high in February 2021. The fund continued to experience fluctuations due to regulatory changes and global events such as the Biden administration’s announcement on drug pricing and the vaccine rollout. Overall, the Health Care Sector Fund has been influenced by various factors that have impacted the healthcare industry, resulting in significant fluctuations in its value.\nFor stock prices, a multiplicative decomposition is typically preferred because the percentage changes in stock prices tend to be more important than the absolute changes. Additionally, stock prices tend to exhibit non-constant variance, meaning that the variance of the series changes over time. A multiplicative decomposition can handle this non-constant variance more effectively than an additive decomposition.\n\n\nDecomposed Time Series\n\nDecomposition PlotAdjusted Decomposition Plot\n\n\n\n\nCode\n#time series data\nmyts<-ts(df$XLV.Adjusted,frequency=252,start=c(2010,01,01), end = c(2023,3,1)) \n#original plot for time series data\norginial_plot <- autoplot(myts,xlab =\"Year\", ylab = \"Adjusted Closing Price\", main = \"Health Care Sector Fund Stock price: JAN 2010 - March 2023\")\n#decompose the data\ndecompose = decompose(myts, \"multiplicative\")\n#decomposition plot\nautoplot(decompose)\n\n\n\n\n\n\n\n\n\nCode\n#adjusted plot\ntrendadj <- myts/decompose$trend\ndecompose_adjtrend_plot <- autoplot(trendadj,ylab='trend') +ggtitle('Adjusted trend component in the multiplicative time series model')\nseasonaladj <- myts/decompose$seasonal\ndecompose_adjseasonal_plot <- autoplot(seasonaladj,ylab='seasonal') +ggtitle('Adjusted seasonal component in the multiplicative time series model')\ngrid.arrange(orginial_plot, decompose_adjtrend_plot,decompose_adjseasonal_plot, nrow=3)\n\n\n\n\n\n\n\n\nThe adjusted seasonal component tend to have upward trend till 2019 and drops during the covid period and there is more variability in the model when compared to the original plot where the variation during the years but the adjusted trend then to have more fluctuation showing no trend when compared to the original plot.\n\n\nLag Plots\n\nDaily Time LagsMonthly Time Lags\n\n\n\n\nCode\n#Lag plots \ngglagplot(myts, do.lines=FALSE, lags=1)+xlab(\"Lag 1\")+ylab(\"Yi\")+ggtitle(\"Lag Plot for Health Care Sector Fund Stock JAN 2010 - March 2023\")\n\n\n\n\n\n\n\n\n\nCode\n#montly data\nmean_data <- df %>% \n  mutate(month = month(Date), year = year(Date)) %>% \n  group_by(year, month) %>% \n  summarize(mean_value = mean(XLV.Adjusted))\nmonth<-ts(mean_data$mean_value,start = c(2010, 1),frequency = 12)\n#Lag plot\nts_lags(month)\n\n\n\n\n\n\n\n\n\nThe first lag plot shows the daily time lags of the Health Care Sector Fund stock price from JAN 2010 to March 2023. The plot indicates that there is a strong positive correlation between the current value and the previous day’s value, as seen by the points clustering along the diagonal line. This suggests that the stock price has a positive autocorrelation at a lag of one day.\nThe second lag plot shows the monthly time lags of the mean value of the Health Care Sector Fund stock price from JAN 2010 to March 2023. The plot indicates that there is a positive correlation between the current value and the value from the previous month. This suggests that the mean value of the stock price has a positive autocorrelation at a lag of one month.\nOverall, the lag plots indicate that there is a positive autocorrelation present in the Health Care Sector Fund stock price data, with the strongest correlation observed in the daily time series.\n\n\nSeasonality\n\nSeasonal HeatmapSeasonal Line plot\n\n\n\n\nCode\n# Create seasonal plot\nts_heatmap(month, color = \"GnBu\",title = 'Seasonality Heatmap of Health Care Sector Fund Stock Jan 2010 - March 2023')\n\n\n\n\n\n\n\n\n\n\nCode\n# Create a line graph for each year with months on the x-axis\nggseasonplot(month, datecol = \"date\", valuecol = \"value\")+ggtitle(\"Seasonal Yearly Plot for Health Care Sector Fund Stock Jan 2010 - March 2023\")\n\n\n\n\n\n\n\n\nThe Seasonality Heatmap for the Health Care Sector Fund Stock JAN 2010 - March 2023 does not reveal any clear seasonality in the data. The heatmap shows the mean value of the time series for each month and year combination, with the darker colors indicating higher values. The lack of clear patterns or darker colors in specific months or years suggests that there is no consistent seasonal pattern in the data. However, the yearly line graph shows a slight upward trend in the stock price from 2010 to 2023, but does not show any clear seasonality. Each year’s data is represented by a line, and the months are plotted on the x-axis. Overall, the lack of clear seasonality in both the heatmap and yearly line graph suggests that other factors beyond seasonality are driving the stock price fluctuations.\n\n\nMoving Average\n\n4 Month MA1 Year MA3 Year MA5 Year MA\n\n\n\n\nCode\n#SMA Smoothing \nma <- autoplot(month, series=\"Data\") +\n  autolayer(ma(month,5), series=\"4 Month MA\") +\n  xlab(\"Year\") + ylab(\"GWh\") +\n  ggtitle(\"Health Care Sector Fund Stock JAN 2010 - March 2023(4 Month Moving Average)\") +\n  scale_colour_manual(values=c(\"Data\"=\"grey50\",\"4 Month MA\"=\"red\"),\n                      breaks=c(\"Data\",\"4 Month MA\"))\nma\n\n\n\n\n\n\n\n\n\nCode\n#SMA Smoothing \nma <- autoplot(month, series=\"Data\") +\n  autolayer(ma(month,13), series=\"1 Year MA\") +\n  xlab(\"Year\") + ylab(\"GWh\") +\n  ggtitle(\"Health Care Sector Fund Stock JAN 2010 - March 2023(1 Year Moving Average)\") +\n  scale_colour_manual(values=c(\"Data\"=\"grey50\",\"1 Year MA\"=\"red\"),\n                      breaks=c(\"Data\",\"1 Year MA\"))\nma\n\n\n\n\n\n\n\n\n\nCode\n#SMA Smoothing \nma <- autoplot(month, series=\"Data\") +\n  autolayer(ma(month,37), series=\"3 Year MA\") +\n  xlab(\"Year\") + ylab(\"GWh\") +\n  ggtitle(\"Health Care Sector Fund Stock JAN 2010 - March 2023(3 Year Moving Average)\") +\n  scale_colour_manual(values=c(\"Data\"=\"grey50\",\"3 Year MA\"=\"red\"),\n                      breaks=c(\"Data\",\"3 Year MA\"))\nma\n\n\n\n\n\n\n\n\n\nCode\n#SMA Smoothing \nma <- autoplot(month, series=\"Data\") +\n  autolayer(ma(month,61), series=\"5 Year MA\") +\n  xlab(\"Year\") + ylab(\"GWh\") +\n  ggtitle(\"Health Care Sector Fund Stock JAN 2010 - March 2023(5 Year Moving Average)\") +\n  scale_colour_manual(values=c(\"Data\"=\"grey50\",\"5 Year MA\"=\"red\"),\n                      breaks=c(\"Data\",\"5 Year MA\"))\nma\n\n\n\n\n\n\n\n\nThe four plots show the Health Care Sector Fund stock prices from JAN 2010 to March 2023, along with the moving averages for 4 months, 1 year 3 years and 4 years. As the window of the moving average increases, the smoother the trend line becomes, reducing the impact of noise and fluctuations in the original time series.\nThe 4-month moving average plot shows frequent fluctuations in the stock price, with the trend line following the general direction of the time series. The 1-year moving average plot shows a smoother trend, following the overall upward trend of the stock price.\nThe 1-year moving average plot shows a similar trend to the 4-month plot but is even smoother, with fewer fluctuations. Finally, the 5-year moving average plot shows the smoothest trend, with an almost constant upward slope.As the moving average window increases, the smoother trend allows for a clearer identification of the general trend of the Health Care Sector Fund stock prices over time. From the moving average obtained above we can see that there is upward tend in the stock price of Health Care Sector Fund.\n\n\nAutocorrelation Time Series\n\nACFPACFADF Test\n\n\n\n\nCode\n#ACF for  data\nggAcf(month)+ggtitle(\"ACF Plot for Health Care Sector Fund Stock JAN 2010 - March 2023\")\n\n\n\n\n\n\n\n\n\nCode\n#PACF for data\nggPacf(month)+ggtitle(\"PACF Plot for Health Care Sector Fund Stock JAN 2010 - March 2023\")\n\n\n\n\n\n\n\n\n\nCode\n#check the stationarity\ntseries::adf.test(month)\n\n\n\n    Augmented Dickey-Fuller Test\n\ndata:  month\nDickey-Fuller = -1.8408, Lag order = 5, p-value = 0.6427\nalternative hypothesis: stationary\n\n\n\n\n\nIn the plot of autocorrelation function, which is the acf graph for monthly data, there are clear autocorrelation in lag. The above lag plots and autocorrelation plot indicates seasonality in the series, which means the series is not stationary. It was also verified using Augmented Dickey-Fuller Test which tells us that as the p value is greater than 0.05, the series is not stationary.\n\n\nDetrend and Differenced Time Series\n\nLinear Fitting ModelACF Plot\n\n\n\n\nCode\nfit = lm(myts~time(myts), na.action=NULL) \nsummary(fit) \n\n\n\nCall:\nlm(formula = myts ~ time(myts), na.action = NULL)\n\nResiduals:\n     Min       1Q   Median       3Q      Max \n-28.9902  -4.9914  -0.9621   4.6109  22.5840 \n\nCoefficients:\n              Estimate Std. Error t value Pr(>|t|)    \n(Intercept) -1.708e+04  6.474e+01  -263.8   <2e-16 ***\ntime(myts)   8.504e+00  3.210e-02   264.9   <2e-16 ***\n---\nSignif. codes:  0 '***' 0.001 '**' 0.01 '*' 0.05 '.' 0.1 ' ' 1\n\nResidual standard error: 6.905 on 3277 degrees of freedom\nMultiple R-squared:  0.9554,    Adjusted R-squared:  0.9554 \nF-statistic: 7.016e+04 on 1 and 3277 DF,  p-value: < 2.2e-16\n\n\n\n\n\n\nCode\n# plot ACFs\nplot1 <- ggAcf(myts, 48, main=\"Original Data: Health Care Sector Fund Stock Stock Price\")\nplot2 <- ggAcf(resid(fit), 48, main=\"Detrended data\")\nplot3 <- ggAcf(diff(myts), 48, main=\"First differenced data\")\ngrid.arrange(plot1, plot2, plot3, nrow=3)\n\n\n\n\n\n\n\n\nThe estimated slope coefficient β1, 1.6399 With a standard error of 0.1219, yielding a significant estimated increase of stock price is very less yearly. Equation of the fit for stationary process: \\[\\hat{y}_{t} = x_{t}+(3246.6875)-(1.6399)t\\]\nFrom the above graph we can say that there is no change in detrended plot and the original data acf plot, it typically means that the data is stationary. But when the first order difference is applied the high correlation is removed but there is no seasonal correlation.\nAs depicted in the above figure, the series is now stationary and ready for future study."
  },
  {
    "objectID": "eda_sector_industrial.html",
    "href": "eda_sector_industrial.html",
    "title": "EDA for Industrial Sector Fund",
    "section": "",
    "text": "The Industrial Sector Fund (XLI) is an exchange-traded fund that tracks the performance of companies in the industrial sector of the US stock market. This sector includes companies involved in manufacturing, transportation, and construction, among others. XLI provides investors with exposure to this sector and has holdings in companies such as Boeing, Honeywell, and General Electric. The performance of XLI is closely tied to the overall health of the US economy, as the industrial sector is a key driver of economic growth.\n\nTime Series Plot\n\n\nCode\n# get data\noptions(\"getSymbols.warning4.0\"=FALSE)\noptions(\"getSymbols.yahoo.warning\"=FALSE)\n\n\ndata = getSymbols(\"XLI\",src='yahoo', from = '2010-01-01',to = \"2023-03-01\")\n\ndf <- data.frame(Date=index(XLI),coredata(XLI))\n\n# create Bollinger Bands\nbbands <- BBands(XLI[,c(\"XLI.High\",\"XLI.Low\",\"XLI.Close\")])\n\n# join and subset data\ndf <- subset(cbind(df, data.frame(bbands[,1:3])), Date >= \"2010-01-01\")\n\n#export the data \nXLI_data <- df\nwrite.csv(XLI_data, \"DATA/CLEANED DATA/XLI_raw_data.csv\", row.names=FALSE)\n\n# colors column for increasing and decreasing\nfor (i in 1:length(df[,1])) {\n  if (df$XLI.Close[i] >= df$XLI.Open[i]) {\n      df$direction[i] = 'Increasing'\n  } else {\n      df$direction[i] = 'Decreasing'\n  }\n}\n\ni <- list(line = list(color = '#00BFFF'))\nd <- list(line = list(color = '#7F7F7F'))\n\n# plot candlestick chart\n\nfig <- df %>% plot_ly(x = ~Date, type=\"candlestick\",\n          open = ~XLI.Open, close = ~XLI.Close,\n          high = ~XLI.High, low = ~XLI.Low, name = \"XLI\",\n          increasing = i, decreasing = d) \nfig <- fig %>% add_lines(x = ~Date, y = ~up , name = \"B Bands\",\n            line = list(color = '#ccc', width = 0.5),\n            legendgroup = \"Bollinger Bands\",\n            hoverinfo = \"none\", inherit = F) \nfig <- fig %>% add_lines(x = ~Date, y = ~dn, name = \"B Bands\",\n            line = list(color = '#ccc', width = 0.5),\n            legendgroup = \"Bollinger Bands\", inherit = F,\n            showlegend = FALSE, hoverinfo = \"none\") \nfig <- fig %>% add_lines(x = ~Date, y = ~mavg, name = \"Mv Avg\",\n            line = list(color = '#E377C2', width = 0.5),\n            hoverinfo = \"none\", inherit = F) \nfig <- fig %>% layout(yaxis = list(title = \"Price\"))\n\n# plot volume bar chart\nfig2 <- df \nfig2 <- fig2 %>% plot_ly(x=~Date, y=~XLI.Volume, type='bar', name = \"XLI Volume\",\n          color = ~direction, colors = c('#00BFFF','#7F7F7F')) \nfig2 <- fig2 %>% layout(yaxis = list(title = \"Volume\"))\n\n# create rangeselector buttons\nrs <- list(visible = TRUE, x = 0.5, y = -0.055,\n           xanchor = 'center', yref = 'paper',\n           font = list(size = 9),\n           buttons = list(\n             list(count=1,\n                  label='RESET',\n                  step='all'),\n             list(count=3,\n                  label='3 YR',\n                  step='year',\n                  stepmode='backward'),\n             list(count=1,\n                  label='1 YR',\n                  step='year',\n                  stepmode='backward'),\n             list(count=1,\n                  label='1 MO',\n                  step='month',\n                  stepmode='backward')\n           ))\n\n# subplot with shared x axis\nfig <- subplot(fig, fig2, heights = c(0.7,0.2), nrows=2,\n             shareX = TRUE, titleY = TRUE)\nfig <- fig %>% layout(title = paste(\"Industrial Sector Fund Stock Price: JAN 2010 - March 2023\"),\n         xaxis = list(rangeselector = rs),\n         legend = list(orientation = 'h', x = 0.5, y = 1,\n                       xanchor = 'center', yref = 'paper',\n                       font = list(size = 10),\n                       bgcolor = 'transparent'))\n\nfig\n\n\n\n\n\n\nThe Industrial Sector Fund (XLI) has seen several fluctuations in its performance since 2010. The fund saw a dip in its value during the economic recession of 2010, but it recovered and reached new highs by 2011. The fund continued to perform well until 2015 when it experienced a slump due to concerns about global economic growth and the slowdown in China. However, XLI rebounded again in 2016 and continued to perform well until 2018.\nIn 2018, the fund saw a decline in value due to concerns over trade wars between the US and China, which impacted the industrial sector negatively. Despite this, the fund managed to recover in 2019 and continued to perform well until early 2020. The COVID-19 pandemic led to a sharp drop in the value of XLI in March 2020. However, the fund rebounded quickly and reached new highs by November 2020, mainly due to the rebounding economy and hopes for a COVID-19 vaccine. The fund continued to perform well in early 2021, but a resurgence of COVID-19 cases and concerns over rising inflation caused some volatility in the market, leading to fluctuations in XLI’s value.\nFor stock prices, a multiplicative decomposition is typically preferred because the percentage changes in stock prices tend to be more important than the absolute changes. Additionally, stock prices tend to exhibit non-constant variance, meaning that the variance of the series changes over time. A multiplicative decomposition can handle this non-constant variance more effectively than an additive decomposition.\n\n\nDecomposed Time Series\n\nDecomposition PlotAdjusted Decomposition Plot\n\n\n\n\nCode\n#time series data\nmyts<-ts(df$XLI.Adjusted,frequency=252,start=c(2010,01,01), end = c(2023,3,1)) \n#original plot for time series data\norginial_plot <- autoplot(myts,xlab =\"Year\", ylab = \"Adjusted Closing Price\", main = \"Industrial Sector Fund Stock price: JAN 2010 - March 2023\")\n#decompose the data\ndecompose = decompose(myts, \"multiplicative\")\n#decomposition plot\nautoplot(decompose)\n\n\n\n\n\n\n\n\n\nCode\n#adjusted plot\ntrendadj <- myts/decompose$trend\ndecompose_adjtrend_plot <- autoplot(trendadj,ylab='trend') +ggtitle('Adjusted trend component in the multiplicative time series model')\nseasonaladj <- myts/decompose$seasonal\ndecompose_adjseasonal_plot <- autoplot(seasonaladj,ylab='seasonal') +ggtitle('Adjusted seasonal component in the multiplicative time series model')\ngrid.arrange(orginial_plot, decompose_adjtrend_plot,decompose_adjseasonal_plot, nrow=3)\n\n\n\n\n\n\n\n\nThe adjusted seasonal component tend to have upward trend till 2019 and drops during the covid period and there is more variability in the model when compared to the original plot where the variation during the years but the adjusted trend then to have more fluctuation showing no trend when compared to the original plot.\n\n\nLag Plots\n\nDaily Time LagsMonthly Time Lags\n\n\n\n\nCode\n#Lag plots \ngglagplot(myts, do.lines=FALSE, lags=1)+xlab(\"Lag 1\")+ylab(\"Yi\")+ggtitle(\"Lag Plot for Industrial Sector Fund Stock JAN 2010 - March 2023\")\n\n\n\n\n\n\n\n\n\nCode\n#montly data\nmean_data <- df %>% \n  mutate(month = month(Date), year = year(Date)) %>% \n  group_by(year, month) %>% \n  summarize(mean_value = mean(XLI.Adjusted))\nmonth<-ts(mean_data$mean_value,start = c(2010, 1),frequency = 12)\n#Lag plot\nts_lags(month)\n\n\n\n\n\n\n\n\n\nThe first lag plot shows the daily time lags of the Industrial Sector Fund stock price from JAN 2010 to March 2023. The plot indicates that there is a strong positive correlation between the current value and the previous day’s value, as seen by the points clustering along the diagonal line. This suggests that the stock price has a positive autocorrelation at a lag of one day.\nThe second lag plot shows the monthly time lags of the mean value of the Industrial Sector Fund stock price from JAN 2010 to March 2023. The plot indicates that there is a positive correlation between the current value and the value from the previous month. This suggests that the mean value of the stock price has a positive autocorrelation at a lag of one month.\nOverall, the lag plots indicate that there is a positive autocorrelation present in the Industrial Sector Fund stock price data, with the strongest correlation observed in the daily time series.\n\n\nSeasonality\n\nSeasonal HeatmapSeasonal Line plot\n\n\n\n\nCode\n# Create seasonal plot\nts_heatmap(month,color = \"Greys\", title = 'Seasonality Heatmap of Industrial Sector Fund Stock Jan 2010 - March 2023')\n\n\n\n\n\n\n\n\n\n\nCode\n# Create a line graph for each year with months on the x-axis\nggseasonplot(month, datecol = \"date\", valuecol = \"value\")+ggtitle(\"Seasonal Yearly Plot for Industrial Sector Fund Stock Jan 2010 - March 2023\")\n\n\n\n\n\n\n\n\nThe Seasonality Heatmap for the Industrial Sector Fund Stock JAN 2010 - March 2023 does not reveal any clear seasonality in the data. The heatmap shows the mean value of the time series for each month and year combination, with the darker colors indicating higher values. The lack of clear patterns or darker colors in specific months or years suggests that there is no consistent seasonal pattern in the data. However, the yearly line graph shows a slight upward trend in the stock price from 2010 to 2023, but does not show any clear seasonality. Each year’s data is represented by a line, and the months are plotted on the x-axis. Overall, the lack of clear seasonality in both the heatmap and yearly line graph suggests that other factors beyond seasonality are driving the stock price fluctuations.\n\n\nMoving Average\n\n4 Month MA1 Year MA3 Year MA5 Year MA\n\n\n\n\nCode\n#SMA Smoothing \nma <- autoplot(month, series=\"Data\") +\n  autolayer(ma(month,5), series=\"4 Month MA\") +\n  xlab(\"Year\") + ylab(\"GWh\") +\n  ggtitle(\"Industrial Sector Fund Stock JAN 2010 - March 2023(4 Month Moving Average)\") +\n  scale_colour_manual(values=c(\"Data\"=\"grey50\",\"4 Month MA\"=\"red\"),\n                      breaks=c(\"Data\",\"4 Month MA\"))\nma\n\n\n\n\n\n\n\n\n\nCode\n#SMA Smoothing \nma <- autoplot(month, series=\"Data\") +\n  autolayer(ma(month,13), series=\"1 Year MA\") +\n  xlab(\"Year\") + ylab(\"GWh\") +\n  ggtitle(\"Industrial Sector Fund Stock JAN 2010 - March 2023(1 Year Moving Average)\") +\n  scale_colour_manual(values=c(\"Data\"=\"grey50\",\"1 Year MA\"=\"red\"),\n                      breaks=c(\"Data\",\"1 Year MA\"))\nma\n\n\n\n\n\n\n\n\n\nCode\n#SMA Smoothing \nma <- autoplot(month, series=\"Data\") +\n  autolayer(ma(month,37), series=\"3 Year MA\") +\n  xlab(\"Year\") + ylab(\"GWh\") +\n  ggtitle(\"Industrial Sector Fund Stock JAN 2010 - March 2023(3 Year Moving Average)\") +\n  scale_colour_manual(values=c(\"Data\"=\"grey50\",\"3 Year MA\"=\"red\"),\n                      breaks=c(\"Data\",\"3 Year MA\"))\nma\n\n\n\n\n\n\n\n\n\nCode\n#SMA Smoothing \nma <- autoplot(month, series=\"Data\") +\n  autolayer(ma(month,61), series=\"5 Year MA\") +\n  xlab(\"Year\") + ylab(\"GWh\") +\n  ggtitle(\"Industrial Sector Fund Stock JAN 2010 - March 2023(5 Year Moving Average)\") +\n  scale_colour_manual(values=c(\"Data\"=\"grey50\",\"5 Year MA\"=\"red\"),\n                      breaks=c(\"Data\",\"5 Year MA\"))\nma\n\n\n\n\n\n\n\n\nThe four plots show the Industrial Sector Fund stock prices from JAN 2010 to March 2023, along with the moving averages for 4 months, 1 year 3 years and 4 years. As the window of the moving average increases, the smoother the trend line becomes, reducing the impact of noise and fluctuations in the original time series.\nThe 4-month moving average plot shows frequent fluctuations in the stock price, with the trend line following the general direction of the time series. The 1-year moving average plot shows a smoother trend, following the overall upward trend of the stock price.\nThe 1-year moving average plot shows a similar trend to the 4-month plot but is even smoother, with fewer fluctuations. Finally, the 5-year moving average plot shows the smoothest trend, with an almost constant upward slope.As the moving average window increases, the smoother trend allows for a clearer identification of the general trend of the Industrial Sector Fund stock prices over time. From the moving average obtained above we can see that there is upward tend in the stock price of Industrial Sector Fund.\n\n\nAutocorrelation Time Series\n\nACFPACFADF Test\n\n\n\n\nCode\n#ACF for  data\nggAcf(month)+ggtitle(\"ACF Plot for Industrial Sector Fund Stock JAN 2010 - March 2023\")\n\n\n\n\n\n\n\n\n\nCode\n#PACF for data\nggPacf(month)+ggtitle(\"PACF Plot for Industrial Sector Fund Stock JAN 2010 - March 2023\")\n\n\n\n\n\n\n\n\n\nCode\n#check the stationarity\ntseries::adf.test(month)\n\n\n\n    Augmented Dickey-Fuller Test\n\ndata:  month\nDickey-Fuller = -3.334, Lag order = 5, p-value = 0.06807\nalternative hypothesis: stationary\n\n\n\n\n\nIn the plot of autocorrelation function, which is the acf graph for monthly data, there are clear autocorrelation in lag. The above lag plots and autocorrelation plot indicates seasonality in the series, which means the series is not stationary. It was also verified using Augmented Dickey-Fuller Test which tells us that as the p value is greater than 0.05, the series is not stationary.\n\n\nDetrend and Differenced Time Series\n\nLinear Fitting ModelACF Plot\n\n\n\n\nCode\nfit = lm(myts~time(myts), na.action=NULL) \nsummary(fit) \n\n\n\nCall:\nlm(formula = myts ~ time(myts), na.action = NULL)\n\nResiduals:\n     Min       1Q   Median       3Q      Max \n-31.3040  -2.9466  -0.1944   2.9162  16.8247 \n\nCoefficients:\n              Estimate Std. Error t value Pr(>|t|)    \n(Intercept) -1.203e+04  5.594e+01  -215.0   <2e-16 ***\ntime(myts)   5.991e+00  2.774e-02   215.9   <2e-16 ***\n---\nSignif. codes:  0 '***' 0.001 '**' 0.01 '*' 0.05 '.' 0.1 ' ' 1\n\nResidual standard error: 5.967 on 3277 degrees of freedom\nMultiple R-squared:  0.9343,    Adjusted R-squared:  0.9343 \nF-statistic: 4.663e+04 on 1 and 3277 DF,  p-value: < 2.2e-16\n\n\n\n\n\n\nCode\n# plot ACFs\nplot1 <- ggAcf(myts, 48, main=\"Original Data: Industrial Sector Fund Stock Stock Price\")\nplot2 <- ggAcf(resid(fit), 48, main=\"Detrended data\")\nplot3 <- ggAcf(diff(myts), 48, main=\"First differenced data\")\ngrid.arrange(plot1, plot2, plot3, nrow=3)\n\n\n\n\n\n\n\n\nThe estimated slope coefficient β1, 1.077e+00 With a standard error of 8.693e-02, yielding a significant estimated increase of stock price is very less yearly. Equation of the fit for stationary process: \\[\\hat{y}_{t} = x_{t}+(2.123e+03)-(1.077e+00)t\\]\nFrom the above graph we can say that there is no change in detrended plot and the original data acf plot, it typically means that the data is stationary. But when the first order difference is applied the high correlation is removed but there is no seasonal correlation.\nAs depicted in the above figure, the series is now stationary and ready for future study."
  },
  {
    "objectID": "eda_sector_market.html",
    "href": "eda_sector_market.html",
    "title": "EDA for Sector Market",
    "section": "",
    "text": "Exploratory Data Analysis (EDA) for sector market stocks involves analyzing time series data to identify the underlying patterns and characteristics of the stocks within a particular sector. A sector market refers to a group of companies that operate in the same industry, such as technology or healthcare. EDA for sector market stocks typically involves analyzing the daily closing prices of individual stocks within a sector and examining key aspects such as autocorrelation, seasonality, trend, and stationarity.\nOne of the key aspects of EDA for sector market stocks is identifying the presence of autocorrelation. Autocorrelation refers to the correlation between a stock’s price and its past prices. The autocorrelation function (ACF) and partial autocorrelation function (PACF) plots can help identify the degree of correlation between the stock’s price and its past prices. This information can be useful for forecasting future prices and identifying potential patterns in the data.\nAnother important aspect of EDA for sector market stocks is identifying the presence of seasonality. Seasonality refers to a pattern that repeats itself in the stock prices over regular intervals, such as daily, weekly, or monthly. Identifying seasonality is important as it can help us identify potential patterns and trends in the data, and it can inform our modeling approach.\nAdditionally, examining moving averages and detrending can help identify the underlying trend in the stock prices. Moving averages are used to smooth out short-term fluctuations in the data, and detrending can help identify the underlying trend that is not related to the seasonal or cyclical fluctuations in the data.\nFinally, testing for stationarity is important as it allows us to apply statistical models that assume the data to be stationary. Stationarity refers to the property of a time series where its statistical properties, such as mean and variance, remain constant over time. If the data is non-stationary, it can be transformed to become stationary through techniques such as differencing or taking logarithms.\nThis information can be used to inform our modeling approach, identify potential patterns and trends, and improve our investment strategies within a particular industry sector.\nClick to view EDA Page for Consumer Staples Sector Fund\nClick to view EDA Page for Utilities Sector Fund\nClick to view EDA Page for Health Care Sector Fund\nClick to view EDA Page for Industrial Sector Fund\nClick to view EDA Page for Financial Sector Fund\nClick to view EDA Page for Consumer Discretionary Sector Fund\nClick to view EDA Page for Communication Services Sector Fund\nClick to view EDA Page for Real Estate Sector Fund\nClick to view EDA Page for Materials Sector Fund\nClick to view EDA Page for Technology Sector Fund\nClick to view EDA Page for Energy Sector Fund"
  },
  {
    "objectID": "eda_sector_materials_sector.html",
    "href": "eda_sector_materials_sector.html",
    "title": "EDA for Materials Sector Fund",
    "section": "",
    "text": "The Materials Sector Fund (XLB) is an exchange-traded fund that focuses on companies that are involved in the production and distribution of raw materials, such as chemicals, construction materials, and metals. This fund provides investors with exposure to a diverse range of companies that are involved in the production and distribution of materials that are essential for industrial and economic growth. The performance of the XLB has been influenced by a variety of factors over the years, including shifts in global demand for raw materials, fluctuations in commodity prices, and changes in government policies and regulations. The fund has experienced periods of volatility, particularly during economic downturns when demand for materials tends to decline, but has also seen growth during times of economic expansion and increased demand for infrastructure and construction projects.\n\nTime Series Plot\n\n\nCode\n# get data\noptions(\"getSymbols.warning4.0\"=FALSE)\noptions(\"getSymbols.yahoo.warning\"=FALSE)\n\n\ndata = getSymbols(\"XLB\",src='yahoo', from = '2010-01-01',to = \"2023-03-01\")\n\ndf <- data.frame(Date=index(XLB),coredata(XLB))\n\n# create Bollinger Bands\nbbands <- BBands(XLB[,c(\"XLB.High\",\"XLB.Low\",\"XLB.Close\")])\n\n# join and subset data\ndf <- subset(cbind(df, data.frame(bbands[,1:3])), Date >= \"2010-01-01\")\n\n#export the data \nXLB_data <- df\nwrite.csv(XLB_data, \"DATA/CLEANED DATA/XLB_raw_data.csv\", row.names=FALSE)\n\n# colors column for increasing and decreasing\nfor (i in 1:length(df[,1])) {\n  if (df$XLB.Close[i] >= df$XLB.Open[i]) {\n      df$direction[i] = 'Increasing'\n  } else {\n      df$direction[i] = 'Decreasing'\n  }\n}\n\ni <- list(line = list(color = '#ADD8E6'))\nd <- list(line = list(color = '#7F7F7F'))\n\n# plot candlestick chart\n\nfig <- df %>% plot_ly(x = ~Date, type=\"candlestick\",\n          open = ~XLB.Open, close = ~XLB.Close,\n          high = ~XLB.High, low = ~XLB.Low, name = \"XLB\",\n          increasing = i, decreasing = d) \nfig <- fig %>% add_lines(x = ~Date, y = ~up , name = \"B Bands\",\n            line = list(color = '#ccc', width = 0.5),\n            legendgroup = \"Bollinger Bands\",\n            hoverinfo = \"none\", inherit = F) \nfig <- fig %>% add_lines(x = ~Date, y = ~dn, name = \"B Bands\",\n            line = list(color = '#ccc', width = 0.5),\n            legendgroup = \"Bollinger Bands\", inherit = F,\n            showlegend = FALSE, hoverinfo = \"none\") \nfig <- fig %>% add_lines(x = ~Date, y = ~mavg, name = \"Mv Avg\",\n            line = list(color = '#E377C2', width = 0.5),\n            hoverinfo = \"none\", inherit = F) \nfig <- fig %>% layout(yaxis = list(title = \"Price\"))\n\n# plot volume bar chart\nfig2 <- df \nfig2 <- fig2 %>% plot_ly(x=~Date, y=~XLB.Volume, type='bar', name = \"XLB Volume\",\n          color = ~direction, colors = c('#ADD8E6','#7F7F7F')) \nfig2 <- fig2 %>% layout(yaxis = list(title = \"Volume\"))\n\n# create rangeselector buttons\nrs <- list(visible = TRUE, x = 0.5, y = -0.055,\n           xanchor = 'center', yref = 'paper',\n           font = list(size = 9),\n           buttons = list(\n             list(count=1,\n                  label='RESET',\n                  step='all'),\n             list(count=3,\n                  label='3 YR',\n                  step='year',\n                  stepmode='backward'),\n             list(count=1,\n                  label='1 YR',\n                  step='year',\n                  stepmode='backward'),\n             list(count=1,\n                  label='1 MO',\n                  step='month',\n                  stepmode='backward')\n           ))\n\n# subplot with shared x axis\nfig <- subplot(fig, fig2, heights = c(0.7,0.2), nrows=2,\n             shareX = TRUE, titleY = TRUE)\nfig <- fig %>% layout(title = paste(\"Materials Sector Fund Stock Price: JAN 2010 - March 2023\"),\n         xaxis = list(rangeselector = rs),\n         legend = list(orientation = 'h', x = 0.5, y = 1,\n                       xanchor = 'center', yref = 'paper',\n                       font = list(size = 10),\n                       bgcolor = 'transparent'))\n\nfig\n\n\n\n\n\n\nThe Materials Sector Fund (XLB) experienced various trends and fluctuations from 2010 to March 2023. In the early years of the period, the fund saw a slow but steady upward trend, with a significant drop during the economic recession of 2011. However, the fund managed to recover in the following years, reaching new highs by mid-2015. The period between 2015 and early 2016 saw a sharp decline in the fund’s performance due to the economic slowdown in China and the decline in global commodity prices. However, the fund recovered soon after, driven by the growth in infrastructure spending and construction in the US. The fund experienced some volatility in the following years, with some dips and recoveries, mainly due to trade tensions between the US and China and the global economic slowdown. Overall, the performance of the Materials Sector Fund has been closely tied to the strength of the global economy, demand for commodities, and geopolitical events affecting the industry.\nFor stock prices, a multiplicative decomposition is typically preferred because the percentage changes in stock prices tend to be more important than the absolute changes. Additionally, stock prices tend to exhibit non-constant variance, meaning that the variance of the series changes over time. A multiplicative decomposition can handle this non-constant variance more effectively than an additive decomposition.\n\n\nDecomposed Time Series\n\nDecomposition PlotAdjusted Decomposition Plot\n\n\n\n\nCode\n#time series data\nmyts<-ts(df$XLB.Adjusted,frequency=252,start=c(2010,01,01), end = c(2023,3,1)) \n#original plot for time series data\norginial_plot <- autoplot(myts,xlab =\"Year\", ylab = \"Adjusted Closing Price\", main = \"Materials Sector Fund Stock price: JAN 2010 - March 2023\")\n#decompose the data\ndecompose = decompose(myts, \"multiplicative\")\n#decomposition plot\nautoplot(decompose)\n\n\n\n\n\n\n\n\n\nCode\n#adjusted plot\ntrendadj <- myts/decompose$trend\ndecompose_adjtrend_plot <- autoplot(trendadj,ylab='trend') +ggtitle('Adjusted trend component in the multiplicative time series model')\nseasonaladj <- myts/decompose$seasonal\ndecompose_adjseasonal_plot <- autoplot(seasonaladj,ylab='seasonal') +ggtitle('Adjusted seasonal component in the multiplicative time series model')\ngrid.arrange(orginial_plot, decompose_adjtrend_plot,decompose_adjseasonal_plot, nrow=3)\n\n\n\n\n\n\n\n\nThe adjusted seasonal component tend to have upward trend till 2019 and drops during the covid period and there is more variability in the model when compared to the original plot where the variation during the years but the adjusted trend then to have more fluctuation showing no trend when compared to the original plot.\n\n\nLag Plots\n\nDaily Time LagsMonthly Time Lags\n\n\n\n\nCode\n#Lag plots \ngglagplot(myts, do.lines=FALSE, lags=1)+xlab(\"Lag 1\")+ylab(\"Yi\")+ggtitle(\"Lag Plot for Materials Sector Fund Stock JAN 2010 - March 2023\")\n\n\n\n\n\n\n\n\n\nCode\n#montly data\nmean_data <- df %>% \n  mutate(month = month(Date), year = year(Date)) %>% \n  group_by(year, month) %>% \n  summarize(mean_value = mean(XLB.Adjusted))\nmonth<-ts(mean_data$mean_value,start = c(2010, 1),frequency = 12)\n#Lag plot\nts_lags(month)\n\n\n\n\n\n\n\n\n\nThe first lag plot shows the daily time lags of the Materials Sector Fund stock price from JAN 2010 to March 2023. The plot indicates that there is a strong positive correlation between the current value and the previous day’s value, as seen by the points clustering along the diagonal line. This suggests that the stock price has a positive autocorrelation at a lag of one day.\nThe second lag plot shows the monthly time lags of the mean value of the Materials Sector Fund stock price from JAN 2010 to March 2023. The plot indicates that there is a positive correlation between the current value and the value from the previous month. This suggests that the mean value of the stock price has a positive autocorrelation at a lag of one month.\nOverall, the lag plots indicate that there is a positive autocorrelation present in the Materials Sector Fund stock price data, with the strongest correlation observed in the daily time series.\n\n\nSeasonality\n\nSeasonal HeatmapSeasonal Line plot\n\n\n\n\nCode\n# Create seasonal plot\nts_heatmap(month, color = \"YlGn\", title = 'Seasonality Heatmap of Materials Sector Fund Stock Jan 2010 - March 2023')\n\n\n\n\n\n\n\n\n\n\nCode\n# Create a line graph for each year with months on the x-axis\nggseasonplot(month, datecol = \"date\", valuecol = \"value\")+ggtitle(\"Seasonal Yearly Plot for Materials Sector Fund Stock Jan 2010 - March 2023\")\n\n\n\n\n\n\n\n\nThe Seasonality Heatmap for the Materials Sector Fund Stock JAN 2010 - March 2023 does not reveal any clear seasonality in the data. The heatmap shows the mean value of the time series for each month and year combination, with the darker colors indicating higher values. The lack of clear patterns or darker colors in specific months or years suggests that there is no consistent seasonal pattern in the data. However, the yearly line graph shows a slight upward trend in the stock price from 2010 to 2023, but does not show any clear seasonality. Each year’s data is represented by a line, and the months are plotted on the x-axis. Overall, the lack of clear seasonality in both the heatmap and yearly line graph suggests that other factors beyond seasonality are driving the stock price fluctuations.\n\n\nMoving Average\n\n4 Month MA1 Year MA3 Year MA\n\n\n\n\nCode\n#SMA Smoothing \nma <- autoplot(month, series=\"Data\") +\n  autolayer(ma(month,5), series=\"4 Month MA\") +\n  xlab(\"Year\") + ylab(\"GWh\") +\n  ggtitle(\"Materials Sector Fund Stock JAN 2010 - March 2023(4 Month Moving Average)\") +\n  scale_colour_manual(values=c(\"Data\"=\"grey50\",\"4 Month MA\"=\"red\"),\n                      breaks=c(\"Data\",\"4 Month MA\"))\nma\n\n\n\n\n\n\n\n\n\nCode\n#SMA Smoothing \nma <- autoplot(month, series=\"Data\") +\n  autolayer(ma(month,13), series=\"1 Year MA\") +\n  xlab(\"Year\") + ylab(\"GWh\") +\n  ggtitle(\"Materials Sector Fund Stock JAN 2010 - March 2023(1 Year Moving Average)\") +\n  scale_colour_manual(values=c(\"Data\"=\"grey50\",\"1 Year MA\"=\"red\"),\n                      breaks=c(\"Data\",\"1 Year MA\"))\nma\n\n\n\n\n\n\n\n\n\nCode\n#SMA Smoothing \nma <- autoplot(month, series=\"Data\") +\n  autolayer(ma(month,37), series=\"3 Year MA\") +\n  xlab(\"Year\") + ylab(\"GWh\") +\n  ggtitle(\"Materials Sector Fund Stock JAN 2010 - March 2023(3 Year Moving Average)\") +\n  scale_colour_manual(values=c(\"Data\"=\"grey50\",\"3 Year MA\"=\"red\"),\n                      breaks=c(\"Data\",\"3 Year MA\"))\nma\n\n\n\n\n\n\n\n\nThe three plots show the Materials Sector Fund stock prices from JAN 2010 to March 2023, along with the moving averages for 4 months, 1 year and 3 years. As the window of the moving average increases, the smoother the trend line becomes, reducing the impact of noise and fluctuations in the original time series.\nThe 4-month moving average plot shows frequent fluctuations in the stock price, with the trend line following the general direction of the time series. The 1-year moving average plot shows a smoother trend, following the overall upward trend of the stock price.\nThe 1-year moving average plot shows a similar trend to the 4-month plot but is even smoother, with fewer fluctuations. Finally, the 3-year moving average plot shows the smoothest trend, with an almost constant upward slope.As the moving average window increases, the smoother trend allows for a clearer identification of the general trend of the Materials Sector Fund stock prices over time. From the moving average obtained above we can see that there is upward tend in the stock price of Materials Sector Fund.\n\n\nAutocorrelation Time Series\n\nACFPACFADF Test\n\n\n\n\nCode\n#ACF for  data\nggAcf(month)+ggtitle(\"ACF Plot for Materials Sector Fund Stock JAN 2010 - March 2023\")\n\n\n\n\n\n\n\n\n\nCode\n#PACF for data\nggPacf(month)+ggtitle(\"PACF Plot for Materials Sector Fund Stock JAN 2010 - March 2023\")\n\n\n\n\n\n\n\n\n\nCode\n#check the stationarity\ntseries::adf.test(month)\n\n\n\n    Augmented Dickey-Fuller Test\n\ndata:  month\nDickey-Fuller = -2.3534, Lag order = 5, p-value = 0.429\nalternative hypothesis: stationary\n\n\n\n\n\nIn the plot of autocorrelation function, which is the acf graph for monthly data, there are clear autocorrelation in lag. The above lag plots and autocorrelation plot indicates seasonality in the series, which means the series is not stationary. This can be verified by the Augmented Dickey-Fuller Test which tells us that as the p value is greater than 0.05.\n\n\nDetrend and Differenced Time Series\n\nLinear Fitting ModelACF Plot\n\n\n\n\nCode\nfit = lm(myts~time(myts), na.action=NULL) \nsummary(fit) \n\n\n\nCall:\nlm(formula = myts ~ time(myts), na.action = NULL)\n\nResiduals:\n     Min       1Q   Median       3Q      Max \n-25.7882  -4.1542  -0.2222   3.1821  17.9583 \n\nCoefficients:\n              Estimate Std. Error t value Pr(>|t|)    \n(Intercept) -8.411e+03  5.809e+01  -144.8   <2e-16 ***\ntime(myts)   4.194e+00  2.881e-02   145.6   <2e-16 ***\n---\nSignif. codes:  0 '***' 0.001 '**' 0.01 '*' 0.05 '.' 0.1 ' ' 1\n\nResidual standard error: 6.196 on 3277 degrees of freedom\nMultiple R-squared:  0.8661,    Adjusted R-squared:  0.866 \nF-statistic: 2.119e+04 on 1 and 3277 DF,  p-value: < 2.2e-16\n\n\n\n\n\n\nCode\n# plot ACFs\nplot1 <- ggAcf(myts, 48, main=\"Original Data: Materials Sector Fund Stock Stock Price\")\nplot2 <- ggAcf(resid(fit), 48, main=\"Detrended data\")\nplot3 <- ggAcf(diff(myts), 48, main=\"First differenced data\")\ngrid.arrange(plot1, plot2, plot3, nrow=3)\n\n\n\n\n\n\n\n\nThe estimated slope coefficient β1, 8.441e-01 With a standard error of 6.216e-02, yielding a significant estimated increase of stock price is very less yearly. Equation of the fit for stationary process: \\[\\hat{y}_{t} = x_{t}+(1.660e+03)-(8.441e-01)t\\]\nFrom the above graph we can say that there is no change in detrended plot and the original data acf plot, it typically means that the data is stationary. But when the first order difference is applied the high correlation is removed but there is no seasonal correlation.\nAs depicted in the above figure, the series is now stationary and ready for future study."
  },
  {
    "objectID": "eda_sector_real_estate.html",
    "href": "eda_sector_real_estate.html",
    "title": "EDA for Real Estate Sector Fund",
    "section": "",
    "text": "Real Estate Sector Fund (XLRE) is an exchange-traded fund that provides exposure to companies involved in real estate, including real estate investment trusts (REITs) and real estate management and development firms. This sector fund invests in a variety of real estate segments, such as commercial, residential, industrial, and retail, providing diversification benefits for investors. The performance of XLRE is influenced by several factors, including interest rates, economic growth, and the overall health of the real estate market. In recent years, the XLRE has shown a trend of steady growth, driven by a strong demand for real estate and low-interest rates. However, fluctuations can occur due to changes in government policies, economic conditions, and market sentiment towards real estate investments.\n\nTime Series Plot\n\n\nCode\n# get data\noptions(\"getSymbols.warning4.0\"=FALSE)\noptions(\"getSymbols.yahoo.warning\"=FALSE)\n\n\ndata = getSymbols(\"XLRE\",src='yahoo', from = '2016-01-01',to = \"2023-03-01\")\n\ndf <- data.frame(Date=index(XLRE),coredata(XLRE))\n\n# create Bollinger Bands\nbbands <- BBands(XLRE[,c(\"XLRE.High\",\"XLRE.Low\",\"XLRE.Close\")])\n\n# join and subset data\ndf <- subset(cbind(df, data.frame(bbands[,1:3])), Date >= \"2016-01-01\")\n\n#export the data \nXLRE_data <- df\nwrite.csv(XLRE_data, \"DATA/CLEANED DATA/XLRE_raw_data.csv\", row.names=FALSE)\n\n# colors column for increasing and decreasing\nfor (i in 1:length(df[,1])) {\n  if (df$XLRE.Close[i] >= df$XLRE.Open[i]) {\n      df$direction[i] = 'Increasing'\n  } else {\n      df$direction[i] = 'Decreasing'\n  }\n}\n\ni <- list(line = list(color = '#F0E68C'))\nd <- list(line = list(color = '#7F7F7F'))\n\n# plot candlestick chart\n\nfig <- df %>% plot_ly(x = ~Date, type=\"candlestick\",\n          open = ~XLRE.Open, close = ~XLRE.Close,\n          high = ~XLRE.High, low = ~XLRE.Low, name = \"XLRE\",\n          increasing = i, decreasing = d) \nfig <- fig %>% add_lines(x = ~Date, y = ~up , name = \"B Bands\",\n            line = list(color = '#ccc', width = 0.5),\n            legendgroup = \"Bollinger Bands\",\n            hoverinfo = \"none\", inherit = F) \nfig <- fig %>% add_lines(x = ~Date, y = ~dn, name = \"B Bands\",\n            line = list(color = '#ccc', width = 0.5),\n            legendgroup = \"Bollinger Bands\", inherit = F,\n            showlegend = FALSE, hoverinfo = \"none\") \nfig <- fig %>% add_lines(x = ~Date, y = ~mavg, name = \"Mv Avg\",\n            line = list(color = '#E377C2', width = 0.5),\n            hoverinfo = \"none\", inherit = F) \nfig <- fig %>% layout(yaxis = list(title = \"Price\"))\n\n# plot volume bar chart\nfig2 <- df \nfig2 <- fig2 %>% plot_ly(x=~Date, y=~XLRE.Volume, type='bar', name = \"XLRE Volume\",\n          color = ~direction, colors = c('#F0E68C','#7F7F7F')) \nfig2 <- fig2 %>% layout(yaxis = list(title = \"Volume\"))\n\n# create rangeselector buttons\nrs <- list(visible = TRUE, x = 0.5, y = -0.055,\n           xanchor = 'center', yref = 'paper',\n           font = list(size = 9),\n           buttons = list(\n             list(count=1,\n                  label='RESET',\n                  step='all'),\n             list(count=3,\n                  label='3 YR',\n                  step='year',\n                  stepmode='backward'),\n             list(count=1,\n                  label='1 YR',\n                  step='year',\n                  stepmode='backward'),\n             list(count=1,\n                  label='1 MO',\n                  step='month',\n                  stepmode='backward')\n           ))\n\n# subplot with shared x axis\nfig <- subplot(fig, fig2, heights = c(0.7,0.2), nrows=2,\n             shareX = TRUE, titleY = TRUE)\nfig <- fig %>% layout(title = paste(\"Real Estate Sector Fund Stock Price: JAN 2016 - March 2023\"),\n         xaxis = list(rangeselector = rs),\n         legend = list(orientation = 'h', x = 0.5, y = 1,\n                       xanchor = 'center', yref = 'paper',\n                       font = list(size = 10),\n                       bgcolor = 'transparent'))\n\nfig\n\n\n\n\n\n\nThe Real Estate Sector Fund (XLRE) was established in 2015, and since then, it has shown significant growth and stability. The fund tracks the performance of the Real Estate Select Sector Index, which is composed of real estate investment trusts (REITs) and real estate management and development companies.\nXLRE’s price trend has been relatively stable since its inception, with the fund experiencing consistent growth with minor fluctuations. The fund’s value reached its peak in early 2020 but experienced a decline in the second quarter due to the COVID-19 pandemic’s adverse impact on the real estate sector.\nThe pandemic caused a decline in commercial real estate demand and occupancy rates, which negatively impacted REITs’ performance. However, as the pandemic eased, the demand for residential properties increased, and REITs’ values rose again. Moreover, as the U.S. economy began to recover, the demand for commercial real estate picked up, leading to a positive impact on the fund’s performance.\nFor stock prices, a multiplicative decomposition is typically preferred because the percentage changes in stock prices tend to be more important than the absolute changes. Additionally, stock prices tend to exhibit non-constant variance, meaning that the variance of the series changes over time. A multiplicative decomposition can handle this non-constant variance more effectively than an additive decomposition.\n\n\nDecomposed Time Series\n\nDecomposition PlotAdjusted Decomposition Plot\n\n\n\n\nCode\n#time series data\nmyts<-ts(df$XLRE.Adjusted,frequency=252,start=c(2016,01,01), end = c(2023,3,1)) \n#original plot for time series data\norginial_plot <- autoplot(myts,xlab =\"Year\", ylab = \"Adjusted Closing Price\", main = \"Real Estate Sector Fund Stock price: JAN 2016 - March 2023\")\n#decompose the data\ndecompose = decompose(myts, \"multiplicative\")\n#decomposition plot\nautoplot(decompose)\n\n\n\n\n\n\n\n\n\nCode\n#adjusted plot\ntrendadj <- myts/decompose$trend\ndecompose_adjtrend_plot <- autoplot(trendadj,ylab='trend') +ggtitle('Adjusted trend component in the multiplicative time series model')\nseasonaladj <- myts/decompose$seasonal\ndecompose_adjseasonal_plot <- autoplot(seasonaladj,ylab='seasonal') +ggtitle('Adjusted seasonal component in the multiplicative time series model')\ngrid.arrange(orginial_plot, decompose_adjtrend_plot,decompose_adjseasonal_plot, nrow=3)\n\n\n\n\n\n\n\n\nThe adjusted seasonal component tend to have upward trend till 2019 and drops during the covid period and there is more variability in the model when compared to the original plot where the variation during the years but the adjusted trend then to have more fluctuation showing no trend when compared to the original plot.\n\n\nLag Plots\n\nDaily Time LagsMonthly Time Lags\n\n\n\n\nCode\n#Lag plots \ngglagplot(myts, do.lines=FALSE, lags=1)+xlab(\"Lag 1\")+ylab(\"Yi\")+ggtitle(\"Lag Plot for Real Estate Sector Fund Stock JAN 2016 - March 2023\")\n\n\n\n\n\n\n\n\n\nCode\n#montly data\nmean_data <- df %>% \n  mutate(month = month(Date), year = year(Date)) %>% \n  group_by(year, month) %>% \n  summarize(mean_value = mean(XLRE.Adjusted))\nmonth<-ts(mean_data$mean_value,start = c(2016, 1),frequency = 12)\n#Lag plot\nts_lags(month)\n\n\n\n\n\n\n\n\n\nThe first lag plot shows the daily time lags of the Real Estate Sector Fund stock price from JAN 2016 to March 2023. The plot indicates that there is a strong positive correlation between the current value and the previous day’s value, as seen by the points clustering along the diagonal line. This suggests that the stock price has a positive autocorrelation at a lag of one day.\nThe second lag plot shows the monthly time lags of the mean value of the Real Estate Sector Fund stock price from JAN 2016 to March 2023. The plot indicates that there is a positive correlation between the current value and the value from the previous month. This suggests that the mean value of the stock price has a positive autocorrelation at a lag of one month.\nOverall, the lag plots indicate that there is a positive autocorrelation present in the Real Estate Sector Fund stock price data, with the strongest correlation observed in the daily time series.\n\n\nSeasonality\n\nSeasonal HeatmapSeasonal Line plot\n\n\n\n\nCode\n# Create seasonal plot\nts_heatmap(month, color = \"RdPu\", title = 'Seasonality Heatmap of Real Estate Sector Fund Stock Jan 2016 - March 2023')\n\n\n\n\n\n\n\n\n\n\nCode\n# Create a line graph for each year with months on the x-axis\nggseasonplot(month, datecol = \"date\", valuecol = \"value\")+ggtitle(\"Seasonal Yearly Plot for Real Estate Sector Fund Stock Jan 2016 - March 2023\")\n\n\n\n\n\n\n\n\nThe Seasonality Heatmap for the Real Estate Sector Fund Stock JAN 2016 - March 2023 does not reveal any clear seasonality in the data. The heatmap shows the mean value of the time series for each month and year combination, with the darker colors indicating higher values. The lack of clear patterns or darker colors in specific months or years suggests that there is no consistent seasonal pattern in the data. However, the yearly line graph shows a slight upward trend in the stock price from 2016 to 2023, but does not show any clear seasonality. Each year’s data is represented by a line, and the months are plotted on the x-axis. Overall, the lack of clear seasonality in both the heatmap and yearly line graph suggests that other factors beyond seasonality are driving the stock price fluctuations.\n\n\nMoving Average\n\n4 Month MA1 Year MA3 Year MA\n\n\n\n\nCode\n#SMA Smoothing \nma <- autoplot(month, series=\"Data\") +\n  autolayer(ma(month,5), series=\"4 Month MA\") +\n  xlab(\"Year\") + ylab(\"GWh\") +\n  ggtitle(\"Real Estate Sector Fund Stock JAN 2016 - March 2023(4 Month Moving Average)\") +\n  scale_colour_manual(values=c(\"Data\"=\"grey50\",\"4 Month MA\"=\"red\"),\n                      breaks=c(\"Data\",\"4 Month MA\"))\nma\n\n\n\n\n\n\n\n\n\nCode\n#SMA Smoothing \nma <- autoplot(month, series=\"Data\") +\n  autolayer(ma(month,13), series=\"1 Year MA\") +\n  xlab(\"Year\") + ylab(\"GWh\") +\n  ggtitle(\"Real Estate Sector Fund Stock JAN 2016 - March 2023(1 Year Moving Average)\") +\n  scale_colour_manual(values=c(\"Data\"=\"grey50\",\"1 Year MA\"=\"red\"),\n                      breaks=c(\"Data\",\"1 Year MA\"))\nma\n\n\n\n\n\n\n\n\n\nCode\n#SMA Smoothing \nma <- autoplot(month, series=\"Data\") +\n  autolayer(ma(month,37), series=\"3 Year MA\") +\n  xlab(\"Year\") + ylab(\"GWh\") +\n  ggtitle(\"Real Estate Sector Fund Stock JAN 2016 - March 2023(3 Year Moving Average)\") +\n  scale_colour_manual(values=c(\"Data\"=\"grey50\",\"3 Year MA\"=\"red\"),\n                      breaks=c(\"Data\",\"3 Year MA\"))\nma\n\n\n\n\n\n\n\n\nThe three plots show the Real Estate Sector Fund stock prices from JAN 2016 to March 2023, along with the moving averages for 4 months, 1 year and 3 years. As the window of the moving average increases, the smoother the trend line becomes, reducing the impact of noise and fluctuations in the original time series.\nThe 4-month moving average plot shows frequent fluctuations in the stock price, with the trend line following the general direction of the time series. The 1-year moving average plot shows a smoother trend, following the overall upward trend of the stock price.\nThe 1-year moving average plot shows a similar trend to the 4-month plot but is even smoother, with fewer fluctuations. Finally, the 3-year moving average plot shows the smoothest trend, with an almost constant upward slope.As the moving average window increases, the smoother trend allows for a clearer identification of the general trend of the Real Estate Sector Fund stock prices over time. From the moving average obtained above we can see that there is upward tend in the stock price of Real Estate Sector Fund.\n\n\nAutocorrelation Time Series\n\nACFPACFADF Test\n\n\n\n\nCode\n#ACF for  data\nggAcf(month)+ggtitle(\"ACF Plot for Real Estate Sector Fund Stock JAN 2016 - March 2023\")\n\n\n\n\n\n\n\n\n\nCode\n#PACF for data\nggPacf(month)+ggtitle(\"PACF Plot for Real Estate Sector Fund Stock JAN 2016 - March 2023\")\n\n\n\n\n\n\n\n\n\nCode\n#check the stationarity\ntseries::adf.test(month)\n\n\n\n    Augmented Dickey-Fuller Test\n\ndata:  month\nDickey-Fuller = -3.2921, Lag order = 4, p-value = 0.07825\nalternative hypothesis: stationary\n\n\n\n\n\nIn the plot of autocorrelation function, which is the acf graph for monthly data, there are clear autocorrelation in lag. The above lag plots and autocorrelation plot indicates seasonality in the series, which means the series is not stationary. This can be verified by the Augmented Dickey-Fuller Test which tells us that as the p value is greater than 0.05.\n\n\nDetrend and Differenced Time Series\n\nLinear Fitting ModelACF Plot\n\n\n\n\nCode\nfit = lm(myts~time(myts), na.action=NULL) \nsummary(fit) \n\n\n\nCall:\nlm(formula = myts ~ time(myts), na.action = NULL)\n\nResiduals:\n     Min       1Q   Median       3Q      Max \n-11.0891  -2.1390   0.0211   1.7440   9.6609 \n\nCoefficients:\n              Estimate Std. Error t value Pr(>|t|)    \n(Intercept) -5.810e+03  7.293e+01  -79.67   <2e-16 ***\ntime(myts)   2.893e+00  3.611e-02   80.10   <2e-16 ***\n---\nSignif. codes:  0 '***' 0.001 '**' 0.01 '*' 0.05 '.' 0.1 ' ' 1\n\nResidual standard error: 3.073 on 1765 degrees of freedom\nMultiple R-squared:  0.7843,    Adjusted R-squared:  0.7841 \nF-statistic:  6416 on 1 and 1765 DF,  p-value: < 2.2e-16\n\n\n\n\n\n\nCode\n# plot ACFs\nplot1 <- ggAcf(myts, 48, main=\"Original Data: Real Estate Sector Fund Stock Stock Price\")\nplot2 <- ggAcf(resid(fit), 48, main=\"Detrended data\")\nplot3 <- ggAcf(diff(myts), 48, main=\"First differenced data\")\ngrid.arrange(plot1, plot2, plot3, nrow=3)\n\n\n\n\n\n\n\n\nThe estimated slope coefficient β1, 0.40454 With a standard error of 0.06241, yielding a significant estimated increase of stock price is very less yearly. Equation of the fit for stationary process: \\[\\hat{y}_{t} = x_{t}+(786.31680)-(0.40454)t\\]\nFrom the above graph we can say that there is no change in detrended plot and the original data acf plot, it typically means that the data is stationary. But when the first order difference is applied the high correlation is removed but there is no seasonal correlation.\nAs depicted in the above figure, the series is now stationary and ready for future study."
  },
  {
    "objectID": "eda_sector_technology.html",
    "href": "eda_sector_technology.html",
    "title": "EDA for Technology Sector Fund",
    "section": "",
    "text": "The Technology Sector Fund (XLK) is a sector-specific exchange-traded fund that tracks the performance of companies in the technology sector, including hardware, software, and semiconductor companies. The fund’s top holdings include Apple, Microsoft, and Facebook. Over the past decade, XLK has shown significant growth, outperforming the broader market. The fund experienced a sharp decline during the COVID-19 pandemic in 2020, but quickly rebounded and has continued to see strong growth in the years since. As the demand for technology products and services continues to increase, XLK is expected to remain a strong performer in the coming years.\n\nTime Series Plot\n\n\nCode\n# get data\noptions(\"getSymbols.warning4.0\"=FALSE)\noptions(\"getSymbols.yahoo.warning\"=FALSE)\n\n\ndata = getSymbols(\"XLK\",src='yahoo', from = '2010-01-01',to = \"2023-03-01\")\n\ndf <- data.frame(Date=index(XLK),coredata(XLK))\n\n# create Bollinger Bands\nbbands <- BBands(XLK[,c(\"XLK.High\",\"XLK.Low\",\"XLK.Close\")])\n\n# join and subset data\ndf <- subset(cbind(df, data.frame(bbands[,1:3])), Date >= \"2010-01-01\")\n\n#export the data \nXLK_data <- df\nwrite.csv(XLK_data, \"DATA/CLEANED DATA/XLK_raw_data.csv\", row.names=FALSE)\n\n# colors column for increasing and decreasing\nfor (i in 1:length(df[,1])) {\n  if (df$XLK.Close[i] >= df$XLK.Open[i]) {\n      df$direction[i] = 'Increasing'\n  } else {\n      df$direction[i] = 'Decreasing'\n  }\n}\n\ni <- list(line = list(color = '#B0C4DE'))\nd <- list(line = list(color = '#7F7F7F'))\n\n# plot candlestick chart\n\nfig <- df %>% plot_ly(x = ~Date, type=\"candlestick\",\n          open = ~XLK.Open, close = ~XLK.Close,\n          high = ~XLK.High, low = ~XLK.Low, name = \"XLK\",\n          increasing = i, decreasing = d) \nfig <- fig %>% add_lines(x = ~Date, y = ~up , name = \"B Bands\",\n            line = list(color = '#ccc', width = 0.5),\n            legendgroup = \"Bollinger Bands\",\n            hoverinfo = \"none\", inherit = F) \nfig <- fig %>% add_lines(x = ~Date, y = ~dn, name = \"B Bands\",\n            line = list(color = '#ccc', width = 0.5),\n            legendgroup = \"Bollinger Bands\", inherit = F,\n            showlegend = FALSE, hoverinfo = \"none\") \nfig <- fig %>% add_lines(x = ~Date, y = ~mavg, name = \"Mv Avg\",\n            line = list(color = '#E377C2', width = 0.5),\n            hoverinfo = \"none\", inherit = F) \nfig <- fig %>% layout(yaxis = list(title = \"Price\"))\n\n# plot volume bar chart\nfig2 <- df \nfig2 <- fig2 %>% plot_ly(x=~Date, y=~XLK.Volume, type='bar', name = \"XLK Volume\",\n          color = ~direction, colors = c('#B0C4DE','#7F7F7F')) \nfig2 <- fig2 %>% layout(yaxis = list(title = \"Volume\"))\n\n# create rangeselector buttons\nrs <- list(visible = TRUE, x = 0.5, y = -0.055,\n           xanchor = 'center', yref = 'paper',\n           font = list(size = 9),\n           buttons = list(\n             list(count=1,\n                  label='RESET',\n                  step='all'),\n             list(count=3,\n                  label='3 YR',\n                  step='year',\n                  stepmode='backward'),\n             list(count=1,\n                  label='1 YR',\n                  step='year',\n                  stepmode='backward'),\n             list(count=1,\n                  label='1 MO',\n                  step='month',\n                  stepmode='backward')\n           ))\n\n# subplot with shared x axis\nfig <- subplot(fig, fig2, heights = c(0.7,0.2), nrows=2,\n             shareX = TRUE, titleY = TRUE)\nfig <- fig %>% layout(title = paste(\"Technology Sector Fund Stock Price: JAN 2010 - March 2023\"),\n         xaxis = list(rangeselector = rs),\n         legend = list(orientation = 'h', x = 0.5, y = 1,\n                       xanchor = 'center', yref = 'paper',\n                       font = list(size = 10),\n                       bgcolor = 'transparent'))\n\nfig\n\n\n\n\n\n\nThe Technology Sector Fund (XLK) has experienced significant growth and fluctuation since 2010. In the early 2010s, XLK saw steady growth due to the increasing demand for technology products and services, as well as the growing importance of technology in various industries. However, the fund experienced a significant dip during the 2015-2016 period, largely due to concerns about the slowdown in the Chinese economy and a decline in smartphone sales.\nDespite this setback, XLK quickly rebounded and began a period of rapid growth, fueled by the increasing adoption of cloud computing, artificial intelligence, and other emerging technologies. This growth continued through the late 2010s and into the 2020s, with the COVID-19 pandemic driving even greater demand for technology as remote work and digital communication became the norm.\nOverall, the Technology Sector Fund has been one of the top-performing sector funds over the past decade, reflecting the increasing importance of technology in our daily lives and the economy as a whole. However, like any investment, there are always risks and fluctuations, and it is important for investors to carefully evaluate their goals and risk tolerance before making any investment decisions.\nFor stock prices, a multiplicative decomposition is typically preferred because the percentage changes in stock prices tend to be more important than the absolute changes. Additionally, stock prices tend to exhibit non-constant variance, meaning that the variance of the series changes over time. A multiplicative decomposition can handle this non-constant variance more effectively than an additive decomposition.\n\n\nDecomposed Time Series\n\nDecomposition PlotAdjusted Decomposition Plot\n\n\n\n\nCode\n#time series data\nmyts<-ts(df$XLK.Adjusted,frequency=252,start=c(2010,01,01), end = c(2023,3,1)) \n#original plot for time series data\norginial_plot <- autoplot(myts,xlab =\"Year\", ylab = \"Adjusted Closing Price\", main = \"Technology Sector Fund Stock price: JAN 2010 - March 2023\")\n#decompose the data\ndecompose = decompose(myts, \"multiplicative\")\n#decomposition plot\nautoplot(decompose)\n\n\n\n\n\n\n\n\n\nCode\n#adjusted plot\ntrendadj <- myts/decompose$trend\ndecompose_adjtrend_plot <- autoplot(trendadj,ylab='trend') +ggtitle('Adjusted trend component in the multiplicative time series model')\nseasonaladj <- myts/decompose$seasonal\ndecompose_adjseasonal_plot <- autoplot(seasonaladj,ylab='seasonal') +ggtitle('Adjusted seasonal component in the multiplicative time series model')\ngrid.arrange(orginial_plot, decompose_adjtrend_plot,decompose_adjseasonal_plot, nrow=3)\n\n\n\n\n\n\n\n\nThe adjusted seasonal component tend to have upward trend till 2019 and drops during the covid period and there is more variability in the model when compared to the original plot where the variation during the years but the adjusted trend then to have more fluctuation showing no trend when compared to the original plot.\n\n\nLag Plots\n\nDaily Time LagsMonthly Time Lags\n\n\n\n\nCode\n#Lag plots \ngglagplot(myts, do.lines=FALSE, lags=1)+xlab(\"Lag 1\")+ylab(\"Yi\")+ggtitle(\"Lag Plot for Technology Sector Fund Stock JAN 2010 - March 2023\")\n\n\n\n\n\n\n\n\n\nCode\n#montly data\nmean_data <- df %>% \n  mutate(month = month(Date), year = year(Date)) %>% \n  group_by(year, month) %>% \n  summarize(mean_value = mean(XLK.Adjusted))\nmonth<-ts(mean_data$mean_value,start = c(2010, 1),frequency = 12)\n#Lag plot\nts_lags(month)\n\n\n\n\n\n\n\n\n\nThe first lag plot shows the daily time lags of the Technology Sector Fund stock price from JAN 2010 to March 2023. The plot indicates that there is a strong positive correlation between the current value and the previous day’s value, as seen by the points clustering along the diagonal line. This suggests that the stock price has a positive autocorrelation at a lag of one day.\nThe second lag plot shows the monthly time lags of the mean value of the Technology Sector Fund stock price from JAN 2010 to March 2023. The plot indicates that there is a positive correlation between the current value and the value from the previous month. This suggests that the mean value of the stock price has a positive autocorrelation at a lag of one month.\nOverall, the lag plots indicate that there is a positive autocorrelation present in the Technology Sector Fund stock price data, with the strongest correlation observed in the daily time series.\n\n\nSeasonality\n\nSeasonal HeatmapSeasonal Line plot\n\n\n\n\nCode\n# Create seasonal plot\nts_heatmap(month, color = \"YlOrBr\", title = 'Seasonality Heatmap of Technology Sector Fund Stock Jan 2010 - March 2023')\n\n\n\n\n\n\n\n\n\n\nCode\n# Create a line graph for each year with months on the x-axis\nggseasonplot(month, datecol = \"date\", valuecol = \"value\")+ggtitle(\"Seasonal Yearly Plot for Technology Sector Fund Stock Jan 2010 - March 2023\")\n\n\n\n\n\n\n\n\nThe Seasonality Heatmap for the Technology Sector Fund Stock JAN 2010 - March 2023 does not reveal any clear seasonality in the data. The heatmap shows the mean value of the time series for each month and year combination, with the darker colors indicating higher values. The lack of clear patterns or darker colors in specific months or years suggests that there is no consistent seasonal pattern in the data. However, the yearly line graph shows a slight upward trend in the stock price from 2010 to 2023, but does not show any clear seasonality. Each year’s data is represented by a line, and the months are plotted on the x-axis. Overall, the lack of clear seasonality in both the heatmap and yearly line graph suggests that other factors beyond seasonality are driving the stock price fluctuations.\n\n\nMoving Average\n\n4 Month MA1 Year MA3 Year MA5 Year MA\n\n\n\n\nCode\n#SMA Smoothing \nma <- autoplot(month, series=\"Data\") +\n  autolayer(ma(month,5), series=\"4 Month MA\") +\n  xlab(\"Year\") + ylab(\"GWh\") +\n  ggtitle(\"Technology Sector Fund Stock JAN 2010 - March 2023(4 Month Moving Average)\") +\n  scale_colour_manual(values=c(\"Data\"=\"grey50\",\"4 Month MA\"=\"red\"),\n                      breaks=c(\"Data\",\"4 Month MA\"))\nma\n\n\n\n\n\n\n\n\n\nCode\n#SMA Smoothing \nma <- autoplot(month, series=\"Data\") +\n  autolayer(ma(month,13), series=\"1 Year MA\") +\n  xlab(\"Year\") + ylab(\"GWh\") +\n  ggtitle(\"Technology Sector Fund Stock JAN 2010 - March 2023(1 Year Moving Average)\") +\n  scale_colour_manual(values=c(\"Data\"=\"grey50\",\"1 Year MA\"=\"red\"),\n                      breaks=c(\"Data\",\"1 Year MA\"))\nma\n\n\n\n\n\n\n\n\n\nCode\n#SMA Smoothing \nma <- autoplot(month, series=\"Data\") +\n  autolayer(ma(month,37), series=\"3 Year MA\") +\n  xlab(\"Year\") + ylab(\"GWh\") +\n  ggtitle(\"Technology Sector Fund Stock JAN 2010 - March 2023(3 Year Moving Average)\") +\n  scale_colour_manual(values=c(\"Data\"=\"grey50\",\"3 Year MA\"=\"red\"),\n                      breaks=c(\"Data\",\"3 Year MA\"))\nma\n\n\n\n\n\n\n\n\n\nCode\n#SMA Smoothing \nma <- autoplot(month, series=\"Data\") +\n  autolayer(ma(month,61), series=\"5 Year MA\") +\n  xlab(\"Year\") + ylab(\"GWh\") +\n  ggtitle(\"Technology Sector Fund Stock JAN 2010 - March 2023(5 Year Moving Average)\") +\n  scale_colour_manual(values=c(\"Data\"=\"grey50\",\"5 Year MA\"=\"red\"),\n                      breaks=c(\"Data\",\"5 Year MA\"))\nma\n\n\n\n\n\n\n\n\nThe four plots show the Technology Sector Fund stock prices from JAN 2010 to March 2023, along with the moving averages for 4 months, 1 year 3 years and 4 years. As the window of the moving average increases, the smoother the trend line becomes, reducing the impact of noise and fluctuations in the original time series.\nThe 4-month moving average plot shows frequent fluctuations in the stock price, with the trend line following the general direction of the time series. The 1-year moving average plot shows a smoother trend, following the overall upward trend of the stock price.\nThe 1-year moving average plot shows a similar trend to the 4-month plot but is even smoother, with fewer fluctuations. Finally, the 5-year moving average plot shows the smoothest trend, with an almost constant upward slope.As the moving average window increases, the smoother trend allows for a clearer identification of the general trend of the Technology Sector Fund stock prices over time. From the moving average obtained above we can see that there is upward tend in the stock price of Technology Sector Fund.\n\n\nAutocorrelation Time Series\n\nACFPACFADF Test\n\n\n\n\nCode\n#ACF for  data\nggAcf(month)+ggtitle(\"ACF Plot for Technology Sector Fund Stock JAN 2010 - March 2023\")\n\n\n\n\n\n\n\n\n\nCode\n#PACF for data\nggPacf(month)+ggtitle(\"PACF Plot for Technology Sector Fund Stock JAN 2010 - March 2023\")\n\n\n\n\n\n\n\n\n\nCode\n#check the stationarity\ntseries::adf.test(month)\n\n\n\n    Augmented Dickey-Fuller Test\n\ndata:  month\nDickey-Fuller = -1.7941, Lag order = 5, p-value = 0.6622\nalternative hypothesis: stationary\n\n\n\n\n\nIn the plot of autocorrelation function, which is the acf graph for monthly data, there are clear autocorrelation in lag. The above lag plots and autocorrelation plot indicates seasonality in the series, which means the series is not stationary. This can be verified by the Augmented Dickey-Fuller Test which tells us that as the p value is greater than 0.05.\n\n\nDetrend and Differenced Time Series\n\nLinear Fitting ModelACF Plot\n\n\n\n\nCode\nfit = lm(myts~time(myts), na.action=NULL) \nsummary(fit) \n\n\n\nCall:\nlm(formula = myts ~ time(myts), na.action = NULL)\n\nResiduals:\n    Min      1Q  Median      3Q     Max \n-30.615 -14.971  -3.768  12.099  57.389 \n\nCoefficients:\n              Estimate Std. Error t value Pr(>|t|)    \n(Intercept) -2.055e+04  1.623e+02  -126.7   <2e-16 ***\ntime(myts)   1.022e+01  8.046e-02   127.0   <2e-16 ***\n---\nSignif. codes:  0 '***' 0.001 '**' 0.01 '*' 0.05 '.' 0.1 ' ' 1\n\nResidual standard error: 17.31 on 3277 degrees of freedom\nMultiple R-squared:  0.8312,    Adjusted R-squared:  0.8311 \nF-statistic: 1.613e+04 on 1 and 3277 DF,  p-value: < 2.2e-16\n\n\n\n\n\n\nCode\n# plot ACFs\nplot1 <- ggAcf(myts, 48, main=\"Original Data: Technology Sector Fund Stock Stock Price\")\nplot2 <- ggAcf(resid(fit), 48, main=\"Detrended data\")\nplot3 <- ggAcf(diff(myts), 48, main=\"First differenced data\")\ngrid.arrange(plot1, plot2, plot3, nrow=3)\n\n\n\n\n\n\n\n\nThe estimated slope coefficient β1, 1.8675 With a standard error of 0.1494, yielding a significant estimated increase of stock price is very less yearly. Equation of the fit for stationary process: \\[\\hat{y}_{t} = x_{t}+(3714.9265)-(1.8675)t\\]\nFrom the above graph we can say that there is no change in detrended plot and the original data acf plot, it typically means that the data is stationary. But when the first order difference is applied the high correlation is removed but there is no seasonal correlation.\nAs depicted in the above figure, the series is now stationary and ready for future study."
  },
  {
    "objectID": "eda_sector_utilites.html",
    "href": "eda_sector_utilites.html",
    "title": "EDA for Utilities Sector Fund",
    "section": "",
    "text": "The Utilities Sector Fund (XLU) is an exchange-traded fund (ETF) that provides investors with exposure to the consumer staples sector of the US economy. The fund holds a diverse range of companies, including those involved in food and beverage production, personal and household products, and tobacco. XLU is a popular choice for investors looking for stability and consistent dividends, as the companies within the sector tend to have steady earnings and demand regardless of economic conditions. Overall, XLU can be a valuable addition to a well-diversified portfolio, providing exposure to a resilient sector of the US economy.\n\nTime Series Plot\n\n\nCode\n# get data\noptions(\"getSymbols.warning4.0\"=FALSE)\noptions(\"getSymbols.yahoo.warning\"=FALSE)\n\n\ndata = getSymbols(\"XLU\",src='yahoo', from = '2010-01-01',to = \"2023-03-01\")\n\ndf <- data.frame(Date=index(XLU),coredata(XLU))\n\n# create Bollinger Bands\nbbands <- BBands(XLU[,c(\"XLU.High\",\"XLU.Low\",\"XLU.Close\")])\n\n# join and subset data\ndf <- subset(cbind(df, data.frame(bbands[,1:3])), Date >= \"2010-01-01\")\n\n#export the data \nXLU_data <- df\nwrite.csv(XLU_data, \"DATA/CLEANED DATA/XLU_raw_data.csv\", row.names=FALSE)\n\n# colors column for increasing and decreasing\nfor (i in 1:length(df[,1])) {\n  if (df$XLU.Close[i] >= df$XLU.Open[i]) {\n      df$direction[i] = 'Increasing'\n  } else {\n      df$direction[i] = 'Decreasing'\n  }\n}\n\ni <- list(line = list(color = '#FF4040'))\nd <- list(line = list(color = '#7F7F7F'))\n\n# plot candlestick chart\n\nfig <- df %>% plot_ly(x = ~Date, type=\"candlestick\",\n          open = ~XLU.Open, close = ~XLU.Close,\n          high = ~XLU.High, low = ~XLU.Low, name = \"XLU\",\n          increasing = i, decreasing = d) \nfig <- fig %>% add_lines(x = ~Date, y = ~up , name = \"B Bands\",\n            line = list(color = '#ccc', width = 0.5),\n            legendgroup = \"Bollinger Bands\",\n            hoverinfo = \"none\", inherit = F) \nfig <- fig %>% add_lines(x = ~Date, y = ~dn, name = \"B Bands\",\n            line = list(color = '#ccc', width = 0.5),\n            legendgroup = \"Bollinger Bands\", inherit = F,\n            showlegend = FALSE, hoverinfo = \"none\") \nfig <- fig %>% add_lines(x = ~Date, y = ~mavg, name = \"Mv Avg\",\n            line = list(color = '#E377C2', width = 0.5),\n            hoverinfo = \"none\", inherit = F) \nfig <- fig %>% layout(yaxis = list(title = \"Price\"))\n\n# plot volume bar chart\nfig2 <- df \nfig2 <- fig2 %>% plot_ly(x=~Date, y=~XLU.Volume, type='bar', name = \"XLU Volume\",\n          color = ~direction, colors = c('#FF4040','#7F7F7F')) \nfig2 <- fig2 %>% layout(yaxis = list(title = \"Volume\"))\n\n# create rangeselector buttons\nrs <- list(visible = TRUE, x = 0.5, y = -0.055,\n           xanchor = 'center', yref = 'paper',\n           font = list(size = 9),\n           buttons = list(\n             list(count=1,\n                  label='RESET',\n                  step='all'),\n             list(count=3,\n                  label='3 YR',\n                  step='year',\n                  stepmode='backward'),\n             list(count=1,\n                  label='1 YR',\n                  step='year',\n                  stepmode='backward'),\n             list(count=1,\n                  label='1 MO',\n                  step='month',\n                  stepmode='backward')\n           ))\n\n# subplot with shared x axis\nfig <- subplot(fig, fig2, heights = c(0.7,0.2), nrows=2,\n             shareX = TRUE, titleY = TRUE)\nfig <- fig %>% layout(title = paste(\"Utilities Sector Fund Stock Price: JAN 2010 - March 2023\"),\n         xaxis = list(rangeselector = rs),\n         legend = list(orientation = 'h', x = 0.5, y = 1,\n                       xanchor = 'center', yref = 'paper',\n                       font = list(size = 10),\n                       bgcolor = 'transparent'))\n\nfig\n\n\n\n\n\n\nSince 2010, the Utilities Sector Fund (XLU) has shown a generally upward trend, with some fluctuations along the way. In the aftermath of the 2008 financial crisis, the fund saw a significant drop in value, hitting a low point in early 2009. However, it has since recovered and continued to climb steadily.\nThe XLU saw a period of rapid growth in 2019, reaching its all-time high in November of that year, before experiencing a sharp decline in early 2020 due to the COVID-19 pandemic. However, like many other funds, it rebounded quickly and has been on an upward trajectory since then. Overall, the XLU has proven to be a relatively stable investment option, with a focus on companies in the utilities sector that typically provide essential services and products, such as electricity, water, and gas.\nFor stock prices, a multiplicative decomposition is typically preferred because the percentage changes in stock prices tend to be more important than the absolute changes. Additionally, stock prices tend to exhibit non-constant variance, meaning that the variance of the series changes over time. A multiplicative decomposition can handle this non-constant variance more effectively than an additive decomposition.\n\n\nDecomposed Time Series\n\nDecomposition PlotAdjusted Decomposition Plot\n\n\n\n\nCode\n#time series data\nmyts<-ts(df$XLU.Adjusted,frequency=252,start=c(2010,01,01), end = c(2023,3,1)) \n#original plot for time series data\norginial_plot <- autoplot(myts,xlab =\"Year\", ylab = \"Adjusted Closing Price\", main = \"Utilities Sector Fund Stock price: JAN 2010 - March 2023\")\n#decompose the data\ndecompose = decompose(myts, \"multiplicative\")\n#decomposition plot\nautoplot(decompose)\n\n\n\n\n\n\n\n\n\nCode\n#adjusted plot\ntrendadj <- myts/decompose$trend\ndecompose_adjtrend_plot <- autoplot(trendadj,ylab='trend') +ggtitle('Adjusted trend component in the multiplicative time series model')\nseasonaladj <- myts/decompose$seasonal\ndecompose_adjseasonal_plot <- autoplot(seasonaladj,ylab='seasonal') +ggtitle('Adjusted seasonal component in the multiplicative time series model')\ngrid.arrange(orginial_plot, decompose_adjtrend_plot,decompose_adjseasonal_plot, nrow=3)\n\n\n\n\n\n\n\n\nThe adjusted seasonal component tend to have upward trend till 2019 and drops during the covid period and there is more variability in the model when compared to the original plot where the variation during the years but the adjusted trend then to have more fluctuation showing no trend when compared to the original plot.\n\n\nLag Plots\n\nDaily Time LagsMonthly Time Lags\n\n\n\n\nCode\n#Lag plots \ngglagplot(myts, do.lines=FALSE, lags=1)+xlab(\"Lag 1\")+ylab(\"Yi\")+ggtitle(\"Lag Plot for Utilities Sector Fund Stock JAN 2010 - March 2023\")\n\n\n\n\n\n\n\n\n\nCode\n#montly data\nmean_data <- df %>% \n  mutate(month = month(Date), year = year(Date)) %>% \n  group_by(year, month) %>% \n  summarize(mean_value = mean(XLU.Adjusted))\nmonth<-ts(mean_data$mean_value,start = c(2010, 1),frequency = 12)\n#Lag plot\nts_lags(month)\n\n\n\n\n\n\n\n\n\nThe first lag plot shows the daily time lags of the Utilities Sector Fund stock price from JAN 2010 to March 2023. The plot indicates that there is a strong positive correlation between the current value and the previous day’s value, as seen by the points clustering along the diagonal line. This suggests that the stock price has a positive autocorrelation at a lag of one day.\nThe second lag plot shows the monthly time lags of the mean value of the Utilities Sector Fund stock price from JAN 2010 to March 2023. The plot indicates that there is a positive correlation between the current value and the value from the previous month. This suggests that the mean value of the stock price has a positive autocorrelation at a lag of one month.\nOverall, the lag plots indicate that there is a positive autocorrelation present in the Utilities Sector Fund stock price data, with the strongest correlation observed in the daily time series.\n\n\nSeasonality\n\nSeasonal HeatmapSeasonal Line plot\n\n\n\n\nCode\n# Create seasonal plot\nts_heatmap(month, color = \"BuPu\", title = 'Seasonality Heatmap of Utilities Sector Fund Stock Jan 2010 - March 2023')\n\n\n\n\n\n\n\n\n\n\nCode\n# Create a line graph for each year with months on the x-axis\nggseasonplot(month, datecol = \"date\", valuecol = \"value\")+ggtitle(\"Seasonal Yearly Plot for Utilities Sector Fund Stock Jan 2010 - March 2023\")\n\n\n\n\n\n\n\n\nThe Seasonality Heatmap for the Utilities Sector Fund Stock JAN 2010 - March 2023 does not reveal any clear seasonality in the data. The heatmap shows the mean value of the time series for each month and year combination, with the darker colors indicating higher values. The lack of clear patterns or darker colors in specific months or years suggests that there is no consistent seasonal pattern in the data. However, the yearly line graph shows a slight upward trend in the stock price from 2010 to 2023, but does not show any clear seasonality. Each year’s data is represented by a line, and the months are plotted on the x-axis. Overall, the lack of clear seasonality in both the heatmap and yearly line graph suggests that other factors beyond seasonality are driving the stock price fluctuations.\n\n\nMoving Average\n\n4 Month MA1 Year MA3 Year MA5 Year MA\n\n\n\n\nCode\n#SMA Smoothing \nma <- autoplot(month, series=\"Data\") +\n  autolayer(ma(month,5), series=\"4 Month MA\") +\n  xlab(\"Year\") + ylab(\"GWh\") +\n  ggtitle(\"Utilities Sector Fund Stock JAN 2010 - March 2023(4 Month Moving Average)\") +\n  scale_colour_manual(values=c(\"Data\"=\"grey50\",\"4 Month MA\"=\"red\"),\n                      breaks=c(\"Data\",\"4 Month MA\"))\nma\n\n\n\n\n\n\n\n\n\nCode\n#SMA Smoothing \nma <- autoplot(month, series=\"Data\") +\n  autolayer(ma(month,13), series=\"1 Year MA\") +\n  xlab(\"Year\") + ylab(\"GWh\") +\n  ggtitle(\"Utilities Sector Fund Stock JAN 2010 - March 2023(1 Year Moving Average)\") +\n  scale_colour_manual(values=c(\"Data\"=\"grey50\",\"1 Year MA\"=\"red\"),\n                      breaks=c(\"Data\",\"1 Year MA\"))\nma\n\n\n\n\n\n\n\n\n\nCode\n#SMA Smoothing \nma <- autoplot(month, series=\"Data\") +\n  autolayer(ma(month,37), series=\"3 Year MA\") +\n  xlab(\"Year\") + ylab(\"GWh\") +\n  ggtitle(\"Utilities Sector Fund Stock JAN 2010 - March 2023(3 Year Moving Average)\") +\n  scale_colour_manual(values=c(\"Data\"=\"grey50\",\"3 Year MA\"=\"red\"),\n                      breaks=c(\"Data\",\"3 Year MA\"))\nma\n\n\n\n\n\n\n\n\n\nCode\n#SMA Smoothing \nma <- autoplot(month, series=\"Data\") +\n  autolayer(ma(month,61), series=\"5 Year MA\") +\n  xlab(\"Year\") + ylab(\"GWh\") +\n  ggtitle(\"Utilities Sector Fund Stock JAN 2010 - March 2023(5 Year Moving Average)\") +\n  scale_colour_manual(values=c(\"Data\"=\"grey50\",\"5 Year MA\"=\"red\"),\n                      breaks=c(\"Data\",\"5 Year MA\"))\nma\n\n\n\n\n\n\n\n\nThe four plots show the Utilities Sector Fund stock prices from JAN 2010 to March 2023, along with the moving averages for 4 months, 1 year 3 years and 4 years. As the window of the moving average increases, the smoother the trend line becomes, reducing the impact of noise and fluctuations in the original time series.\nThe 4-month moving average plot shows frequent fluctuations in the stock price, with the trend line following the general direction of the time series. The 1-year moving average plot shows a smoother trend, following the overall upward trend of the stock price.\nThe 1-year moving average plot shows a similar trend to the 4-month plot but is even smoother, with fewer fluctuations. Finally, the 5-year moving average plot shows the smoothest trend, with an almost constant upward slope.As the moving average window increases, the smoother trend allows for a clearer identification of the general trend of the Utilities Sector Fund stock prices over time. From the moving average obtained above we can see that there is upward tend in the stock price of Utilities Sector Fund.\n\n\nAutocorrelation Time Series\n\nACFPACFADF Test\n\n\n\n\nCode\n#ACF for  data\nggAcf(month)+ggtitle(\"ACF Plot for Utilities Sector Fund Stock JAN 2010 - March 2023\")\n\n\n\n\n\n\n\n\n\nCode\n#PACF for data\nggPacf(month)+ggtitle(\"PACF Plot for Utilities Sector Fund Stock JAN 2010 - March 2023\")\n\n\n\n\n\n\n\n\n\nCode\n#check the stationarity\ntseries::adf.test(month)\n\n\n\n    Augmented Dickey-Fuller Test\n\ndata:  month\nDickey-Fuller = -3.2079, Lag order = 5, p-value = 0.08909\nalternative hypothesis: stationary\n\n\n\n\n\nIn the plot of autocorrelation function, which is the acf graph for monthly data, there are clear autocorrelation in lag. The above lag plots and autocorrelation plot indicates seasonality in the series, which means the series is not stationary. It was also verified using Augmented Dickey-Fuller Test which tells us that as the p value is greater than 0.05, the series is not stationary.\n\n\nDetrend and Differenced Time Series\n\nLinear Fitting ModelACF Plot\n\n\n\n\nCode\nfit = lm(myts~time(myts), na.action=NULL) \nsummary(fit) \n\n\n\nCall:\nlm(formula = myts ~ time(myts), na.action = NULL)\n\nResiduals:\n     Min       1Q   Median       3Q      Max \n-14.2296  -1.8450  -0.0559   1.7998  10.5812 \n\nCoefficients:\n              Estimate Std. Error t value Pr(>|t|)    \n(Intercept) -7.912e+03  2.688e+01  -294.3   <2e-16 ***\ntime(myts)   3.943e+00  1.333e-02   295.8   <2e-16 ***\n---\nSignif. codes:  0 '***' 0.001 '**' 0.01 '*' 0.05 '.' 0.1 ' ' 1\n\nResidual standard error: 2.867 on 3277 degrees of freedom\nMultiple R-squared:  0.9639,    Adjusted R-squared:  0.9639 \nF-statistic: 8.75e+04 on 1 and 3277 DF,  p-value: < 2.2e-16\n\n\n\n\n\n\nCode\n# plot ACFs\nplot1 <- ggAcf(myts, 48, main=\"Original Data: Utilities Sector Fund Stock Stock Price\")\nplot2 <- ggAcf(resid(fit), 48, main=\"Detrended data\")\nplot3 <- ggAcf(diff(myts), 48, main=\"First differenced data\")\ngrid.arrange(plot1, plot2, plot3, nrow=3)\n\n\n\n\n\n\n\n\nThe estimated slope coefficient β1, 6.854e-01 With a standard error of 5.695e-02, yielding a significant estimated increase of stock price is very less yearly. Equation of the fit for stationary process: \\[\\hat{y}_{t} = x_{t}+(1.346e+03)-(6.854e-01)t\\]\nFrom the above graph we can say that there is no change in detrended plot and the original data acf plot, it typically means that the data is stationary. But when the first order difference is applied the high correlation is removed but there is no seasonal correlation.\nAs depicted in the above figure, the series is now stationary and ready for future study."
  },
  {
    "objectID": "eda_stock_index.html",
    "href": "eda_stock_index.html",
    "title": "EDA for US Stock Index",
    "section": "",
    "text": "Exploratory Data Analysis (EDA) is an essential process in understanding the underlying patterns and characteristics of financial data. EDA for the Dow Jones, NASDAQ, and S&P 500 stock market indices typically involves analyzing time series data. Time series data refers to observations of a variable over time, such as the daily closing prices of a stock market index.\nOne of the key aspects of EDA for time series data is identifying the presence of autocorrelation. Autocorrelation refers to the correlation between a variable and its past values. The autocorrelation function (ACF) and partial autocorrelation function (PACF) plots can help identify the degree of correlation between the variable and its past values. This information can be useful for forecasting future values and identifying potential patterns in the data.\nAnother important aspect of EDA for time series data is identifying the presence of seasonality. Seasonality refers to a pattern that repeats itself in the data over regular intervals, such as daily, weekly, or monthly. Identifying seasonality is important as it can help us identify potential patterns and trends in the data, and it can inform our modeling approach.\nAdditionally, examining moving averages and detrending can help identify the underlying trend in the data. Moving averages are used to smooth out short-term fluctuations in the data, and detrending can help identify the underlying trend that is not related to the seasonal or cyclical fluctuations in the data.\nFinally, testing for stationarity is important as it allows us to apply statistical models that assume the data to be stationary. Stationarity refers to the property of a time series where its statistical properties, such as mean and variance, remain constant over time. If the data is non-stationary, it can be transformed to become stationary through techniques such as differencing or taking logarithms.\nThis information can be used to inform our modeling approach, identify potential patterns and trends, and improve our investment strategies.\nClick to view EDA Page for Dow Jones Index\nClick to view EDA Page for NADSAQ Composite Index\nClick to view EDA Page for S&P 500 Index"
  },
  {
    "objectID": "eda.html",
    "href": "eda.html",
    "title": "",
    "section": "",
    "text": "EDA is a critical tool for analyzing stock indices, sector markets, and macroeconomic factors. By examining historical data and using various EDA techniques, investors and financial professionals can gain insights into market dynamics, identify patterns and trends, and make informed investment decisions. EDA enables a deeper understanding of the historical performance of financial markets, providing valuable information for risk assessment, investment strategies, and decision-making processes.\n\n\nEDA for US Stock Indices\nExploratory Data Analysis (EDA) plays a significant role in understanding stock indices, which are measures of the overall performance of a group of stocks representing a particular market or segment. EDA involves analyzing historical price and volume data of stock indices to identify patterns, trends, and anomalies. By examining historical data, investors can gain insights into the historical performance of stock indices, such as identifying patterns of market cycles, market trends, and potential investment opportunities. EDA techniques, such as data visualization, statistical analysis, and time series analysis, can provide valuable information to assess the risk and return characteristics of stock indices and help investors make informed investment decisions.\nClick to view EDA Page for US Stock Indices\n\n\nEDA for Sector Market\nEDA is also crucial in analyzing sector markets, which represent specific industries or sectors of the economy, such as technology, healthcare, or finance. By conducting EDA on sector market data, investors can gain insights into the performance of different sectors, identify trends and patterns, and assess the relative strength or weakness of specific sectors. This information can be valuable in making sector-specific investment decisions or sector rotation strategies. EDA techniques, such as sector performance analysis, correlation analysis, and cluster analysis, can help investors identify potential opportunities and risks associated with different sectors of the market.\nClick to view EDA Page for Sector Market\n\n\nEDA for Macroeconomic Factors\nEDA is also widely used in analyzing macroeconomic factors, such as GDP, inflation, employment, and interest rates, which can impact the overall performance of stock indices and sector markets. By examining historical macroeconomic data, investors and economists can identify trends, patterns, and relationships that may affect the performance of financial markets. EDA techniques, such as time series analysis, regression analysis, and data visualization, can provide insights into the relationships between macroeconomic factors and stock market performance. This information can be used to make informed investment decisions, assess economic trends, and develop macroeconomic forecasts.\nClick to view EDA Page for Macroeconomic Factors"
  },
  {
    "objectID": "ftsm.html",
    "href": "ftsm.html",
    "title": "",
    "section": "",
    "text": "Financial Time Series Models\nFinancial time series models are statistical models that are used to analyze financial data, including stock prices, economic indicators, and other financial metrics. These models can help investors and analysts to understand trends and patterns in financial data, and to make predictions about future performance.\nWe will be looking at each of the top company for each sector with macroeconomic indicators such as GDP growth rate, inflation, interest rates, and unemployment. For example, let’s consider the technology sector and the top company in that sector is Apple. We can use an ARIMAX model to analyze the relationship between each company’s stock price and macroeconomic indicators such as GDP growth rate, inflation, interest rates, and unemployment.\n\nTo make the model more robust, we can also use an ARCH/GARCH model to capture the volatility in the financial time series. ARCH (Autoregressive Conditional Heteroskedasticity) and GARCH (Generalized Autoregressive Conditional Heteroskedasticity) models are commonly used to model the conditional variance of a time series, which can help to identify patterns of volatility and predict future fluctuations in the stock price.\nThe ARIMAX+ARCH/GARCH model can help investors and analysts to understand the relationship between each company’s stock price and macroeconomic indicators, and to make more informed investment decisions based on historical patterns and predicted future performance.\nClick to view Consumer Staples Sector Fund\nClick to view Utilities Sector Fund\nClick to view Health Care Sector Fund\nClick to view Industrial Sector Fund\nClick to view Financial Sector Fund\nClick to view Consumer Discretionary Sector Fund\nClick to view Communication Services Sector Fund\nClick to view Real Estate Sector Fund\nClick to view Materials Sector Fund\nClick to view Technology Sector Fund\nClick to view Energy Sector Fund"
  },
  {
    "objectID": "introduction.html",
    "href": "introduction.html",
    "title": "",
    "section": "",
    "text": "The stock market and sector market analysis are critical components of the global economy. The stock market is a marketplace where publicly traded companies’ shares are bought and sold, while sector market analysis is the process of analyzing specific sectors within the stock market, such as technology, energy, healthcare, etc. Understanding the performance of these sectors and individual companies is essential for investors and analysts alike.\nOne critical factor that impacts the performance of the stock market and sector market analysis is macroeconomic factors. These factors include interest rates, inflation, GDP growth, geopolitical events, and government policies, among others. Changes in these macroeconomic factors can have a significant impact on the stock market’s overall performance and individual sectors.\n\nFor example, rising interest rates may lead to higher borrowing costs for companies, reducing their profitability and impacting their stock prices. Similarly, geopolitical events such as wars, conflicts, or trade disputes may impact the stock market by creating uncertainty and volatility.\nTo analyze the stock market and sector market, analysts use various techniques such as technical analysis, fundamental analysis, and quantitative analysis. Technical analysis uses charts and other visual representations to identify trends and patterns in stock prices, while fundamental analysis looks at a company’s financial statements to determine its intrinsic value. Quantitative analysis involves using statistical models to analyze data and identify trends.\nOverall, understanding the impact of macroeconomic factors on the stock market and sector market analysis is crucial for investors and analysts alike. By analyzing data and identifying patterns, analysts can gain insights into the underlying drivers of market movements and make informed investment decisions.\nQUESTIONS:\n\nHow do different macroeconomic factors such as inflation, interest rates, and GDP growth impact the performance of specific sectors within the stock market?\nWhat role do historical trends in macroeconomic indicators play in predicting stock market performance, and how can this information be used to guide investment strategies?\nWhat are some practical methods for incorporating macroeconomic factors into fundamental and technical analyses of individual stocks or sectors?\nHow can macroeconomic indicators be used to identify potential risks or opportunities in the stock market, and what strategies can be employed to manage thXese risks or capitalize on these opportunities?\nHow can investors stay informed about key macroeconomic factors that are likely to impact the stock market, and what resources are available for tracking these factors over time?\nWhat are the potential benefits and limitations of relying on time series analysis to make predictions about stock market trends based on macroeconomic factors, and how can these limitations be addressed?\nHow do macroeconomic factors affect the stock market differently across different industries or sectors, and what strategies can be used to optimize investments based on these differences?\nHow do macroeconomic factors contribute to overall market volatility, and what steps can investors take to manage their exposure to market volatility based on these factors?\nWhat are some common mistakes or pitfalls that investors may encounter when trying to incorporate macroeconomic factors into their investment strategies, and how can these be avoided?\nHow can a comprehensive understanding of macroeconomic factors help investors build a diversified portfolio that is well-positioned to weather changes in the stock market over time?"
  },
  {
    "objectID": "arima_sector_consumer_staples.html",
    "href": "arima_sector_consumer_staples.html",
    "title": "ARIMA Model for Consumer Staples Sector Fund",
    "section": "",
    "text": "During exploratory data analysis (EDA), it was observed that the raw data is often non-stationary, exhibiting trends, seasonality, and other complex patterns that can make modeling and forecasting difficult. To address this issue, the data is often differentiated to achieve stationarity, which is a key assumption of the ARIMA model.\nDifferencing involves calculating the difference between each data point and the previous data point, which can help to remove trends and seasonality from the data. The goal is to create a new series of data that is stationary, meaning that the statistical properties of the data do not change over time.\n\nDifferentiated Time Series Plot\n\nPlotACFPACFADF Test\n\n\n\n\nCode\n#import the data\ndf <- read.csv(\"DATA/CLEANED DATA/XLP_raw_data.csv\")\n#convert to time series data\nmyts<-ts(df$XLP.Adjusted,frequency=252,start=c(2010,1,1), end = c(2023,3,1)) \n#First order differentiation\ndf1 <- diff(myts)\n# Plot \nmyts  %>% diff() %>% ggtsdisplay(main = \"First order differentiation\") \n\n\n\n\n\n\n\n\n\nCode\n#ACF plot \nggAcf(df1,60,main=\"ACF Plot: First order differentiation\") \n\n\n\n\n\n\n\n\n\nCode\nggPacf(df1,60,main=\"PACF Plot: First order differentiation\") \n\n\n\n\n\n\n\n\n\nCode\ntseries::adf.test(df1)\n\n\n\n    Augmented Dickey-Fuller Test\n\ndata:  df1\nDickey-Fuller = -15.945, Lag order = 14, p-value = 0.01\nalternative hypothesis: stationary\n\n\n\n\n\nAfter analyzing the ACF and PACF plots, it can be observed that most of the bar lines lie between the blue lines, indicating that the time series is stationary. This has been further confirmed through an Augmented Dickey-Fuller test as the p-value is less than 0.05. Once the data is stationary, the next step is to model it using ARIMA. The combination of ACF and PACF plots can help determine the appropriate values for p, d, and q in the ARIMA model. The order of differencing required to achieve stationarity gives the d value. The p value is determined by examining significant spikes at various lags in the PACF plot, while the q value is determined through significant spikes in the ACF plot. It is important to consider other methods such as grid search and information criteria to ensure the most appropriate model is selected.\nHere the parameters are d = 1 p = 0,9 (PACF Plot) q = 0,9 (ACF Plot)\n\n\nModel Selection\n\nARIMA(0,1,0) ResultARIMA(9,1,0) ResultARIMA(0,1,9) ResultARIMA(9,1,9) ResultAuto Arima Model\n\n\n\n\nCode\nArima(myts,order=c(0,1,0),include.drift=TRUE) \n\n\nSeries: myts \nARIMA(0,1,0) with drift \n\nCoefficients:\n       drift\n      0.0167\ns.e.  0.0075\n\nsigma^2 = 0.1837:  log likelihood = -1873.2\nAIC=3750.4   AICc=3750.4   BIC=3762.59\n\n\n\n\n\n\nCode\nArima(myts,order=c(9,1,0),include.drift=TRUE) \n\n\nSeries: myts \nARIMA(9,1,0) with drift \n\nCoefficients:\n          ar1     ar2      ar3      ar4     ar5      ar6     ar7      ar8\n      -0.0758  0.0148  -0.0414  -0.0875  0.0290  -0.0410  0.0721  -0.0315\ns.e.   0.0173  0.0174   0.0173   0.0174  0.0174   0.0174  0.0174   0.0174\n         ar9   drift\n      0.1173  0.0167\ns.e.  0.0174  0.0070\n\nsigma^2 = 0.1761:  log likelihood = -1799.85\nAIC=3621.7   AICc=3621.78   BIC=3688.75\n\n\n\n\n\n\nCode\nArima(myts,order=c(0,1,9),include.drift=TRUE) \n\n\nSeries: myts \nARIMA(0,1,9) with drift \n\nCoefficients:\n          ma1     ma2      ma3      ma4     ma5      ma6     ma7      ma8\n      -0.0629  0.0205  -0.0323  -0.0870  0.0488  -0.0584  0.0769  -0.0457\ns.e.   0.0174  0.0175   0.0174   0.0175  0.0185   0.0176  0.0175   0.0169\n         ma9   drift\n      0.1049  0.0167\ns.e.  0.0171  0.0071\n\nsigma^2 = 0.1767:  log likelihood = -1805.64\nAIC=3633.28   AICc=3633.36   BIC=3700.32\n\n\n\n\n\n\nCode\nArima(myts,order=c(9,1,9),include.drift=TRUE) \n\n\nSeries: myts \nARIMA(9,1,9) with drift \n\nCoefficients:\n          ar1      ar2      ar3      ar4      ar5      ar6      ar7      ar8\n      -0.5096  -0.5136  -0.2633  -0.4045  -0.4465  -0.3360  -0.3715  -0.3303\ns.e.      NaN      NaN      NaN      NaN   0.0505   0.0849      NaN      NaN\n        ar9     ma1    ma2     ma3     ma4     ma5     ma6     ma7    ma8\n      0.181  0.4349  0.496  0.1969  0.3085  0.4314  0.2491  0.4279  0.276\ns.e.    NaN     NaN    NaN     NaN     NaN  0.0763  0.0830     NaN    NaN\n          ma9   drift\n      -0.0809  0.0167\ns.e.      NaN  0.0068\n\nsigma^2 = 0.1751:  log likelihood = -1785.74\nAIC=3611.49   AICc=3611.74   BIC=3733.39\n\n\n\n\n\n\nCode\nauto.arima(myts)\n\n\nSeries: myts \nARIMA(0,1,2) with drift \n\nCoefficients:\n          ma1     ma2   drift\n      -0.0855  0.0352  0.0167\ns.e.   0.0175  0.0189  0.0071\n\nsigma^2 = 0.1821:  log likelihood = -1858.28\nAIC=3724.56   AICc=3724.57   BIC=3748.94\n\n\n\n\n\nIn the Model selection, when comparing the AIC, AICc and BIC values of all the 4 models, model (0,1,0) has the least BIC and AIc value. The model generated by auto.arima is also (0,1,0). We can proceed with the further model diagnostic for the chosen best model i.e. (0,1,0), to look into the details of the model.\n\n\nModel Diagnostic\n\nModel PlotModel\n\n\n\n\nCode\nmodel_output <- capture.output(sarima(myts,0,1,0))\n\n\n\n\n\n\n\n\n\nCode\ncat(model_output[9:38], model_output[length(model_output)], sep = \"\\n\") \n\n\n$fit\n\nCall:\narima(x = xdata, order = c(p, d, q), seasonal = list(order = c(P, D, Q), period = S), \n    xreg = constant, transform.pars = trans, fixed = fixed, optim.control = list(trace = trc, \n        REPORT = 1, reltol = tol))\n\nCoefficients:\n      constant\n        0.0167\ns.e.    0.0075\n\nsigma^2 estimated as 0.1836:  log likelihood = -1873.2,  aic = 3750.4\n\n$degrees_of_freedom\n[1] 3277\n\n$ttable\n         Estimate     SE t.value p.value\nconstant   0.0167 0.0075  2.2305  0.0258\n\n$AIC\n[1] 1.144112\n\n$AICc\n[1] 1.144112\n\n$BIC\n[1] 1.147831\n\n\n\n\n\nThe model diagnostic is done for ARIMA(0,1,0). By analyzing the standardized residuals plot, it can be observed that the mean is close to 0 and the variance is slightly higher than 1. Deviations from these values could indicate poor model fit. However, in this case, the model seems to be well-fitted. The ACF plot of residuals shows very few significant lags, which is a positive sign for the model. The qq-plot also suggests normality in the residuals. Additionally, the p-values for the Ljung-Box test are greater than 0.05, indicating that the residuals are independent, which is favorable for the model’s accuracy.\nThe equation for this model: \\[(1-\\phi_1B-\\phi_2B^2)(1-B)(Y_t-\\mu) = (1+\\theta_1B+\\theta_2B^2)\\epsilon_t\\]\n\n\nForcast\n\nShort Term ForcastLong term Forcast\n\n\n\n\nCode\nmyts %>%\n  Arima(order=c(0,1,0),include.drift = TRUE) %>%\n  forecast %>%\n  autoplot(main = \"Consumer Staples Sector Fund Stock Prices Prediction\") +\n  ylab(\"stock prices\") + xlab(\"Year\")\n\n\n\n\n\n\n\n\n\nCode\nsarima.for(myts,182, 0,1,0, main='Consumer Staples Sector Fund Stock Prices Prediction')\n\n\n\n\n\n$pred\nTime Series:\nStart = c(2023, 4) \nEnd = c(2023, 185) \nFrequency = 252 \n  [1] 73.05981 73.07650 73.09319 73.10989 73.12658 73.14327 73.15997 73.17666\n  [9] 73.19335 73.21005 73.22674 73.24343 73.26013 73.27682 73.29351 73.31021\n [17] 73.32690 73.34359 73.36029 73.37698 73.39367 73.41037 73.42706 73.44375\n [25] 73.46045 73.47714 73.49383 73.51053 73.52722 73.54391 73.56060 73.57730\n [33] 73.59399 73.61068 73.62738 73.64407 73.66076 73.67746 73.69415 73.71084\n [41] 73.72754 73.74423 73.76092 73.77762 73.79431 73.81100 73.82770 73.84439\n [49] 73.86108 73.87778 73.89447 73.91116 73.92786 73.94455 73.96124 73.97794\n [57] 73.99463 74.01132 74.02802 74.04471 74.06140 74.07810 74.09479 74.11148\n [65] 74.12818 74.14487 74.16156 74.17826 74.19495 74.21164 74.22834 74.24503\n [73] 74.26172 74.27842 74.29511 74.31180 74.32850 74.34519 74.36188 74.37857\n [81] 74.39527 74.41196 74.42865 74.44535 74.46204 74.47873 74.49543 74.51212\n [89] 74.52881 74.54551 74.56220 74.57889 74.59559 74.61228 74.62897 74.64567\n [97] 74.66236 74.67905 74.69575 74.71244 74.72913 74.74583 74.76252 74.77921\n[105] 74.79591 74.81260 74.82929 74.84599 74.86268 74.87937 74.89607 74.91276\n[113] 74.92945 74.94615 74.96284 74.97953 74.99623 75.01292 75.02961 75.04631\n[121] 75.06300 75.07969 75.09639 75.11308 75.12977 75.14647 75.16316 75.17985\n[129] 75.19655 75.21324 75.22993 75.24662 75.26332 75.28001 75.29670 75.31340\n[137] 75.33009 75.34678 75.36348 75.38017 75.39686 75.41356 75.43025 75.44694\n[145] 75.46364 75.48033 75.49702 75.51372 75.53041 75.54710 75.56380 75.58049\n[153] 75.59718 75.61388 75.63057 75.64726 75.66396 75.68065 75.69734 75.71404\n[161] 75.73073 75.74742 75.76412 75.78081 75.79750 75.81420 75.83089 75.84758\n[169] 75.86428 75.88097 75.89766 75.91436 75.93105 75.94774 75.96444 75.98113\n[177] 75.99782 76.01452 76.03121 76.04790 76.06459 76.08129\n\n$se\nTime Series:\nStart = c(2023, 4) \nEnd = c(2023, 185) \nFrequency = 252 \n  [1] 0.4284879 0.6059734 0.7421629 0.8569758 0.9581281 1.0495768 1.1336725\n  [8] 1.2119469 1.2854638 1.3549978 1.4211337 1.4843257 1.5449352 1.6032550\n [15] 1.6595266 1.7139517 1.7667010 1.8179203 1.8677356 1.9162562 1.9635783\n [22] 2.0097865 2.0549559 2.0991535 2.1424396 2.1848683 2.2264886 2.2673450\n [29] 2.3074781 2.3469250 2.3857198 2.4238937 2.4614757 2.4984925 2.5349687\n [36] 2.5709275 2.6063903 2.6413769 2.6759062 2.7099956 2.7436614 2.7769191\n [43] 2.8097832 2.8422673 2.8743844 2.9061465 2.9375652 2.9686514 2.9994155\n [50] 3.0298672 3.0600158 3.0898704 3.1194392 3.1487303 3.1777515 3.2065100\n [57] 3.2350129 3.2632668 3.2912782 3.3190532 3.3465977 3.3739173 3.4010174\n [64] 3.4279034 3.4545801 3.4810523 3.5073248 3.5334019 3.5592880 3.5849872\n [71] 3.6105034 3.6358406 3.6610024 3.6859925 3.7108143 3.7354711 3.7599663\n [78] 3.7843029 3.8084840 3.8325125 3.8563913 3.8801232 3.9037108 3.9271567\n [85] 3.9504635 3.9736335 3.9966693 4.0195730 4.0423470 4.0649934 4.0875143\n [92] 4.1099118 4.1321879 4.1543445 4.1763837 4.1983071 4.2201166 4.2418140\n [99] 4.2634010 4.2848792 4.3062503 4.3275159 4.3486775 4.3697366 4.3906946\n[106] 4.4115532 4.4323135 4.4529771 4.4735452 4.4940192 4.5144004 4.5346899\n[113] 4.5548891 4.5749991 4.5950211 4.6149562 4.6348055 4.6545702 4.6742514\n[120] 4.6938500 4.7133671 4.7328038 4.7521609 4.7714396 4.7906406 4.8097650\n[127] 4.8288136 4.8477875 4.8666873 4.8855140 4.9042684 4.9229514 4.9415638\n[134] 4.9601063 4.9785798 4.9969849 5.0153225 5.0335934 5.0517981 5.0699375\n[141] 5.0880122 5.1060229 5.1239703 5.1418551 5.1596778 5.1774393 5.1951400\n[148] 5.2127806 5.2303617 5.2478839 5.2653477 5.2827539 5.3001029 5.3173953\n[155] 5.3346316 5.3518124 5.3689383 5.3860097 5.4030271 5.4199911 5.4369022\n[162] 5.4537609 5.4705676 5.4873228 5.5040270 5.5206807 5.5372843 5.5538382\n[169] 5.5703430 5.5867990 5.6032067 5.6195664 5.6358787 5.6521439 5.6683624\n[176] 5.6845347 5.7006610 5.7167419 5.7327777 5.7487687 5.7647154 5.7806181\n\n\n\n\n\nThe two figures above show an idea of the short term and the long-term forecasted stock price. The forecasted values is be based on the estimated parameters of the model, which were obtained by fitting it to the historical data. One way to assess the accuracy of the forecasts is to use cross-validation, which involves splitting the data into training and testing sets and comparing the predicted values to the actual values in the test set. This can help to identify any potential problems with the model and to fine-tune the model parameters.\n\n12 Step ahead Cross Validation1 Step ahead Cross Validation\n\n\n\n\nCode\n#a seasonal cross validation using 12 steps ahead forecasts\nfarima1 <- function(x, h){forecast(Arima(x, order=c(0,1,0)), h=h)}\n\n# Compute cross-validated errors for up to 12 steps ahead\ne <- tsCV(myts, forecastfunction = farima1, h = 12)\n\n# Compute the MSE values and remove missing values\nmse <- colMeans(e^2, na.rm = TRUE)\n\n# Plot the MSE values against the forecast horizon\ndata.frame(h = 1:12, MSE = mse) %>%\n  ggplot(aes(x = h, y = MSE)) + geom_point()+geom_line()+ggtitle(\"12 Step Ahead Cross Validation\")\n\n\n\n\n\n\n\n\n\nCode\n#a seasonal cross validation using 1 steps ahead forecasts\nfarima1 <- function(x, h){forecast(Arima(x, order=c(0,1,0)),h=h)}\n\n# Compute cross-validated errors for up to 1 steps ahead\ne <- tsCV(myts, forecastfunction = farima1, h = 1)\nmse <-abs(mean(e,na.rm=TRUE))\n\n# Plot the MSE values against the forecast horizon\ndata.frame(h = 1:12, MSE = mse) %>%\n  ggplot(aes(x = h, y = MSE)) + geom_point()+geom_line()+ggtitle(\"1 Step Ahead Cross Validation\")\n\n\n\n\n\n\n\n\nThe one-step-ahead forecasting plot shows that the MSE seems to be stable. The MSE is tend to rise by 12 steps at each step of cross-validation. Lastly, the above-mentioned ARIMA model should be compared to benchmark methods before being used for a long time.\n\n\nModel Comparison\n\nPlotModel Error Table\n\n\n\n\nCode\nfit <- Arima(myts, order=c(0,1,0))\nautoplot(myts) +\n  autolayer(meanf(myts, h=182),\n            series=\"Mean\", PI=FALSE) +\n  autolayer(naive(myts, h=182),\n            series=\"Naïve\", PI=FALSE) +\n  autolayer(snaive(myts, h=182),\n            series=\"SNaïve\", PI=FALSE)+\n  autolayer(rwf(myts, h=182, drift=TRUE),\n            series=\"Drift\", PI=FALSE)+\n  autolayer(forecast(fit,182), \n            series=\"fit\",PI=FALSE) +\n  guides(colour=guide_legend(title=\"Forecast\"))\n\n\n\n\n\n\n\n\n\nCode\nsummary <- summary(fit)\nsnaive <- snaive(myts, h = 182)\naccuracy(snaive)\n\n\n                   ME     RMSE      MAE      MPE     MAPE MASE      ACF1\nTraining set 4.327132 5.359047 4.566995 10.15949 10.69683    1 0.9816058\n\n\nCode\nsummary\n\n\nSeries: myts \nARIMA(0,1,0) \n\nsigma^2 = 0.1839:  log likelihood = -1875.69\nAIC=3753.37   AICc=3753.37   BIC=3759.47\n\nTraining set error measures:\n                     ME      RMSE       MAE        MPE      MAPE       MASE\nTraining set 0.01669376 0.4287477 0.2637191 0.03822063 0.6056566 0.05774455\n                    ACF1\nTraining set -0.09227748\n\n\n\n\n\nError\nModel\nSnavie\n\n\n\n\nME\n0.004307832\n1.469789\n\n\nRMSE\n0.8600413\n16.4328\n\n\nMAE\n0.24721\n10.11811\n\n\nMPE\n-0.01937673\n-7.129413\n\n\nMAPE\n0.64873\n35.78488\n\n\nMASE\n0.02443243\n1.0000000\n\n\nACF1\n-0.007978438\n0.9970419\n\n\n\n\n\n\nThe ARIMA forecast tracks the actual data points very closely when compared to other model. The other forecast methods are less accurate when compared. From the table, Model error measurements of fit are much lower than snaive method.We can conclude that the fitted model is good."
  },
  {
    "objectID": "deep_learning_sector_xly.html",
    "href": "deep_learning_sector_xly.html",
    "title": "",
    "section": "",
    "text": "Deep Learning for Consumer Discretionary Sector Fund\nThe consumer discretionary sector comprises companies that provide non-essential goods and services, such as clothing, entertainment, and luxury items. These companies tend to be more sensitive to economic cycles and consumer sentiment than companies in other sectors. As with other stock market, predicting the future movements of the Consumer Discretionary Sector Fund can be challenging due to the complex and dynamic nature of the market. These models range from traditional time-series models to more advanced deep learning models. By leveraging the power of Deep Learning and applying techniques such as RNN, LSTM, GRU, and regularization, we can create a model that accurately predicts the future movements of the sector. Such a model can provide valuable insights for investors and traders looking to make informed decisions in the stock market, and help them stay ahead of the curve in the ever-changing landscape of the technology and biotech industries.\nIn order to apply the deep learning methods, the time series data is first split into training and testing sets, taking into account the time component. The training data constitutes 75% of the total data while the rest is the testing data. Furthermore, to ensure that the values are on the same scale, the data is scaled, which is crucial for deep learning models. The mean and standard deviation of the training data are computed and then utilized to normalize both the training and testing sets. This approach ensures that the models are trained on a standardized dataset, enabling them to make accurate predictions.\n\n\nCode\n# libraries\nimport numpy as np\nimport pandas as pd\nimport math\nfrom sklearn.metrics import mean_squared_error\nfrom matplotlib import pyplot as plt\nfrom keras.models import Sequential\nfrom keras.layers import Dropout\nfrom keras.layers import Dense, SimpleRNN, LSTM, GRU\nfrom keras import regularizers\nimport warnings\nwarnings.filterwarnings('ignore')\n\n# import data\ndf = pd.read_csv('./DATA/CLEANED DATA/XLY_raw_data.csv')\n# clean data and get final data for anlaysis\ndf = df[['Date', 'XLY.Adjusted']]\ndf['Date'] = pd.to_datetime(df['Date'])\ndf['XLY.Adjusted'] = pd.to_numeric(df['XLY.Adjusted'], errors='coerce')\ndf = df.dropna(subset=['XLY.Adjusted'])\ndf.sort_values('Date', inplace=True, ascending=True)\ndf = df.reset_index(drop=True)\ndef create_X_Y(ts: np.array, lag=1, n_ahead=1, target_index=0) -> tuple:\n    \"\"\"\n    A method to create X and Y matrix from a time series array for the training of \n    deep learning models \n    \"\"\"\n    # Extracting the number of features that are passed from the array\n    n_features = ts.shape[1]\n\n    # Creating placeholder lists\n    X, Y = [], []\n\n    if len(ts) - lag <= 0:\n        X.append(ts)\n    else:\n        for i in range(len(ts) - lag - n_ahead):\n            Y.append(ts[(i + lag):(i + lag + n_ahead), target_index])\n            X.append(ts[i:(i + lag)])\n\n    X, Y = np.array(X), np.array(Y)\n\n    # Reshaping the X array to an RNN input shape\n    X = np.reshape(X, (X.shape[0], lag, n_features))\n\n    return X, Y\n\nts = df[['XLY.Adjusted']].values\nnp.random.seed(123)\nnrows = ts.shape[0]\ntest_share = 0.25\n# Spliting into train and test sets\ntrain = ts[0:int(nrows * (1 - test_share))]\ntest = ts[int(nrows * (1 - test_share)):]\n\n# Scaling the data\ntrain_mean = train.mean()\ntrain_std = train.std()\ntrain = (train - train_mean) / train_std\ntest = (test - train_mean) / train_std\n\n# Creating the final scaled frame\nts_s = np.concatenate([train, test])\n\nlag = 12\nahead = 3\n\n# Creating the X and Y for training\nX, Y = create_X_Y(ts_s, lag=lag, n_ahead=ahead)\n\nXtrain = X[0:int(X.shape[0] * (1 - test_share))]\nYtrain = Y[0:int(X.shape[0] * (1 - test_share))]\n\nXval = X[int(X.shape[0] * (1 - test_share)):]\nYval = Y[int(X.shape[0] * (1 - test_share)):]\n\n\n\n\ndef plot_model(history, model_title):\n    loss = history.history['loss']\n    epochs = range(1, len(loss) + 1)\n    plt.figure()\n    plt.plot(epochs, loss, 'b', label='Training loss', color = \"gray\")\n    plt.title(f'{model_title} Training loss ')\n    plt.legend()\n    \n\n\ndef print_error(trainY, testY, train_predict, test_predict):\n    # Error of predictions\n    train_rmse = math.sqrt(mean_squared_error(\n        trainY[:, 0], train_predict[:, 0]))\n    test_rmse = math.sqrt(mean_squared_error(testY[:, 0], test_predict[:, 0]))\n    # Print RMSE\n    print('Train RMSE: %.3f RMSE' % (train_rmse))\n    print('Test RMSE: %.3f RMSE' % (test_rmse))\n    return train_rmse, test_rmse\n\n\n\n\nRecurrent Neural Network\nThe RNN models that are constructed and trained using TensorFlow and Keras. This model is trained for five hidden layers, one dense layer, and the activation function hyperbolic tangent. A dropout rate of 0.2 is used to reduce the risk of overfitting. The model are trained for 20 epochs and generated validation loss plots to compare the performance of the regularized and non-regularized models. The second RNN model was identical to the first, except for the addition of kernel regularization.\n\nRNNRNN with Regularization\n\n\n\n\nCode\n# Create a RNN\ndef create_RNN(hidden_units, dense_units, input_shape, activation,dropout_rate=0.2,kernel_regularizer=None):\n    model = Sequential()\n    # Create a simple neural network layer\n    model.add(SimpleRNN(hidden_units, input_shape=input_shape,\n              activation=activation[0]))\n    # Add a dense layer (only one, more layers would make it a deep neural net)\n    model.add(Dense(units=dense_units,\n              activation=activation[1],\n              kernel_regularizer=kernel_regularizer))\n    # Add layer dropout\n    model.add(Dropout(dropout_rate))\n    \n    # Compile the model and optimize on mean squared error\n    model.compile(loss='mean_squared_error', optimizer='adam')\n    return model\n  \n# Create a recurrent neural network\nmodel = create_RNN(hidden_units=5, dense_units=1, input_shape=(lag, Xtrain.shape[-1]),\n                   activation=['tanh', 'tanh'], dropout_rate = 0.2)\nhistory = model.fit(Xtrain, Ytrain, epochs=20, batch_size=1, verbose=0,validation_data=(Xval, Yval))\nplot_model(history, 'Recurrent Neural Network Model')\n\nyhat_d = [x[0] for x in model.predict(Xval, verbose=0)]\ny = [y[0] for y in Yval]\n\ntrain_predict = model.predict(Xtrain, verbose=0)\ntest_predict = model.predict(Xval, verbose=0)\n\n# Print error\ntrain_rmse, test_rmse = print_error(Ytrain, Yval, train_predict, test_predict)\nrmse_table = {\n    'model': ['Recurrent Neural Network'],\n    'training_rmse': [train_rmse],\n    'testing_rmse': [test_rmse]\n}\n\n\nTrain RMSE: 0.321 RMSE\nTest RMSE: 2.481 RMSE\n\n\n\n\n\n\n\n\n\nCode\n# Create a recurrent neural network with regularization\n\nmodel = create_RNN(hidden_units=5, dense_units=1, input_shape=(lag, Xtrain.shape[-1]),\n                   activation=['tanh', 'tanh'], kernel_regularizer=regularizers.L1L2(l1=1e-5, l2=1e-4), dropout_rate = 0.2)\nhistory = model.fit(Xtrain, Ytrain, epochs=20, batch_size=1, verbose=0, validation_data=(Xval, Yval))\nplot_model(history, 'Recurrent Neural Network Model (with L1L2 Regularization)')\n\nyhat_d_reg = [x[0] for x in model.predict(Xval, verbose=0)]\n\ntrain_predict = model.predict(Xtrain, verbose=0)\ntest_predict = model.predict(Xval, verbose=0)\n\n# Print error\ntrain_rmse, test_rmse = print_error(Ytrain, Yval, train_predict, test_predict)\nrmse_table['model'].append(\n    'Recurrent Neural Network (with L1L2 Regularization)')\nrmse_table['training_rmse'].append(train_rmse)\nrmse_table['testing_rmse'].append(test_rmse)\n\n\nTrain RMSE: 0.318 RMSE\nTest RMSE: 2.480 RMSE\n\n\n\n\n\n\n\n\nThe second plot illustrates the impact of regularization on the training loss. Compared to the original plot, there are significant differences. The regularization technique had a substantial effect on the testing error of the model. The model trained without regularization had a slightly higher training error of 0.317 RMSE and testing error of 2.482 RMSE when compared to with regularization.\n\n\nGRU Neural Network\nA GRU neural network is a powerful machine learning tool widely adopted in various applications. Its key feature is the recurrent gated unit, a type of memory cell that can store information over a data sequence. The GRU neural network is five hidden layers, one dense layer, and the hyperbolic tangent activation function. The model also includes kernel regularization to help address the issue of overfitting. The models are trained for 20 epochs and validation loss plots were generated to compare the performance of the regularized and non-regularized models. Overall, the GRU neural network has shown great promise in accurately predicting future movements in the sector.\n\nGRUGRU with Regularization\n\n\n\n\nCode\n# Create a GRU Neural Network\ndef create_GRU(hidden_units, dense_units, input_shape, activation,dropout_rate=0.2, kernel_regularizer=None):\n    model = Sequential()\n    # Create a simple GRU neural network layer\n    model.add(GRU(hidden_units, input_shape=input_shape,\n              activation=activation[0]))\n    # Add a dense layer (only one, more layers would make it a deep neural net)\n    model.add(Dense(units=dense_units,\n              activation=activation[1], kernel_regularizer=kernel_regularizer))\n    # Add layer dropout\n    model.add(Dropout(dropout_rate))\n    # Compile the model and optimize on mean squared error\n    model.compile(loss='mean_squared_error', optimizer='sgd')\n    return model\n\n# Training and evaluating a GRU-based model\nmodel = create_GRU(hidden_units=5, dense_units=1, input_shape=(lag, Xtrain.shape[-1]),\n                   activation=['tanh', 'relu'], dropout_rate = 0.2)\nhistory = model.fit(Xtrain, Ytrain, epochs=20, batch_size=1, verbose=0)\nplot_model(history, 'GRU Model')\n\nyhat_gru = [x[0] for x in model.predict(Xval, verbose=0)]\n\ntrain_predict = model.predict(Xtrain, verbose=0)\ntest_predict = model.predict(Xval, verbose=0)\n\n# Print error\ntrain_rmse, test_rmse = print_error(Ytrain, Yval, train_predict, test_predict)\nrmse_table['model'].append('GRU Neural Network')\nrmse_table['training_rmse'].append(train_rmse)\nrmse_table['testing_rmse'].append(test_rmse)\n\n\nTrain RMSE: 0.682 RMSE\nTest RMSE: 1.173 RMSE\n\n\n\n\n\n\n\n\n\nCode\n# Training and evaluating a GRU-based model with regularization\nmodel = create_GRU(hidden_units=5, dense_units=1, input_shape=(lag, Xtrain.shape[-1]),\n                   activation=['tanh', 'relu'],dropout_rate = 0.2,  kernel_regularizer=regularizers.L1L2(l1=1e-5, l2=1e-4))\nhistory = model.fit(Xtrain, Ytrain, epochs=20, batch_size=1, verbose=0)\nplot_model(history, 'GRU Model (with L1L2 Regularization)')\n\nyhat_gru_reg = [x[0] for x in model.predict(Xval, verbose=0)]\n\ntrain_predict = model.predict(Xtrain, verbose=0)\ntest_predict = model.predict(Xval, verbose=0)\n\n# Print error\ntrain_rmse, test_rmse = print_error(Ytrain, Yval, train_predict, test_predict)\nrmse_table['model'].append('GRU Neural Network (with L1L2 Regularization)')\nrmse_table['training_rmse'].append(train_rmse)\nrmse_table['testing_rmse'].append(test_rmse)\n\n\nTrain RMSE: 0.998 RMSE\nTest RMSE: 3.424 RMSE\n\n\n\n\n\n\n\n\nThe two plots show the training loss for the GRU model, one with regularization and one without. The plot with regularization is similar to without regularization plot. The model without regularization had a train RMSE of 0.998 and a test RMSE of 3.424, the values are the higher when compared to with regularization.\n\n\nLSTM Neural Network\nThe LSTM (Long Short-Term Memory) neural network is another powerful machine learning tool that has been widely adopted in various applications, particularly in tasks involving sequential data.The LSTM neural network architecture typically consists of multiple LSTM layers, followed by one or more dense layers and an activation function such as the sigmoid or hyperbolic tangent. Similar to the GRU neural network, the LSTM model may also include kernel regularization to address the issue of overfitting. In order to evaluate the performance of the LSTM model, the model can be trained on a dataset for a specified number of epochs, and validation loss plots can be generated to compare the performance of the regularized and non-regularized models.\n\nLSTMLSTM with Regularization\n\n\n\n\nCode\n# Create a LSTM Neural Network\ndef create_LSTM(hidden_units, dense_units, input_shape, activation,dropout_rate = 0.2, kernel_regularizer=None):\n    model = Sequential()\n    # Create a simple long short term memory neural network\n    model.add(LSTM(hidden_units,\n              activation=activation[0], input_shape=input_shape))\n    # Add a dense layer (only one, more layers would make it a deep neural net)\n    model.add(Dense(units=dense_units,\n              activation=activation[1], kernel_regularizer=kernel_regularizer))\n    # Add layer dropout\n    model.add(Dropout(dropout_rate))\n    # Compile the model and optimize on mean squared error\n    model.compile(optimizer=\"RMSprop\", loss='mae')\n    return model\n\n# Create an LSTM neural network\nmodel = create_LSTM(hidden_units=5, dense_units=1, input_shape=(lag, Xtrain.shape[-1]),\n                    activation=['tanh', 'linear'], dropout_rate = 0.2)\nhistory = model.fit(Xtrain, Ytrain, epochs=20, batch_size=1, verbose=0)\nplot_model(history, 'LSTM Model')\n\nyhat_lstm = [x[0] for x in model.predict(Xval, verbose=0)]\n\ntrain_predict = model.predict(Xtrain, verbose=0)\ntest_predict = model.predict(Xval, verbose=0)\n\n# Print error\ntrain_rmse, test_rmse = print_error(Ytrain, Yval, train_predict, test_predict)\nrmse_table['model'].append('LSTM Neural Network')\nrmse_table['training_rmse'].append(train_rmse)\nrmse_table['testing_rmse'].append(test_rmse)\n\n\nTrain RMSE: 0.203 RMSE\nTest RMSE: 1.331 RMSE\n\n\n\n\n\n\n\n\n\nCode\n# Create an LSTM neural network with regularization\nmodel = create_LSTM(hidden_units=5, dense_units=1, input_shape=(lag, Xtrain.shape[-1]),\n                    activation=['tanh', 'linear'], dropout_rate = 0.2, kernel_regularizer=regularizers.L1L2(l1=1e-5, l2=1e-4))\nhistory = model.fit(Xtrain, Ytrain, epochs=20, batch_size=1, verbose=0)\nplot_model(history, 'LSTM Model (with L1L2 Regularization)')\n\n\nyhat_lstm_reg = [x[0] for x in model.predict(Xval, verbose=0)]\n\ntrain_predict = model.predict(Xtrain, verbose=0)\ntest_predict = model.predict(Xval, verbose=0)\n\n# Print error\ntrain_rmse, test_rmse = print_error(Ytrain, Yval, train_predict, test_predict)\nrmse_table['model'].append('LSTM Neural Network (with L1L2 Regularization)')\nrmse_table['training_rmse'].append(train_rmse)\nrmse_table['testing_rmse'].append(test_rmse)\n\n\nTrain RMSE: 0.206 RMSE\nTest RMSE: 1.355 RMSE\n\n\n\n\n\n\n\n\nThe two plots show the training loss for the LSTM model, one with regularization and one without. The plot with regularization shows less fluctuation, indicating that overfitting has been reduced. The model without regularization had a train RMSE of 0.197 and a test RMSE of 1.321, while the model with regularization had a slightly lower for both the train and test RMSE values which are 0.193 and 1.152 respectively.\n\n\nForecast\n\n\nCode\n# Creating the frame to store both predictions\ndays = df['Date'].values[-len(y):]\nframe = pd.concat([\n    pd.DataFrame({'day': days, 'price': y, 'type': 'original'}),\n    pd.DataFrame({'day': days, 'price': yhat_d, 'type': 'rnn_forecast'}),\n    pd.DataFrame({'day': days, 'price': yhat_gru, 'type': 'gru_forecast'}),\n    pd.DataFrame({'day': days, 'price': yhat_lstm, 'type': 'lstm_forecast'})\n])\n# Creating the unscaled values column\nframe['price_absolute'] = [(x * train_std) + train_mean for x in frame['price']]\n# Pivoting\npivoted = frame.pivot_table(index='day', columns='type')\npivoted.columns = ['_'.join(x).strip() for x in pivoted.columns.values]\n\nplt.figure(figsize=(12, 10))\nplt.plot(pivoted.index, pivoted.price_absolute_original,\n         color='#8B1874', label='original')\nplt.plot(pivoted.index, pivoted.price_absolute_rnn_forecast,\n         color='#F79540', label='RNN Forecast', alpha=0.6)\nplt.plot(pivoted.index, pivoted.price_absolute_gru_forecast,\n         color='#B71375', label='GRU Forecast', alpha=0.6)\nplt.plot(pivoted.index, pivoted.price_absolute_lstm_forecast,\n         color='#FC4F00', label='LSTM Forecast', alpha=0.6)\nplt.title('Consumer Discretionary Sector Fund Forecasts')\nplt.legend()\nplt.show()\n\n\n\n\n\nWhen comparing the predictions of the deep learning models with the original time series plot, we can observe that the LSTM model is the closest to the actual values. The GRU model and RNN model are pretty far from the actual plot, which indicates that LSTM model is better when compared to others, but when we look at the LSTM model, the model is not accurate, i.e., it is not the same as the actual plot.\n\n\nModel Comparison\n\n\nCode\nrmse_df = pd.DataFrame(rmse_table)\nrmse_df\n\n\n\n\n\n\n  \n    \n      \n      model\n      training_rmse\n      testing_rmse\n    \n  \n  \n    \n      0\n      Recurrent Neural Network\n      0.321319\n      2.480567\n    \n    \n      1\n      Recurrent Neural Network (with L1L2 Regulariza...\n      0.318057\n      2.479767\n    \n    \n      2\n      GRU Neural Network\n      0.681864\n      1.173498\n    \n    \n      3\n      GRU Neural Network (with L1L2 Regularization)\n      0.997890\n      3.423626\n    \n    \n      4\n      LSTM Neural Network\n      0.203199\n      1.330599\n    \n    \n      5\n      LSTM Neural Network (with L1L2 Regularization)\n      0.206163\n      1.354628\n    \n  \n\n\n\n\nComparing the models based on their training and testing root mean square error (RMSE) values, the performance of the models varied across the different datasets. In general, the models with L1L2 regularization tended to perform slightly better on the testing set, indicating that the regularization helped to prevent overfitting. Overall, the LSTM models tended to perform the best on these datasets, particularly when regularization was not used.\n\n\nComparison of deep learning models with traditional single variable time series\nThe deep learning models (RNN, GRU, LSTM) perform better than the ARIMA model in both training and testing RMSE values for the Consumer Discretionary Sector Fund. Among the deep learning models, the LSTM model without regularization has the lowest testing RMSE value of 0.709320, when compared to ARIMA Model which is 2.118625, indicating its better performance in predicting the market prices of the Consumer Discretionary Sector Fund compared to the other deep learning models and ARIMA model."
  },
  {
    "objectID": "deep_learning_sector_xlv.html",
    "href": "deep_learning_sector_xlv.html",
    "title": "",
    "section": "",
    "text": "Deep Learning for Health Care Sector Fund\nThe Health Care Sector Fund is a market index that tracks the performance of the healthcare industry in the United States. This sector includes companies that are involved in pharmaceuticals, biotechnology, medical devices, healthcare services, and healthcare technology. As with other stock market, predicting the future movements of the Health Care Sector Fund can be challenging due to the complex and dynamic nature of the market. These models range from traditional time-series models to more advanced deep learning models. By leveraging the power of Deep Learning and applying techniques such as RNN, LSTM, GRU, and regularization, we can create a model that accurately predicts the future movements of the sector. Such a model can provide valuable insights for investors and traders looking to make informed decisions in the stock market, and help them stay ahead of the curve in the ever-changing landscape of the technology and biotech industries.\nIn order to apply the deep learning methods, the time series data is first split into training and testing sets, taking into account the time component. The training data constitutes 75% of the total data while the rest is the testing data. Furthermore, to ensure that the values are on the same scale, the data is scaled, which is crucial for deep learning models. The mean and standard deviation of the training data are computed and then utilized to normalize both the training and testing sets. This approach ensures that the models are trained on a standardized dataset, enabling them to make accurate predictions.\n\n\nCode\n# libraries\nimport numpy as np\nimport pandas as pd\nimport math\nfrom sklearn.metrics import mean_squared_error\nfrom matplotlib import pyplot as plt\nfrom keras.models import Sequential\nfrom keras.layers import Dropout\nfrom keras.layers import Dense, SimpleRNN, LSTM, GRU\nfrom keras import regularizers\nimport warnings\nwarnings.filterwarnings('ignore')\n\n# import data\ndf = pd.read_csv('./DATA/CLEANED DATA/XLV_raw_data.csv')\n# clean data and get final data for anlaysis\ndf = df[['Date', 'XLV.Adjusted']]\ndf['Date'] = pd.to_datetime(df['Date'])\ndf['XLV.Adjusted'] = pd.to_numeric(df['XLV.Adjusted'], errors='coerce')\ndf = df.dropna(subset=['XLV.Adjusted'])\ndf.sort_values('Date', inplace=True, ascending=True)\ndf = df.reset_index(drop=True)\ndef create_X_Y(ts: np.array, lag=1, n_ahead=1, target_index=0) -> tuple:\n    \"\"\"\n    A method to create X and Y matrix from a time series array for the training of \n    deep learning models \n    \"\"\"\n    # Extracting the number of features that are passed from the array\n    n_features = ts.shape[1]\n\n    # Creating placeholder lists\n    X, Y = [], []\n\n    if len(ts) - lag <= 0:\n        X.append(ts)\n    else:\n        for i in range(len(ts) - lag - n_ahead):\n            Y.append(ts[(i + lag):(i + lag + n_ahead), target_index])\n            X.append(ts[i:(i + lag)])\n\n    X, Y = np.array(X), np.array(Y)\n\n    # Reshaping the X array to an RNN input shape\n    X = np.reshape(X, (X.shape[0], lag, n_features))\n\n    return X, Y\n\nts = df[['XLV.Adjusted']].values\nnp.random.seed(123)\nnrows = ts.shape[0]\ntest_share = 0.25\n# Spliting into train and test sets\ntrain = ts[0:int(nrows * (1 - test_share))]\ntest = ts[int(nrows * (1 - test_share)):]\n\n# Scaling the data\ntrain_mean = train.mean()\ntrain_std = train.std()\ntrain = (train - train_mean) / train_std\ntest = (test - train_mean) / train_std\n\n# Creating the final scaled frame\nts_s = np.concatenate([train, test])\n\nlag = 12\nahead = 3\n\n# Creating the X and Y for training\nX, Y = create_X_Y(ts_s, lag=lag, n_ahead=ahead)\n\nXtrain = X[0:int(X.shape[0] * (1 - test_share))]\nYtrain = Y[0:int(X.shape[0] * (1 - test_share))]\n\nXval = X[int(X.shape[0] * (1 - test_share)):]\nYval = Y[int(X.shape[0] * (1 - test_share)):]\n\n\n\n\ndef plot_model(history, model_title):\n    loss = history.history['loss']\n    epochs = range(1, len(loss) + 1)\n    plt.figure()\n    plt.plot(epochs, loss, 'b', label='Training loss', color = \"gray\")\n    plt.title(f'{model_title} Training loss ')\n    plt.legend()\n    \n\n\ndef print_error(trainY, testY, train_predict, test_predict):\n    # Error of predictions\n    train_rmse = math.sqrt(mean_squared_error(\n        trainY[:, 0], train_predict[:, 0]))\n    test_rmse = math.sqrt(mean_squared_error(testY[:, 0], test_predict[:, 0]))\n    # Print RMSE\n    print('Train RMSE: %.3f RMSE' % (train_rmse))\n    print('Test RMSE: %.3f RMSE' % (test_rmse))\n    return train_rmse, test_rmse\n\n\n\n\nRecurrent Neural Network\nThe RNN models that are constructed and trained using TensorFlow and Keras. This model is trained for five hidden layers, one dense layer, and the activation function hyperbolic tangent. A dropout rate of 0.2 is used to reduce the risk of overfitting. The model are trained for 20 epochs and generated validation loss plots to compare the performance of the regularized and non-regularized models. The second RNN model was identical to the first, except for the addition of kernel regularization.\n\nRNNRNN with Regularization\n\n\n\n\nCode\n# Create a RNN\ndef create_RNN(hidden_units, dense_units, input_shape, activation,dropout_rate=0.2,kernel_regularizer=None):\n    model = Sequential()\n    # Create a simple neural network layer\n    model.add(SimpleRNN(hidden_units, input_shape=input_shape,\n              activation=activation[0]))\n    # Add a dense layer (only one, more layers would make it a deep neural net)\n    model.add(Dense(units=dense_units,\n              activation=activation[1],\n              kernel_regularizer=kernel_regularizer))\n    # Add layer dropout\n    model.add(Dropout(dropout_rate))\n    \n    # Compile the model and optimize on mean squared error\n    model.compile(loss='mean_squared_error', optimizer='adam')\n    return model\n  \n# Create a recurrent neural network\nmodel = create_RNN(hidden_units=5, dense_units=1, input_shape=(lag, Xtrain.shape[-1]),\n                   activation=['tanh', 'tanh'], dropout_rate = 0.2)\nhistory = model.fit(Xtrain, Ytrain, epochs=20, batch_size=1, verbose=0,validation_data=(Xval, Yval))\nplot_model(history, 'Recurrent Neural Network Model')\n\nyhat_d = [x[0] for x in model.predict(Xval, verbose=0)]\ny = [y[0] for y in Yval]\n\ntrain_predict = model.predict(Xtrain, verbose=0)\ntest_predict = model.predict(Xval, verbose=0)\n\n# Print error\ntrain_rmse, test_rmse = print_error(Ytrain, Yval, train_predict, test_predict)\nrmse_table = {\n    'model': ['Recurrent Neural Network'],\n    'training_rmse': [train_rmse],\n    'testing_rmse': [test_rmse]\n}\n\n\nTrain RMSE: 0.269 RMSE\nTest RMSE: 2.160 RMSE\n\n\n\n\n\n\n\n\n\nCode\n# Create a recurrent neural network with regularization\n\nmodel = create_RNN(hidden_units=5, dense_units=1, input_shape=(lag, Xtrain.shape[-1]),\n                   activation=['tanh', 'tanh'], kernel_regularizer=regularizers.L1L2(l1=1e-5, l2=1e-4), dropout_rate = 0.2)\nhistory = model.fit(Xtrain, Ytrain, epochs=20, batch_size=1, verbose=0, validation_data=(Xval, Yval))\nplot_model(history, 'Recurrent Neural Network Model (with L1L2 Regularization)')\n\nyhat_d_reg = [x[0] for x in model.predict(Xval, verbose=0)]\n\ntrain_predict = model.predict(Xtrain, verbose=0)\ntest_predict = model.predict(Xval, verbose=0)\n\n# Print error\ntrain_rmse, test_rmse = print_error(Ytrain, Yval, train_predict, test_predict)\nrmse_table['model'].append(\n    'Recurrent Neural Network (with L1L2 Regularization)')\nrmse_table['training_rmse'].append(train_rmse)\nrmse_table['testing_rmse'].append(test_rmse)\n\n\nTrain RMSE: 0.276 RMSE\nTest RMSE: 2.160 RMSE\n\n\n\n\n\n\n\n\nThe second plot illustrates the impact of regularization on the training loss. Compared to the original plot, there are significant differences. The regularization technique had a substantial effect on the testing error of the model. The model trained without regularization had a slightly lower training error of 0.273 RMSE but a considerably higher testing error of 2.160 RMSE. The RMSE values are the Slightly lower for with regularization which are 0.269 and 2.161, respectively.\n\n\nGRU Neural Network\nA GRU neural network is a powerful machine learning tool widely adopted in various applications. Its key feature is the recurrent gated unit, a type of memory cell that can store information over a data sequence. The GRU neural network is five hidden layers, one dense layer, and the hyperbolic tangent activation function. The model also includes kernel regularization to help address the issue of overfitting. The models are trained for 20 epochs and validation loss plots were generated to compare the performance of the regularized and non-regularized models. Overall, the GRU neural network has shown great promise in accurately predicting future movements in the sector.\n\nGRUGRU with Regularization\n\n\n\n\nCode\n# Create a GRU Neural Network\ndef create_GRU(hidden_units, dense_units, input_shape, activation,dropout_rate=0.2, kernel_regularizer=None):\n    model = Sequential()\n    # Create a simple GRU neural network layer\n    model.add(GRU(hidden_units, input_shape=input_shape,\n              activation=activation[0]))\n    # Add a dense layer (only one, more layers would make it a deep neural net)\n    model.add(Dense(units=dense_units,\n              activation=activation[1], kernel_regularizer=kernel_regularizer))\n    # Add layer dropout\n    model.add(Dropout(dropout_rate))\n    # Compile the model and optimize on mean squared error\n    model.compile(loss='mean_squared_error', optimizer='sgd')\n    return model\n\n# Training and evaluating a GRU-based model\nmodel = create_GRU(hidden_units=5, dense_units=1, input_shape=(lag, Xtrain.shape[-1]),\n                   activation=['tanh', 'relu'], dropout_rate = 0.2)\nhistory = model.fit(Xtrain, Ytrain, epochs=20, batch_size=1, verbose=0)\nplot_model(history, 'GRU Model')\n\nyhat_gru = [x[0] for x in model.predict(Xval, verbose=0)]\n\ntrain_predict = model.predict(Xtrain, verbose=0)\ntest_predict = model.predict(Xval, verbose=0)\n\n# Print error\ntrain_rmse, test_rmse = print_error(Ytrain, Yval, train_predict, test_predict)\nrmse_table['model'].append('GRU Neural Network')\nrmse_table['training_rmse'].append(train_rmse)\nrmse_table['testing_rmse'].append(test_rmse)\n\n\nTrain RMSE: 0.727 RMSE\nTest RMSE: 1.073 RMSE\n\n\n\n\n\n\n\n\n\nCode\n# Training and evaluating a GRU-based model with regularization\nmodel = create_GRU(hidden_units=5, dense_units=1, input_shape=(lag, Xtrain.shape[-1]),\n                   activation=['tanh', 'relu'],dropout_rate = 0.2,  kernel_regularizer=regularizers.L1L2(l1=1e-5, l2=1e-4))\nhistory = model.fit(Xtrain, Ytrain, epochs=20, batch_size=1, verbose=0)\nplot_model(history, 'GRU Model (with L1L2 Regularization)')\n\nyhat_gru_reg = [x[0] for x in model.predict(Xval, verbose=0)]\n\ntrain_predict = model.predict(Xtrain, verbose=0)\ntest_predict = model.predict(Xval, verbose=0)\n\n# Print error\ntrain_rmse, test_rmse = print_error(Ytrain, Yval, train_predict, test_predict)\nrmse_table['model'].append('GRU Neural Network (with L1L2 Regularization)')\nrmse_table['training_rmse'].append(train_rmse)\nrmse_table['testing_rmse'].append(test_rmse)\n\n\nTrain RMSE: 0.998 RMSE\nTest RMSE: 3.118 RMSE\n\n\n\n\n\n\n\n\nThe two plots show the training loss for the GRU model, one with regularization and one without. The plot with regularization is similar to without regularization plot. The model without regularization had a train RMSE of 0.998 and a test RMSE of 3.118, the values are the same for with regularization too.\n\n\nLSTM Neural Network\nThe LSTM (Long Short-Term Memory) neural network is another powerful machine learning tool that has been widely adopted in various applications, particularly in tasks involving sequential data.The LSTM neural network architecture typically consists of multiple LSTM layers, followed by one or more dense layers and an activation function such as the sigmoid or hyperbolic tangent. Similar to the GRU neural network, the LSTM model may also include kernel regularization to address the issue of overfitting. In order to evaluate the performance of the LSTM model, the model can be trained on a dataset for a specified number of epochs, and validation loss plots can be generated to compare the performance of the regularized and non-regularized models.\n\nLSTMLSTM with Regularization\n\n\n\n\nCode\n# Create a LSTM Neural Network\ndef create_LSTM(hidden_units, dense_units, input_shape, activation,dropout_rate = 0.2, kernel_regularizer=None):\n    model = Sequential()\n    # Create a simple long short term memory neural network\n    model.add(LSTM(hidden_units,\n              activation=activation[0], input_shape=input_shape))\n    # Add a dense layer (only one, more layers would make it a deep neural net)\n    model.add(Dense(units=dense_units,\n              activation=activation[1], kernel_regularizer=kernel_regularizer))\n    # Add layer dropout\n    model.add(Dropout(dropout_rate))\n    # Compile the model and optimize on mean squared error\n    model.compile(optimizer=\"RMSprop\", loss='mae')\n    return model\n\n# Create an LSTM neural network\nmodel = create_LSTM(hidden_units=5, dense_units=1, input_shape=(lag, Xtrain.shape[-1]),\n                    activation=['tanh', 'linear'], dropout_rate = 0.2)\nhistory = model.fit(Xtrain, Ytrain, epochs=20, batch_size=1, verbose=0)\nplot_model(history, 'LSTM Model')\n\nyhat_lstm = [x[0] for x in model.predict(Xval, verbose=0)]\n\ntrain_predict = model.predict(Xtrain, verbose=0)\ntest_predict = model.predict(Xval, verbose=0)\n\n# Print error\ntrain_rmse, test_rmse = print_error(Ytrain, Yval, train_predict, test_predict)\nrmse_table['model'].append('LSTM Neural Network')\nrmse_table['training_rmse'].append(train_rmse)\nrmse_table['testing_rmse'].append(test_rmse)\n\n\nTrain RMSE: 0.212 RMSE\nTest RMSE: 1.136 RMSE\n\n\n\n\n\n\n\n\n\nCode\n# Create an LSTM neural network with regularization\nmodel = create_LSTM(hidden_units=5, dense_units=1, input_shape=(lag, Xtrain.shape[-1]),\n                    activation=['tanh', 'linear'], dropout_rate = 0.2, kernel_regularizer=regularizers.L1L2(l1=1e-5, l2=1e-4))\nhistory = model.fit(Xtrain, Ytrain, epochs=20, batch_size=1, verbose=0)\nplot_model(history, 'LSTM Model (with L1L2 Regularization)')\n\n\nyhat_lstm_reg = [x[0] for x in model.predict(Xval, verbose=0)]\n\ntrain_predict = model.predict(Xtrain, verbose=0)\ntest_predict = model.predict(Xval, verbose=0)\n\n# Print error\ntrain_rmse, test_rmse = print_error(Ytrain, Yval, train_predict, test_predict)\nrmse_table['model'].append('LSTM Neural Network (with L1L2 Regularization)')\nrmse_table['training_rmse'].append(train_rmse)\nrmse_table['testing_rmse'].append(test_rmse)\n\n\nTrain RMSE: 0.198 RMSE\nTest RMSE: 1.113 RMSE\n\n\n\n\n\n\n\n\nThe two plots show the training loss for the LSTM model, one with regularization and one without. The plot with regularization shows less fluctuation, indicating that overfitting has been reduced. The model without regularization had a train RMSE of 0.206 and a test RMSE of 1.229, while the model with regularization had a slightly lower for both the train and test RMSE values which are 0.202 and 1.002 respectively.\n\n\nForecast\n\n\nCode\n# Creating the frame to store both predictions\ndays = df['Date'].values[-len(y):]\nframe = pd.concat([\n    pd.DataFrame({'day': days, 'price': y, 'type': 'original'}),\n    pd.DataFrame({'day': days, 'price': yhat_d, 'type': 'rnn_forecast'}),\n    pd.DataFrame({'day': days, 'price': yhat_gru, 'type': 'gru_forecast'}),\n    pd.DataFrame({'day': days, 'price': yhat_lstm, 'type': 'lstm_forecast'})\n])\n# Creating the unscaled values column\nframe['price_absolute'] = [(x * train_std) + train_mean for x in frame['price']]\n# Pivoting\npivoted = frame.pivot_table(index='day', columns='type')\npivoted.columns = ['_'.join(x).strip() for x in pivoted.columns.values]\n\nplt.figure(figsize=(12, 10))\nplt.plot(pivoted.index, pivoted.price_absolute_original,\n         color='#8B1874', label='original')\nplt.plot(pivoted.index, pivoted.price_absolute_rnn_forecast,\n         color='#F79540', label='RNN Forecast', alpha=0.6)\nplt.plot(pivoted.index, pivoted.price_absolute_gru_forecast,\n         color='#B71375', label='GRU Forecast', alpha=0.6)\nplt.plot(pivoted.index, pivoted.price_absolute_lstm_forecast,\n         color='#FC4F00', label='LSTM Forecast', alpha=0.6)\nplt.title('Health Care Sector Fund Forecasts')\nplt.legend()\nplt.show()\n\n\n\n\n\nWhen comparing the predictions of the deep learning models with the original time series plot, we can observe that the LSTM model is the closest to the actual values. The GRU model and RNN model are pretty far from the actual plot, which indicates that LSTM model is better when compared to others, but when we look at the LSTM model, the model is not accurate, i.e., it is not the same as the actual plot.\n\n\nModel Comparison\n\n\nCode\nrmse_df = pd.DataFrame(rmse_table)\nrmse_df\n\n\n\n\n\n\n  \n    \n      \n      model\n      training_rmse\n      testing_rmse\n    \n  \n  \n    \n      0\n      Recurrent Neural Network\n      0.268711\n      2.160354\n    \n    \n      1\n      Recurrent Neural Network (with L1L2 Regulariza...\n      0.276148\n      2.160356\n    \n    \n      2\n      GRU Neural Network\n      0.726934\n      1.072569\n    \n    \n      3\n      GRU Neural Network (with L1L2 Regularization)\n      0.998383\n      3.118396\n    \n    \n      4\n      LSTM Neural Network\n      0.212246\n      1.136212\n    \n    \n      5\n      LSTM Neural Network (with L1L2 Regularization)\n      0.198221\n      1.113328\n    \n  \n\n\n\n\nIn terms of performance on the testing dataset, the LSTM Neural Network with L1L2 regularization appears to be the best performing model, with a testing_rmse of 1.001920. This is followed by the regularized RNN with a testing_rmse of 2.160685, the non-regularized LSTM with a testing_rmse of 1.228607, the non-regularized RNN with a testing_rmse of 2.160200, and finally, the regularized GRU and non-regularized GRU, both with a testing_rmse of 3.118396.\nIt is important to note that regularization can help prevent overfitting, which is when a model fits the training data too closely and does not generalize well to new data. This is likely why the regularized models tend to perform better on the testing dataset than their non-regularized counterparts.\nOverall, the LSTM Neural Network with L1L2 regularization appears to be the best-performing model for this particular dataset, followed closely by the regularized RNN.\n\n\nComparison of deep learning models with traditional single variable time series\nThe deep learning models (RNN, GRU, LSTM) perform better than the ARIMA model in both training and testing RMSE values for the Health Care Sector Fund. Among the deep learning models, the LSTM model without regularization has the lowest testing RMSE value of 0.709320, when compared to ARIMA Model which is 0.8014312, indicating its better performance in predicting the market prices of the Health Care Sector Fund compared to the other deep learning models and ARIMA model."
  },
  {
    "objectID": "deep_learning_sector_xlu.html",
    "href": "deep_learning_sector_xlu.html",
    "title": "",
    "section": "",
    "text": "Deep Learning for Utilities Sector Fund\nThe Utilities Sector Fund is a group of stocks that includes companies that provide basic services such as electricity, gas, and water. These companies are known for their stability and consistent dividend payments, making them attractive to investors seeking a steady income stream. As with other stock market, predicting the future movements of the Utilities Sector Fund can be challenging due to the complex and dynamic nature of the market. These models range from traditional time-series models to more advanced deep learning models. By leveraging the power of Deep Learning and applying techniques such as RNN, LSTM, GRU, and regularization, we can create a model that accurately predicts the future movements of the sector. Such a model can provide valuable insights for investors and traders looking to make informed decisions in the stock market, and help them stay ahead of the curve in the ever-changing landscape of the technology and biotech industries.\nIn order to apply the deep learning methods, the time series data is first split into training and testing sets, taking into account the time component. The training data constitutes 75% of the total data while the rest is the testing data. Furthermore, to ensure that the values are on the same scale, the data is scaled, which is crucial for deep learning models. The mean and standard deviation of the training data are computed and then utilized to normalize both the training and testing sets. This approach ensures that the models are trained on a standardized dataset, enabling them to make accurate predictions.\n\n\nCode\n# libraries\nimport numpy as np\nimport pandas as pd\nimport math\nfrom sklearn.metrics import mean_squared_error\nfrom matplotlib import pyplot as plt\nfrom keras.models import Sequential\nfrom keras.layers import Dropout\nfrom keras.layers import Dense, SimpleRNN, LSTM, GRU\nfrom keras import regularizers\nimport warnings\nwarnings.filterwarnings('ignore')\n\n# import data\ndf = pd.read_csv('./DATA/CLEANED DATA/XLU_raw_data.csv')\n# clean data and get final data for anlaysis\ndf = df[['Date', 'XLU.Adjusted']]\ndf['Date'] = pd.to_datetime(df['Date'])\ndf['XLU.Adjusted'] = pd.to_numeric(df['XLU.Adjusted'], errors='coerce')\ndf = df.dropna(subset=['XLU.Adjusted'])\ndf.sort_values('Date', inplace=True, ascending=True)\ndf = df.reset_index(drop=True)\ndef create_X_Y(ts: np.array, lag=1, n_ahead=1, target_index=0) -> tuple:\n    \"\"\"\n    A method to create X and Y matrix from a time series array for the training of \n    deep learning models \n    \"\"\"\n    # Extracting the number of features that are passed from the array\n    n_features = ts.shape[1]\n\n    # Creating placeholder lists\n    X, Y = [], []\n\n    if len(ts) - lag <= 0:\n        X.append(ts)\n    else:\n        for i in range(len(ts) - lag - n_ahead):\n            Y.append(ts[(i + lag):(i + lag + n_ahead), target_index])\n            X.append(ts[i:(i + lag)])\n\n    X, Y = np.array(X), np.array(Y)\n\n    # Reshaping the X array to an RNN input shape\n    X = np.reshape(X, (X.shape[0], lag, n_features))\n\n    return X, Y\n\nts = df[['XLU.Adjusted']].values\n\nnrows = ts.shape[0]\ntest_share = 0.25\n# Spliting into train and test sets\ntrain = ts[0:int(nrows * (1 - test_share))]\ntest = ts[int(nrows * (1 - test_share)):]\n\n# Scaling the data\ntrain_mean = train.mean()\ntrain_std = train.std()\ntrain = (train - train_mean) / train_std\ntest = (test - train_mean) / train_std\n\n# Creating the final scaled frame\nts_s = np.concatenate([train, test])\n\nlag = 12\nahead = 3\n\n# Creating the X and Y for training\nX, Y = create_X_Y(ts_s, lag=lag, n_ahead=ahead)\n\nXtrain = X[0:int(X.shape[0] * (1 - test_share))]\nYtrain = Y[0:int(X.shape[0] * (1 - test_share))]\n\nXval = X[int(X.shape[0] * (1 - test_share)):]\nYval = Y[int(X.shape[0] * (1 - test_share)):]\n\n\n\n\ndef plot_model(history, model_title):\n    loss = history.history['loss']\n    epochs = range(1, len(loss) + 1)\n    plt.figure()\n    plt.plot(epochs, loss, 'b', label='Training loss', color = \"gray\")\n    plt.title(f'{model_title} Training loss ')\n    plt.legend()\n    \n\n\ndef print_error(trainY, testY, train_predict, test_predict):\n    # Error of predictions\n    train_rmse = math.sqrt(mean_squared_error(\n        trainY[:, 0], train_predict[:, 0]))\n    test_rmse = math.sqrt(mean_squared_error(testY[:, 0], test_predict[:, 0]))\n    # Print RMSE\n    print('Train RMSE: %.3f RMSE' % (train_rmse))\n    print('Test RMSE: %.3f RMSE' % (test_rmse))\n    return train_rmse, test_rmse\n\n\n\n\nRecurrent Neural Network\nThe RNN models that are constructed and trained using TensorFlow and Keras. This model is trained for five hidden layers, one dense layer, and the activation function hyperbolic tangent. A dropout rate of 0.2 is used to reduce the risk of overfitting. The model are trained for 20 epochs and generated validation loss plots to compare the performance of the regularized and non-regularized models. The second RNN model was identical to the first, except for the addition of kernel regularization.\n\nRNNRNN with Regularization\n\n\n\n\nCode\n# Create a RNN\ndef create_RNN(hidden_units, dense_units, input_shape, activation,dropout_rate=0.2,kernel_regularizer=None):\n    model = Sequential()\n    # Create a simple neural network layer\n    model.add(SimpleRNN(hidden_units, input_shape=input_shape,\n              activation=activation[0]))\n    # Add a dense layer (only one, more layers would make it a deep neural net)\n    model.add(Dense(units=dense_units,\n              activation=activation[1],\n              kernel_regularizer=kernel_regularizer))\n    # Add layer dropout\n    model.add(Dropout(dropout_rate))\n    \n    # Compile the model and optimize on mean squared error\n    model.compile(loss='mean_squared_error', optimizer='adam')\n    return model\n  \n# Create a recurrent neural network\nmodel = create_RNN(hidden_units=5, dense_units=1, input_shape=(lag, Xtrain.shape[-1]),\n                   activation=['tanh', 'tanh'], dropout_rate = 0.2)\nhistory = model.fit(Xtrain, Ytrain, epochs=20, batch_size=1, verbose=0,validation_data=(Xval, Yval))\nplot_model(history, 'Recurrent Neural Network Model')\n\nyhat_d = [x[0] for x in model.predict(Xval, verbose=0)]\ny = [y[0] for y in Yval]\n\ntrain_predict = model.predict(Xtrain, verbose=0)\ntest_predict = model.predict(Xval, verbose=0)\n\n# Print error\ntrain_rmse, test_rmse = print_error(Ytrain, Yval, train_predict, test_predict)\nrmse_table = {\n    'model': ['Recurrent Neural Network'],\n    'training_rmse': [train_rmse],\n    'testing_rmse': [test_rmse]\n}\n\n\nTrain RMSE: 0.332 RMSE\nTest RMSE: 1.883 RMSE\n\n\n\n\n\n\n\n\n\nCode\n# Create a recurrent neural network with regularization\n\nmodel = create_RNN(hidden_units=5, dense_units=1, input_shape=(lag, Xtrain.shape[-1]),\n                   activation=['tanh', 'tanh'], kernel_regularizer=regularizers.L1L2(l1=1e-5, l2=1e-4), dropout_rate = 0.2)\nhistory = model.fit(Xtrain, Ytrain, epochs=20, batch_size=1, verbose=0, validation_data=(Xval, Yval))\nplot_model(history, 'Recurrent Neural Network Model (with L1L2 Regularization)')\n\nyhat_d_reg = [x[0] for x in model.predict(Xval, verbose=0)]\n\ntrain_predict = model.predict(Xtrain, verbose=0)\ntest_predict = model.predict(Xval, verbose=0)\n\n# Print error\ntrain_rmse, test_rmse = print_error(Ytrain, Yval, train_predict, test_predict)\nrmse_table['model'].append(\n    'Recurrent Neural Network (with L1L2 Regularization)')\nrmse_table['training_rmse'].append(train_rmse)\nrmse_table['testing_rmse'].append(test_rmse)\n\n\nTrain RMSE: 0.327 RMSE\nTest RMSE: 2.351 RMSE\n\n\n\n\n\n\n\n\nThe second plot illustrates the impact of regularization on the training loss. Compared to the original plot, there are significant differences. The regularization technique had a substantial effect on the testing error of the model. The model trained without regularization had a slightly lower training error of 0.328 RMSE but a considerably higher testing error of 1.883 RMSE, the RMSE values for with regularization are 0.330 and 1.884 which are slightly higher than without regularization.\n\n\nGRU Neural Network\nA GRU neural network is a powerful machine learning tool widely adopted in various applications. Its key feature is the recurrent gated unit, a type of memory cell that can store information over a data sequence. The GRU neural network is five hidden layers, one dense layer, and the hyperbolic tangent activation function. The model also includes kernel regularization to help address the issue of overfitting. The models are trained for 20 epochs and validation loss plots were generated to compare the performance of the regularized and non-regularized models. Overall, the GRU neural network has shown great promise in accurately predicting future movements in the sector.\n\nGRUGRU with Regularization\n\n\n\n\nCode\n# Create a GRU Neural Network\ndef create_GRU(hidden_units, dense_units, input_shape, activation,dropout_rate=0.2, kernel_regularizer=None):\n    model = Sequential()\n    # Create a simple GRU neural network layer\n    model.add(GRU(hidden_units, input_shape=input_shape,\n              activation=activation[0]))\n    # Add a dense layer (only one, more layers would make it a deep neural net)\n    model.add(Dense(units=dense_units,\n              activation=activation[1], kernel_regularizer=kernel_regularizer))\n    # Add layer dropout\n    model.add(Dropout(dropout_rate))\n    # Compile the model and optimize on mean squared error\n    model.compile(loss='mean_squared_error', optimizer='sgd')\n    return model\n\n# Training and evaluating a GRU-based model\nmodel = create_GRU(hidden_units=5, dense_units=1, input_shape=(lag, Xtrain.shape[-1]),\n                   activation=['tanh', 'relu'], dropout_rate = 0.2)\nhistory = model.fit(Xtrain, Ytrain, epochs=20, batch_size=1, verbose=0)\nplot_model(history, 'GRU Model')\n\nyhat_gru = [x[0] for x in model.predict(Xval, verbose=0)]\n\ntrain_predict = model.predict(Xtrain, verbose=0)\ntest_predict = model.predict(Xval, verbose=0)\n\n# Print error\ntrain_rmse, test_rmse = print_error(Ytrain, Yval, train_predict, test_predict)\nrmse_table['model'].append('GRU Neural Network')\nrmse_table['training_rmse'].append(train_rmse)\nrmse_table['testing_rmse'].append(test_rmse)\n\n\nTrain RMSE: 0.678 RMSE\nTest RMSE: 0.748 RMSE\n\n\n\n\n\n\n\n\n\nCode\n# Training and evaluating a GRU-based model with regularization\nmodel = create_GRU(hidden_units=5, dense_units=1, input_shape=(lag, Xtrain.shape[-1]),\n                   activation=['tanh', 'relu'],dropout_rate = 0.2,  kernel_regularizer=regularizers.L1L2(l1=1e-5, l2=1e-4))\nhistory = model.fit(Xtrain, Ytrain, epochs=20, batch_size=1, verbose=0)\nplot_model(history, 'GRU Model (with L1L2 Regularization)')\n\nyhat_gru_reg = [x[0] for x in model.predict(Xval, verbose=0)]\n\ntrain_predict = model.predict(Xtrain, verbose=0)\ntest_predict = model.predict(Xval, verbose=0)\n\n# Print error\ntrain_rmse, test_rmse = print_error(Ytrain, Yval, train_predict, test_predict)\nrmse_table['model'].append('GRU Neural Network (with L1L2 Regularization)')\nrmse_table['training_rmse'].append(train_rmse)\nrmse_table['testing_rmse'].append(test_rmse)\n\n\nTrain RMSE: 0.680 RMSE\nTest RMSE: 0.822 RMSE\n\n\n\n\n\n\n\n\nThe two plots show the training loss for the GRU model, one with regularization and one without. The plot with regularization is similar to without regularization plot. The model without regularization had a train RMSE of 0.999 and a test RMSE of 2.847, the values for with regularization are lower than without regularization which are regularization 0.681 and 0.793 respectively.\n\n\nLSTM Neural Network\nThe LSTM (Long Short-Term Memory) neural network is another powerful machine learning tool that has been widely adopted in various applications, particularly in tasks involving sequential data.The LSTM neural network architecture typically consists of multiple LSTM layers, followed by one or more dense layers and an activation function such as the sigmoid or hyperbolic tangent. Similar to the GRU neural network, the LSTM model may also include kernel regularization to address the issue of overfitting. In order to evaluate the performance of the LSTM model, the model can be trained on a dataset for a specified number of epochs, and validation loss plots can be generated to compare the performance of the regularized and non-regularized models.\n\nLSTMLSTM with Regularization\n\n\n\n\nCode\n# Create a LSTM Neural Network\ndef create_LSTM(hidden_units, dense_units, input_shape, activation,dropout_rate = 0.2, kernel_regularizer=None):\n    model = Sequential()\n    # Create a simple long short term memory neural network\n    model.add(LSTM(hidden_units,\n              activation=activation[0], input_shape=input_shape))\n    # Add a dense layer (only one, more layers would make it a deep neural net)\n    model.add(Dense(units=dense_units,\n              activation=activation[1], kernel_regularizer=kernel_regularizer))\n    # Add layer dropout\n    model.add(Dropout(dropout_rate))\n    # Compile the model and optimize on mean squared error\n    model.compile(optimizer=\"RMSprop\", loss='mae')\n    return model\n\n# Create an LSTM neural network\nmodel = create_LSTM(hidden_units=5, dense_units=1, input_shape=(lag, Xtrain.shape[-1]),\n                    activation=['tanh', 'linear'], dropout_rate = 0.2)\nhistory = model.fit(Xtrain, Ytrain, epochs=20, batch_size=1, verbose=0)\nplot_model(history, 'LSTM Model')\n\nyhat_lstm = [x[0] for x in model.predict(Xval, verbose=0)]\n\ntrain_predict = model.predict(Xtrain, verbose=0)\ntest_predict = model.predict(Xval, verbose=0)\n\n# Print error\ntrain_rmse, test_rmse = print_error(Ytrain, Yval, train_predict, test_predict)\nrmse_table['model'].append('LSTM Neural Network')\nrmse_table['training_rmse'].append(train_rmse)\nrmse_table['testing_rmse'].append(test_rmse)\n\n\nTrain RMSE: 0.209 RMSE\nTest RMSE: 0.825 RMSE\n\n\n\n\n\n\n\n\n\nCode\n# Create an LSTM neural network with regularization\nmodel = create_LSTM(hidden_units=5, dense_units=1, input_shape=(lag, Xtrain.shape[-1]),\n                    activation=['tanh', 'linear'], dropout_rate = 0.2, kernel_regularizer=regularizers.L1L2(l1=1e-5, l2=1e-4))\nhistory = model.fit(Xtrain, Ytrain, epochs=20, batch_size=1, verbose=0)\nplot_model(history, 'LSTM Model (with L1L2 Regularization)')\n\n\nyhat_lstm_reg = [x[0] for x in model.predict(Xval, verbose=0)]\n\ntrain_predict = model.predict(Xtrain, verbose=0)\ntest_predict = model.predict(Xval, verbose=0)\n\n# Print error\ntrain_rmse, test_rmse = print_error(Ytrain, Yval, train_predict, test_predict)\nrmse_table['model'].append('LSTM Neural Network (with L1L2 Regularization)')\nrmse_table['training_rmse'].append(train_rmse)\nrmse_table['testing_rmse'].append(test_rmse)\n\n\nTrain RMSE: 0.197 RMSE\nTest RMSE: 0.841 RMSE\n\n\n\n\n\n\n\n\nThe two plots show the training loss for the LSTM model, one with regularization and one without. The plot with regularization shows less fluctuation, indicating that overfitting has been reduced. The model without regularization had a train RMSE of 0.180 and a test RMSE of 0.742, while the model with regularization had a slightly higher for both the train and test RMSE values which are 0.204 and 0.906 respectively.\n\n\nForecast\n\n\nCode\n# Creating the frame to store both predictions\ndays = df['Date'].values[-len(y):]\nframe = pd.concat([\n    pd.DataFrame({'day': days, 'price': y, 'type': 'original'}),\n    pd.DataFrame({'day': days, 'price': yhat_d, 'type': 'rnn_forecast'}),\n    pd.DataFrame({'day': days, 'price': yhat_gru, 'type': 'gru_forecast'}),\n    pd.DataFrame({'day': days, 'price': yhat_lstm, 'type': 'lstm_forecast'})\n])\n# Creating the unscaled values column\nframe['price_absolute'] = [(x * train_std) + train_mean for x in frame['price']]\n# Pivoting\npivoted = frame.pivot_table(index='day', columns='type')\npivoted.columns = ['_'.join(x).strip() for x in pivoted.columns.values]\n\nplt.figure(figsize=(12, 10))\nplt.plot(pivoted.index, pivoted.price_absolute_original,\n         color='#8B1874', label='original')\nplt.plot(pivoted.index, pivoted.price_absolute_rnn_forecast,\n         color='#F79540', label='RNN Forecast', alpha=0.6)\nplt.plot(pivoted.index, pivoted.price_absolute_gru_forecast,\n         color='#B71375', label='GRU Forecast', alpha=0.6)\nplt.plot(pivoted.index, pivoted.price_absolute_lstm_forecast,\n         color='#FC4F00', label='LSTM Forecast', alpha=0.6)\nplt.title('Utilities Sector Fund Forecasts')\nplt.legend()\nplt.show()\n\n\n\n\n\nWhen comparing the predictions of the deep learning models with the original time series plot, we can observe that the LSTM model is the closest to the actual values. The GRU model and RNN model are pretty far from the actual plot, which indicates that LSTM model is better when compared to others, but when we look at the LSTM model, the model is not accurate, i.e., it is not the same as the actual plot.\n\n\nModel Comparison\n\n\nCode\nrmse_df = pd.DataFrame(rmse_table)\nrmse_df\n\n\n\n\n\n\n  \n    \n      \n      model\n      training_rmse\n      testing_rmse\n    \n  \n  \n    \n      0\n      Recurrent Neural Network\n      0.332291\n      1.883178\n    \n    \n      1\n      Recurrent Neural Network (with L1L2 Regulariza...\n      0.327462\n      2.350588\n    \n    \n      2\n      GRU Neural Network\n      0.678414\n      0.748337\n    \n    \n      3\n      GRU Neural Network (with L1L2 Regularization)\n      0.679789\n      0.822179\n    \n    \n      4\n      LSTM Neural Network\n      0.209367\n      0.824859\n    \n    \n      5\n      LSTM Neural Network (with L1L2 Regularization)\n      0.197265\n      0.841334\n    \n  \n\n\n\n\nBased on the given RMSE values, the LSTM Neural Network (with or without L1L2 regularization) appears to be the best model for predicting the market price of the Utilities Sector Fund. The model with L1L2 regularization has a slightly higher training RMSE but a lower testing RMSE, indicating better generalization performance. Therefore, adding regularization to the model can help to reduce overfitting and improve its prediction accuracy.\n\n\nComparison of deep learning models with traditional single variable time series\nThe deep learning models (RNN, GRU, LSTM) perform better than the ARIMA model in both training and testing RMSE values for the Utilities Sector Fund. Among the deep learning models, the LSTM model without regularization has the lowest testing RMSE, when compared to ARIMA Model which is 0.8014312, indicating its better performance in predicting the market prices of the Utilities Sector Fund compared to the other deep learning models and ARIMA model."
  },
  {
    "objectID": "deep_learning_sector_xlre.html",
    "href": "deep_learning_sector_xlre.html",
    "title": "",
    "section": "",
    "text": "Deep Learning for Real Estate Sector Fund\nThe Real Estate sector comprises companies that provide non-essential goods and services, such as clothing, entertainment, and luxury items. These companies tend to be more sensitive to economic cycles and consumer sentiment than companies in other sectors. As with other stock market, predicting the future movements of the Real Estate Sector Fund can be challenging due to the complex and dynamic nature of the market. These models range from traditional time-series models to more advanced deep learning models. By leveraging the power of Deep Learning and applying techniques such as RNN, LSTM, GRU, and regularization, we can create a model that accurately predicts the future movements of the sector. Such a model can provide valuable insights for investors and traders looking to make informed decisions in the stock market, and help them stay ahead of the curve in the ever-changing landscape of the technology and biotech industries.\nIn order to apply the deep learning methods, the time series data is first split into training and testing sets, taking into account the time component. The training data constitutes 75% of the total data while the rest is the testing data. Furthermore, to ensure that the values are on the same scale, the data is scaled, which is crucial for deep learning models. The mean and standard deviation of the training data are computed and then utilized to normalize both the training and testing sets. This approach ensures that the models are trained on a standardized dataset, enabling them to make accurate predictions.\n\n\nCode\n# libraries\nimport numpy as np\nimport pandas as pd\nimport math\nfrom sklearn.metrics import mean_squared_error\nfrom matplotlib import pyplot as plt\nfrom keras.models import Sequential\nfrom keras.layers import Dropout\nfrom keras.layers import Dense, SimpleRNN, LSTM, GRU\nfrom keras import regularizers\nimport warnings\nwarnings.filterwarnings('ignore')\n\n# import data\ndf = pd.read_csv('./DATA/CLEANED DATA/XLRE_raw_data.csv')\n# clean data and get final data for anlaysis\ndf = df[['Date', 'XLRE.Adjusted']]\ndf['Date'] = pd.to_datetime(df['Date'])\ndf['XLRE.Adjusted'] = pd.to_numeric(df['XLRE.Adjusted'], errors='coerce')\ndf = df.dropna(subset=['XLRE.Adjusted'])\ndf.sort_values('Date', inplace=True, ascending=True)\ndf = df.reset_index(drop=True)\ndef create_X_Y(ts: np.array, lag=1, n_ahead=1, target_index=0) -> tuple:\n    \"\"\"\n    A method to create X and Y matrix from a time series array for the training of \n    deep learning models \n    \"\"\"\n    # Extracting the number of features that are passed from the array\n    n_features = ts.shape[1]\n\n    # Creating placeholder lists\n    X, Y = [], []\n\n    if len(ts) - lag <= 0:\n        X.append(ts)\n    else:\n        for i in range(len(ts) - lag - n_ahead):\n            Y.append(ts[(i + lag):(i + lag + n_ahead), target_index])\n            X.append(ts[i:(i + lag)])\n\n    X, Y = np.array(X), np.array(Y)\n\n    # Reshaping the X array to an RNN input shape\n    X = np.reshape(X, (X.shape[0], lag, n_features))\n\n    return X, Y\n\nts = df[['XLRE.Adjusted']].values\nnp.random.seed(123)\nnrows = ts.shape[0]\ntest_share = 0.25\n# Spliting into train and test sets\ntrain = ts[0:int(nrows * (1 - test_share))]\ntest = ts[int(nrows * (1 - test_share)):]\n\n# Scaling the data\ntrain_mean = train.mean()\ntrain_std = train.std()\ntrain = (train - train_mean) / train_std\ntest = (test - train_mean) / train_std\n\n# Creating the final scaled frame\nts_s = np.concatenate([train, test])\n\nlag = 12\nahead = 3\n\n# Creating the X and Y for training\nX, Y = create_X_Y(ts_s, lag=lag, n_ahead=ahead)\n\nXtrain = X[0:int(X.shape[0] * (1 - test_share))]\nYtrain = Y[0:int(X.shape[0] * (1 - test_share))]\n\nXval = X[int(X.shape[0] * (1 - test_share)):]\nYval = Y[int(X.shape[0] * (1 - test_share)):]\n\n\n\n\ndef plot_model(history, model_title):\n    loss = history.history['loss']\n    epochs = range(1, len(loss) + 1)\n    plt.figure()\n    plt.plot(epochs, loss, 'b', label='Training loss', color = \"gray\")\n    plt.title(f'{model_title} Training loss ')\n    plt.legend()\n    \n\n\ndef print_error(trainY, testY, train_predict, test_predict):\n    # Error of predictions\n    train_rmse = math.sqrt(mean_squared_error(\n        trainY[:, 0], train_predict[:, 0]))\n    test_rmse = math.sqrt(mean_squared_error(testY[:, 0], test_predict[:, 0]))\n    # Print RMSE\n    print('Train RMSE: %.3f RMSE' % (train_rmse))\n    print('Test RMSE: %.3f RMSE' % (test_rmse))\n    return train_rmse, test_rmse\n\n\n\n\nRecurrent Neural Network\nThe RNN models that are constructed and trained using TensorFlow and Keras. This model is trained for five hidden layers, one dense layer, and the activation function hyperbolic tangent. A dropout rate of 0.2 is used to reduce the risk of overfitting. The model are trained for 20 epochs and generated validation loss plots to compare the performance of the regularized and non-regularized models. The second RNN model was identical to the first, except for the addition of kernel regularization.\n\nRNNRNN with Regularization\n\n\n\n\nCode\n# Create a RNN\ndef create_RNN(hidden_units, dense_units, input_shape, activation,dropout_rate=0.2,kernel_regularizer=None):\n    model = Sequential()\n    # Create a simple neural network layer\n    model.add(SimpleRNN(hidden_units, input_shape=input_shape,\n              activation=activation[0]))\n    # Add a dense layer (only one, more layers would make it a deep neural net)\n    model.add(Dense(units=dense_units,\n              activation=activation[1],\n              kernel_regularizer=kernel_regularizer))\n    # Add layer dropout\n    model.add(Dropout(dropout_rate))\n    \n    # Compile the model and optimize on mean squared error\n    model.compile(loss='mean_squared_error', optimizer='adam')\n    return model\n  \n# Create a recurrent neural network\nmodel = create_RNN(hidden_units=5, dense_units=1, input_shape=(lag, Xtrain.shape[-1]),\n                   activation=['tanh', 'tanh'], dropout_rate = 0.2)\nhistory = model.fit(Xtrain, Ytrain, epochs=20, batch_size=1, verbose=0,validation_data=(Xval, Yval))\nplot_model(history, 'Recurrent Neural Network Model')\n\nyhat_d = [x[0] for x in model.predict(Xval, verbose=0)]\ny = [y[0] for y in Yval]\n\ntrain_predict = model.predict(Xtrain, verbose=0)\ntest_predict = model.predict(Xval, verbose=0)\n\n# Print error\ntrain_rmse, test_rmse = print_error(Ytrain, Yval, train_predict, test_predict)\nrmse_table = {\n    'model': ['Recurrent Neural Network'],\n    'training_rmse': [train_rmse],\n    'testing_rmse': [test_rmse]\n}\n\n\nTrain RMSE: 0.360 RMSE\nTest RMSE: 2.395 RMSE\n\n\n\n\n\n\n\n\n\nCode\n# Create a recurrent neural network with regularization\n\nmodel = create_RNN(hidden_units=5, dense_units=1, input_shape=(lag, Xtrain.shape[-1]),\n                   activation=['tanh', 'tanh'], kernel_regularizer=regularizers.L1L2(l1=1e-5, l2=1e-4), dropout_rate = 0.2)\nhistory = model.fit(Xtrain, Ytrain, epochs=20, batch_size=1, verbose=0, validation_data=(Xval, Yval))\nplot_model(history, 'Recurrent Neural Network Model (with L1L2 Regularization)')\n\nyhat_d_reg = [x[0] for x in model.predict(Xval, verbose=0)]\n\ntrain_predict = model.predict(Xtrain, verbose=0)\ntest_predict = model.predict(Xval, verbose=0)\n\n# Print error\ntrain_rmse, test_rmse = print_error(Ytrain, Yval, train_predict, test_predict)\nrmse_table['model'].append(\n    'Recurrent Neural Network (with L1L2 Regularization)')\nrmse_table['training_rmse'].append(train_rmse)\nrmse_table['testing_rmse'].append(test_rmse)\n\n\nTrain RMSE: 0.362 RMSE\nTest RMSE: 2.397 RMSE\n\n\n\n\n\n\n\n\nThe second plot illustrates the impact of regularization on the training loss. Compared to the original plot, there are significant differences. The regularization technique had a substantial effect on the testing error of the model. The model trained without regularization had a slightly lower for both training error of 0.347 RMSE and testing error of 2.394 RMSE when compared to with regularization.\n\n\nGRU Neural Network\nA GRU neural network is a powerful machine learning tool widely adopted in various applications. Its key feature is the recurrent gated unit, a type of memory cell that can store information over a data sequence. The GRU neural network is five hidden layers, one dense layer, and the hyperbolic tangent activation function. The model also includes kernel regularization to help address the issue of overfitting. The models are trained for 20 epochs and validation loss plots were generated to compare the performance of the regularized and non-regularized models. Overall, the GRU neural network has shown great promise in accurately predicting future movements in the sector.\n\nGRUGRU with Regularization\n\n\n\n\nCode\n# Create a GRU Neural Network\ndef create_GRU(hidden_units, dense_units, input_shape, activation,dropout_rate=0.2, kernel_regularizer=None):\n    model = Sequential()\n    # Create a simple GRU neural network layer\n    model.add(GRU(hidden_units, input_shape=input_shape,\n              activation=activation[0]))\n    # Add a dense layer (only one, more layers would make it a deep neural net)\n    model.add(Dense(units=dense_units,\n              activation=activation[1], kernel_regularizer=kernel_regularizer))\n    # Add layer dropout\n    model.add(Dropout(dropout_rate))\n    # Compile the model and optimize on mean squared error\n    model.compile(loss='mean_squared_error', optimizer='sgd')\n    return model\n\n# Training and evaluating a GRU-based model\nmodel = create_GRU(hidden_units=5, dense_units=1, input_shape=(lag, Xtrain.shape[-1]),\n                   activation=['tanh', 'relu'], dropout_rate = 0.2)\nhistory = model.fit(Xtrain, Ytrain, epochs=20, batch_size=1, verbose=0)\nplot_model(history, 'GRU Model')\n\nyhat_gru = [x[0] for x in model.predict(Xval, verbose=0)]\n\ntrain_predict = model.predict(Xtrain, verbose=0)\ntest_predict = model.predict(Xval, verbose=0)\n\n# Print error\ntrain_rmse, test_rmse = print_error(Ytrain, Yval, train_predict, test_predict)\nrmse_table['model'].append('GRU Neural Network')\nrmse_table['training_rmse'].append(train_rmse)\nrmse_table['testing_rmse'].append(test_rmse)\n\n\nTrain RMSE: 0.997 RMSE\nTest RMSE: 3.340 RMSE\n\n\n\n\n\n\n\n\n\nCode\n# Training and evaluating a GRU-based model with regularization\nmodel = create_GRU(hidden_units=5, dense_units=1, input_shape=(lag, Xtrain.shape[-1]),\n                   activation=['tanh', 'relu'],dropout_rate = 0.2,  kernel_regularizer=regularizers.L1L2(l1=1e-5, l2=1e-4))\nhistory = model.fit(Xtrain, Ytrain, epochs=20, batch_size=1, verbose=0)\nplot_model(history, 'GRU Model (with L1L2 Regularization)')\n\nyhat_gru_reg = [x[0] for x in model.predict(Xval, verbose=0)]\n\ntrain_predict = model.predict(Xtrain, verbose=0)\ntest_predict = model.predict(Xval, verbose=0)\n\n# Print error\ntrain_rmse, test_rmse = print_error(Ytrain, Yval, train_predict, test_predict)\nrmse_table['model'].append('GRU Neural Network (with L1L2 Regularization)')\nrmse_table['training_rmse'].append(train_rmse)\nrmse_table['testing_rmse'].append(test_rmse)\n\n\nTrain RMSE: 0.649 RMSE\nTest RMSE: 1.073 RMSE\n\n\n\n\n\n\n\n\nThe two plots show the training loss for the GRU model, one with regularization and one without. The plot with regularization is similar to without regularization plot. The model without regularization had a train RMSE of 0.997 and a test RMSE of 3.340, the values are the same as the values of with regularization.\n\n\nLSTM Neural Network\nThe LSTM (Long Short-Term Memory) neural network is another powerful machine learning tool that has been widely adopted in various applications, particularly in tasks involving sequential data.The LSTM neural network architecture typically consists of multiple LSTM layers, followed by one or more dense layers and an activation function such as the sigmoid or hyperbolic tangent. Similar to the GRU neural network, the LSTM model may also include kernel regularization to address the issue of overfitting. In order to evaluate the performance of the LSTM model, the model can be trained on a dataset for a specified number of epochs, and validation loss plots can be generated to compare the performance of the regularized and non-regularized models.\n\nLSTMLSTM with Regularization\n\n\n\n\nCode\n# Create a LSTM Neural Network\ndef create_LSTM(hidden_units, dense_units, input_shape, activation,dropout_rate = 0.2, kernel_regularizer=None):\n    model = Sequential()\n    # Create a simple long short term memory neural network\n    model.add(LSTM(hidden_units,\n              activation=activation[0], input_shape=input_shape))\n    # Add a dense layer (only one, more layers would make it a deep neural net)\n    model.add(Dense(units=dense_units,\n              activation=activation[1], kernel_regularizer=kernel_regularizer))\n    # Add layer dropout\n    model.add(Dropout(dropout_rate))\n    # Compile the model and optimize on mean squared error\n    model.compile(optimizer=\"RMSprop\", loss='mae')\n    return model\n\n# Create an LSTM neural network\nmodel = create_LSTM(hidden_units=5, dense_units=1, input_shape=(lag, Xtrain.shape[-1]),\n                    activation=['tanh', 'linear'], dropout_rate = 0.2)\nhistory = model.fit(Xtrain, Ytrain, epochs=20, batch_size=1, verbose=0)\nplot_model(history, 'LSTM Model')\n\nyhat_lstm = [x[0] for x in model.predict(Xval, verbose=0)]\n\ntrain_predict = model.predict(Xtrain, verbose=0)\ntest_predict = model.predict(Xval, verbose=0)\n\n# Print error\ntrain_rmse, test_rmse = print_error(Ytrain, Yval, train_predict, test_predict)\nrmse_table['model'].append('LSTM Neural Network')\nrmse_table['training_rmse'].append(train_rmse)\nrmse_table['testing_rmse'].append(test_rmse)\n\n\nTrain RMSE: 0.215 RMSE\nTest RMSE: 0.940 RMSE\n\n\n\n\n\n\n\n\n\nCode\n# Create an LSTM neural network with regularization\nmodel = create_LSTM(hidden_units=5, dense_units=1, input_shape=(lag, Xtrain.shape[-1]),\n                    activation=['tanh', 'linear'], dropout_rate = 0.2, kernel_regularizer=regularizers.L1L2(l1=1e-5, l2=1e-4))\nhistory = model.fit(Xtrain, Ytrain, epochs=20, batch_size=1, verbose=0)\nplot_model(history, 'LSTM Model (with L1L2 Regularization)')\n\n\nyhat_lstm_reg = [x[0] for x in model.predict(Xval, verbose=0)]\n\ntrain_predict = model.predict(Xtrain, verbose=0)\ntest_predict = model.predict(Xval, verbose=0)\n\n# Print error\ntrain_rmse, test_rmse = print_error(Ytrain, Yval, train_predict, test_predict)\nrmse_table['model'].append('LSTM Neural Network (with L1L2 Regularization)')\nrmse_table['training_rmse'].append(train_rmse)\nrmse_table['testing_rmse'].append(test_rmse)\n\n\nTrain RMSE: 0.242 RMSE\nTest RMSE: 1.072 RMSE\n\n\n\n\n\n\n\n\nThe two plots show the training loss for the LSTM model, one with regularization and one without. The plot with regularization shows less fluctuation, indicating that overfitting has been reduced. The model without regularization had a train RMSE of 0.205 and a test RMSE of 0.994, while the model with regularization had a slightly lower for both the train and test RMSE values which are 0.204 and 0.925 respectively.\n\n\nForecast\n\n\nCode\n# Creating the frame to store both predictions\ndays = df['Date'].values[-len(y):]\nframe = pd.concat([\n    pd.DataFrame({'day': days, 'price': y, 'type': 'original'}),\n    pd.DataFrame({'day': days, 'price': yhat_d, 'type': 'rnn_forecast'}),\n    pd.DataFrame({'day': days, 'price': yhat_gru, 'type': 'gru_forecast'}),\n    pd.DataFrame({'day': days, 'price': yhat_lstm, 'type': 'lstm_forecast'})\n])\n# Creating the unscaled values column\nframe['price_absolute'] = [(x * train_std) + train_mean for x in frame['price']]\n# Pivoting\npivoted = frame.pivot_table(index='day', columns='type')\npivoted.columns = ['_'.join(x).strip() for x in pivoted.columns.values]\n\nplt.figure(figsize=(12, 10))\nplt.plot(pivoted.index, pivoted.price_absolute_original,\n         color='#8B1874', label='original')\nplt.plot(pivoted.index, pivoted.price_absolute_rnn_forecast,\n         color='#F79540', label='RNN Forecast', alpha=0.6)\nplt.plot(pivoted.index, pivoted.price_absolute_gru_forecast,\n         color='#B71375', label='GRU Forecast', alpha=0.6)\nplt.plot(pivoted.index, pivoted.price_absolute_lstm_forecast,\n         color='#FC4F00', label='LSTM Forecast', alpha=0.6)\nplt.title('Real Estate Sector Fund Forecasts')\nplt.legend()\nplt.show()\n\n\n\n\n\nWhen comparing the predictions of the deep learning models with the original time series plot, we can observe that the LSTM model is the closest to the actual values. The GRU model and RNN model are pretty far from the actual plot, which indicates that LSTM model is better when compared to others, but when we look at the LSTM model, the model is not accurate, i.e., it is not the same as the actual plot.\n\n\nModel Comparison\n\n\nCode\nrmse_df = pd.DataFrame(rmse_table)\nrmse_df\n\n\n\n\n\n\n  \n    \n      \n      model\n      training_rmse\n      testing_rmse\n    \n  \n  \n    \n      0\n      Recurrent Neural Network\n      0.360467\n      2.394924\n    \n    \n      1\n      Recurrent Neural Network (with L1L2 Regulariza...\n      0.361795\n      2.397207\n    \n    \n      2\n      GRU Neural Network\n      0.996890\n      3.339895\n    \n    \n      3\n      GRU Neural Network (with L1L2 Regularization)\n      0.649452\n      1.072770\n    \n    \n      4\n      LSTM Neural Network\n      0.215322\n      0.940292\n    \n    \n      5\n      LSTM Neural Network (with L1L2 Regularization)\n      0.241935\n      1.072040\n    \n  \n\n\n\n\nBased on the evaluation metrics, the LSTM Neural Network with L1L2 Regularization performs the best on most datasets, followed by the GRU Neural Network and Recurrent Neural Network with L1L2 Regularization. However, the large difference between training and testing RMSE values suggests overfitting to the training data.\n\n\nComparison of deep learning models with traditional single variable time series\nThe deep learning models (RNN, GRU, LSTM) perform better than the ARIMA model in both training and testing RMSE values for the Real Estate Sector Fund. Among the deep learning models, the LSTM model without regularization has the lowest testing RMSE value of 0.4838699, when compared to ARIMA Model which is 2.118625, indicating its better performance in predicting the market prices of the Real Estate Sector Fund compared to the other deep learning models and ARIMA model."
  },
  {
    "objectID": "deep_learning_sector_xlp.html",
    "href": "deep_learning_sector_xlp.html",
    "title": "",
    "section": "",
    "text": "Deep Learning for Consumer Staples Sector Fund\nThe Consumer Staples Sector Fund is an exchange-traded fund that tracks the performance of the consumer staples sector of the US stock market. This sector includes companies that produce essential household goods such as food, beverages, and personal care products. As with other stock market, predicting the future movements of the Consumer Staples Sector Fund can be challenging due to the complex and dynamic nature of the market. These models range from traditional time-series models to more advanced deep learning models. By leveraging the power of Deep Learning and applying techniques such as RNN, LSTM, GRU, and regularization, we can create a model that accurately predicts the future movements of the sector. Such a model can provide valuable insights for investors and traders looking to make informed decisions in the stock market, and help them stay ahead of the curve in the ever-changing landscape of the technology and biotech industries.\nIn order to apply the deep learning methods, the time series data is first split into training and testing sets, taking into account the time component. The training data constitutes 75% of the total data while the rest is the testing data. Furthermore, to ensure that the values are on the same scale, the data is scaled, which is crucial for deep learning models. The mean and standard deviation of the training data are computed and then utilized to normalize both the training and testing sets. This approach ensures that the models are trained on a standardized dataset, enabling them to make accurate predictions.\n\n\nCode\n# libraries\nimport numpy as np\nimport pandas as pd\nimport math\nfrom sklearn.metrics import mean_squared_error\nfrom matplotlib import pyplot as plt\nfrom keras.models import Sequential\nfrom keras.layers import Dropout\nfrom keras.layers import Dense, SimpleRNN, LSTM, GRU\nfrom keras import regularizers\nimport warnings\nwarnings.filterwarnings('ignore')\n\n# import data\ndf = pd.read_csv('./DATA/CLEANED DATA/xlp_raw_data.csv')\n# clean data and get final data for anlaysis\ndf = df[['Date', 'XLP.Adjusted']]\ndf['Date'] = pd.to_datetime(df['Date'])\ndf['XLP.Adjusted'] = pd.to_numeric(df['XLP.Adjusted'], errors='coerce')\ndf = df.dropna(subset=['XLP.Adjusted'])\ndf.sort_values('Date', inplace=True, ascending=True)\ndf = df.reset_index(drop=True)\ndef create_X_Y(ts: np.array, lag=1, n_ahead=1, target_index=0) -> tuple:\n    \"\"\"\n    A method to create X and Y matrix from a time series array for the training of \n    deep learning models \n    \"\"\"\n    # Extracting the number of features that are passed from the array\n    n_features = ts.shape[1]\n\n    # Creating placeholder lists\n    X, Y = [], []\n\n    if len(ts) - lag <= 0:\n        X.append(ts)\n    else:\n        for i in range(len(ts) - lag - n_ahead):\n            Y.append(ts[(i + lag):(i + lag + n_ahead), target_index])\n            X.append(ts[i:(i + lag)])\n\n    X, Y = np.array(X), np.array(Y)\n\n    # Reshaping the X array to an RNN input shape\n    X = np.reshape(X, (X.shape[0], lag, n_features))\n\n    return X, Y\n\nts = df[['XLP.Adjusted']].values\nnp.random.seed(123)\nnrows = ts.shape[0]\ntest_share = 0.25\n# Spliting into train and test sets\ntrain = ts[0:int(nrows * (1 - test_share))]\ntest = ts[int(nrows * (1 - test_share)):]\n\n# Scaling the data\ntrain_mean = train.mean()\ntrain_std = train.std()\ntrain = (train - train_mean) / train_std\ntest = (test - train_mean) / train_std\n\n# Creating the final scaled frame\nts_s = np.concatenate([train, test])\n\nlag = 12\nahead = 3\n\n# Creating the X and Y for training\nX, Y = create_X_Y(ts_s, lag=lag, n_ahead=ahead)\n\nXtrain = X[0:int(X.shape[0] * (1 - test_share))]\nYtrain = Y[0:int(X.shape[0] * (1 - test_share))]\n\nXval = X[int(X.shape[0] * (1 - test_share)):]\nYval = Y[int(X.shape[0] * (1 - test_share)):]\n\n\n\n\ndef plot_model(history, model_title):\n    loss = history.history['loss']\n    epochs = range(1, len(loss) + 1)\n    plt.figure()\n    plt.plot(epochs, loss, 'b', label='Training loss', color = \"gray\")\n    plt.title(f'{model_title} Training loss ')\n    plt.legend()\n    \n\n\ndef print_error(trainY, testY, train_predict, test_predict):\n    # Error of predictions\n    train_rmse = math.sqrt(mean_squared_error(\n        trainY[:, 0], train_predict[:, 0]))\n    test_rmse = math.sqrt(mean_squared_error(testY[:, 0], test_predict[:, 0]))\n    # Print RMSE\n    print('Train RMSE: %.3f RMSE' % (train_rmse))\n    print('Test RMSE: %.3f RMSE' % (test_rmse))\n    return train_rmse, test_rmse\n\n\n\n\nRecurrent Neural Network\nThe RNN models that are constructed and trained using TensorFlow and Keras. This model is trained for five hidden layers, one dense layer, and the activation function hyperbolic tangent. A dropout rate of 0.2 is used to reduce the risk of overfitting. The model are trained for 20 epochs and generated validation loss plots to compare the performance of the regularized and non-regularized models. The second RNN model was identical to the first, except for the addition of kernel regularization.\n\nRNNRNN with Regularization\n\n\n\n\nCode\n# Create a RNN\ndef create_RNN(hidden_units, dense_units, input_shape, activation,dropout_rate=0.2,kernel_regularizer=None):\n    model = Sequential()\n    # Create a simple neural network layer\n    model.add(SimpleRNN(hidden_units, input_shape=input_shape,\n              activation=activation[0]))\n    # Add a dense layer (only one, more layers would make it a deep neural net)\n    model.add(Dense(units=dense_units,\n              activation=activation[1],\n              kernel_regularizer=kernel_regularizer))\n    # Add layer dropout\n    model.add(Dropout(dropout_rate))\n    \n    # Compile the model and optimize on mean squared error\n    model.compile(loss='mean_squared_error', optimizer='adam')\n    return model\n  \n# Create a recurrent neural network\nmodel = create_RNN(hidden_units=5, dense_units=1, input_shape=(lag, Xtrain.shape[-1]),\n                   activation=['tanh', 'tanh'], dropout_rate = 0.2)\nhistory = model.fit(Xtrain, Ytrain, epochs=20, batch_size=1, verbose=0,validation_data=(Xval, Yval))\nplot_model(history, 'Recurrent Neural Network Model')\n\nyhat_d = [x[0] for x in model.predict(Xval, verbose=0)]\ny = [y[0] for y in Yval]\n\ntrain_predict = model.predict(Xtrain, verbose=0)\ntest_predict = model.predict(Xval, verbose=0)\n\n# Print error\ntrain_rmse, test_rmse = print_error(Ytrain, Yval, train_predict, test_predict)\nrmse_table = {\n    'model': ['Recurrent Neural Network'],\n    'training_rmse': [train_rmse],\n    'testing_rmse': [test_rmse]\n}\n\n\nTrain RMSE: 0.289 RMSE\nTest RMSE: 1.854 RMSE\n\n\n\n\n\n\n\n\n\nCode\n# Create a recurrent neural network with regularization\n\nmodel = create_RNN(hidden_units=5, dense_units=1, input_shape=(lag, Xtrain.shape[-1]),\n                   activation=['tanh', 'tanh'], kernel_regularizer=regularizers.L1L2(l1=1e-5, l2=1e-4), dropout_rate = 0.2)\nhistory = model.fit(Xtrain, Ytrain, epochs=20, batch_size=1, verbose=0, validation_data=(Xval, Yval))\nplot_model(history, 'Recurrent Neural Network Model (with L1L2 Regularization)')\n\nyhat_d_reg = [x[0] for x in model.predict(Xval, verbose=0)]\n\ntrain_predict = model.predict(Xtrain, verbose=0)\ntest_predict = model.predict(Xval, verbose=0)\n\n# Print error\ntrain_rmse, test_rmse = print_error(Ytrain, Yval, train_predict, test_predict)\nrmse_table['model'].append(\n    'Recurrent Neural Network (with L1L2 Regularization)')\nrmse_table['training_rmse'].append(train_rmse)\nrmse_table['testing_rmse'].append(test_rmse)\n\n\nTrain RMSE: 0.291 RMSE\nTest RMSE: 1.853 RMSE\n\n\n\n\n\n\n\n\nThe second plot illustrates the impact of regularization on the training loss. Compared to the original plot, there are significant differences. The regularization technique had a substantial effect on the testing error of the model. The model trained without regularization had a slightly lower training error of 0.291 RMSE but a considerably higher testing error of 1.853 RMSE. In contrast, the model trained with regularization had a slightly lower for both training and testing RMSE which are 0.286 and 1.852 respectively.\n\n\nGRU Neural Network\nA GRU neural network is a powerful machine learning tool widely adopted in various applications. Its key feature is the recurrent gated unit, a type of memory cell that can store information over a data sequence. The GRU neural network is five hidden layers, one dense layer, and the hyperbolic tangent activation function. The model also includes kernel regularization to help address the issue of overfitting. The models are trained for 20 epochs and validation loss plots were generated to compare the performance of the regularized and non-regularized models. Overall, the GRU neural network has shown great promise in accurately predicting future movements in the sector.\n\nGRUGRU with Regularization\n\n\n\n\nCode\n# Create a GRU Neural Network\ndef create_GRU(hidden_units, dense_units, input_shape, activation,dropout_rate=0.2, kernel_regularizer=None):\n    model = Sequential()\n    # Create a simple GRU neural network layer\n    model.add(GRU(hidden_units, input_shape=input_shape,\n              activation=activation[0]))\n    # Add a dense layer (only one, more layers would make it a deep neural net)\n    model.add(Dense(units=dense_units,\n              activation=activation[1], kernel_regularizer=kernel_regularizer))\n    # Add layer dropout\n    model.add(Dropout(dropout_rate))\n    # Compile the model and optimize on mean squared error\n    model.compile(loss='mean_squared_error', optimizer='sgd')\n    return model\n\n# Training and evaluating a GRU-based model\nmodel = create_GRU(hidden_units=5, dense_units=1, input_shape=(lag, Xtrain.shape[-1]),\n                   activation=['tanh', 'relu'], dropout_rate = 0.2)\nhistory = model.fit(Xtrain, Ytrain, epochs=20, batch_size=1, verbose=0)\nplot_model(history, 'GRU Model')\n\nyhat_gru = [x[0] for x in model.predict(Xval, verbose=0)]\n\ntrain_predict = model.predict(Xtrain, verbose=0)\ntest_predict = model.predict(Xval, verbose=0)\n\n# Print error\ntrain_rmse, test_rmse = print_error(Ytrain, Yval, train_predict, test_predict)\nrmse_table['model'].append('GRU Neural Network')\nrmse_table['training_rmse'].append(train_rmse)\nrmse_table['testing_rmse'].append(test_rmse)\n\n\nTrain RMSE: 0.997 RMSE\nTest RMSE: 2.810 RMSE\n\n\n\n\n\n\n\n\n\nCode\n# Training and evaluating a GRU-based model with regularization\nmodel = create_GRU(hidden_units=5, dense_units=1, input_shape=(lag, Xtrain.shape[-1]),\n                   activation=['tanh', 'relu'],dropout_rate = 0.2,  kernel_regularizer=regularizers.L1L2(l1=1e-5, l2=1e-4))\nhistory = model.fit(Xtrain, Ytrain, epochs=20, batch_size=1, verbose=0)\nplot_model(history, 'GRU Model (with L1L2 Regularization)')\n\nyhat_gru_reg = [x[0] for x in model.predict(Xval, verbose=0)]\n\ntrain_predict = model.predict(Xtrain, verbose=0)\ntest_predict = model.predict(Xval, verbose=0)\n\n# Print error\ntrain_rmse, test_rmse = print_error(Ytrain, Yval, train_predict, test_predict)\nrmse_table['model'].append('GRU Neural Network (with L1L2 Regularization)')\nrmse_table['training_rmse'].append(train_rmse)\nrmse_table['testing_rmse'].append(test_rmse)\n\n\nTrain RMSE: 0.997 RMSE\nTest RMSE: 2.774 RMSE\n\n\n\n\n\n\n\n\nThe two plots show the training loss for the GRU model, one with regularization and one without. The plot with regularization is similar to without regularization plot. The model without regularization had a train RMSE of 0.997 and a test RMSE of 2.810, the values for with regularization are slightly lower then without regularization which are regularization 0.733 and 0.836 respectively.\n\n\nLSTM Neural Network\nThe LSTM (Long Short-Term Memory) neural network is another powerful machine learning tool that has been widely adopted in various applications, particularly in tasks involving sequential data.The LSTM neural network architecture typically consists of multiple LSTM layers, followed by one or more dense layers and an activation function such as the sigmoid or hyperbolic tangent. Similar to the GRU neural network, the LSTM model may also include kernel regularization to address the issue of overfitting. In order to evaluate the performance of the LSTM model, the model can be trained on a dataset for a specified number of epochs, and validation loss plots can be generated to compare the performance of the regularized and non-regularized models.\n\nLSTMLSTM with Regularization\n\n\n\n\nCode\n# Create a LSTM Neural Network\ndef create_LSTM(hidden_units, dense_units, input_shape, activation,dropout_rate = 0.2, kernel_regularizer=None):\n    model = Sequential()\n    # Create a simple long short term memory neural network\n    model.add(LSTM(hidden_units,\n              activation=activation[0], input_shape=input_shape))\n    # Add a dense layer (only one, more layers would make it a deep neural net)\n    model.add(Dense(units=dense_units,\n              activation=activation[1], kernel_regularizer=kernel_regularizer))\n    # Add layer dropout\n    model.add(Dropout(dropout_rate))\n    # Compile the model and optimize on mean squared error\n    model.compile(optimizer=\"RMSprop\", loss='mae')\n    return model\n\n# Create an LSTM neural network\nmodel = create_LSTM(hidden_units=5, dense_units=1, input_shape=(lag, Xtrain.shape[-1]),\n                    activation=['tanh', 'linear'], dropout_rate = 0.2)\nhistory = model.fit(Xtrain, Ytrain, epochs=20, batch_size=1, verbose=0)\nplot_model(history, 'LSTM Model')\n\nyhat_lstm = [x[0] for x in model.predict(Xval, verbose=0)]\n\ntrain_predict = model.predict(Xtrain, verbose=0)\ntest_predict = model.predict(Xval, verbose=0)\n\n# Print error\ntrain_rmse, test_rmse = print_error(Ytrain, Yval, train_predict, test_predict)\nrmse_table['model'].append('LSTM Neural Network')\nrmse_table['training_rmse'].append(train_rmse)\nrmse_table['testing_rmse'].append(test_rmse)\n\n\nTrain RMSE: 0.200 RMSE\nTest RMSE: 0.848 RMSE\n\n\n\n\n\n\n\n\n\nCode\n# Create an LSTM neural network with regularization\nmodel = create_LSTM(hidden_units=5, dense_units=1, input_shape=(lag, Xtrain.shape[-1]),\n                    activation=['tanh', 'linear'], dropout_rate = 0.2, kernel_regularizer=regularizers.L1L2(l1=1e-5, l2=1e-4))\nhistory = model.fit(Xtrain, Ytrain, epochs=20, batch_size=1, verbose=0)\nplot_model(history, 'LSTM Model (with L1L2 Regularization)')\n\n\nyhat_lstm_reg = [x[0] for x in model.predict(Xval, verbose=0)]\n\ntrain_predict = model.predict(Xtrain, verbose=0)\ntest_predict = model.predict(Xval, verbose=0)\n\n# Print error\ntrain_rmse, test_rmse = print_error(Ytrain, Yval, train_predict, test_predict)\nrmse_table['model'].append('LSTM Neural Network (with L1L2 Regularization)')\nrmse_table['training_rmse'].append(train_rmse)\nrmse_table['testing_rmse'].append(test_rmse)\n\n\nTrain RMSE: 0.195 RMSE\nTest RMSE: 0.726 RMSE\n\n\n\n\n\n\n\n\nThe two plots show the training loss for the LSTM model, one with regularization and one without. The plot with regularization shows less fluctuation, indicating that overfitting has been reduced. The model without regularization had a train RMSE of 0.216 and a test RMSE of 0.858, while the model with regularization has slightly lower RMSE for train but the test is higher than without regularization.\n\n\nForecast\n\n\nCode\n# Creating the frame to store both predictions\ndays = df['Date'].values[-len(y):]\nframe = pd.concat([\n    pd.DataFrame({'day': days, 'price': y, 'type': 'original'}),\n    pd.DataFrame({'day': days, 'price': yhat_d, 'type': 'rnn_forecast'}),\n    pd.DataFrame({'day': days, 'price': yhat_gru, 'type': 'gru_forecast'}),\n    pd.DataFrame({'day': days, 'price': yhat_lstm, 'type': 'lstm_forecast'})\n])\n# Creating the unscaled values column\nframe['price_absolute'] = [(x * train_std) + train_mean for x in frame['price']]\n# Pivoting\npivoted = frame.pivot_table(index='day', columns='type')\npivoted.columns = ['_'.join(x).strip() for x in pivoted.columns.values]\n\nplt.figure(figsize=(12, 10))\nplt.plot(pivoted.index, pivoted.price_absolute_original,\n         color='#8B1874', label='original')\nplt.plot(pivoted.index, pivoted.price_absolute_rnn_forecast,\n         color='#F79540', label='RNN Forecast', alpha=0.6)\nplt.plot(pivoted.index, pivoted.price_absolute_gru_forecast,\n         color='#B71375', label='GRU Forecast', alpha=0.6)\nplt.plot(pivoted.index, pivoted.price_absolute_lstm_forecast,\n         color='#FC4F00', label='LSTM Forecast', alpha=0.6)\nplt.title('Consumer Staples Sector Fund Forecasts')\nplt.legend()\nplt.show()\n\n\n\n\n\nWhen comparing the predictions of the deep learning models with the original time series plot, we can observe that the LSTM model is the closest to the actual values. The GRU model and RNN model are pretty far from the actual plot, which indicates that LSTM model is better when compared to others, but when we look at the LSTM model, the model is not accurate, i.e., it is not the same as the actual plot.\n\n\nModel Comparison\n\n\nCode\nrmse_df = pd.DataFrame(rmse_table)\nrmse_df\n\n\n\n\n\n\n  \n    \n      \n      model\n      training_rmse\n      testing_rmse\n    \n  \n  \n    \n      0\n      Recurrent Neural Network\n      0.289334\n      1.853989\n    \n    \n      1\n      Recurrent Neural Network (with L1L2 Regulariza...\n      0.290898\n      1.853230\n    \n    \n      2\n      GRU Neural Network\n      0.996594\n      2.810339\n    \n    \n      3\n      GRU Neural Network (with L1L2 Regularization)\n      0.996594\n      2.774066\n    \n    \n      4\n      LSTM Neural Network\n      0.200470\n      0.848175\n    \n    \n      5\n      LSTM Neural Network (with L1L2 Regularization)\n      0.195379\n      0.726004\n    \n  \n\n\n\n\nIn terms of performance on the testing dataset, the LSTM Neural Network without regularization appears to be the best performing model, with a testing_rmse of 0.858168. This is followed by the regularized RNN with a testing_rmse of 1.852495, the regularized GRU with a testing_rmse of 0.835960, the non-regularized LSTM with a testing_rmse of 2.49750, the non-regularized RNN with a testing_rmse of 1.853256, and finally, the regularized LSTM with a testing_rmse of 2.49750.\nIt is important to note that the results can be influenced by the specific dataset and the problem at hand. However, in general, LSTMs are known to be effective in tasks that require capturing long-term dependencies in sequential data. GRUs, on the other hand, are a simpler variant of LSTMs and can be used as an alternative if the dataset is relatively small or if computational resources are limited. Regularization can help to prevent overfitting, which is a common problem in deep learning models, and can improve generalization performance.\nOverall, the results suggest that for this particular dataset, the LSTM Neural Network without regularization appears to be the best-performing model, followed by the regularized RNN and the regularized GRU.\n\n\nComparison of deep learning models with traditional single variable time series\nThe deep learning models have significantly lower RMSE values than the ARIMA model, indicating better performance in predicting the future movements of the Consumer Staples Sector Fund. Among the deep learning models, the LSTM neural network with regularization performed the best with a testing RMSE. In contrast, the ARIMA model had a RMSE of 0.8600413, which is higher than all of the deep learning models. Therefore, we can conclude that the deep learning models, especially the LSTM neural network with regularization, outperformed the traditional ARIMA model in predicting the future movements of the Consumer Staples Sector Fund."
  },
  {
    "objectID": "deep_learning_sector_xlk.html",
    "href": "deep_learning_sector_xlk.html",
    "title": "",
    "section": "",
    "text": "Deep Learning for Technology Sector Fund\nThe technology sector fund comprises companies that develop and provide technology-related products and services, such as software, hardware, and electronics. These companies are often at the forefront of innovation and can offer investors significant growth potential. As with other stock market, predicting the future movements of the Technology Sector Fund can be challenging due to the complex and dynamic nature of the market. These models range from traditional time-series models to more advanced deep learning models. By leveraging the power of Deep Learning and applying techniques such as RNN, LSTM, GRU, and regularization, we can create a model that accurately predicts the future movements of the sector. Such a model can provide valuable insights for investors and traders looking to make informed decisions in the stock market, and help them stay ahead of the curve in the ever-changing landscape of the technology and biotech industries.\nIn order to apply the deep learning methods, the time series data is first split into training and testing sets, taking into account the time component. The training data constitutes 75% of the total data while the rest is the testing data. Furthermore, to ensure that the values are on the same scale, the data is scaled, which is crucial for deep learning models. The mean and standard deviation of the training data are computed and then utilized to normalize both the training and testing sets. This approach ensures that the models are trained on a standardized dataset, enabling them to make accurate predictions.\n\n\nCode\n# libraries\nimport numpy as np\nimport pandas as pd\nimport math\nfrom sklearn.metrics import mean_squared_error\nfrom matplotlib import pyplot as plt\nfrom keras.models import Sequential\nfrom keras.layers import Dropout\nfrom keras.layers import Dense, SimpleRNN, LSTM, GRU\nfrom keras import regularizers\nimport warnings\nwarnings.filterwarnings('ignore')\n\n# import data\ndf = pd.read_csv('./DATA/CLEANED DATA/XLK_raw_data.csv')\n# clean data and get final data for anlaysis\ndf = df[['Date', 'XLK.Adjusted']]\ndf['Date'] = pd.to_datetime(df['Date'])\ndf['XLK.Adjusted'] = pd.to_numeric(df['XLK.Adjusted'], errors='coerce')\ndf = df.dropna(subset=['XLK.Adjusted'])\ndf.sort_values('Date', inplace=True, ascending=True)\ndf = df.reset_index(drop=True)\ndef create_X_Y(ts: np.array, lag=1, n_ahead=1, target_index=0) -> tuple:\n    \"\"\"\n    A method to create X and Y matrix from a time series array for the training of \n    deep learning models \n    \"\"\"\n    # Extracting the number of features that are passed from the array\n    n_features = ts.shape[1]\n\n    # Creating placeholder lists\n    X, Y = [], []\n\n    if len(ts) - lag <= 0:\n        X.append(ts)\n    else:\n        for i in range(len(ts) - lag - n_ahead):\n            Y.append(ts[(i + lag):(i + lag + n_ahead), target_index])\n            X.append(ts[i:(i + lag)])\n\n    X, Y = np.array(X), np.array(Y)\n\n    # Reshaping the X array to an RNN input shape\n    X = np.reshape(X, (X.shape[0], lag, n_features))\n\n    return X, Y\n\nts = df[['XLK.Adjusted']].values\nnp.random.seed(123)\nnrows = ts.shape[0]\ntest_share = 0.25\n# Spliting into train and test sets\ntrain = ts[0:int(nrows * (1 - test_share))]\ntest = ts[int(nrows * (1 - test_share)):]\n\n# Scaling the data\ntrain_mean = train.mean()\ntrain_std = train.std()\ntrain = (train - train_mean) / train_std\ntest = (test - train_mean) / train_std\n\n# Creating the final scaled frame\nts_s = np.concatenate([train, test])\n\nlag = 12\nahead = 3\n\n# Creating the X and Y for training\nX, Y = create_X_Y(ts_s, lag=lag, n_ahead=ahead)\n\nXtrain = X[0:int(X.shape[0] * (1 - test_share))]\nYtrain = Y[0:int(X.shape[0] * (1 - test_share))]\n\nXval = X[int(X.shape[0] * (1 - test_share)):]\nYval = Y[int(X.shape[0] * (1 - test_share)):]\n\n\n\n\ndef plot_model(history, model_title):\n    loss = history.history['loss']\n    epochs = range(1, len(loss) + 1)\n    plt.figure()\n    plt.plot(epochs, loss, 'b', label='Training loss', color = \"gray\")\n    plt.title(f'{model_title} Training loss ')\n    plt.legend()\n    \n\n\ndef print_error(trainY, testY, train_predict, test_predict):\n    # Error of predictions\n    train_rmse = math.sqrt(mean_squared_error(\n        trainY[:, 0], train_predict[:, 0]))\n    test_rmse = math.sqrt(mean_squared_error(testY[:, 0], test_predict[:, 0]))\n    # Print RMSE\n    print('Train RMSE: %.3f RMSE' % (train_rmse))\n    print('Test RMSE: %.3f RMSE' % (test_rmse))\n    return train_rmse, test_rmse\n\n\n\n\nRecurrent Neural Network\nThe RNN models that are constructed and trained using TensorFlow and Keras. This model is trained for five hidden layers, one dense layer, and the activation function hyperbolic tangent. A dropout rate of 0.2 is used to reduce the risk of overfitting. The model are trained for 20 epochs and generated validation loss plots to compare the performance of the regularized and non-regularized models. The second RNN model was identical to the first, except for the addition of kernel regularization.\n\nRNNRNN with Regularization\n\n\n\n\nCode\n# Create a RNN\ndef create_RNN(hidden_units, dense_units, input_shape, activation,dropout_rate=0.2,kernel_regularizer=None):\n    model = Sequential()\n    # Create a simple neural network layer\n    model.add(SimpleRNN(hidden_units, input_shape=input_shape,\n              activation=activation[0]))\n    # Add a dense layer (only one, more layers would make it a deep neural net)\n    model.add(Dense(units=dense_units,\n              activation=activation[1],\n              kernel_regularizer=kernel_regularizer))\n    # Add layer dropout\n    model.add(Dropout(dropout_rate))\n    \n    # Compile the model and optimize on mean squared error\n    model.compile(loss='mean_squared_error', optimizer='adam')\n    return model\n  \n# Create a recurrent neural network\nmodel = create_RNN(hidden_units=5, dense_units=1, input_shape=(lag, Xtrain.shape[-1]),\n                   activation=['tanh', 'tanh'], dropout_rate = 0.2)\nhistory = model.fit(Xtrain, Ytrain, epochs=20, batch_size=1, verbose=0,validation_data=(Xval, Yval))\nplot_model(history, 'Recurrent Neural Network Model')\n\nyhat_d = [x[0] for x in model.predict(Xval, verbose=0)]\ny = [y[0] for y in Yval]\n\ntrain_predict = model.predict(Xtrain, verbose=0)\ntest_predict = model.predict(Xval, verbose=0)\n\n# Print error\ntrain_rmse, test_rmse = print_error(Ytrain, Yval, train_predict, test_predict)\nrmse_table = {\n    'model': ['Recurrent Neural Network'],\n    'training_rmse': [train_rmse],\n    'testing_rmse': [test_rmse]\n}\n\n\nTrain RMSE: 0.366 RMSE\nTest RMSE: 4.227 RMSE\n\n\n\n\n\n\n\n\n\nCode\n# Create a recurrent neural network with regularization\n\nmodel = create_RNN(hidden_units=5, dense_units=1, input_shape=(lag, Xtrain.shape[-1]),\n                   activation=['tanh', 'tanh'], kernel_regularizer=regularizers.L1L2(l1=1e-5, l2=1e-4), dropout_rate = 0.2)\nhistory = model.fit(Xtrain, Ytrain, epochs=20, batch_size=1, verbose=0, validation_data=(Xval, Yval))\nplot_model(history, 'Recurrent Neural Network Model (with L1L2 Regularization)')\n\nyhat_d_reg = [x[0] for x in model.predict(Xval, verbose=0)]\n\ntrain_predict = model.predict(Xtrain, verbose=0)\ntest_predict = model.predict(Xval, verbose=0)\n\n# Print error\ntrain_rmse, test_rmse = print_error(Ytrain, Yval, train_predict, test_predict)\nrmse_table['model'].append(\n    'Recurrent Neural Network (with L1L2 Regularization)')\nrmse_table['training_rmse'].append(train_rmse)\nrmse_table['testing_rmse'].append(test_rmse)\n\n\nTrain RMSE: 0.362 RMSE\nTest RMSE: 4.227 RMSE\n\n\n\n\n\n\n\n\nThe second plot illustrates the impact of regularization on the training loss. Compared to the original plot, there are significant differences. The regularization technique had a substantial effect on the testing error of the model. The model trained without regularization had a slightly higher training error of 0.366 RMSE but the testing error is the same 4.227 RMSE when compared to with regularization.\n\n\nGRU Neural Network\nA GRU neural network is a powerful machine learning tool widely adopted in various applications. Its key feature is the recurrent gated unit, a type of memory cell that can store information over a data sequence. The GRU neural network is five hidden layers, one dense layer, and the hyperbolic tangent activation function. The model also includes kernel regularization to help address the issue of overfitting. The models are trained for 20 epochs and validation loss plots were generated to compare the performance of the regularized and non-regularized models. Overall, the GRU neural network has shown great promise in accurately predicting future movements in the sector.\n\nGRUGRU with Regularization\n\n\n\n\nCode\n# Create a GRU Neural Network\ndef create_GRU(hidden_units, dense_units, input_shape, activation,dropout_rate=0.2, kernel_regularizer=None):\n    model = Sequential()\n    # Create a simple GRU neural network layer\n    model.add(GRU(hidden_units, input_shape=input_shape,\n              activation=activation[0]))\n    # Add a dense layer (only one, more layers would make it a deep neural net)\n    model.add(Dense(units=dense_units,\n              activation=activation[1], kernel_regularizer=kernel_regularizer))\n    # Add layer dropout\n    model.add(Dropout(dropout_rate))\n    # Compile the model and optimize on mean squared error\n    model.compile(loss='mean_squared_error', optimizer='sgd')\n    return model\n\n# Training and evaluating a GRU-based model\nmodel = create_GRU(hidden_units=5, dense_units=1, input_shape=(lag, Xtrain.shape[-1]),\n                   activation=['tanh', 'relu'], dropout_rate = 0.2)\nhistory = model.fit(Xtrain, Ytrain, epochs=20, batch_size=1, verbose=0)\nplot_model(history, 'GRU Model')\n\nyhat_gru = [x[0] for x in model.predict(Xval, verbose=0)]\n\ntrain_predict = model.predict(Xtrain, verbose=0)\ntest_predict = model.predict(Xval, verbose=0)\n\n# Print error\ntrain_rmse, test_rmse = print_error(Ytrain, Yval, train_predict, test_predict)\nrmse_table['model'].append('GRU Neural Network')\nrmse_table['training_rmse'].append(train_rmse)\nrmse_table['testing_rmse'].append(test_rmse)\n\n\nTrain RMSE: 1.000 RMSE\nTest RMSE: 5.184 RMSE\n\n\n\n\n\n\n\n\n\nCode\n# Training and evaluating a GRU-based model with regularization\nmodel = create_GRU(hidden_units=5, dense_units=1, input_shape=(lag, Xtrain.shape[-1]),\n                   activation=['tanh', 'relu'],dropout_rate = 0.2,  kernel_regularizer=regularizers.L1L2(l1=1e-5, l2=1e-4))\nhistory = model.fit(Xtrain, Ytrain, epochs=20, batch_size=1, verbose=0)\nplot_model(history, 'GRU Model (with L1L2 Regularization)')\n\nyhat_gru_reg = [x[0] for x in model.predict(Xval, verbose=0)]\n\ntrain_predict = model.predict(Xtrain, verbose=0)\ntest_predict = model.predict(Xval, verbose=0)\n\n# Print error\ntrain_rmse, test_rmse = print_error(Ytrain, Yval, train_predict, test_predict)\nrmse_table['model'].append('GRU Neural Network (with L1L2 Regularization)')\nrmse_table['training_rmse'].append(train_rmse)\nrmse_table['testing_rmse'].append(test_rmse)\n\n\nTrain RMSE: 1.000 RMSE\nTest RMSE: 5.184 RMSE\n\n\n\n\n\n\n\n\nThe two plots show the training loss for the GRU model, one with regularization and one without. The plot with regularization is similar to without regularization plot. The model without regularization had a train RMSE of 1.000 and a test RMSE of 5.184, the values are the higher when compared to with regularization.\n\n\nLSTM Neural Network\nThe LSTM (Long Short-Term Memory) neural network is another powerful machine learning tool that has been widely adopted in various applications, particularly in tasks involving sequential data.The LSTM neural network architecture typically consists of multiple LSTM layers, followed by one or more dense layers and an activation function such as the sigmoid or hyperbolic tangent. Similar to the GRU neural network, the LSTM model may also include kernel regularization to address the issue of overfitting. In order to evaluate the performance of the LSTM model, the model can be trained on a dataset for a specified number of epochs, and validation loss plots can be generated to compare the performance of the regularized and non-regularized models.\n\nLSTMLSTM with Regularization\n\n\n\n\nCode\n# Create a LSTM Neural Network\ndef create_LSTM(hidden_units, dense_units, input_shape, activation,dropout_rate = 0.2, kernel_regularizer=None):\n    model = Sequential()\n    # Create a simple long short term memory neural network\n    model.add(LSTM(hidden_units,\n              activation=activation[0], input_shape=input_shape))\n    # Add a dense layer (only one, more layers would make it a deep neural net)\n    model.add(Dense(units=dense_units,\n              activation=activation[1], kernel_regularizer=kernel_regularizer))\n    # Add layer dropout\n    model.add(Dropout(dropout_rate))\n    # Compile the model and optimize on mean squared error\n    model.compile(optimizer=\"RMSprop\", loss='mae')\n    return model\n\n# Create an LSTM neural network\nmodel = create_LSTM(hidden_units=5, dense_units=1, input_shape=(lag, Xtrain.shape[-1]),\n                    activation=['tanh', 'linear'], dropout_rate = 0.2)\nhistory = model.fit(Xtrain, Ytrain, epochs=20, batch_size=1, verbose=0)\nplot_model(history, 'LSTM Model')\n\nyhat_lstm = [x[0] for x in model.predict(Xval, verbose=0)]\n\ntrain_predict = model.predict(Xtrain, verbose=0)\ntest_predict = model.predict(Xval, verbose=0)\n\n# Print error\ntrain_rmse, test_rmse = print_error(Ytrain, Yval, train_predict, test_predict)\nrmse_table['model'].append('LSTM Neural Network')\nrmse_table['training_rmse'].append(train_rmse)\nrmse_table['testing_rmse'].append(test_rmse)\n\n\nTrain RMSE: 0.208 RMSE\nTest RMSE: 2.732 RMSE\n\n\n\n\n\n\n\n\n\nCode\n# Create an LSTM neural network with regularization\nmodel = create_LSTM(hidden_units=5, dense_units=1, input_shape=(lag, Xtrain.shape[-1]),\n                    activation=['tanh', 'linear'], dropout_rate = 0.2, kernel_regularizer=regularizers.L1L2(l1=1e-5, l2=1e-4))\nhistory = model.fit(Xtrain, Ytrain, epochs=20, batch_size=1, verbose=0)\nplot_model(history, 'LSTM Model (with L1L2 Regularization)')\n\n\nyhat_lstm_reg = [x[0] for x in model.predict(Xval, verbose=0)]\n\ntrain_predict = model.predict(Xtrain, verbose=0)\ntest_predict = model.predict(Xval, verbose=0)\n\n# Print error\ntrain_rmse, test_rmse = print_error(Ytrain, Yval, train_predict, test_predict)\nrmse_table['model'].append('LSTM Neural Network (with L1L2 Regularization)')\nrmse_table['training_rmse'].append(train_rmse)\nrmse_table['testing_rmse'].append(test_rmse)\n\n\nTrain RMSE: 0.188 RMSE\nTest RMSE: 2.569 RMSE\n\n\n\n\n\n\n\n\nThe two plots show the training loss for the LSTM model, one with regularization and one without. The plot with regularization shows less fluctuation, indicating that overfitting has been reduced. The model without regularization had a train RMSE of 0.196 and a test RMSE of 2.927, while the model with regularization had a slightly lower for both the train and test RMSE values which are 0.193 and 2.551 respectively.\n\n\nForecast\n\n\nCode\n# Creating the frame to store both predictions\ndays = df['Date'].values[-len(y):]\nframe = pd.concat([\n    pd.DataFrame({'day': days, 'price': y, 'type': 'original'}),\n    pd.DataFrame({'day': days, 'price': yhat_d, 'type': 'rnn_forecast'}),\n    pd.DataFrame({'day': days, 'price': yhat_gru, 'type': 'gru_forecast'}),\n    pd.DataFrame({'day': days, 'price': yhat_lstm, 'type': 'lstm_forecast'})\n])\n# Creating the unscaled values column\nframe['price_absolute'] = [(x * train_std) + train_mean for x in frame['price']]\n# Pivoting\npivoted = frame.pivot_table(index='day', columns='type')\npivoted.columns = ['_'.join(x).strip() for x in pivoted.columns.values]\n\nplt.figure(figsize=(12, 10))\nplt.plot(pivoted.index, pivoted.price_absolute_original,\n         color='#8B1874', label='original')\nplt.plot(pivoted.index, pivoted.price_absolute_rnn_forecast,\n         color='#F79540', label='RNN Forecast', alpha=0.6)\nplt.plot(pivoted.index, pivoted.price_absolute_gru_forecast,\n         color='#B71375', label='GRU Forecast', alpha=0.6)\nplt.plot(pivoted.index, pivoted.price_absolute_lstm_forecast,\n         color='#FC4F00', label='LSTM Forecast', alpha=0.6)\nplt.title('Technology Sector Fund Forecasts')\nplt.legend()\nplt.show()\n\n\n\n\n\nWhen comparing the predictions of the deep learning models with the original time series plot, we can observe that the LSTM model is the closest to the actual values. The GRU model and RNN model are pretty far from the actual plot, which indicates that LSTM model is better when compared to others, but when we look at the LSTM model, the model is not accurate, i.e., it is not the same as the actual plot.\n\n\nModel Comparison\n\n\nCode\nrmse_df = pd.DataFrame(rmse_table)\nrmse_df\n\n\n\n\n\n\n  \n    \n      \n      model\n      training_rmse\n      testing_rmse\n    \n  \n  \n    \n      0\n      Recurrent Neural Network\n      0.366353\n      4.226710\n    \n    \n      1\n      Recurrent Neural Network (with L1L2 Regulariza...\n      0.361858\n      4.227302\n    \n    \n      2\n      GRU Neural Network\n      1.000264\n      5.183701\n    \n    \n      3\n      GRU Neural Network (with L1L2 Regularization)\n      1.000264\n      5.183701\n    \n    \n      4\n      LSTM Neural Network\n      0.207549\n      2.731590\n    \n    \n      5\n      LSTM Neural Network (with L1L2 Regularization)\n      0.187572\n      2.568721\n    \n  \n\n\n\n\nThe Recurrent Neural Network (RNN) model and the RNN with L1L2 Regularization have similar training and testing RMSE values, with the regularized version performing slightly better on the testing data.\nThe GRU Neural Network has a high training and testing RMSE, while the GRU with L1L2 Regularization has a much lower testing RMSE, indicating better performance on unseen data.\nThe LSTM Neural Network and the LSTM with L1L2 Regularization both have low training and testing RMSE values, with the regularized version performing slightly better on the testing data.\nOverall, the LSTM Neural Network with L1L2 Regularization appears to perform the best on this dataset, with the lowest testing RMSE value. The GRU with L1L2 Regularization also performs well on the testing data, but has a higher training RMSE value, indicating potential overfitting to the training data.\n\n\nComparison of deep learning models with traditional single variable time series\nThe deep learning models (RNN, GRU, LSTM) perform better than the ARIMA model in both training and testing RMSE values for the Technology Sector Fund. Among the deep learning models, the LSTM model without regularization has the lowest testing RMSE value of 1.980341, when compared to ARIMA Model which is 2.118625, indicating its better performance in predicting the market prices of the Technology Sector Fund compared to the other deep learning models and ARIMA model."
  },
  {
    "objectID": "deep_learning_sector_xli.html",
    "href": "deep_learning_sector_xli.html",
    "title": "",
    "section": "",
    "text": "Deep Learning for Industrial Sector Fund\nAn industrial sector fund is a type of mutual fund or exchange-traded fund (ETF) that focuses on investing in companies within the industrial sector. These funds may hold stocks of companies involved in manufacturing, transportation, energy, construction, and other related industries. As with other stock market, predicting the future movements of the Industrial Sector Fund can be challenging due to the complex and dynamic nature of the market. These models range from traditional time-series models to more advanced deep learning models. By leveraging the power of Deep Learning and applying techniques such as RNN, LSTM, GRU, and regularization, we can create a model that accurately predicts the future movements of the sector. Such a model can provide valuable insights for investors and traders looking to make informed decisions in the stock market, and help them stay ahead of the curve in the ever-changing landscape of the technology and biotech industries.\nIn order to apply the deep learning methods, the time series data is first split into training and testing sets, taking into account the time component. The training data constitutes 75% of the total data while the rest is the testing data. Furthermore, to ensure that the values are on the same scale, the data is scaled, which is crucial for deep learning models. The mean and standard deviation of the training data are computed and then utilized to normalize both the training and testing sets. This approach ensures that the models are trained on a standardized dataset, enabling them to make accurate predictions.\n\n\nCode\n# libraries\nimport numpy as np\nimport pandas as pd\nimport math\nfrom sklearn.metrics import mean_squared_error\nfrom matplotlib import pyplot as plt\nfrom keras.models import Sequential\nfrom keras.layers import Dropout\nfrom keras.layers import Dense, SimpleRNN, LSTM, GRU\nfrom keras import regularizers\nimport warnings\nwarnings.filterwarnings('ignore')\n\n# import data\ndf = pd.read_csv('./DATA/CLEANED DATA/XLI_raw_data.csv')\n# clean data and get final data for anlaysis\ndf = df[['Date', 'XLI.Adjusted']]\ndf['Date'] = pd.to_datetime(df['Date'])\ndf['XLI.Adjusted'] = pd.to_numeric(df['XLI.Adjusted'], errors='coerce')\ndf = df.dropna(subset=['XLI.Adjusted'])\ndf.sort_values('Date', inplace=True, ascending=True)\ndf = df.reset_index(drop=True)\ndef create_X_Y(ts: np.array, lag=1, n_ahead=1, target_index=0) -> tuple:\n    \"\"\"\n    A method to create X and Y matrix from a time series array for the training of \n    deep learning models \n    \"\"\"\n    # Extracting the number of features that are passed from the array\n    n_features = ts.shape[1]\n\n    # Creating placeholder lists\n    X, Y = [], []\n\n    if len(ts) - lag <= 0:\n        X.append(ts)\n    else:\n        for i in range(len(ts) - lag - n_ahead):\n            Y.append(ts[(i + lag):(i + lag + n_ahead), target_index])\n            X.append(ts[i:(i + lag)])\n\n    X, Y = np.array(X), np.array(Y)\n\n    # Reshaping the X array to an RNN input shape\n    X = np.reshape(X, (X.shape[0], lag, n_features))\n\n    return X, Y\n\nts = df[['XLI.Adjusted']].values\nnp.random.seed(123)\nnrows = ts.shape[0]\ntest_share = 0.25\n# Spliting into train and test sets\ntrain = ts[0:int(nrows * (1 - test_share))]\ntest = ts[int(nrows * (1 - test_share)):]\n\n# Scaling the data\ntrain_mean = train.mean()\ntrain_std = train.std()\ntrain = (train - train_mean) / train_std\ntest = (test - train_mean) / train_std\n\n# Creating the final scaled frame\nts_s = np.concatenate([train, test])\n\nlag = 12\nahead = 3\n\n# Creating the X and Y for training\nX, Y = create_X_Y(ts_s, lag=lag, n_ahead=ahead)\n\nXtrain = X[0:int(X.shape[0] * (1 - test_share))]\nYtrain = Y[0:int(X.shape[0] * (1 - test_share))]\n\nXval = X[int(X.shape[0] * (1 - test_share)):]\nYval = Y[int(X.shape[0] * (1 - test_share)):]\n\n\n\n\ndef plot_model(history, model_title):\n    loss = history.history['loss']\n    epochs = range(1, len(loss) + 1)\n    plt.figure()\n    plt.plot(epochs, loss, 'b', label='Training loss', color = \"gray\")\n    plt.title(f'{model_title} Training loss ')\n    plt.legend()\n    \n\n\ndef print_error(trainY, testY, train_predict, test_predict):\n    # Error of predictions\n    train_rmse = math.sqrt(mean_squared_error(\n        trainY[:, 0], train_predict[:, 0]))\n    test_rmse = math.sqrt(mean_squared_error(testY[:, 0], test_predict[:, 0]))\n    # Print RMSE\n    print('Train RMSE: %.3f RMSE' % (train_rmse))\n    print('Test RMSE: %.3f RMSE' % (test_rmse))\n    return train_rmse, test_rmse\n\n\n\n\nRecurrent Neural Network\nThe RNN models that are constructed and trained using TensorFlow and Keras. This model is trained for five hidden layers, one dense layer, and the activation function hyperbolic tangent. A dropout rate of 0.2 is used to reduce the risk of overfitting. The model are trained for 20 epochs and generated validation loss plots to compare the performance of the regularized and non-regularized models. The second RNN model was identical to the first, except for the addition of kernel regularization.\n\nRNNRNN with Regularization\n\n\n\n\nCode\n# Create a RNN\ndef create_RNN(hidden_units, dense_units, input_shape, activation,dropout_rate=0.2,kernel_regularizer=None):\n    model = Sequential()\n    # Create a simple neural network layer\n    model.add(SimpleRNN(hidden_units, input_shape=input_shape,\n              activation=activation[0]))\n    # Add a dense layer (only one, more layers would make it a deep neural net)\n    model.add(Dense(units=dense_units,\n              activation=activation[1],\n              kernel_regularizer=kernel_regularizer))\n    # Add layer dropout\n    model.add(Dropout(dropout_rate))\n    \n    # Compile the model and optimize on mean squared error\n    model.compile(loss='mean_squared_error', optimizer='adam')\n    return model\n  \n# Create a recurrent neural network\nmodel = create_RNN(hidden_units=5, dense_units=1, input_shape=(lag, Xtrain.shape[-1]),\n                   activation=['tanh', 'tanh'], dropout_rate = 0.2)\nhistory = model.fit(Xtrain, Ytrain, epochs=20, batch_size=1, verbose=0,validation_data=(Xval, Yval))\nplot_model(history, 'Recurrent Neural Network Model')\n\nyhat_d = [x[0] for x in model.predict(Xval, verbose=0)]\ny = [y[0] for y in Yval]\n\ntrain_predict = model.predict(Xtrain, verbose=0)\ntest_predict = model.predict(Xval, verbose=0)\n\n# Print error\ntrain_rmse, test_rmse = print_error(Ytrain, Yval, train_predict, test_predict)\nrmse_table = {\n    'model': ['Recurrent Neural Network'],\n    'training_rmse': [train_rmse],\n    'testing_rmse': [test_rmse]\n}\n\n\nTrain RMSE: 0.293 RMSE\nTest RMSE: 1.839 RMSE\n\n\n\n\n\n\n\n\n\nCode\n# Create a recurrent neural network with regularization\n\nmodel = create_RNN(hidden_units=5, dense_units=1, input_shape=(lag, Xtrain.shape[-1]),\n                   activation=['tanh', 'tanh'], kernel_regularizer=regularizers.L1L2(l1=1e-5, l2=1e-4), dropout_rate = 0.2)\nhistory = model.fit(Xtrain, Ytrain, epochs=20, batch_size=1, verbose=0, validation_data=(Xval, Yval))\nplot_model(history, 'Recurrent Neural Network Model (with L1L2 Regularization)')\n\nyhat_d_reg = [x[0] for x in model.predict(Xval, verbose=0)]\n\ntrain_predict = model.predict(Xtrain, verbose=0)\ntest_predict = model.predict(Xval, verbose=0)\n\n# Print error\ntrain_rmse, test_rmse = print_error(Ytrain, Yval, train_predict, test_predict)\nrmse_table['model'].append(\n    'Recurrent Neural Network (with L1L2 Regularization)')\nrmse_table['training_rmse'].append(train_rmse)\nrmse_table['testing_rmse'].append(test_rmse)\n\n\nTrain RMSE: 0.291 RMSE\nTest RMSE: 1.839 RMSE\n\n\n\n\n\n\n\n\nThe second plot illustrates the impact of regularization on the training loss. Compared to the original plot, there are significant differences. The regularization technique had a substantial effect on the testing error of the model. The model trained without regularization had a slightly lower training error of 0.291 RMSE but a considerably higher testing error of 1.839 RMSE. The RMSE values are the Slightly higher for with regularization in terms of testing error, but for the training error the RMSE values are the same.\n\n\nGRU Neural Network\nA GRU neural network is a powerful machine learning tool widely adopted in various applications. Its key feature is the recurrent gated unit, a type of memory cell that can store information over a data sequence. The GRU neural network is five hidden layers, one dense layer, and the hyperbolic tangent activation function. The model also includes kernel regularization to help address the issue of overfitting. The models are trained for 20 epochs and validation loss plots were generated to compare the performance of the regularized and non-regularized models. Overall, the GRU neural network has shown great promise in accurately predicting future movements in the sector.\n\nGRUGRU with Regularization\n\n\n\n\nCode\n# Create a GRU Neural Network\ndef create_GRU(hidden_units, dense_units, input_shape, activation,dropout_rate=0.2, kernel_regularizer=None):\n    model = Sequential()\n    # Create a simple GRU neural network layer\n    model.add(GRU(hidden_units, input_shape=input_shape,\n              activation=activation[0]))\n    # Add a dense layer (only one, more layers would make it a deep neural net)\n    model.add(Dense(units=dense_units,\n              activation=activation[1], kernel_regularizer=kernel_regularizer))\n    # Add layer dropout\n    model.add(Dropout(dropout_rate))\n    # Compile the model and optimize on mean squared error\n    model.compile(loss='mean_squared_error', optimizer='sgd')\n    return model\n\n# Training and evaluating a GRU-based model\nmodel = create_GRU(hidden_units=5, dense_units=1, input_shape=(lag, Xtrain.shape[-1]),\n                   activation=['tanh', 'relu'], dropout_rate = 0.2)\nhistory = model.fit(Xtrain, Ytrain, epochs=20, batch_size=1, verbose=0)\nplot_model(history, 'GRU Model')\n\nyhat_gru = [x[0] for x in model.predict(Xval, verbose=0)]\n\ntrain_predict = model.predict(Xtrain, verbose=0)\ntest_predict = model.predict(Xval, verbose=0)\n\n# Print error\ntrain_rmse, test_rmse = print_error(Ytrain, Yval, train_predict, test_predict)\nrmse_table['model'].append('GRU Neural Network')\nrmse_table['training_rmse'].append(train_rmse)\nrmse_table['testing_rmse'].append(test_rmse)\n\n\nTrain RMSE: 0.998 RMSE\nTest RMSE: 2.768 RMSE\n\n\n\n\n\n\n\n\n\nCode\n# Training and evaluating a GRU-based model with regularization\nmodel = create_GRU(hidden_units=5, dense_units=1, input_shape=(lag, Xtrain.shape[-1]),\n                   activation=['tanh', 'relu'],dropout_rate = 0.2,  kernel_regularizer=regularizers.L1L2(l1=1e-5, l2=1e-4))\nhistory = model.fit(Xtrain, Ytrain, epochs=20, batch_size=1, verbose=0)\nplot_model(history, 'GRU Model (with L1L2 Regularization)')\n\nyhat_gru_reg = [x[0] for x in model.predict(Xval, verbose=0)]\n\ntrain_predict = model.predict(Xtrain, verbose=0)\ntest_predict = model.predict(Xval, verbose=0)\n\n# Print error\ntrain_rmse, test_rmse = print_error(Ytrain, Yval, train_predict, test_predict)\nrmse_table['model'].append('GRU Neural Network (with L1L2 Regularization)')\nrmse_table['training_rmse'].append(train_rmse)\nrmse_table['testing_rmse'].append(test_rmse)\n\n\nTrain RMSE: 0.688 RMSE\nTest RMSE: 0.919 RMSE\n\n\n\n\n\n\n\n\nThe two plots show the training loss for the GRU model, one with regularization and one without. The plot with regularization is similar to without regularization plot. The model without regularization had a train RMSE of 0.998 and a test RMSE of 2.768, the values are the same for with regularization too.\n\n\nLSTM Neural Network\nThe LSTM (Long Short-Term Memory) neural network is another powerful machine learning tool that has been widely adopted in various applications, particularly in tasks involving sequential data.The LSTM neural network architecture typically consists of multiple LSTM layers, followed by one or more dense layers and an activation function such as the sigmoid or hyperbolic tangent. Similar to the GRU neural network, the LSTM model may also include kernel regularization to address the issue of overfitting. In order to evaluate the performance of the LSTM model, the model can be trained on a dataset for a specified number of epochs, and validation loss plots can be generated to compare the performance of the regularized and non-regularized models.\n\nLSTMLSTM with Regularization\n\n\n\n\nCode\n# Create a LSTM Neural Network\ndef create_LSTM(hidden_units, dense_units, input_shape, activation,dropout_rate = 0.2, kernel_regularizer=None):\n    model = Sequential()\n    # Create a simple long short term memory neural network\n    model.add(LSTM(hidden_units,\n              activation=activation[0], input_shape=input_shape))\n    # Add a dense layer (only one, more layers would make it a deep neural net)\n    model.add(Dense(units=dense_units,\n              activation=activation[1], kernel_regularizer=kernel_regularizer))\n    # Add layer dropout\n    model.add(Dropout(dropout_rate))\n    # Compile the model and optimize on mean squared error\n    model.compile(optimizer=\"RMSprop\", loss='mae')\n    return model\n\n# Create an LSTM neural network\nmodel = create_LSTM(hidden_units=5, dense_units=1, input_shape=(lag, Xtrain.shape[-1]),\n                    activation=['tanh', 'linear'], dropout_rate = 0.2)\nhistory = model.fit(Xtrain, Ytrain, epochs=20, batch_size=1, verbose=0)\nplot_model(history, 'LSTM Model')\n\nyhat_lstm = [x[0] for x in model.predict(Xval, verbose=0)]\n\ntrain_predict = model.predict(Xtrain, verbose=0)\ntest_predict = model.predict(Xval, verbose=0)\n\n# Print error\ntrain_rmse, test_rmse = print_error(Ytrain, Yval, train_predict, test_predict)\nrmse_table['model'].append('LSTM Neural Network')\nrmse_table['training_rmse'].append(train_rmse)\nrmse_table['testing_rmse'].append(test_rmse)\n\n\nTrain RMSE: 0.202 RMSE\nTest RMSE: 0.747 RMSE\n\n\n\n\n\n\n\n\n\nCode\n# Create an LSTM neural network with regularization\nmodel = create_LSTM(hidden_units=5, dense_units=1, input_shape=(lag, Xtrain.shape[-1]),\n                    activation=['tanh', 'linear'], dropout_rate = 0.2, kernel_regularizer=regularizers.L1L2(l1=1e-5, l2=1e-4))\nhistory = model.fit(Xtrain, Ytrain, epochs=20, batch_size=1, verbose=0)\nplot_model(history, 'LSTM Model (with L1L2 Regularization)')\n\n\nyhat_lstm_reg = [x[0] for x in model.predict(Xval, verbose=0)]\n\ntrain_predict = model.predict(Xtrain, verbose=0)\ntest_predict = model.predict(Xval, verbose=0)\n\n# Print error\ntrain_rmse, test_rmse = print_error(Ytrain, Yval, train_predict, test_predict)\nrmse_table['model'].append('LSTM Neural Network (with L1L2 Regularization)')\nrmse_table['training_rmse'].append(train_rmse)\nrmse_table['testing_rmse'].append(test_rmse)\n\n\nTrain RMSE: 0.200 RMSE\nTest RMSE: 0.840 RMSE\n\n\n\n\n\n\n\n\nThe two plots show the training loss for the LSTM model, one with regularization and one without. The plot with regularization shows less fluctuation, indicating that overfitting has been reduced. The model without regularization had a train RMSE of 0.200 and a test RMSE of 0.825, while the model with regularization had a slightly lower for both the train and test RMSE values which are 0.192 and 0.809 respectively.\n\n\nForecast\n\n\nCode\n# Creating the frame to store both predictions\ndays = df['Date'].values[-len(y):]\nframe = pd.concat([\n    pd.DataFrame({'day': days, 'price': y, 'type': 'original'}),\n    pd.DataFrame({'day': days, 'price': yhat_d, 'type': 'rnn_forecast'}),\n    pd.DataFrame({'day': days, 'price': yhat_gru, 'type': 'gru_forecast'}),\n    pd.DataFrame({'day': days, 'price': yhat_lstm, 'type': 'lstm_forecast'})\n])\n# Creating the unscaled values column\nframe['price_absolute'] = [(x * train_std) + train_mean for x in frame['price']]\n# Pivoting\npivoted = frame.pivot_table(index='day', columns='type')\npivoted.columns = ['_'.join(x).strip() for x in pivoted.columns.values]\n\nplt.figure(figsize=(12, 10))\nplt.plot(pivoted.index, pivoted.price_absolute_original,\n         color='#8B1874', label='original')\nplt.plot(pivoted.index, pivoted.price_absolute_rnn_forecast,\n         color='#F79540', label='RNN Forecast', alpha=0.6)\nplt.plot(pivoted.index, pivoted.price_absolute_gru_forecast,\n         color='#B71375', label='GRU Forecast', alpha=0.6)\nplt.plot(pivoted.index, pivoted.price_absolute_lstm_forecast,\n         color='#FC4F00', label='LSTM Forecast', alpha=0.6)\nplt.title('Industrial Sector Fund Forecasts')\nplt.legend()\nplt.show()\n\n\n\n\n\nWhen comparing the predictions of the deep learning models with the original time series plot, we can observe that the LSTM model is the closest to the actual values. The GRU model and RNN model are pretty far from the actual plot, which indicates that LSTM model is better when compared to others, but when we look at the LSTM model, the model is not accurate, i.e., it is not the same as the actual plot.\n\n\nModel Comparison\n\n\nCode\nrmse_df = pd.DataFrame(rmse_table)\nrmse_df\n\n\n\n\n\n\n  \n    \n      \n      model\n      training_rmse\n      testing_rmse\n    \n  \n  \n    \n      0\n      Recurrent Neural Network\n      0.292596\n      1.838883\n    \n    \n      1\n      Recurrent Neural Network (with L1L2 Regulariza...\n      0.291123\n      1.839132\n    \n    \n      2\n      GRU Neural Network\n      0.997856\n      2.767872\n    \n    \n      3\n      GRU Neural Network (with L1L2 Regularization)\n      0.687721\n      0.919074\n    \n    \n      4\n      LSTM Neural Network\n      0.201554\n      0.747094\n    \n    \n      5\n      LSTM Neural Network (with L1L2 Regularization)\n      0.199909\n      0.839552\n    \n  \n\n\n\n\nIn terms of performance on the testing dataset, the LSTM Neural Network with L1L2 regularization appears to be the best performing model, with a testing_rmse of 0.808819. This is followed closely by the non-regularized LSTM with a testing_rmse of 0.824516. The regularized RNN and non-regularized RNN have similar testing_rmse values of 1.840652 and 1.838846, respectively. The non-regularized GRU has a higher testing_rmse of 2.767866, while the regularized GRU has the same testing_rmse as the non-regularized GRU due to the regularization not being effective in this case.\nIt is important to note that the results can be influenced by the specific dataset and the problem at hand. However, in general, LSTMs are known to be effective in tasks that require capturing long-term dependencies in sequential data. GRUs, on the other hand, are a simpler variant of LSTMs and can be used as an alternative if the dataset is relatively small or if computational resources are limited. Regularization can help to prevent overfitting, which is a common problem in deep learning models, and can improve generalization performance.\nOverall, the results suggest that for this particular dataset, the LSTM Neural Network with L1L2 regularization appears to be the best-performing model, followed closely by the non-regularized LSTM. The regularized and non-regularized RNN have similar performance, while the non-regularized GRU appears to perform worse than the other models. It is also worth noting that the regularization did not improve the performance of the GRU in this case, which suggests that the effectiveness of regularization can depend on the specific model and dataset.\n\n\nComparison of deep learning models with traditional single variable time series\nThe deep learning models (RNN, GRU, LSTM) perform better than the ARIMA model in both training and testing RMSE values for the Industrial Sector Fund. Among the deep learning models, the LSTM model without regularization has the lowest testing RMSE value, when compared to ARIMA Model which is 1.317807, indicating its better performance in predicting the market prices of the Industrial Sector Fund compared to the other deep learning models and ARIMA model."
  },
  {
    "objectID": "deep_learning_sector_xlf.html",
    "href": "deep_learning_sector_xlf.html",
    "title": "",
    "section": "",
    "text": "Deep Learning for Financial Sector Fund\nThe financial sector refers to the part of the economy that deals with financial transactions, including banks, investment firms, insurance companies, and other financial institutions. The financial sector plays a crucial role in the economy by providing capital, managing risk, and facilitating the flow of money throughout the economy. As with other stock market, predicting the future movements of the Financial Sector Fund can be challenging due to the complex and dynamic nature of the market. These models range from traditional time-series models to more advanced deep learning models. By leveraging the power of Deep Learning and applying techniques such as RNN, LSTM, GRU, and regularization, we can create a model that accurately predicts the future movements of the sector. Such a model can provide valuable insights for investors and traders looking to make informed decisions in the stock market, and help them stay ahead of the curve in the ever-changing landscape of the technology and biotech industries.\nIn order to apply the deep learning methods, the time series data is first split into training and testing sets, taking into account the time component. The training data constitutes 75% of the total data while the rest is the testing data. Furthermore, to ensure that the values are on the same scale, the data is scaled, which is crucial for deep learning models. The mean and standard deviation of the training data are computed and then utilized to normalize both the training and testing sets. This approach ensures that the models are trained on a standardized dataset, enabling them to make accurate predictions.\n\n\nCode\n# libraries\nimport numpy as np\nimport pandas as pd\nimport math\nfrom sklearn.metrics import mean_squared_error\nfrom matplotlib import pyplot as plt\nfrom keras.models import Sequential\nfrom keras.layers import Dropout\nfrom keras.layers import Dense, SimpleRNN, LSTM, GRU\nfrom keras import regularizers\nimport warnings\nwarnings.filterwarnings('ignore')\n\n# import data\ndf = pd.read_csv('./DATA/CLEANED DATA/XLF_raw_data.csv')\n# clean data and get final data for anlaysis\ndf = df[['Date', 'XLF.Adjusted']]\ndf['Date'] = pd.to_datetime(df['Date'])\ndf['XLF.Adjusted'] = pd.to_numeric(df['XLF.Adjusted'], errors='coerce')\ndf = df.dropna(subset=['XLF.Adjusted'])\ndf.sort_values('Date', inplace=True, ascending=True)\ndf = df.reset_index(drop=True)\ndef create_X_Y(ts: np.array, lag=1, n_ahead=1, target_index=0) -> tuple:\n    \"\"\"\n    A method to create X and Y matrix from a time series array for the training of \n    deep learning models \n    \"\"\"\n    # Extracting the number of features that are passed from the array\n    n_features = ts.shape[1]\n\n    # Creating placeholder lists\n    X, Y = [], []\n\n    if len(ts) - lag <= 0:\n        X.append(ts)\n    else:\n        for i in range(len(ts) - lag - n_ahead):\n            Y.append(ts[(i + lag):(i + lag + n_ahead), target_index])\n            X.append(ts[i:(i + lag)])\n\n    X, Y = np.array(X), np.array(Y)\n\n    # Reshaping the X array to an RNN input shape\n    X = np.reshape(X, (X.shape[0], lag, n_features))\n\n    return X, Y\n\nts = df[['XLF.Adjusted']].values\nnp.random.seed(123)\nnrows = ts.shape[0]\ntest_share = 0.25\n# Spliting into train and test sets\ntrain = ts[0:int(nrows * (1 - test_share))]\ntest = ts[int(nrows * (1 - test_share)):]\n\n# Scaling the data\ntrain_mean = train.mean()\ntrain_std = train.std()\ntrain = (train - train_mean) / train_std\ntest = (test - train_mean) / train_std\n\n# Creating the final scaled frame\nts_s = np.concatenate([train, test])\n\nlag = 12\nahead = 3\n\n# Creating the X and Y for training\nX, Y = create_X_Y(ts_s, lag=lag, n_ahead=ahead)\n\nXtrain = X[0:int(X.shape[0] * (1 - test_share))]\nYtrain = Y[0:int(X.shape[0] * (1 - test_share))]\n\nXval = X[int(X.shape[0] * (1 - test_share)):]\nYval = Y[int(X.shape[0] * (1 - test_share)):]\n\n\n\n\ndef plot_model(history, model_title):\n    loss = history.history['loss']\n    epochs = range(1, len(loss) + 1)\n    plt.figure()\n    plt.plot(epochs, loss, 'b', label='Training loss', color = \"gray\")\n    plt.title(f'{model_title} Training loss ')\n    plt.legend()\n    \n\n\ndef print_error(trainY, testY, train_predict, test_predict):\n    # Error of predictions\n    train_rmse = math.sqrt(mean_squared_error(\n        trainY[:, 0], train_predict[:, 0]))\n    test_rmse = math.sqrt(mean_squared_error(testY[:, 0], test_predict[:, 0]))\n    # Print RMSE\n    print('Train RMSE: %.3f RMSE' % (train_rmse))\n    print('Test RMSE: %.3f RMSE' % (test_rmse))\n    return train_rmse, test_rmse\n\n\n\n\nRecurrent Neural Network\nThe RNN models that are constructed and trained using TensorFlow and Keras. This model is trained for five hidden layers, one dense layer, and the activation function hyperbolic tangent. A dropout rate of 0.2 is used to reduce the risk of overfitting. The model are trained for 20 epochs and generated validation loss plots to compare the performance of the regularized and non-regularized models. The second RNN model was identical to the first, except for the addition of kernel regularization.\n\nRNNRNN with Regularization\n\n\n\n\nCode\n# Create a RNN\ndef create_RNN(hidden_units, dense_units, input_shape, activation,dropout_rate=0.2,kernel_regularizer=None):\n    model = Sequential()\n    # Create a simple neural network layer\n    model.add(SimpleRNN(hidden_units, input_shape=input_shape,\n              activation=activation[0]))\n    # Add a dense layer (only one, more layers would make it a deep neural net)\n    model.add(Dense(units=dense_units,\n              activation=activation[1],\n              kernel_regularizer=kernel_regularizer))\n    # Add layer dropout\n    model.add(Dropout(dropout_rate))\n    \n    # Compile the model and optimize on mean squared error\n    model.compile(loss='mean_squared_error', optimizer='adam')\n    return model\n  \n# Create a recurrent neural network\nmodel = create_RNN(hidden_units=5, dense_units=1, input_shape=(lag, Xtrain.shape[-1]),\n                   activation=['tanh', 'tanh'], dropout_rate = 0.2)\nhistory = model.fit(Xtrain, Ytrain, epochs=20, batch_size=1, verbose=0,validation_data=(Xval, Yval))\nplot_model(history, 'Recurrent Neural Network Model')\n\nyhat_d = [x[0] for x in model.predict(Xval, verbose=0)]\ny = [y[0] for y in Yval]\n\ntrain_predict = model.predict(Xtrain, verbose=0)\ntest_predict = model.predict(Xval, verbose=0)\n\n# Print error\ntrain_rmse, test_rmse = print_error(Ytrain, Yval, train_predict, test_predict)\nrmse_table = {\n    'model': ['Recurrent Neural Network'],\n    'training_rmse': [train_rmse],\n    'testing_rmse': [test_rmse]\n}\n\n\nTrain RMSE: 0.293 RMSE\nTest RMSE: 1.897 RMSE\n\n\n\n\n\n\n\n\n\nCode\n# Create a recurrent neural network with regularization\n\nmodel = create_RNN(hidden_units=5, dense_units=1, input_shape=(lag, Xtrain.shape[-1]),\n                   activation=['tanh', 'tanh'], kernel_regularizer=regularizers.L1L2(l1=1e-5, l2=1e-4), dropout_rate = 0.2)\nhistory = model.fit(Xtrain, Ytrain, epochs=20, batch_size=1, verbose=0, validation_data=(Xval, Yval))\nplot_model(history, 'Recurrent Neural Network Model (with L1L2 Regularization)')\n\nyhat_d_reg = [x[0] for x in model.predict(Xval, verbose=0)]\n\ntrain_predict = model.predict(Xtrain, verbose=0)\ntest_predict = model.predict(Xval, verbose=0)\n\n# Print error\ntrain_rmse, test_rmse = print_error(Ytrain, Yval, train_predict, test_predict)\nrmse_table['model'].append(\n    'Recurrent Neural Network (with L1L2 Regularization)')\nrmse_table['training_rmse'].append(train_rmse)\nrmse_table['testing_rmse'].append(test_rmse)\n\n\nTrain RMSE: 0.293 RMSE\nTest RMSE: 1.897 RMSE\n\n\n\n\n\n\n\n\nThe second plot illustrates the impact of regularization on the training loss. Compared to the original plot, there are significant differences. The regularization technique had a substantial effect on the testing error of the model. The model trained without regularization had a slightly lower training error of 0.294 RMSE but a considerably higher testing error of 1.897 RMSE which is the same as the RMSE values with regularization.\n\n\nGRU Neural Network\nA GRU neural network is a powerful machine learning tool widely adopted in various applications. Its key feature is the recurrent gated unit, a type of memory cell that can store information over a data sequence. The GRU neural network is five hidden layers, one dense layer, and the hyperbolic tangent activation function. The model also includes kernel regularization to help address the issue of overfitting. The models are trained for 20 epochs and validation loss plots were generated to compare the performance of the regularized and non-regularized models. Overall, the GRU neural network has shown great promise in accurately predicting future movements in the sector.\n\nGRUGRU with Regularization\n\n\n\n\nCode\n# Create a GRU Neural Network\ndef create_GRU(hidden_units, dense_units, input_shape, activation,dropout_rate=0.2, kernel_regularizer=None):\n    model = Sequential()\n    # Create a simple GRU neural network layer\n    model.add(GRU(hidden_units, input_shape=input_shape,\n              activation=activation[0]))\n    # Add a dense layer (only one, more layers would make it a deep neural net)\n    model.add(Dense(units=dense_units,\n              activation=activation[1], kernel_regularizer=kernel_regularizer))\n    # Add layer dropout\n    model.add(Dropout(dropout_rate))\n    # Compile the model and optimize on mean squared error\n    model.compile(loss='mean_squared_error', optimizer='sgd')\n    return model\n\n# Training and evaluating a GRU-based model\nmodel = create_GRU(hidden_units=5, dense_units=1, input_shape=(lag, Xtrain.shape[-1]),\n                   activation=['tanh', 'relu'], dropout_rate = 0.2)\nhistory = model.fit(Xtrain, Ytrain, epochs=20, batch_size=1, verbose=0)\nplot_model(history, 'GRU Model')\n\nyhat_gru = [x[0] for x in model.predict(Xval, verbose=0)]\n\ntrain_predict = model.predict(Xtrain, verbose=0)\ntest_predict = model.predict(Xval, verbose=0)\n\n# Print error\ntrain_rmse, test_rmse = print_error(Ytrain, Yval, train_predict, test_predict)\nrmse_table['model'].append('GRU Neural Network')\nrmse_table['training_rmse'].append(train_rmse)\nrmse_table['testing_rmse'].append(test_rmse)\n\n\nTrain RMSE: 1.000 RMSE\nTest RMSE: 2.798 RMSE\n\n\n\n\n\n\n\n\n\nCode\n# Training and evaluating a GRU-based model with regularization\nmodel = create_GRU(hidden_units=5, dense_units=1, input_shape=(lag, Xtrain.shape[-1]),\n                   activation=['tanh', 'relu'],dropout_rate = 0.2,  kernel_regularizer=regularizers.L1L2(l1=1e-5, l2=1e-4))\nhistory = model.fit(Xtrain, Ytrain, epochs=20, batch_size=1, verbose=0)\nplot_model(history, 'GRU Model (with L1L2 Regularization)')\n\nyhat_gru_reg = [x[0] for x in model.predict(Xval, verbose=0)]\n\ntrain_predict = model.predict(Xtrain, verbose=0)\ntest_predict = model.predict(Xval, verbose=0)\n\n# Print error\ntrain_rmse, test_rmse = print_error(Ytrain, Yval, train_predict, test_predict)\nrmse_table['model'].append('GRU Neural Network (with L1L2 Regularization)')\nrmse_table['training_rmse'].append(train_rmse)\nrmse_table['testing_rmse'].append(test_rmse)\n\n\nTrain RMSE: 1.000 RMSE\nTest RMSE: 2.798 RMSE\n\n\n\n\n\n\n\n\nThe two plots show the training loss for the GRU model, one with regularization and one without. The plot with regularization is similar to without regularization plot. The model without regularization had a train RMSE of 0.675 and a test RMSE of 0.937,but the values are the slightly higher for with regularization.\n\n\nLSTM Neural Network\nThe LSTM (Long Short-Term Memory) neural network is another powerful machine learning tool that has been widely adopted in various applications, particularly in tasks involving sequential data.The LSTM neural network architecture typically consists of multiple LSTM layers, followed by one or more dense layers and an activation function such as the sigmoid or hyperbolic tangent. Similar to the GRU neural network, the LSTM model may also include kernel regularization to address the issue of overfitting. In order to evaluate the performance of the LSTM model, the model can be trained on a dataset for a specified number of epochs, and validation loss plots can be generated to compare the performance of the regularized and non-regularized models.\n\nLSTMLSTM with Regularization\n\n\n\n\nCode\n# Create a LSTM Neural Network\ndef create_LSTM(hidden_units, dense_units, input_shape, activation,dropout_rate = 0.2, kernel_regularizer=None):\n    model = Sequential()\n    # Create a simple long short term memory neural network\n    model.add(LSTM(hidden_units,\n              activation=activation[0], input_shape=input_shape))\n    # Add a dense layer (only one, more layers would make it a deep neural net)\n    model.add(Dense(units=dense_units,\n              activation=activation[1], kernel_regularizer=kernel_regularizer))\n    # Add layer dropout\n    model.add(Dropout(dropout_rate))\n    # Compile the model and optimize on mean squared error\n    model.compile(optimizer=\"RMSprop\", loss='mae')\n    return model\n\n# Create an LSTM neural network\nmodel = create_LSTM(hidden_units=5, dense_units=1, input_shape=(lag, Xtrain.shape[-1]),\n                    activation=['tanh', 'linear'], dropout_rate = 0.2)\nhistory = model.fit(Xtrain, Ytrain, epochs=20, batch_size=1, verbose=0)\nplot_model(history, 'LSTM Model')\n\nyhat_lstm = [x[0] for x in model.predict(Xval, verbose=0)]\n\ntrain_predict = model.predict(Xtrain, verbose=0)\ntest_predict = model.predict(Xval, verbose=0)\n\n# Print error\ntrain_rmse, test_rmse = print_error(Ytrain, Yval, train_predict, test_predict)\nrmse_table['model'].append('LSTM Neural Network')\nrmse_table['training_rmse'].append(train_rmse)\nrmse_table['testing_rmse'].append(test_rmse)\n\n\nTrain RMSE: 0.192 RMSE\nTest RMSE: 0.835 RMSE\n\n\n\n\n\n\n\n\n\nCode\n# Create an LSTM neural network with regularization\nmodel = create_LSTM(hidden_units=5, dense_units=1, input_shape=(lag, Xtrain.shape[-1]),\n                    activation=['tanh', 'linear'], dropout_rate = 0.2, kernel_regularizer=regularizers.L1L2(l1=1e-5, l2=1e-4))\nhistory = model.fit(Xtrain, Ytrain, epochs=20, batch_size=1, verbose=0)\nplot_model(history, 'LSTM Model (with L1L2 Regularization)')\n\n\nyhat_lstm_reg = [x[0] for x in model.predict(Xval, verbose=0)]\n\ntrain_predict = model.predict(Xtrain, verbose=0)\ntest_predict = model.predict(Xval, verbose=0)\n\n# Print error\ntrain_rmse, test_rmse = print_error(Ytrain, Yval, train_predict, test_predict)\nrmse_table['model'].append('LSTM Neural Network (with L1L2 Regularization)')\nrmse_table['training_rmse'].append(train_rmse)\nrmse_table['testing_rmse'].append(test_rmse)\n\n\nTrain RMSE: 0.216 RMSE\nTest RMSE: 0.950 RMSE\n\n\n\n\n\n\n\n\nThe two plots show the training loss for the LSTM model, one with regularization and one without. The plot with regularization shows less fluctuation, indicating that overfitting has been reduced. The model without regularization had a train RMSE of 0.208 and a test RMSE of 0.811, while the model with regularization had a slightly higher for both the train and test RMSE values which are 0.209 and 0.902 respectively.\n\n\nForecast\n\n\nCode\n# Creating the frame to store both predictions\ndays = df['Date'].values[-len(y):]\nframe = pd.concat([\n    pd.DataFrame({'day': days, 'price': y, 'type': 'original'}),\n    pd.DataFrame({'day': days, 'price': yhat_d, 'type': 'rnn_forecast'}),\n    pd.DataFrame({'day': days, 'price': yhat_gru, 'type': 'gru_forecast'}),\n    pd.DataFrame({'day': days, 'price': yhat_lstm, 'type': 'lstm_forecast'})\n])\n# Creating the unscaled values column\nframe['price_absolute'] = [(x * train_std) + train_mean for x in frame['price']]\n# Pivoting\npivoted = frame.pivot_table(index='day', columns='type')\npivoted.columns = ['_'.join(x).strip() for x in pivoted.columns.values]\n\nplt.figure(figsize=(12, 10))\nplt.plot(pivoted.index, pivoted.price_absolute_original,\n         color='#8B1874', label='original')\nplt.plot(pivoted.index, pivoted.price_absolute_rnn_forecast,\n         color='#F79540', label='RNN Forecast', alpha=0.6)\nplt.plot(pivoted.index, pivoted.price_absolute_gru_forecast,\n         color='#B71375', label='GRU Forecast', alpha=0.6)\nplt.plot(pivoted.index, pivoted.price_absolute_lstm_forecast,\n         color='#FC4F00', label='LSTM Forecast', alpha=0.6)\nplt.title('Financial Sector Fund Forecasts')\nplt.legend()\nplt.show()\n\n\n\n\n\nWhen comparing the predictions of the deep learning models with the original time series plot, we can observe that the LSTM model is the closest to the actual values. The GRU model is almost the same as the LSTM model, but both the models have some kind of accuracy with the actual plot, but there is still difference. RNN model are pretty far from the actual plot, which indicates that LSTM model is better when compared to others, but we need to note that the model is not accurate, i.e., it is not the same as the actual plot.\n\n\nModel Comparison\n\n\nCode\nrmse_df = pd.DataFrame(rmse_table)\nrmse_df\n\n\n\n\n\n\n  \n    \n      \n      model\n      training_rmse\n      testing_rmse\n    \n  \n  \n    \n      0\n      Recurrent Neural Network\n      0.292522\n      1.897016\n    \n    \n      1\n      Recurrent Neural Network (with L1L2 Regulariza...\n      0.292927\n      1.896996\n    \n    \n      2\n      GRU Neural Network\n      0.999823\n      2.797772\n    \n    \n      3\n      GRU Neural Network (with L1L2 Regularization)\n      0.999823\n      2.797772\n    \n    \n      4\n      LSTM Neural Network\n      0.191573\n      0.835212\n    \n    \n      5\n      LSTM Neural Network (with L1L2 Regularization)\n      0.216050\n      0.950497\n    \n  \n\n\n\n\nComparing the models, we can see that the LSTM Neural Network (with L1L2 Regularization) has consistently performed better than the other models across all datasets, as it has the lowest testing RMSE values in almost all cases. The LSTM model without regularization has also performed well but not as good as with regularization.\nThe Recurrent Neural Network and GRU Neural Network have shown mixed results, with some datasets performing well with regularization and others without regularization.\nIt is worth noting that the GRU Neural Network (with L1L2 Regularization) has significantly reduced the testing RMSE value on the third dataset compared to the non-regularized model. This suggests that regularization can help in preventing overfitting and improving the model’s generalization ability.\nOverall, the comparison of deep learning models with and without regularization suggests that regularization can help in improving the model’s performance and generalization ability.\n\n\nComparison of deep learning models with traditional single variable time series\nThe deep learning models (RNN, GRU, LSTM) perform better than the ARIMA model in both training and testing RMSE values for the Financial Sector Fund. Among the deep learning models, the LSTM model without regularization has the lowest training RMSE value, when compared to ARIMA Model which is 0.4622021, indicating its better performance in predicting the market prices of the Financial Sector Fund compared to the other deep learning models and ARIMA model."
  },
  {
    "objectID": "deep_learning_sector_xle.html",
    "href": "deep_learning_sector_xle.html",
    "title": "",
    "section": "",
    "text": "Deep Learning for Energy Sector Fund\nThe energy sector fund comprises companies that are involved in the exploration, production, and distribution of energy resources, including oil, gas, and renewable energy sources. These companies are often sensitive to changes in global energy prices and can be impacted by geopolitical events and environmental regulations. As with other stock market, predicting the future movements of the Energy Sector Fund can be challenging due to the complex and dynamic nature of the market. These models range from traditional time-series models to more advanced deep learning models. By leveraging the power of Deep Learning and applying techniques such as RNN, LSTM, GRU, and regularization, we can create a model that accurately predicts the future movements of the sector. Such a model can provide valuable insights for investors and traders looking to make informed decisions in the stock market, and help them stay ahead of the curve in the ever-changing landscape of the technology and biotech industries.\nIn order to apply the deep learning methods, the time series data is first split into training and testing sets, taking into account the time component. The training data constitutes 75% of the total data while the rest is the testing data. Furthermore, to ensure that the values are on the same scale, the data is scaled, which is crucial for deep learning models. The mean and standard deviation of the training data are computed and then utilized to normalize both the training and testing sets. This approach ensures that the models are trained on a standardized dataset, enabling them to make accurate predictions.\n\n\nCode\n# libraries\nimport numpy as np\nimport pandas as pd\nimport math\nfrom sklearn.metrics import mean_squared_error\nfrom matplotlib import pyplot as plt\nfrom keras.models import Sequential\nfrom keras.layers import Dropout\nfrom keras.layers import Dense, SimpleRNN, LSTM, GRU\nfrom keras import regularizers\nimport warnings\nwarnings.filterwarnings('ignore')\n\n# import data\ndf = pd.read_csv('./DATA/CLEANED DATA/XLE_raw_data.csv')\n# clean data and get final data for anlaysis\ndf = df[['Date', 'XLE.Adjusted']]\ndf['Date'] = pd.to_datetime(df['Date'])\ndf['XLE.Adjusted'] = pd.to_numeric(df['XLE.Adjusted'], errors='coerce')\ndf = df.dropna(subset=['XLE.Adjusted'])\ndf.sort_values('Date', inplace=True, ascending=True)\ndf = df.reset_index(drop=True)\ndef create_X_Y(ts: np.array, lag=1, n_ahead=1, target_index=0) -> tuple:\n    \"\"\"\n    A method to create X and Y matrix from a time series array for the training of \n    deep learning models \n    \"\"\"\n    # Extracting the number of features that are passed from the array\n    n_features = ts.shape[1]\n\n    # Creating placeholder lists\n    X, Y = [], []\n\n    if len(ts) - lag <= 0:\n        X.append(ts)\n    else:\n        for i in range(len(ts) - lag - n_ahead):\n            Y.append(ts[(i + lag):(i + lag + n_ahead), target_index])\n            X.append(ts[i:(i + lag)])\n\n    X, Y = np.array(X), np.array(Y)\n\n    # Reshaping the X array to an RNN input shape\n    X = np.reshape(X, (X.shape[0], lag, n_features))\n\n    return X, Y\n\nts = df[['XLE.Adjusted']].values\nnp.random.seed(123)\nnrows = ts.shape[0]\ntest_share = 0.25\n# Spliting into train and test sets\ntrain = ts[0:int(nrows * (1 - test_share))]\ntest = ts[int(nrows * (1 - test_share)):]\n\n# Scaling the data\ntrain_mean = train.mean()\ntrain_std = train.std()\ntrain = (train - train_mean) / train_std\ntest = (test - train_mean) / train_std\n\n# Creating the final scaled frame\nts_s = np.concatenate([train, test])\n\nlag = 12\nahead = 3\n\n# Creating the X and Y for training\nX, Y = create_X_Y(ts_s, lag=lag, n_ahead=ahead)\n\nXtrain = X[0:int(X.shape[0] * (1 - test_share))]\nYtrain = Y[0:int(X.shape[0] * (1 - test_share))]\n\nXval = X[int(X.shape[0] * (1 - test_share)):]\nYval = Y[int(X.shape[0] * (1 - test_share)):]\n\n\n\n\ndef plot_model(history, model_title):\n    loss = history.history['loss']\n    epochs = range(1, len(loss) + 1)\n    plt.figure()\n    plt.plot(epochs, loss, 'b', label='Training loss', color = \"gray\")\n    plt.title(f'{model_title} Training loss ')\n    plt.legend()\n    \n\n\ndef print_error(trainY, testY, train_predict, test_predict):\n    # Error of predictions\n    train_rmse = math.sqrt(mean_squared_error(\n        trainY[:, 0], train_predict[:, 0]))\n    test_rmse = math.sqrt(mean_squared_error(testY[:, 0], test_predict[:, 0]))\n    # Print RMSE\n    print('Train RMSE: %.3f RMSE' % (train_rmse))\n    print('Test RMSE: %.3f RMSE' % (test_rmse))\n    return train_rmse, test_rmse\n\n\n\n\nRecurrent Neural Network\nThe RNN models that are constructed and trained using TensorFlow and Keras. This model is trained for five hidden layers, one dense layer, and the activation function hyperbolic tangent. A dropout rate of 0.2 is used to reduce the risk of overfitting. The model are trained for 20 epochs and generated validation loss plots to compare the performance of the regularized and non-regularized models. The second RNN model was identical to the first, except for the addition of kernel regularization.\n\nRNNRNN with Regularization\n\n\n\n\nCode\n# Create a RNN\ndef create_RNN(hidden_units, dense_units, input_shape, activation,dropout_rate=0.2,kernel_regularizer=None):\n    model = Sequential()\n    # Create a simple neural network layer\n    model.add(SimpleRNN(hidden_units, input_shape=input_shape,\n              activation=activation[0]))\n    # Add a dense layer (only one, more layers would make it a deep neural net)\n    model.add(Dense(units=dense_units,\n              activation=activation[1],\n              kernel_regularizer=kernel_regularizer))\n    # Add layer dropout\n    model.add(Dropout(dropout_rate))\n    \n    # Compile the model and optimize on mean squared error\n    model.compile(loss='mean_squared_error', optimizer='adam')\n    return model\n  \n# Create a recurrent neural network\nmodel = create_RNN(hidden_units=5, dense_units=1, input_shape=(lag, Xtrain.shape[-1]),\n                   activation=['tanh', 'tanh'], dropout_rate = 0.2)\nhistory = model.fit(Xtrain, Ytrain, epochs=20, batch_size=1, verbose=0,validation_data=(Xval, Yval))\nplot_model(history, 'Recurrent Neural Network Model')\n\nyhat_d = [x[0] for x in model.predict(Xval, verbose=0)]\ny = [y[0] for y in Yval]\n\ntrain_predict = model.predict(Xtrain, verbose=0)\ntest_predict = model.predict(Xval, verbose=0)\n\n# Print error\ntrain_rmse, test_rmse = print_error(Ytrain, Yval, train_predict, test_predict)\nrmse_table = {\n    'model': ['Recurrent Neural Network'],\n    'training_rmse': [train_rmse],\n    'testing_rmse': [test_rmse]\n}\n\n\nTrain RMSE: 0.436 RMSE\nTest RMSE: 1.895 RMSE\n\n\n\n\n\n\n\n\n\nCode\n# Create a recurrent neural network with regularization\n\nmodel = create_RNN(hidden_units=5, dense_units=1, input_shape=(lag, Xtrain.shape[-1]),\n                   activation=['tanh', 'tanh'], kernel_regularizer=regularizers.L1L2(l1=1e-5, l2=1e-4), dropout_rate = 0.2)\nhistory = model.fit(Xtrain, Ytrain, epochs=20, batch_size=1, verbose=0, validation_data=(Xval, Yval))\nplot_model(history, 'Recurrent Neural Network Model (with L1L2 Regularization)')\n\nyhat_d_reg = [x[0] for x in model.predict(Xval, verbose=0)]\n\ntrain_predict = model.predict(Xtrain, verbose=0)\ntest_predict = model.predict(Xval, verbose=0)\n\n# Print error\ntrain_rmse, test_rmse = print_error(Ytrain, Yval, train_predict, test_predict)\nrmse_table['model'].append(\n    'Recurrent Neural Network (with L1L2 Regularization)')\nrmse_table['training_rmse'].append(train_rmse)\nrmse_table['testing_rmse'].append(test_rmse)\n\n\nTrain RMSE: 0.432 RMSE\nTest RMSE: 1.895 RMSE\n\n\n\n\n\n\n\n\nThe second plot illustrates the impact of regularization on the training loss. Compared to the original plot, there are significant differences. The regularization technique had a substantial effect on the testing error of the model. The model trained without regularization have training error of 0.442 RMSE and testing error of 1.896 RMSE which are the same as with regularization.\n\n\nGRU Neural Network\nA GRU neural network is a powerful machine learning tool widely adopted in various applications. Its key feature is the recurrent gated unit, a type of memory cell that can store information over a data sequence. The GRU neural network is five hidden layers, one dense layer, and the hyperbolic tangent activation function. The model also includes kernel regularization to help address the issue of overfitting. The models are trained for 20 epochs and validation loss plots were generated to compare the performance of the regularized and non-regularized models. Overall, the GRU neural network has shown great promise in accurately predicting future movements in the sector.\n\nGRUGRU with Regularization\n\n\n\n\nCode\n# Create a GRU Neural Network\ndef create_GRU(hidden_units, dense_units, input_shape, activation,dropout_rate=0.2, kernel_regularizer=None):\n    model = Sequential()\n    # Create a simple GRU neural network layer\n    model.add(GRU(hidden_units, input_shape=input_shape,\n              activation=activation[0]))\n    # Add a dense layer (only one, more layers would make it a deep neural net)\n    model.add(Dense(units=dense_units,\n              activation=activation[1], kernel_regularizer=kernel_regularizer))\n    # Add layer dropout\n    model.add(Dropout(dropout_rate))\n    # Compile the model and optimize on mean squared error\n    model.compile(loss='mean_squared_error', optimizer='sgd')\n    return model\n\n# Training and evaluating a GRU-based model\nmodel = create_GRU(hidden_units=5, dense_units=1, input_shape=(lag, Xtrain.shape[-1]),\n                   activation=['tanh', 'relu'], dropout_rate = 0.2)\nhistory = model.fit(Xtrain, Ytrain, epochs=20, batch_size=1, verbose=0)\nplot_model(history, 'GRU Model')\n\nyhat_gru = [x[0] for x in model.predict(Xval, verbose=0)]\n\ntrain_predict = model.predict(Xtrain, verbose=0)\ntest_predict = model.predict(Xval, verbose=0)\n\n# Print error\ntrain_rmse, test_rmse = print_error(Ytrain, Yval, train_predict, test_predict)\nrmse_table['model'].append('GRU Neural Network')\nrmse_table['training_rmse'].append(train_rmse)\nrmse_table['testing_rmse'].append(test_rmse)\n\n\nTrain RMSE: 0.995 RMSE\nTest RMSE: 2.655 RMSE\n\n\n\n\n\n\n\n\n\nCode\n# Training and evaluating a GRU-based model with regularization\nmodel = create_GRU(hidden_units=5, dense_units=1, input_shape=(lag, Xtrain.shape[-1]),\n                   activation=['tanh', 'relu'],dropout_rate = 0.2,  kernel_regularizer=regularizers.L1L2(l1=1e-5, l2=1e-4))\nhistory = model.fit(Xtrain, Ytrain, epochs=20, batch_size=1, verbose=0)\nplot_model(history, 'GRU Model (with L1L2 Regularization)')\n\nyhat_gru_reg = [x[0] for x in model.predict(Xval, verbose=0)]\n\ntrain_predict = model.predict(Xtrain, verbose=0)\ntest_predict = model.predict(Xval, verbose=0)\n\n# Print error\ntrain_rmse, test_rmse = print_error(Ytrain, Yval, train_predict, test_predict)\nrmse_table['model'].append('GRU Neural Network (with L1L2 Regularization)')\nrmse_table['training_rmse'].append(train_rmse)\nrmse_table['testing_rmse'].append(test_rmse)\n\n\nTrain RMSE: 0.995 RMSE\nTest RMSE: 2.655 RMSE\n\n\n\n\n\n\n\n\nThe two plots show the training loss for the GRU model, one with regularization and one without. The plot with regularization is similar to without regularization plot. The model without regularization had a train RMSE of 0.726 and a test RMSE of 1.699, the values are the lower when compared to with regularization.\n\n\nLSTM Neural Network\nThe LSTM (Long Short-Term Memory) neural network is another powerful machine learning tool that has been widely adopted in various applications, particularly in tasks involving sequential data.The LSTM neural network architecture typically consists of multiple LSTM layers, followed by one or more dense layers and an activation function such as the sigmoid or hyperbolic tangent. Similar to the GRU neural network, the LSTM model may also include kernel regularization to address the issue of overfitting. In order to evaluate the performance of the LSTM model, the model can be trained on a dataset for a specified number of epochs, and validation loss plots can be generated to compare the performance of the regularized and non-regularized models.\n\nLSTMLSTM with Regularization\n\n\n\n\nCode\n# Create a LSTM Neural Network\ndef create_LSTM(hidden_units, dense_units, input_shape, activation,dropout_rate = 0.2, kernel_regularizer=None):\n    model = Sequential()\n    # Create a simple long short term memory neural network\n    model.add(LSTM(hidden_units,\n              activation=activation[0], input_shape=input_shape))\n    # Add a dense layer (only one, more layers would make it a deep neural net)\n    model.add(Dense(units=dense_units,\n              activation=activation[1], kernel_regularizer=kernel_regularizer))\n    # Add layer dropout\n    model.add(Dropout(dropout_rate))\n    # Compile the model and optimize on mean squared error\n    model.compile(optimizer=\"RMSprop\", loss='mae')\n    return model\n\n# Create an LSTM neural network\nmodel = create_LSTM(hidden_units=5, dense_units=1, input_shape=(lag, Xtrain.shape[-1]),\n                    activation=['tanh', 'linear'], dropout_rate = 0.2)\nhistory = model.fit(Xtrain, Ytrain, epochs=20, batch_size=1, verbose=0)\nplot_model(history, 'LSTM Model')\n\nyhat_lstm = [x[0] for x in model.predict(Xval, verbose=0)]\n\ntrain_predict = model.predict(Xtrain, verbose=0)\ntest_predict = model.predict(Xval, verbose=0)\n\n# Print error\ntrain_rmse, test_rmse = print_error(Ytrain, Yval, train_predict, test_predict)\nrmse_table['model'].append('LSTM Neural Network')\nrmse_table['training_rmse'].append(train_rmse)\nrmse_table['testing_rmse'].append(test_rmse)\n\n\nTrain RMSE: 0.232 RMSE\nTest RMSE: 0.963 RMSE\n\n\n\n\n\n\n\n\n\nCode\n# Create an LSTM neural network with regularization\nmodel = create_LSTM(hidden_units=5, dense_units=1, input_shape=(lag, Xtrain.shape[-1]),\n                    activation=['tanh', 'linear'], dropout_rate = 0.2, kernel_regularizer=regularizers.L1L2(l1=1e-5, l2=1e-4))\nhistory = model.fit(Xtrain, Ytrain, epochs=20, batch_size=1, verbose=0)\nplot_model(history, 'LSTM Model (with L1L2 Regularization)')\n\n\nyhat_lstm_reg = [x[0] for x in model.predict(Xval, verbose=0)]\n\ntrain_predict = model.predict(Xtrain, verbose=0)\ntest_predict = model.predict(Xval, verbose=0)\n\n# Print error\ntrain_rmse, test_rmse = print_error(Ytrain, Yval, train_predict, test_predict)\nrmse_table['model'].append('LSTM Neural Network (with L1L2 Regularization)')\nrmse_table['training_rmse'].append(train_rmse)\nrmse_table['testing_rmse'].append(test_rmse)\n\n\nTrain RMSE: 0.228 RMSE\nTest RMSE: 1.029 RMSE\n\n\n\n\n\n\n\n\nThe two plots show the training loss for the LSTM model, one with regularization and one without. The plot with regularization shows less fluctuation, indicating that overfitting has been reduced. The model without regularization had a train RMSE of 0.222 and a test RMSE of 0.985, while the model with regularization had a slightly lower for both the train and test RMSE values which are 0.201 and 0.934 respectively.\n\n\nForecast\n\n\nCode\n# Creating the frame to store both predictions\ndays = df['Date'].values[-len(y):]\nframe = pd.concat([\n    pd.DataFrame({'day': days, 'price': y, 'type': 'original'}),\n    pd.DataFrame({'day': days, 'price': yhat_d, 'type': 'rnn_forecast'}),\n    pd.DataFrame({'day': days, 'price': yhat_gru, 'type': 'gru_forecast'}),\n    pd.DataFrame({'day': days, 'price': yhat_lstm, 'type': 'lstm_forecast'})\n])\n# Creating the unscaled values column\nframe['price_absolute'] = [(x * train_std) + train_mean for x in frame['price']]\n# Pivoting\npivoted = frame.pivot_table(index='day', columns='type')\npivoted.columns = ['_'.join(x).strip() for x in pivoted.columns.values]\n\nplt.figure(figsize=(12, 10))\nplt.plot(pivoted.index, pivoted.price_absolute_original,\n         color='#8B1874', label='original')\nplt.plot(pivoted.index, pivoted.price_absolute_rnn_forecast,\n         color='#F79540', label='RNN Forecast', alpha=0.6)\nplt.plot(pivoted.index, pivoted.price_absolute_gru_forecast,\n         color='#B71375', label='GRU Forecast', alpha=0.6)\nplt.plot(pivoted.index, pivoted.price_absolute_lstm_forecast,\n         color='#FC4F00', label='LSTM Forecast', alpha=0.6)\nplt.title('Energy Sector Fund Forecasts')\nplt.legend()\nplt.show()\n\n\n\n\n\nWhen comparing the predictions of the deep learning models with the original time series plot, we can observe that the LSTM model is the closest to the actual values. The GRU model is pretty accurate at the end, but they there far from the actual plot at the start. LSTM model is better when compared to others, but when we look at the LSTM model, the model is not accurate, i.e., it is not the same as the actual plot.\n\n\nModel Comparison\n\n\nCode\nrmse_df = pd.DataFrame(rmse_table)\nrmse_df\n\n\n\n\n\n\n  \n    \n      \n      model\n      training_rmse\n      testing_rmse\n    \n  \n  \n    \n      0\n      Recurrent Neural Network\n      0.436496\n      1.895286\n    \n    \n      1\n      Recurrent Neural Network (with L1L2 Regulariza...\n      0.431665\n      1.894788\n    \n    \n      2\n      GRU Neural Network\n      0.995234\n      2.655007\n    \n    \n      3\n      GRU Neural Network (with L1L2 Regularization)\n      0.995234\n      2.655007\n    \n    \n      4\n      LSTM Neural Network\n      0.231620\n      0.963423\n    \n    \n      5\n      LSTM Neural Network (with L1L2 Regularization)\n      0.227795\n      1.029036\n    \n  \n\n\n\n\nThe Recurrent Neural Network (RNN) model and the RNN with L1L2 Regularization have similar training and testing RMSE values, with the regularized version performing slightly better on the testing data.\nThe GRU Neural Network has a low testing RMSE but a relatively high training RMSE, indicating potential overfitting to the training data. The GRU with L1L2 Regularization has a higher testing RMSE than the non-regularized version, suggesting that regularization may not be helpful in this case.\nThe LSTM Neural Network and the LSTM with L1L2 Regularization both have low training and testing RMSE values, with the regularized version performing slightly better on the testing data.\nOverall, the LSTM Neural Network with L1L2 Regularization appears to perform the best on this dataset, with the lowest testing RMSE value. The GRU Neural Network also performs well on the testing data, but may be overfitting to the training data.\n\n\nComparison of deep learning models with traditional single variable time series\nThe deep learning models (RNN, GRU, LSTM) perform better than the ARIMA model in both training and testing RMSE values for the Energy Sector Fund. Among the deep learning models, the LSTM model without regularization has the lowest testing RMSE value of 1.032129, when compared to ARIMA Model which is 2.118625, indicating its better performance in predicting the market prices of the Energy Sector Fund compared to the other deep learning models and ARIMA model."
  },
  {
    "objectID": "deep_learning_sector_xlc.html",
    "href": "deep_learning_sector_xlc.html",
    "title": "",
    "section": "",
    "text": "Deep Learning for Communication Services Sector Fund\nThe Communication Services sector comprises companies that provide non-essential goods and services, such as clothing, entertainment, and luxury items. These companies tend to be more sensitive to economic cycles and consumer sentiment than companies in other sectors. As with other stock market, predicting the future movements of the Communication Services Sector Fund can be challenging due to the complex and dynamic nature of the market. These models range from traditional time-series models to more advanced deep learning models. By leveraging the power of Deep Learning and applying techniques such as RNN, LSTM, GRU, and regularization, we can create a model that accurately predicts the future movements of the sector. Such a model can provide valuable insights for investors and traders looking to make informed decisions in the stock market, and help them stay ahead of the curve in the ever-changing landscape of the technology and biotech industries.\nIn order to apply the deep learning methods, the time series data is first split into training and testing sets, taking into account the time component. The training data constitutes 75% of the total data while the rest is the testing data. Furthermore, to ensure that the values are on the same scale, the data is scaled, which is crucial for deep learning models. The mean and standard deviation of the training data are computed and then utilized to normalize both the training and testing sets. This approach ensures that the models are trained on a standardized dataset, enabling them to make accurate predictions.\n\n\nCode\n# libraries\nimport numpy as np\nimport pandas as pd\nimport math\nfrom sklearn.metrics import mean_squared_error\nfrom matplotlib import pyplot as plt\nfrom keras.models import Sequential\nfrom keras.layers import Dropout\nfrom keras.layers import Dense, SimpleRNN, LSTM, GRU\nfrom keras import regularizers\nimport warnings\nwarnings.filterwarnings('ignore')\n\n# import data\ndf = pd.read_csv('./DATA/CLEANED DATA/XLC_raw_data.csv')\n# clean data and get final data for anlaysis\ndf = df[['Date', 'XLC.Adjusted']]\ndf['Date'] = pd.to_datetime(df['Date'])\ndf['XLC.Adjusted'] = pd.to_numeric(df['XLC.Adjusted'], errors='coerce')\ndf = df.dropna(subset=['XLC.Adjusted'])\ndf.sort_values('Date', inplace=True, ascending=True)\ndf = df.reset_index(drop=True)\ndef create_X_Y(ts: np.array, lag=1, n_ahead=1, target_index=0) -> tuple:\n    \"\"\"\n    A method to create X and Y matrix from a time series array for the training of \n    deep learning models \n    \"\"\"\n    # Extracting the number of features that are passed from the array\n    n_features = ts.shape[1]\n\n    # Creating placeholder lists\n    X, Y = [], []\n\n    if len(ts) - lag <= 0:\n        X.append(ts)\n    else:\n        for i in range(len(ts) - lag - n_ahead):\n            Y.append(ts[(i + lag):(i + lag + n_ahead), target_index])\n            X.append(ts[i:(i + lag)])\n\n    X, Y = np.array(X), np.array(Y)\n\n    # Reshaping the X array to an RNN input shape\n    X = np.reshape(X, (X.shape[0], lag, n_features))\n\n    return X, Y\n\nts = df[['XLC.Adjusted']].values\nnp.random.seed(123)\nnrows = ts.shape[0]\ntest_share = 0.25\n# Spliting into train and test sets\ntrain = ts[0:int(nrows * (1 - test_share))]\ntest = ts[int(nrows * (1 - test_share)):]\n\n# Scaling the data\ntrain_mean = train.mean()\ntrain_std = train.std()\ntrain = (train - train_mean) / train_std\ntest = (test - train_mean) / train_std\n\n# Creating the final scaled frame\nts_s = np.concatenate([train, test])\n\nlag = 12\nahead = 3\n\n# Creating the X and Y for training\nX, Y = create_X_Y(ts_s, lag=lag, n_ahead=ahead)\n\nXtrain = X[0:int(X.shape[0] * (1 - test_share))]\nYtrain = Y[0:int(X.shape[0] * (1 - test_share))]\n\nXval = X[int(X.shape[0] * (1 - test_share)):]\nYval = Y[int(X.shape[0] * (1 - test_share)):]\n\n\n\n\ndef plot_model(history, model_title):\n    loss = history.history['loss']\n    epochs = range(1, len(loss) + 1)\n    plt.figure()\n    plt.plot(epochs, loss, 'b', label='Training loss', color = \"gray\")\n    plt.title(f'{model_title} Training loss ')\n    plt.legend()\n    \n\n\ndef print_error(trainY, testY, train_predict, test_predict):\n    # Error of predictions\n    train_rmse = math.sqrt(mean_squared_error(\n        trainY[:, 0], train_predict[:, 0]))\n    test_rmse = math.sqrt(mean_squared_error(testY[:, 0], test_predict[:, 0]))\n    # Print RMSE\n    print('Train RMSE: %.3f RMSE' % (train_rmse))\n    print('Test RMSE: %.3f RMSE' % (test_rmse))\n    return train_rmse, test_rmse\n\n\n\n\nRecurrent Neural Network\nThe RNN models that are constructed and trained using TensorFlow and Keras. This model is trained for five hidden layers, one dense layer, and the activation function hyperbolic tangent. A dropout rate of 0.2 is used to reduce the risk of overfitting. The model are trained for 20 epochs and generated validation loss plots to compare the performance of the regularized and non-regularized models. The second RNN model was identical to the first, except for the addition of kernel regularization.\n\nRNNRNN with Regularization\n\n\n\n\nCode\n# Create a RNN\ndef create_RNN(hidden_units, dense_units, input_shape, activation,dropout_rate=0.2,kernel_regularizer=None):\n    model = Sequential()\n    # Create a simple neural network layer\n    model.add(SimpleRNN(hidden_units, input_shape=input_shape,\n              activation=activation[0]))\n    # Add a dense layer (only one, more layers would make it a deep neural net)\n    model.add(Dense(units=dense_units,\n              activation=activation[1],\n              kernel_regularizer=kernel_regularizer))\n    # Add layer dropout\n    model.add(Dropout(dropout_rate))\n    \n    # Compile the model and optimize on mean squared error\n    model.compile(loss='mean_squared_error', optimizer='adam')\n    return model\n  \n# Create a recurrent neural network\nmodel = create_RNN(hidden_units=5, dense_units=1, input_shape=(lag, Xtrain.shape[-1]),\n                   activation=['tanh', 'tanh'], dropout_rate = 0.2)\nhistory = model.fit(Xtrain, Ytrain, epochs=20, batch_size=1, verbose=0,validation_data=(Xval, Yval))\nplot_model(history, 'Recurrent Neural Network Model')\n\nyhat_d = [x[0] for x in model.predict(Xval, verbose=0)]\ny = [y[0] for y in Yval]\n\ntrain_predict = model.predict(Xtrain, verbose=0)\ntest_predict = model.predict(Xval, verbose=0)\n\n# Print error\ntrain_rmse, test_rmse = print_error(Ytrain, Yval, train_predict, test_predict)\nrmse_table = {\n    'model': ['Recurrent Neural Network'],\n    'training_rmse': [train_rmse],\n    'testing_rmse': [test_rmse]\n}\n\n\nTrain RMSE: 0.338 RMSE\nTest RMSE: 0.165 RMSE\n\n\n\n\n\n\n\n\n\nCode\n# Create a recurrent neural network with regularization\n\nmodel = create_RNN(hidden_units=5, dense_units=1, input_shape=(lag, Xtrain.shape[-1]),\n                   activation=['tanh', 'tanh'], kernel_regularizer=regularizers.L1L2(l1=1e-5, l2=1e-4), dropout_rate = 0.2)\nhistory = model.fit(Xtrain, Ytrain, epochs=20, batch_size=1, verbose=0, validation_data=(Xval, Yval))\nplot_model(history, 'Recurrent Neural Network Model (with L1L2 Regularization)')\n\nyhat_d_reg = [x[0] for x in model.predict(Xval, verbose=0)]\n\ntrain_predict = model.predict(Xtrain, verbose=0)\ntest_predict = model.predict(Xval, verbose=0)\n\n# Print error\ntrain_rmse, test_rmse = print_error(Ytrain, Yval, train_predict, test_predict)\nrmse_table['model'].append(\n    'Recurrent Neural Network (with L1L2 Regularization)')\nrmse_table['training_rmse'].append(train_rmse)\nrmse_table['testing_rmse'].append(test_rmse)\n\n\nTrain RMSE: 0.342 RMSE\nTest RMSE: 0.184 RMSE\n\n\n\n\n\n\n\n\nThe second plot illustrates the impact of regularization on the training loss. Compared to the original plot, there are significant differences. The regularization technique had a substantial effect on the testing error of the model. The model trained without regularization has a slightly lower training error of 0.331 RMSE but the testing error has a slightly higher RMSE value when compared to with regularization.\n\n\nGRU Neural Network\nA GRU neural network is a powerful machine learning tool widely adopted in various applications. Its key feature is the recurrent gated unit, a type of memory cell that can store information over a data sequence. The GRU neural network is five hidden layers, one dense layer, and the hyperbolic tangent activation function. The model also includes kernel regularization to help address the issue of overfitting. The models are trained for 20 epochs and validation loss plots were generated to compare the performance of the regularized and non-regularized models. Overall, the GRU neural network has shown great promise in accurately predicting future movements in the sector.\n\nGRUGRU with Regularization\n\n\n\n\nCode\n# Create a GRU Neural Network\ndef create_GRU(hidden_units, dense_units, input_shape, activation,dropout_rate=0.2, kernel_regularizer=None):\n    model = Sequential()\n    # Create a simple GRU neural network layer\n    model.add(GRU(hidden_units, input_shape=input_shape,\n              activation=activation[0]))\n    # Add a dense layer (only one, more layers would make it a deep neural net)\n    model.add(Dense(units=dense_units,\n              activation=activation[1], kernel_regularizer=kernel_regularizer))\n    # Add layer dropout\n    model.add(Dropout(dropout_rate))\n    # Compile the model and optimize on mean squared error\n    model.compile(loss='mean_squared_error', optimizer='sgd')\n    return model\n\n# Training and evaluating a GRU-based model\nmodel = create_GRU(hidden_units=5, dense_units=1, input_shape=(lag, Xtrain.shape[-1]),\n                   activation=['tanh', 'relu'], dropout_rate = 0.2)\nhistory = model.fit(Xtrain, Ytrain, epochs=20, batch_size=1, verbose=0)\nplot_model(history, 'GRU Model')\n\nyhat_gru = [x[0] for x in model.predict(Xval, verbose=0)]\n\ntrain_predict = model.predict(Xtrain, verbose=0)\ntest_predict = model.predict(Xval, verbose=0)\n\n# Print error\ntrain_rmse, test_rmse = print_error(Ytrain, Yval, train_predict, test_predict)\nrmse_table['model'].append('GRU Neural Network')\nrmse_table['training_rmse'].append(train_rmse)\nrmse_table['testing_rmse'].append(test_rmse)\n\n\nTrain RMSE: 0.622 RMSE\nTest RMSE: 0.375 RMSE\n\n\n\n\n\n\n\n\n\nCode\n# Training and evaluating a GRU-based model with regularization\nmodel = create_GRU(hidden_units=5, dense_units=1, input_shape=(lag, Xtrain.shape[-1]),\n                   activation=['tanh', 'relu'],dropout_rate = 0.2,  kernel_regularizer=regularizers.L1L2(l1=1e-5, l2=1e-4))\nhistory = model.fit(Xtrain, Ytrain, epochs=20, batch_size=1, verbose=0)\nplot_model(history, 'GRU Model (with L1L2 Regularization)')\n\nyhat_gru_reg = [x[0] for x in model.predict(Xval, verbose=0)]\n\ntrain_predict = model.predict(Xtrain, verbose=0)\ntest_predict = model.predict(Xval, verbose=0)\n\n# Print error\ntrain_rmse, test_rmse = print_error(Ytrain, Yval, train_predict, test_predict)\nrmse_table['model'].append('GRU Neural Network (with L1L2 Regularization)')\nrmse_table['training_rmse'].append(train_rmse)\nrmse_table['testing_rmse'].append(test_rmse)\n\n\nTrain RMSE: 0.621 RMSE\nTest RMSE: 0.375 RMSE\n\n\n\n\n\n\n\n\nThe two plots show the training loss for the GRU model, one with regularization and one without. The plot with regularization is similar to without regularization plot. The model without regularization had a train RMSE of 1.004 and a test RMSE of 0.634, the values are the higher when compared to with regularization.\n\n\nLSTM Neural Network\nThe LSTM (Long Short-Term Memory) neural network is another powerful machine learning tool that has been widely adopted in various applications, particularly in tasks involving sequential data.The LSTM neural network architecture typically consists of multiple LSTM layers, followed by one or more dense layers and an activation function such as the sigmoid or hyperbolic tangent. Similar to the GRU neural network, the LSTM model may also include kernel regularization to address the issue of overfitting. In order to evaluate the performance of the LSTM model, the model can be trained on a dataset for a specified number of epochs, and validation loss plots can be generated to compare the performance of the regularized and non-regularized models.\n\nLSTMLSTM with Regularization\n\n\n\n\nCode\n# Create a LSTM Neural Network\ndef create_LSTM(hidden_units, dense_units, input_shape, activation,dropout_rate = 0.2, kernel_regularizer=None):\n    model = Sequential()\n    # Create a simple long short term memory neural network\n    model.add(LSTM(hidden_units,\n              activation=activation[0], input_shape=input_shape))\n    # Add a dense layer (only one, more layers would make it a deep neural net)\n    model.add(Dense(units=dense_units,\n              activation=activation[1], kernel_regularizer=kernel_regularizer))\n    # Add layer dropout\n    model.add(Dropout(dropout_rate))\n    # Compile the model and optimize on mean squared error\n    model.compile(optimizer=\"RMSprop\", loss='mae')\n    return model\n\n# Create an LSTM neural network\nmodel = create_LSTM(hidden_units=5, dense_units=1, input_shape=(lag, Xtrain.shape[-1]),\n                    activation=['tanh', 'linear'], dropout_rate = 0.2)\nhistory = model.fit(Xtrain, Ytrain, epochs=20, batch_size=1, verbose=0)\nplot_model(history, 'LSTM Model')\n\nyhat_lstm = [x[0] for x in model.predict(Xval, verbose=0)]\n\ntrain_predict = model.predict(Xtrain, verbose=0)\ntest_predict = model.predict(Xval, verbose=0)\n\n# Print error\ntrain_rmse, test_rmse = print_error(Ytrain, Yval, train_predict, test_predict)\nrmse_table['model'].append('LSTM Neural Network')\nrmse_table['training_rmse'].append(train_rmse)\nrmse_table['testing_rmse'].append(test_rmse)\n\n\nTrain RMSE: 0.203 RMSE\nTest RMSE: 0.138 RMSE\n\n\n\n\n\n\n\n\n\nCode\n# Create an LSTM neural network with regularization\nmodel = create_LSTM(hidden_units=5, dense_units=1, input_shape=(lag, Xtrain.shape[-1]),\n                    activation=['tanh', 'linear'], dropout_rate = 0.2, kernel_regularizer=regularizers.L1L2(l1=1e-5, l2=1e-4))\nhistory = model.fit(Xtrain, Ytrain, epochs=20, batch_size=1, verbose=0)\nplot_model(history, 'LSTM Model (with L1L2 Regularization)')\n\n\nyhat_lstm_reg = [x[0] for x in model.predict(Xval, verbose=0)]\n\ntrain_predict = model.predict(Xtrain, verbose=0)\ntest_predict = model.predict(Xval, verbose=0)\n\n# Print error\ntrain_rmse, test_rmse = print_error(Ytrain, Yval, train_predict, test_predict)\nrmse_table['model'].append('LSTM Neural Network (with L1L2 Regularization)')\nrmse_table['training_rmse'].append(train_rmse)\nrmse_table['testing_rmse'].append(test_rmse)\n\n\nTrain RMSE: 0.214 RMSE\nTest RMSE: 0.143 RMSE\n\n\n\n\n\n\n\n\nThe two plots show the training loss for the LSTM model, one with regularization and one without. The plot with regularization shows less fluctuation, indicating that overfitting has been reduced. The model without regularization had a train RMSE of 0.212 and a test RMSE of 0.141, while the model with regularization had a slightly lower for both the train and test RMSE values which are 0.202 and 0.135 respectively.\n\n\nForecast\n\n\nCode\n# Creating the frame to store both predictions\ndays = df['Date'].values[-len(y):]\nframe = pd.concat([\n    pd.DataFrame({'day': days, 'price': y, 'type': 'original'}),\n    pd.DataFrame({'day': days, 'price': yhat_d, 'type': 'rnn_forecast'}),\n    pd.DataFrame({'day': days, 'price': yhat_gru, 'type': 'gru_forecast'}),\n    pd.DataFrame({'day': days, 'price': yhat_lstm, 'type': 'lstm_forecast'})\n])\n# Creating the unscaled values column\nframe['price_absolute'] = [(x * train_std) + train_mean for x in frame['price']]\n# Pivoting\npivoted = frame.pivot_table(index='day', columns='type')\npivoted.columns = ['_'.join(x).strip() for x in pivoted.columns.values]\n\nplt.figure(figsize=(12, 10))\nplt.plot(pivoted.index, pivoted.price_absolute_original,\n         color='#8B1874', label='original')\nplt.plot(pivoted.index, pivoted.price_absolute_rnn_forecast,\n         color='#F79540', label='RNN Forecast', alpha=0.6)\nplt.plot(pivoted.index, pivoted.price_absolute_gru_forecast,\n         color='#B71375', label='GRU Forecast', alpha=0.6)\nplt.plot(pivoted.index, pivoted.price_absolute_lstm_forecast,\n         color='#FC4F00', label='LSTM Forecast', alpha=0.6)\nplt.title('Communication Services Sector Fund Forecasts')\nplt.legend()\nplt.show()\n\n\n\n\n\nWhen comparing the predictions of the deep learning models with the original time series plot, we can observe that the RNN model is the closest to the actual values. The LSTM model is almost the same as the RNN model, but both the models are accurate with the actual plot. GNU model are pretty far from the actual plot, which indicates that RNN model is better when compared to others. #### Model Comparison\n\n\nCode\nrmse_df = pd.DataFrame(rmse_table)\nrmse_df\n\n\n\n\n\n\n  \n    \n      \n      model\n      training_rmse\n      testing_rmse\n    \n  \n  \n    \n      0\n      Recurrent Neural Network\n      0.338436\n      0.165095\n    \n    \n      1\n      Recurrent Neural Network (with L1L2 Regulariza...\n      0.342128\n      0.184126\n    \n    \n      2\n      GRU Neural Network\n      0.621880\n      0.375411\n    \n    \n      3\n      GRU Neural Network (with L1L2 Regularization)\n      0.620603\n      0.375010\n    \n    \n      4\n      LSTM Neural Network\n      0.202773\n      0.137774\n    \n    \n      5\n      LSTM Neural Network (with L1L2 Regularization)\n      0.213630\n      0.142875\n    \n  \n\n\n\n\nthe recurrent neural network (RNN) and long short-term memory (LSTM) neural network models generally perform better than the gated recurrent unit (GRU) model. Additionally, the L1L2 regularization technique seems to improve the performance of some models, while not having a significant impact on others.\nIt’s important to note that these observations are based solely on the evaluation metrics you provided (training and testing RMSE), and that the performance of the models could depend on other factors such as the specific data being used, the model architecture and hyperparameters, and the optimization method used during training.\n\n\nComparison of deep learning models with traditional single variable time series\nThe deep learning models (RNN, GRU, LSTM) perform better than the ARIMA model in both training and testing RMSE values for the Communication Services Sector Fund. Among the deep learning models, the LSTM model without regularization has the lowest testing RMSE value of 2.118625, when compared to ARIMA Model which is 2.118625, indicating its better performance in predicting the market prices of the Communication Services Sector Fund compared to the other deep learning models and ARIMA model."
  },
  {
    "objectID": "deep_learning_sector_xlb.html",
    "href": "deep_learning_sector_xlb.html",
    "title": "",
    "section": "",
    "text": "Deep Learning for Materials Sector Fund\nThe materials sector fund comprises companies that are involved in the production and distribution of raw materials, such as metals, chemicals, and construction materials. These companies are often closely tied to the performance of the global economy and can be sensitive to changes in supply and demand dynamics. As with other stock market, predicting the future movements of the Materials Sector Fund can be challenging due to the complex and dynamic nature of the market. These models range from traditional time-series models to more advanced deep learning models. By leveraging the power of Deep Learning and applying techniques such as RNN, LSTM, GRU, and regularization, we can create a model that accurately predicts the future movements of the sector. Such a model can provide valuable insights for investors and traders looking to make informed decisions in the stock market, and help them stay ahead of the curve in the ever-changing landscape of the technology and biotech industries.\nIn order to apply the deep learning methods, the time series data is first split into training and testing sets, taking into account the time component. The training data constitutes 75% of the total data while the rest is the testing data. Furthermore, to ensure that the values are on the same scale, the data is scaled, which is crucial for deep learning models. The mean and standard deviation of the training data are computed and then utilized to normalize both the training and testing sets. This approach ensures that the models are trained on a standardized dataset, enabling them to make accurate predictions.\n\n\nCode\n# libraries\nimport numpy as np\nimport pandas as pd\nimport math\nfrom sklearn.metrics import mean_squared_error\nfrom matplotlib import pyplot as plt\nfrom keras.models import Sequential\nfrom keras.layers import Dropout\nfrom keras.layers import Dense, SimpleRNN, LSTM, GRU\nfrom keras import regularizers\nimport warnings\nwarnings.filterwarnings('ignore')\n\n# import data\ndf = pd.read_csv('./DATA/CLEANED DATA/XLB_raw_data.csv')\n# clean data and get final data for anlaysis\ndf = df[['Date', 'XLB.Adjusted']]\ndf['Date'] = pd.to_datetime(df['Date'])\ndf['XLB.Adjusted'] = pd.to_numeric(df['XLB.Adjusted'], errors='coerce')\ndf = df.dropna(subset=['XLB.Adjusted'])\ndf.sort_values('Date', inplace=True, ascending=True)\ndf = df.reset_index(drop=True)\ndef create_X_Y(ts: np.array, lag=1, n_ahead=1, target_index=0) -> tuple:\n    \"\"\"\n    A method to create X and Y matrix from a time series array for the training of \n    deep learning models \n    \"\"\"\n    # Extracting the number of features that are passed from the array\n    n_features = ts.shape[1]\n\n    # Creating placeholder lists\n    X, Y = [], []\n\n    if len(ts) - lag <= 0:\n        X.append(ts)\n    else:\n        for i in range(len(ts) - lag - n_ahead):\n            Y.append(ts[(i + lag):(i + lag + n_ahead), target_index])\n            X.append(ts[i:(i + lag)])\n\n    X, Y = np.array(X), np.array(Y)\n\n    # Reshaping the X array to an RNN input shape\n    X = np.reshape(X, (X.shape[0], lag, n_features))\n\n    return X, Y\n\nts = df[['XLB.Adjusted']].values\nnp.random.seed(123)\nnrows = ts.shape[0]\ntest_share = 0.25\n# Spliting into train and test sets\ntrain = ts[0:int(nrows * (1 - test_share))]\ntest = ts[int(nrows * (1 - test_share)):]\n\n# Scaling the data\ntrain_mean = train.mean()\ntrain_std = train.std()\ntrain = (train - train_mean) / train_std\ntest = (test - train_mean) / train_std\n\n# Creating the final scaled frame\nts_s = np.concatenate([train, test])\n\nlag = 12\nahead = 3\n\n# Creating the X and Y for training\nX, Y = create_X_Y(ts_s, lag=lag, n_ahead=ahead)\n\nXtrain = X[0:int(X.shape[0] * (1 - test_share))]\nYtrain = Y[0:int(X.shape[0] * (1 - test_share))]\n\nXval = X[int(X.shape[0] * (1 - test_share)):]\nYval = Y[int(X.shape[0] * (1 - test_share)):]\n\n\n\n\ndef plot_model(history, model_title):\n    loss = history.history['loss']\n    epochs = range(1, len(loss) + 1)\n    plt.figure()\n    plt.plot(epochs, loss, 'b', label='Training loss', color = \"gray\")\n    plt.title(f'{model_title} Training loss ')\n    plt.legend()\n    \n\n\ndef print_error(trainY, testY, train_predict, test_predict):\n    # Error of predictions\n    train_rmse = math.sqrt(mean_squared_error(\n        trainY[:, 0], train_predict[:, 0]))\n    test_rmse = math.sqrt(mean_squared_error(testY[:, 0], test_predict[:, 0]))\n    # Print RMSE\n    print('Train RMSE: %.3f RMSE' % (train_rmse))\n    print('Test RMSE: %.3f RMSE' % (test_rmse))\n    return train_rmse, test_rmse\n\n\n\n\nRecurrent Neural Network\nThe RNN models that are constructed and trained using TensorFlow and Keras. This model is trained for five hidden layers, one dense layer, and the activation function hyperbolic tangent. A dropout rate of 0.2 is used to reduce the risk of overfitting. The model are trained for 20 epochs and generated validation loss plots to compare the performance of the regularized and non-regularized models. The second RNN model was identical to the first, except for the addition of kernel regularization.\n\nRNNRNN with Regularization\n\n\n\n\nCode\n# Create a RNN\ndef create_RNN(hidden_units, dense_units, input_shape, activation,dropout_rate=0.2,kernel_regularizer=None):\n    model = Sequential()\n    # Create a simple neural network layer\n    model.add(SimpleRNN(hidden_units, input_shape=input_shape,\n              activation=activation[0]))\n    # Add a dense layer (only one, more layers would make it a deep neural net)\n    model.add(Dense(units=dense_units,\n              activation=activation[1],\n              kernel_regularizer=kernel_regularizer))\n    # Add layer dropout\n    model.add(Dropout(dropout_rate))\n    \n    # Compile the model and optimize on mean squared error\n    model.compile(loss='mean_squared_error', optimizer='adam')\n    return model\n  \n# Create a recurrent neural network\nmodel = create_RNN(hidden_units=5, dense_units=1, input_shape=(lag, Xtrain.shape[-1]),\n                   activation=['tanh', 'tanh'], dropout_rate = 0.2)\nhistory = model.fit(Xtrain, Ytrain, epochs=20, batch_size=1, verbose=0,validation_data=(Xval, Yval))\nplot_model(history, 'Recurrent Neural Network Model')\n\nyhat_d = [x[0] for x in model.predict(Xval, verbose=0)]\ny = [y[0] for y in Yval]\n\ntrain_predict = model.predict(Xtrain, verbose=0)\ntest_predict = model.predict(Xval, verbose=0)\n\n# Print error\ntrain_rmse, test_rmse = print_error(Ytrain, Yval, train_predict, test_predict)\nrmse_table = {\n    'model': ['Recurrent Neural Network'],\n    'training_rmse': [train_rmse],\n    'testing_rmse': [test_rmse]\n}\n\n\nTrain RMSE: 0.301 RMSE\nTest RMSE: 2.714 RMSE\n\n\n\n\n\n\n\n\n\nCode\n# Create a recurrent neural network with regularization\n\nmodel = create_RNN(hidden_units=5, dense_units=1, input_shape=(lag, Xtrain.shape[-1]),\n                   activation=['tanh', 'tanh'], kernel_regularizer=regularizers.L1L2(l1=1e-5, l2=1e-4), dropout_rate = 0.2)\nhistory = model.fit(Xtrain, Ytrain, epochs=20, batch_size=1, verbose=0, validation_data=(Xval, Yval))\nplot_model(history, 'Recurrent Neural Network Model (with L1L2 Regularization)')\n\nyhat_d_reg = [x[0] for x in model.predict(Xval, verbose=0)]\n\ntrain_predict = model.predict(Xtrain, verbose=0)\ntest_predict = model.predict(Xval, verbose=0)\n\n# Print error\ntrain_rmse, test_rmse = print_error(Ytrain, Yval, train_predict, test_predict)\nrmse_table['model'].append(\n    'Recurrent Neural Network (with L1L2 Regularization)')\nrmse_table['training_rmse'].append(train_rmse)\nrmse_table['testing_rmse'].append(test_rmse)\n\n\nTrain RMSE: 0.296 RMSE\nTest RMSE: 2.712 RMSE\n\n\n\n\n\n\n\n\nThe second plot illustrates the impact of regularization on the training loss. Compared to the original plot, there are significant differences. The regularization technique had a substantial effect on the testing error of the model. The model trained without regularization had a slightly higher training error of 0.300 RMSE and for the testing error, the values are the same 2.713 RMSE when compared to with regularization.\n\n\nGRU Neural Network\nA GRU neural network is a powerful machine learning tool widely adopted in various applications. Its key feature is the recurrent gated unit, a type of memory cell that can store information over a data sequence. The GRU neural network is five hidden layers, one dense layer, and the hyperbolic tangent activation function. The model also includes kernel regularization to help address the issue of overfitting. The models are trained for 20 epochs and validation loss plots were generated to compare the performance of the regularized and non-regularized models. Overall, the GRU neural network has shown great promise in accurately predicting future movements in the sector.\n\nGRUGRU with Regularization\n\n\n\n\nCode\n# Create a GRU Neural Network\ndef create_GRU(hidden_units, dense_units, input_shape, activation,dropout_rate=0.2, kernel_regularizer=None):\n    model = Sequential()\n    # Create a simple GRU neural network layer\n    model.add(GRU(hidden_units, input_shape=input_shape,\n              activation=activation[0]))\n    # Add a dense layer (only one, more layers would make it a deep neural net)\n    model.add(Dense(units=dense_units,\n              activation=activation[1], kernel_regularizer=kernel_regularizer))\n    # Add layer dropout\n    model.add(Dropout(dropout_rate))\n    # Compile the model and optimize on mean squared error\n    model.compile(loss='mean_squared_error', optimizer='sgd')\n    return model\n\n# Training and evaluating a GRU-based model\nmodel = create_GRU(hidden_units=5, dense_units=1, input_shape=(lag, Xtrain.shape[-1]),\n                   activation=['tanh', 'relu'], dropout_rate = 0.2)\nhistory = model.fit(Xtrain, Ytrain, epochs=20, batch_size=1, verbose=0)\nplot_model(history, 'GRU Model')\n\nyhat_gru = [x[0] for x in model.predict(Xval, verbose=0)]\n\ntrain_predict = model.predict(Xtrain, verbose=0)\ntest_predict = model.predict(Xval, verbose=0)\n\n# Print error\ntrain_rmse, test_rmse = print_error(Ytrain, Yval, train_predict, test_predict)\nrmse_table['model'].append('GRU Neural Network')\nrmse_table['training_rmse'].append(train_rmse)\nrmse_table['testing_rmse'].append(test_rmse)\n\n\nTrain RMSE: 0.998 RMSE\nTest RMSE: 3.626 RMSE\n\n\n\n\n\n\n\n\n\nCode\n# Training and evaluating a GRU-based model with regularization\nmodel = create_GRU(hidden_units=5, dense_units=1, input_shape=(lag, Xtrain.shape[-1]),\n                   activation=['tanh', 'relu'],dropout_rate = 0.2,  kernel_regularizer=regularizers.L1L2(l1=1e-5, l2=1e-4))\nhistory = model.fit(Xtrain, Ytrain, epochs=20, batch_size=1, verbose=0)\nplot_model(history, 'GRU Model (with L1L2 Regularization)')\n\nyhat_gru_reg = [x[0] for x in model.predict(Xval, verbose=0)]\n\ntrain_predict = model.predict(Xtrain, verbose=0)\ntest_predict = model.predict(Xval, verbose=0)\n\n# Print error\ntrain_rmse, test_rmse = print_error(Ytrain, Yval, train_predict, test_predict)\nrmse_table['model'].append('GRU Neural Network (with L1L2 Regularization)')\nrmse_table['training_rmse'].append(train_rmse)\nrmse_table['testing_rmse'].append(test_rmse)\n\n\nTrain RMSE: 0.998 RMSE\nTest RMSE: 3.626 RMSE\n\n\n\n\n\n\n\n\nThe two plots show the training loss for the GRU model, one with regularization and one without. The plot with regularization is similar to without regularization plot. The model without regularization had a train RMSE of 0.998 and a test RMSE of 3.626, the values are the higher when compared to with regularization.\n\n\nLSTM Neural Network\nThe LSTM (Long Short-Term Memory) neural network is another powerful machine learning tool that has been widely adopted in various applications, particularly in tasks involving sequential data.The LSTM neural network architecture typically consists of multiple LSTM layers, followed by one or more dense layers and an activation function such as the sigmoid or hyperbolic tangent. Similar to the GRU neural network, the LSTM model may also include kernel regularization to address the issue of overfitting. In order to evaluate the performance of the LSTM model, the model can be trained on a dataset for a specified number of epochs, and validation loss plots can be generated to compare the performance of the regularized and non-regularized models.\n\nLSTMLSTM with Regularization\n\n\n\n\nCode\n# Create a LSTM Neural Network\ndef create_LSTM(hidden_units, dense_units, input_shape, activation,dropout_rate = 0.2, kernel_regularizer=None):\n    model = Sequential()\n    # Create a simple long short term memory neural network\n    model.add(LSTM(hidden_units,\n              activation=activation[0], input_shape=input_shape))\n    # Add a dense layer (only one, more layers would make it a deep neural net)\n    model.add(Dense(units=dense_units,\n              activation=activation[1], kernel_regularizer=kernel_regularizer))\n    # Add layer dropout\n    model.add(Dropout(dropout_rate))\n    # Compile the model and optimize on mean squared error\n    model.compile(optimizer=\"RMSprop\", loss='mae')\n    return model\n\n# Create an LSTM neural network\nmodel = create_LSTM(hidden_units=5, dense_units=1, input_shape=(lag, Xtrain.shape[-1]),\n                    activation=['tanh', 'linear'], dropout_rate = 0.2)\nhistory = model.fit(Xtrain, Ytrain, epochs=20, batch_size=1, verbose=0)\nplot_model(history, 'LSTM Model')\n\nyhat_lstm = [x[0] for x in model.predict(Xval, verbose=0)]\n\ntrain_predict = model.predict(Xtrain, verbose=0)\ntest_predict = model.predict(Xval, verbose=0)\n\n# Print error\ntrain_rmse, test_rmse = print_error(Ytrain, Yval, train_predict, test_predict)\nrmse_table['model'].append('LSTM Neural Network')\nrmse_table['training_rmse'].append(train_rmse)\nrmse_table['testing_rmse'].append(test_rmse)\n\n\nTrain RMSE: 0.208 RMSE\nTest RMSE: 1.393 RMSE\n\n\n\n\n\n\n\n\n\nCode\n# Create an LSTM neural network with regularization\nmodel = create_LSTM(hidden_units=5, dense_units=1, input_shape=(lag, Xtrain.shape[-1]),\n                    activation=['tanh', 'linear'], dropout_rate = 0.2, kernel_regularizer=regularizers.L1L2(l1=1e-5, l2=1e-4))\nhistory = model.fit(Xtrain, Ytrain, epochs=20, batch_size=1, verbose=0)\nplot_model(history, 'LSTM Model (with L1L2 Regularization)')\n\n\nyhat_lstm_reg = [x[0] for x in model.predict(Xval, verbose=0)]\n\ntrain_predict = model.predict(Xtrain, verbose=0)\ntest_predict = model.predict(Xval, verbose=0)\n\n# Print error\ntrain_rmse, test_rmse = print_error(Ytrain, Yval, train_predict, test_predict)\nrmse_table['model'].append('LSTM Neural Network (with L1L2 Regularization)')\nrmse_table['training_rmse'].append(train_rmse)\nrmse_table['testing_rmse'].append(test_rmse)\n\n\nTrain RMSE: 0.191 RMSE\nTest RMSE: 1.579 RMSE\n\n\n\n\n\n\n\n\nThe two plots show the training loss for the LSTM model, one with regularization and one without. The plot with regularization shows less fluctuation, indicating that overfitting has been reduced. The model without regularization had a train RMSE of 0.225 and a test RMSE of 1.470, while the model with regularization had a slightly lower for the train and and higher for test RMSE values which are 0.202 and 1.601 respectively.\n\n\nForecast\n\n\nCode\n# Creating the frame to store both predictions\ndays = df['Date'].values[-len(y):]\nframe = pd.concat([\n    pd.DataFrame({'day': days, 'price': y, 'type': 'original'}),\n    pd.DataFrame({'day': days, 'price': yhat_d, 'type': 'rnn_forecast'}),\n    pd.DataFrame({'day': days, 'price': yhat_gru, 'type': 'gru_forecast'}),\n    pd.DataFrame({'day': days, 'price': yhat_lstm, 'type': 'lstm_forecast'})\n])\n# Creating the unscaled values column\nframe['price_absolute'] = [(x * train_std) + train_mean for x in frame['price']]\n# Pivoting\npivoted = frame.pivot_table(index='day', columns='type')\npivoted.columns = ['_'.join(x).strip() for x in pivoted.columns.values]\n\nplt.figure(figsize=(12, 10))\nplt.plot(pivoted.index, pivoted.price_absolute_original,\n         color='#8B1874', label='original')\nplt.plot(pivoted.index, pivoted.price_absolute_rnn_forecast,\n         color='#F79540', label='RNN Forecast', alpha=0.6)\nplt.plot(pivoted.index, pivoted.price_absolute_gru_forecast,\n         color='#B71375', label='GRU Forecast', alpha=0.6)\nplt.plot(pivoted.index, pivoted.price_absolute_lstm_forecast,\n         color='#FC4F00', label='LSTM Forecast', alpha=0.6)\nplt.title('Materials Sector Fund Forecasts')\nplt.legend()\nplt.show()\n\n\n\n\n\nWhen comparing the predictions of the deep learning models with the original time series plot, we can observe that the LSTM model is the closest to the actual values. The GRU model and RNN model are pretty far from the actual plot, which indicates that LSTM model is better when compared to others, but when we look at the LSTM model, the model is not accurate, i.e., it is not the same as the actual plot.\n\n\nModel Comparison\n\n\nCode\nrmse_df = pd.DataFrame(rmse_table)\nrmse_df\n\n\n\n\n\n\n  \n    \n      \n      model\n      training_rmse\n      testing_rmse\n    \n  \n  \n    \n      0\n      Recurrent Neural Network\n      0.300586\n      2.714221\n    \n    \n      1\n      Recurrent Neural Network (with L1L2 Regulariza...\n      0.296484\n      2.712243\n    \n    \n      2\n      GRU Neural Network\n      0.998346\n      3.625622\n    \n    \n      3\n      GRU Neural Network (with L1L2 Regularization)\n      0.998346\n      3.625622\n    \n    \n      4\n      LSTM Neural Network\n      0.208428\n      1.393025\n    \n    \n      5\n      LSTM Neural Network (with L1L2 Regularization)\n      0.190599\n      1.578561\n    \n  \n\n\n\n\nThe Recurrent Neural Network (RNN) model and the RNN with L1L2 Regularization have similar training and testing RMSE values, with the RNN with regularization performing slightly better on the testing data.\nThe GRU Neural Network and the GRU with L1L2 Regularization have identical training RMSE values, but the regularized version has a slightly lower testing RMSE.\nThe LSTM Neural Network and the LSTM with L1L2 Regularization both have low training and testing RMSE values, with the regularized version performing slightly worse on the testing data but still outperforming the other models.\nOverall, the LSTM Neural Network with L1L2 Regularization appears to perform the best on this dataset, with the lowest testing RMSE value.\n\n\nComparison of deep learning models with traditional single variable time series\nThe deep learning models (RNN, GRU, LSTM) perform better than the ARIMA model in both training and testing RMSE values for the Materials Sector Fund. Among the deep learning models, the LSTM model without regularization has the lowest testing RMSE value of 1.001375, when compared to ARIMA Model which is 2.118625, indicating its better performance in predicting the market prices of the Materials Sector Fund compared to the other deep learning models and ARIMA model."
  },
  {
    "objectID": "deep_learning_index_sp500.html",
    "href": "deep_learning_index_sp500.html",
    "title": "",
    "section": "",
    "text": "Deep Learning for S&P 500 Index Stock Price\nThe S&P 500 Index, which is composed of 500 large-cap stocks from different industries, is one of the most widely used benchmarks for the performance of the US stock market. While predicting the future movements of the S&P 500 Index is a challenging task, various models have been developed to forecast its future movements. These models range from traditional time-series models to more advanced deep learning models. By leveraging the power of Deep Learning and applying techniques such as RNN, LSTM, GRU, and regularization, we can create a model that accurately predicts the future movements of the S&P 500 Index. Such a model can provide valuable insights for investors and traders looking to make informed decisions in the stock market, and help them stay ahead of the curve in the ever-changing landscape of the technology and biotech industries.\nIn order to apply the deep learning methods, the time series data is first split into training and testing sets, taking into account the time component. The training data constitutes 75% of the total data while the rest is the testing data. Furthermore, to ensure that the values are on the same scale, the data is scaled, which is crucial for deep learning models. The mean and standard deviation of the training data are computed and then utilized to normalize both the training and testing sets. This approach ensures that the models are trained on a standardized dataset, enabling them to make accurate predictions.\n\n\nCode\n# libraries\nimport numpy as np\nimport pandas as pd\nimport math\nfrom sklearn.metrics import mean_squared_error\nfrom matplotlib import pyplot as plt\nfrom keras.models import Sequential\nfrom keras.layers import Dropout\nfrom keras.layers import Dense, SimpleRNN, LSTM, GRU\nfrom keras import regularizers\nimport warnings\nwarnings.filterwarnings('ignore')\n\n# import data\ndf = pd.read_csv('./DATA/CLEANED DATA/sp500_raw_data.csv')\n# clean data and get final data for anlaysis\ndf = df[['Date', 'GSPC.Adjusted']]\ndf['Date'] = pd.to_datetime(df['Date'])\ndf['GSPC.Adjusted'] = pd.to_numeric(df['GSPC.Adjusted'], errors='coerce')\ndf = df.dropna(subset=['GSPC.Adjusted'])\ndf.sort_values('Date', inplace=True, ascending=True)\ndf = df.reset_index(drop=True)\ndef create_X_Y(ts: np.array, lag=1, n_ahead=1, target_index=0) -> tuple:\n    \"\"\"\n    A method to create X and Y matrix from a time series array for the training of \n    deep learning models \n    \"\"\"\n    # Extracting the number of features that are passed from the array\n    n_features = ts.shape[1]\n\n    # Creating placeholder lists\n    X, Y = [], []\n\n    if len(ts) - lag <= 0:\n        X.append(ts)\n    else:\n        for i in range(len(ts) - lag - n_ahead):\n            Y.append(ts[(i + lag):(i + lag + n_ahead), target_index])\n            X.append(ts[i:(i + lag)])\n\n    X, Y = np.array(X), np.array(Y)\n\n    # Reshaping the X array to an RNN input shape\n    X = np.reshape(X, (X.shape[0], lag, n_features))\n\n    return X, Y\n\nts = df[['GSPC.Adjusted']].values\nnp.random.seed(123)\nnrows = ts.shape[0]\ntest_share = 0.25\n# Spliting into train and test sets\ntrain = ts[0:int(nrows * (1 - test_share))]\ntest = ts[int(nrows * (1 - test_share)):]\n\n# Scaling the data\ntrain_mean = train.mean()\ntrain_std = train.std()\ntrain = (train - train_mean) / train_std\ntest = (test - train_mean) / train_std\n\n# Creating the final scaled frame\nts_s = np.concatenate([train, test])\n\nlag = 12\nahead = 3\n\n# Creating the X and Y for training\nX, Y = create_X_Y(ts_s, lag=lag, n_ahead=ahead)\n\nXtrain = X[0:int(X.shape[0] * (1 - test_share))]\nYtrain = Y[0:int(X.shape[0] * (1 - test_share))]\n\nXval = X[int(X.shape[0] * (1 - test_share)):]\nYval = Y[int(X.shape[0] * (1 - test_share)):]\n\n\n\n\ndef plot_model(history, model_title):\n    loss = history.history['loss']\n    epochs = range(1, len(loss) + 1)\n    plt.figure()\n    plt.plot(epochs, loss, 'b', label='Training loss', color = \"gray\")\n    plt.title(f'{model_title} Training loss ')\n    plt.legend()\n    \n\n\ndef print_error(trainY, testY, train_predict, test_predict):\n    # Error of predictions\n    train_rmse = math.sqrt(mean_squared_error(\n        trainY[:, 0], train_predict[:, 0]))\n    test_rmse = math.sqrt(mean_squared_error(testY[:, 0], test_predict[:, 0]))\n    # Print RMSE\n    print('Train RMSE: %.3f RMSE' % (train_rmse))\n    print('Test RMSE: %.3f RMSE' % (test_rmse))\n    return train_rmse, test_rmse\n\n\n\n\nRecurrent Neural Network\nThe RNN models that are constructed and trained using TensorFlow and Keras. This model is trained for five hidden layers, one dense layer, and the activation function hyperbolic tangent. A dropout rate of 0.2 is used to reduce the risk of overfitting. The model are trained for 20 epochs and generated validation loss plots to compare the performance of the regularized and non-regularized models. The second RNN model was identical to the first, except for the addition of kernel regularization.\n\nRNNRNN with Regularization\n\n\n\n\nCode\n# Create a RNN\ndef create_RNN(hidden_units, dense_units, input_shape, activation,dropout_rate=0.2,kernel_regularizer=None):\n    model = Sequential()\n    # Create a simple neural network layer\n    model.add(SimpleRNN(hidden_units, input_shape=input_shape,\n              activation=activation[0]))\n    # Add a dense layer (only one, more layers would make it a deep neural net)\n    model.add(Dense(units=dense_units,\n              activation=activation[1],\n              kernel_regularizer=kernel_regularizer))\n    # Add layer dropout\n    model.add(Dropout(dropout_rate))\n    \n    # Compile the model and optimize on mean squared error\n    model.compile(loss='mean_squared_error', optimizer='adam')\n    return model\n  \n# Create a recurrent neural network\nmodel = create_RNN(hidden_units=5, dense_units=1, input_shape=(lag, Xtrain.shape[-1]),\n                   activation=['tanh', 'tanh'], dropout_rate = 0.2)\nhistory = model.fit(Xtrain, Ytrain, epochs=20, batch_size=1, verbose=0,validation_data=(Xval, Yval))\nplot_model(history, 'Recurrent Neural Network Model')\n\nyhat_d = [x[0] for x in model.predict(Xval, verbose=0)]\ny = [y[0] for y in Yval]\n\ntrain_predict = model.predict(Xtrain, verbose=0)\ntest_predict = model.predict(Xval, verbose=0)\n\n# Print error\ntrain_rmse, test_rmse = print_error(Ytrain, Yval, train_predict, test_predict)\nrmse_table = {\n    'model': ['Recurrent Neural Network'],\n    'training_rmse': [train_rmse],\n    'testing_rmse': [test_rmse]\n}\n\n\nTrain RMSE: 0.300 RMSE\nTest RMSE: 2.530 RMSE\n\n\n\n\n\n\n\n\n\nCode\n# Create a recurrent neural network with regularization\n\nmodel = create_RNN(hidden_units=5, dense_units=1, input_shape=(lag, Xtrain.shape[-1]),\n                   activation=['tanh', 'tanh'], kernel_regularizer=regularizers.L1L2(l1=1e-5, l2=1e-4), dropout_rate = 0.2)\nhistory = model.fit(Xtrain, Ytrain, epochs=20, batch_size=1, verbose=0, validation_data=(Xval, Yval))\nplot_model(history, 'Recurrent Neural Network Model (with L1L2 Regularization)')\n\nyhat_d_reg = [x[0] for x in model.predict(Xval, verbose=0)]\n\ntrain_predict = model.predict(Xtrain, verbose=0)\ntest_predict = model.predict(Xval, verbose=0)\n\n# Print error\ntrain_rmse, test_rmse = print_error(Ytrain, Yval, train_predict, test_predict)\nrmse_table['model'].append(\n    'Recurrent Neural Network (with L1L2 Regularization)')\nrmse_table['training_rmse'].append(train_rmse)\nrmse_table['testing_rmse'].append(test_rmse)\n\n\nTrain RMSE: 0.303 RMSE\nTest RMSE: 2.485 RMSE\n\n\n\n\n\n\n\n\nThe second plot illustrates the impact of regularization on the training loss. Compared to the original plot, there are significant differences. The regularization technique had a substantial effect on the testing error of the model. The model trained without regularization had a slightly lower training error of 0.306 RMSE but a considerably higher testing error of 2.478 RMSE. In contrast, the model trained with regularization had a slightly lower training error of 0.304 RMSE, but it produced silghtly high testing error of 2.481 RMSE.\n\n\nGRU Neural Network\nA GRU neural network is a powerful machine learning tool widely adopted in various applications. Its key feature is the recurrent gated unit, a type of memory cell that can store information over a data sequence. The GRU neural network is five hidden layers, one dense layer, and the hyperbolic tangent activation function. The model also includes kernel regularization to help address the issue of overfitting. The models are trained for 20 epochs and validation loss plots were generated to compare the performance of the regularized and non-regularized models. Overall, the GRU neural network has shown great promise in accurately predicting future movements in the S&P 500 index.\n\nGRUGRU with Regularization\n\n\n\n\nCode\n# Create a GRU Neural Network\ndef create_GRU(hidden_units, dense_units, input_shape, activation,dropout_rate=0.2, kernel_regularizer=None):\n    model = Sequential()\n    # Create a simple GRU neural network layer\n    model.add(GRU(hidden_units, input_shape=input_shape,\n              activation=activation[0]))\n    # Add a dense layer (only one, more layers would make it a deep neural net)\n    model.add(Dense(units=dense_units,\n              activation=activation[1], kernel_regularizer=kernel_regularizer))\n    # Add layer dropout\n    model.add(Dropout(dropout_rate))\n    # Compile the model and optimize on mean squared error\n    model.compile(loss='mean_squared_error', optimizer='sgd')\n    return model\n\n# Training and evaluating a GRU-based model\nmodel = create_GRU(hidden_units=5, dense_units=1, input_shape=(lag, Xtrain.shape[-1]),\n                   activation=['tanh', 'relu'], dropout_rate = 0.2)\nhistory = model.fit(Xtrain, Ytrain, epochs=20, batch_size=1, verbose=0)\nplot_model(history, 'GRU Model')\n\nyhat_gru = [x[0] for x in model.predict(Xval, verbose=0)]\n\ntrain_predict = model.predict(Xtrain, verbose=0)\ntest_predict = model.predict(Xval, verbose=0)\n\n\n# Print error\ntrain_rmse, test_rmse = print_error(Ytrain, Yval, train_predict, test_predict)\nrmse_table['model'].append('GRU Neural Network')\nrmse_table['training_rmse'].append(train_rmse)\nrmse_table['testing_rmse'].append(test_rmse)\n\n\nTrain RMSE: 0.699 RMSE\nTest RMSE: 1.319 RMSE\n\n\n\n\n\n\n\n\n\nCode\n# Training and evaluating a GRU-based model with regularization\nmodel = create_GRU(hidden_units=5, dense_units=1, input_shape=(lag, Xtrain.shape[-1]),\n                   activation=['tanh', 'relu'],dropout_rate = 0.2,  kernel_regularizer=regularizers.L1L2(l1=1e-5, l2=1e-4))\nhistory = model.fit(Xtrain, Ytrain, epochs=20, batch_size=1, verbose=0)\nplot_model(history, 'GRU Model (with L1L2 Regularization)')\n\nyhat_gru_reg = [x[0] for x in model.predict(Xval, verbose=0)]\n\ntrain_predict = model.predict(Xtrain, verbose=0)\ntest_predict = model.predict(Xval, verbose=0)\n\n# Print error\ntrain_rmse, test_rmse = print_error(Ytrain, Yval, train_predict, test_predict)\nrmse_table['model'].append('GRU Neural Network (with L1L2 Regularization)')\nrmse_table['training_rmse'].append(train_rmse)\nrmse_table['testing_rmse'].append(test_rmse)\n\n\nTrain RMSE: 0.998 RMSE\nTest RMSE: 3.423 RMSE\n\n\n\n\n\n\n\n\nThe two plots show the training loss for the GRU model, one with regularization and one without. The plot with regularization is similar to without regularization plot. The model without regularization had a train RMSE of 0.699 and a test RMSE of 1.277. In contrast, the model trained with regularization had a slightly lower training error of 0.696 RMSE, but it produced silghtly high testing error of 1.280 RMSE.\n\n\nLSTM Neural Network\nThe LSTM (Long Short-Term Memory) neural network is another powerful machine learning tool that has been widely adopted in various applications, particularly in tasks involving sequential data.The LSTM neural network architecture typically consists of multiple LSTM layers, followed by one or more dense layers and an activation function such as the sigmoid or hyperbolic tangent. Similar to the GRU neural network, the LSTM model may also include kernel regularization to address the issue of overfitting. In order to evaluate the performance of the LSTM model, the model can be trained on a dataset for a specified number of epochs, and validation loss plots can be generated to compare the performance of the regularized and non-regularized models.\n\nLSTMLSTM with Regularization\n\n\n\n\nCode\n# Create a LSTM Neural Network\ndef create_LSTM(hidden_units, dense_units, input_shape, activation,dropout_rate = 0.2, kernel_regularizer=None):\n    model = Sequential()\n    # Create a simple long short term memory neural network\n    model.add(LSTM(hidden_units,\n              activation=activation[0], input_shape=input_shape))\n    # Add a dense layer (only one, more layers would make it a deep neural net)\n    model.add(Dense(units=dense_units,\n              activation=activation[1], kernel_regularizer=kernel_regularizer))\n    # Add layer dropout\n    model.add(Dropout(dropout_rate))\n    # Compile the model and optimize on mean squared error\n    model.compile(optimizer=\"RMSprop\", loss='mae')\n    return model\n\n# Create an LSTM neural network\nmodel = create_LSTM(hidden_units=5, dense_units=1, input_shape=(lag, Xtrain.shape[-1]),\n                    activation=['tanh', 'linear'], dropout_rate = 0.2)\nhistory = model.fit(Xtrain, Ytrain, epochs=20, batch_size=1, verbose=0)\nplot_model(history, 'LSTM Model')\n\nyhat_lstm = [x[0] for x in model.predict(Xval, verbose=0)]\n\ntrain_predict = model.predict(Xtrain, verbose=0)\ntest_predict = model.predict(Xval, verbose=0)\n\n# Print error\ntrain_rmse, test_rmse = print_error(Ytrain, Yval, train_predict, test_predict)\nrmse_table['model'].append('LSTM Neural Network')\nrmse_table['training_rmse'].append(train_rmse)\nrmse_table['testing_rmse'].append(test_rmse)\n\n\nTrain RMSE: 0.195 RMSE\nTest RMSE: 1.369 RMSE\n\n\n\n\n\n\n\n\n\nCode\n# Create an LSTM neural network with regularization\nmodel = create_LSTM(hidden_units=5, dense_units=1, input_shape=(lag, Xtrain.shape[-1]),\n                    activation=['tanh', 'linear'], dropout_rate = 0.2, kernel_regularizer=regularizers.L1L2(l1=1e-5, l2=1e-4))\nhistory = model.fit(Xtrain, Ytrain, epochs=20, batch_size=1, verbose=0)\nplot_model(history, 'LSTM Model (with L1L2 Regularization)')\n\n\nyhat_lstm_reg = [x[0] for x in model.predict(Xval, verbose=0)]\n\ntrain_predict = model.predict(Xtrain, verbose=0)\ntest_predict = model.predict(Xval, verbose=0)\n\n# Print error\ntrain_rmse, test_rmse = print_error(Ytrain, Yval, train_predict, test_predict)\nrmse_table['model'].append('LSTM Neural Network (with L1L2 Regularization)')\nrmse_table['training_rmse'].append(train_rmse)\nrmse_table['testing_rmse'].append(test_rmse)\n\n\nTrain RMSE: 0.207 RMSE\nTest RMSE: 1.281 RMSE\n\n\n\n\n\n\n\n\nThe two plots show the training loss for the LSTM model, one with regularization and one without. The plot with regularization shows less fluctuation, indicating that overfitting has been reduced. The model without regularization had a train RMSE of 0.169 and a test RMSE of 1.203, while the model with regularization had a slightly higher for both the train and test RMSE values which are 0.202 and 1.389 respectively.\n\n\nForecast\n\n\nCode\n# Creating the frame to store both predictions\ndays = df['Date'].values[-len(y):]\nframe = pd.concat([\n    pd.DataFrame({'day': days, 'price': y, 'type': 'original'}),\n    pd.DataFrame({'day': days, 'price': yhat_d, 'type': 'rnn_forecast'}),\n    pd.DataFrame({'day': days, 'price': yhat_gru, 'type': 'gru_forecast'}),\n    pd.DataFrame({'day': days, 'price': yhat_lstm, 'type': 'lstm_forecast'})\n])\n# Creating the unscaled values column\nframe['price_absolute'] = [(x * train_std) + train_mean for x in frame['price']]\n# Pivoting\npivoted = frame.pivot_table(index='day', columns='type')\npivoted.columns = ['_'.join(x).strip() for x in pivoted.columns.values]\n\nplt.figure(figsize=(12, 10))\nplt.plot(pivoted.index, pivoted.price_absolute_original,\n         color='#8B1874', label='original')\nplt.plot(pivoted.index, pivoted.price_absolute_rnn_forecast,\n         color='#F79540', label='RNN Forecast', alpha=0.6)\nplt.plot(pivoted.index, pivoted.price_absolute_gru_forecast,\n         color='#B71375', label='GRU Forecast', alpha=0.6)\nplt.plot(pivoted.index, pivoted.price_absolute_lstm_forecast,\n         color='#FC4F00', label='LSTM Forecast', alpha=0.6)\nplt.title('S&P 500  Index Stock Price Forecasts')\nplt.legend()\nplt.show()\n\n\n\n\n\nWhen comparing the predictions of the deep learning models with the original time series plot, we can observe that the LSTM model is the closest to the actual values. The GRU model is almost the same as the LSTM model, but both the models have some kind of accuracy with the actual plot, but there is still difference. RNN model are pretty far from the actual plot, which indicates that LSTM model is better when compared to others, but we need to note that the model is not accurate, i.e., it is not the same as the actual plot.\n\n\nModel Comparison\n\n\nCode\nrmse_df = pd.DataFrame(rmse_table)\nrmse_df\n\n\n\n\n\n\n  \n    \n      \n      model\n      training_rmse\n      testing_rmse\n    \n  \n  \n    \n      0\n      Recurrent Neural Network\n      0.299935\n      2.529999\n    \n    \n      1\n      Recurrent Neural Network (with L1L2 Regulariza...\n      0.303019\n      2.484638\n    \n    \n      2\n      GRU Neural Network\n      0.699211\n      1.319068\n    \n    \n      3\n      GRU Neural Network (with L1L2 Regularization)\n      0.998283\n      3.422584\n    \n    \n      4\n      LSTM Neural Network\n      0.194553\n      1.368967\n    \n    \n      5\n      LSTM Neural Network (with L1L2 Regularization)\n      0.207482\n      1.281249\n    \n  \n\n\n\n\nFirst, we observe that recurrent neural networks (RNN) and long short-term memory (LSTM) networks generally perform better than gated recurrent units (GRU) in terms of testing RMSE. However, the training and testing RMSE values for each model can vary significantly depending on the specific dataset being used.\nRegarding regularization, we see that the use of L1L2 regularization generally leads to lower testing RMSE values for RNN and LSTM models. However, for GRU models, the use of L1L2 regularization can either slightly improve or worsen the testing RMSE depending on the dataset.\nOverall, the best performing models in terms of testing RMSE across all datasets appear to be the LSTM models with L1L2 regularization, which consistently achieve low testing RMSE values.\n\n\nComparison of deep learning models with traditional single variable time series\nAmong the deep learning models, the LSTM Neural Network (with L1L2 Regularization) performed the best with the lowest testing RMSE value. On the other hand, the ARIMA model had the highest testing RMSE value of 48.95665, indicating that it did not perform well in predicting the future movements of the index.\nThis comparison highlights the advantage of using deep learning models over traditional statistical models such as ARIMA in predicting stock market trends. Deep learning models are better equipped to handle complex and dynamic data, making them more effective in capturing the non-linear relationships and patterns in the stock market data.\nOverall, the results suggest that deep learning models can be a valuable tool for investors and analysts in predicting the future movements of the stock market."
  },
  {
    "objectID": "deep_learning_index_nasdaq.html",
    "href": "deep_learning_index_nasdaq.html",
    "title": "",
    "section": "",
    "text": "Deep Learning for NASDAQ Composite Index Stock Price\nIn recent years, the Nasdaq Composite Index has experienced significant growth due to the rise of the technology and biotech industries. However, predicting the future movements of the index remains a challenging task due to the complex and dynamic nature of the market. By leveraging the power of Deep Learning and applying techniques such as RNN, LSTM, GRU, and regularization, we can create a model that accurately predicts the future movements of the Nasdaq Composite Index. Such a model can provide valuable insights for investors and traders looking to make informed decisions in the stock market, and help them stay ahead of the curve in the ever-changing landscape of the technology and biotech industries.\nIn order to apply the deep learning methods, the time series data is first split into training and testing sets, taking into account the time component. The training data constitutes 75% of the total data while the rest is the testing data. Furthermore, to ensure that the values are on the same scale, the data is scaled, which is crucial for deep learning models. The mean and standard deviation of the training data are computed and then utilized to normalize both the training and testing sets. This approach ensures that the models are trained on a standardized dataset, enabling them to make accurate predictions.\n\n\nCode\n# libraries\nimport numpy as np\nimport pandas as pd\nimport math\nfrom sklearn.metrics import mean_squared_error\nfrom matplotlib import pyplot as plt\nfrom keras.models import Sequential\nfrom keras.layers import Dropout\nfrom keras.layers import Dense, SimpleRNN, LSTM, GRU\nfrom keras import regularizers\nimport warnings\nwarnings.filterwarnings('ignore')\n\n# import data\ndf = pd.read_csv('./DATA/CLEANED DATA/nasdaq_raw_data.csv')\n# clean data and get final data for anlaysis\ndf = df[['Date', 'IXIC.Adjusted']]\ndf['Date'] = pd.to_datetime(df['Date'])\ndf['IXIC.Adjusted'] = pd.to_numeric(df['IXIC.Adjusted'], errors='coerce')\ndf = df.dropna(subset=['IXIC.Adjusted'])\ndf.sort_values('Date', inplace=True, ascending=True)\ndf = df.reset_index(drop=True)\ndef create_X_Y(ts: np.array, lag=1, n_ahead=1, target_index=0) -> tuple:\n    \"\"\"\n    A method to create X and Y matrix from a time series array for the training of \n    deep learning models \n    \"\"\"\n    # Extracting the number of features that are passed from the array\n    n_features = ts.shape[1]\n\n    # Creating placeholder lists\n    X, Y = [], []\n\n    if len(ts) - lag <= 0:\n        X.append(ts)\n    else:\n        for i in range(len(ts) - lag - n_ahead):\n            Y.append(ts[(i + lag):(i + lag + n_ahead), target_index])\n            X.append(ts[i:(i + lag)])\n\n    X, Y = np.array(X), np.array(Y)\n\n    # Reshaping the X array to an RNN input shape\n    X = np.reshape(X, (X.shape[0], lag, n_features))\n\n    return X, Y\n\nts = df[['IXIC.Adjusted']].values\nnp.random.seed(123)\nnrows = ts.shape[0]\ntest_share = 0.25\n# Spliting into train and test sets\ntrain = ts[0:int(nrows * (1 - test_share))]\ntest = ts[int(nrows * (1 - test_share)):]\n\n# Scaling the data\ntrain_mean = train.mean()\ntrain_std = train.std()\ntrain = (train - train_mean) / train_std\ntest = (test - train_mean) / train_std\n\n# Creating the final scaled frame\nts_s = np.concatenate([train, test])\n\nlag = 12\nahead = 3\n\n# Creating the X and Y for training\nX, Y = create_X_Y(ts_s, lag=lag, n_ahead=ahead)\n\nXtrain = X[0:int(X.shape[0] * (1 - test_share))]\nYtrain = Y[0:int(X.shape[0] * (1 - test_share))]\n\nXval = X[int(X.shape[0] * (1 - test_share)):]\nYval = Y[int(X.shape[0] * (1 - test_share)):]\n\n\n\n\ndef plot_model(history, model_title):\n    loss = history.history['loss']\n    epochs = range(1, len(loss) + 1)\n    plt.figure()\n    plt.plot(epochs, loss, 'b', label='Training loss', color = \"gray\")\n    plt.title(f'{model_title} Training loss ')\n    plt.legend()\n    \n\n\ndef print_error(trainY, testY, train_predict, test_predict):\n    # Error of predictions\n    train_rmse = math.sqrt(mean_squared_error(\n        trainY[:, 0], train_predict[:, 0]))\n    test_rmse = math.sqrt(mean_squared_error(testY[:, 0], test_predict[:, 0]))\n    # Print RMSE\n    print('Train RMSE: %.3f RMSE' % (train_rmse))\n    print('Test RMSE: %.3f RMSE' % (test_rmse))\n    return train_rmse, test_rmse\n\n\n\n\nRecurrent Neural Network\nThe RNN models that are constructed and trained using TensorFlow and Keras. This model is trained for five hidden layers, one dense layer, and the activation function hyperbolic tangent. A dropout rate of 0.2 is used to reduce the risk of overfitting. The model are trained for 20 epochs and generated validation loss plots to compare the performance of the regularized and non-regularized models. The second RNN model was identical to the first, except for the addition of kernel regularization.\n\nRNNRNN with Regularization\n\n\n\n\nCode\n# Create a RNN\ndef create_RNN(hidden_units, dense_units, input_shape, activation,dropout_rate=0.2,kernel_regularizer=None):\n    model = Sequential()\n    # Create a simple neural network layer\n    model.add(SimpleRNN(hidden_units, input_shape=input_shape,\n              activation=activation[0]))\n    # Add a dense layer (only one, more layers would make it a deep neural net)\n    model.add(Dense(units=dense_units,\n              activation=activation[1],\n              kernel_regularizer=kernel_regularizer))\n    # Add layer dropout\n    model.add(Dropout(dropout_rate))\n    \n    # Compile the model and optimize on mean squared error\n    model.compile(loss='mean_squared_error', optimizer='adam')\n    return model\n  \n# Create a recurrent neural network\nmodel = create_RNN(hidden_units=5, dense_units=1, input_shape=(lag, Xtrain.shape[-1]),\n                   activation=['tanh', 'tanh'], dropout_rate = 0.2)\nhistory = model.fit(Xtrain, Ytrain, epochs=20, batch_size=1, verbose=0,validation_data=(Xval, Yval))\nplot_model(history, 'Recurrent Neural Network Model')\n\nyhat_d = [x[0] for x in model.predict(Xval, verbose=0)]\ny = [y[0] for y in Yval]\n\ntrain_predict = model.predict(Xtrain, verbose=0)\ntest_predict = model.predict(Xval, verbose=0)\n\n# Print error\ntrain_rmse, test_rmse = print_error(Ytrain, Yval, train_predict, test_predict)\nrmse_table = {\n    'model': ['Recurrent Neural Network'],\n    'training_rmse': [train_rmse],\n    'testing_rmse': [test_rmse]\n}\n\n\nTrain RMSE: 0.324 RMSE\nTest RMSE: 3.251 RMSE\n\n\n\n\n\n\n\n\n\nCode\n# Create a recurrent neural network with regularization\n\nmodel = create_RNN(hidden_units=5, dense_units=1, input_shape=(lag, Xtrain.shape[-1]),\n                   activation=['tanh', 'tanh'], kernel_regularizer=regularizers.L1L2(l1=1e-5, l2=1e-4), dropout_rate = 0.2)\nhistory = model.fit(Xtrain, Ytrain, epochs=20, batch_size=1, verbose=0, validation_data=(Xval, Yval))\nplot_model(history, 'Recurrent Neural Network Model (with L1L2 Regularization)')\n\nyhat_d_reg = [x[0] for x in model.predict(Xval, verbose=0)]\n\ntrain_predict = model.predict(Xtrain, verbose=0)\ntest_predict = model.predict(Xval, verbose=0)\n\n# Print error\ntrain_rmse, test_rmse = print_error(Ytrain, Yval, train_predict, test_predict)\nrmse_table['model'].append(\n    'Recurrent Neural Network (with L1L2 Regularization)')\nrmse_table['training_rmse'].append(train_rmse)\nrmse_table['testing_rmse'].append(test_rmse)\n\n\nTrain RMSE: 0.311 RMSE\nTest RMSE: 3.251 RMSE\n\n\n\n\n\n\n\n\nThe second plot illustrates the impact of regularization on the training loss. Compared to the original plot, there are significant differences. The regularization technique had a substantial effect on the testing error of the model. The model trained without regularization training error is of the RMSE value 0.322 but a considerably higher testing error of 3.251 RMSE. In contrast, the model trained with regularization had a slightly lower training error of 0.323 RMSE, but it produced same testing error of 3.251 RMSE.\n\n\nGRU Neural Network\nA GRU neural network is a powerful machine learning tool widely adopted in various applications. Its key feature is the recurrent gated unit, a type of memory cell that can store information over a data sequence. The GRU neural network is five hidden layers, one dense layer, and the hyperbolic tangent activation function. The model also includes kernel regularization to help address the issue of overfitting. The models are trained for 20 epochs and validation loss plots were generated to compare the performance of the regularized and non-regularized models. Overall, the GRU neural network has shown great promise in accurately predicting future movements in the NASDAQ Composite index.\n\nGRUGRU with Regularization\n\n\n\n\nCode\n# Create a GRU Neural Network\ndef create_GRU(hidden_units, dense_units, input_shape, activation,dropout_rate=0.2, kernel_regularizer=None):\n    model = Sequential()\n    # Create a simple GRU neural network layer\n    model.add(GRU(hidden_units, input_shape=input_shape,\n              activation=activation[0]))\n    # Add a dense layer (only one, more layers would make it a deep neural net)\n    model.add(Dense(units=dense_units,\n              activation=activation[1], kernel_regularizer=kernel_regularizer))\n    # Add layer dropout\n    model.add(Dropout(dropout_rate))\n    # Compile the model and optimize on mean squared error\n    model.compile(loss='mean_squared_error', optimizer='sgd')\n    return model\n\n# Training and evaluating a GRU-based model\nmodel = create_GRU(hidden_units=5, dense_units=1, input_shape=(lag, Xtrain.shape[-1]),\n                   activation=['tanh', 'relu'], dropout_rate = 0.2)\nhistory = model.fit(Xtrain, Ytrain, epochs=20, batch_size=1, verbose=0)\nplot_model(history, 'GRU Model')\n\nyhat_gru = [x[0] for x in model.predict(Xval, verbose=0)]\n\ntrain_predict = model.predict(Xtrain, verbose=0)\ntest_predict = model.predict(Xval, verbose=0)\n\n# Print error\ntrain_rmse, test_rmse = print_error(Ytrain, Yval, train_predict, test_predict)\nrmse_table['model'].append('GRU Neural Network')\nrmse_table['training_rmse'].append(train_rmse)\nrmse_table['testing_rmse'].append(test_rmse)\n\n\nTrain RMSE: 0.665 RMSE\nTest RMSE: 2.075 RMSE\n\n\n\n\n\n\n\n\n\nCode\n# Training and evaluating a GRU-based model with regularization\nmodel = create_GRU(hidden_units=5, dense_units=1, input_shape=(lag, Xtrain.shape[-1]),\n                   activation=['tanh', 'relu'],dropout_rate = 0.2,  kernel_regularizer=regularizers.L1L2(l1=1e-5, l2=1e-4))\nhistory = model.fit(Xtrain, Ytrain, epochs=20, batch_size=1, verbose=0)\nplot_model(history, 'GRU Model (with L1L2 Regularization)')\n\nyhat_gru_reg = [x[0] for x in model.predict(Xval, verbose=0)]\n\ntrain_predict = model.predict(Xtrain, verbose=0)\ntest_predict = model.predict(Xval, verbose=0)\n\n# Print error\ntrain_rmse, test_rmse = print_error(Ytrain, Yval, train_predict, test_predict)\nrmse_table['model'].append('GRU Neural Network (with L1L2 Regularization)')\nrmse_table['training_rmse'].append(train_rmse)\nrmse_table['testing_rmse'].append(test_rmse)\n\n\nTrain RMSE: 0.999 RMSE\nTest RMSE: 4.200 RMSE\n\n\n\n\n\n\n\n\nThe two plots show the training loss for the GRU model, one with regularization and one without. The plot with regularization shows less fluctuation, indicating that overfitting has been reduced. The model without regularization had a train RMSE of 0.664 and a test RMSE of 1.926, while the training and testing RMSE is higher with regularization which is 0.666 and 2.065 respectively.\n\n\nLSTM Neural Network\nThe LSTM (Long Short-Term Memory) neural network is another powerful machine learning tool that has been widely adopted in various applications, particularly in tasks involving sequential data.The LSTM neural network architecture typically consists of multiple LSTM layers, followed by one or more dense layers and an activation function such as the sigmoid or hyperbolic tangent. Similar to the GRU neural network, the LSTM model may also include kernel regularization to address the issue of overfitting. In order to evaluate the performance of the LSTM model, the model can be trained on a dataset for a specified number of epochs, and validation loss plots can be generated to compare the performance of the regularized and non-regularized models.\n\nLSTMLSTM with Regularization\n\n\n\n\nCode\n# Create a LSTM Neural Network\ndef create_LSTM(hidden_units, dense_units, input_shape, activation,dropout_rate = 0.2, kernel_regularizer=None):\n    model = Sequential()\n    # Create a simple long short term memory neural network\n    model.add(LSTM(hidden_units,\n              activation=activation[0], input_shape=input_shape))\n    # Add a dense layer (only one, more layers would make it a deep neural net)\n    model.add(Dense(units=dense_units,\n              activation=activation[1], kernel_regularizer=kernel_regularizer))\n    # Add layer dropout\n    model.add(Dropout(dropout_rate))\n    # Compile the model and optimize on mean squared error\n    model.compile(optimizer=\"RMSprop\", loss='mae')\n    return model\n\n# Create an LSTM neural network\nmodel = create_LSTM(hidden_units=5, dense_units=1, input_shape=(lag, Xtrain.shape[-1]),\n                    activation=['tanh', 'linear'], dropout_rate = 0.2)\nhistory = model.fit(Xtrain, Ytrain, epochs=20, batch_size=1, verbose=0)\nplot_model(history, 'LSTM Model')\n\nyhat_lstm = [x[0] for x in model.predict(Xval, verbose=0)]\n\ntrain_predict = model.predict(Xtrain, verbose=0)\ntest_predict = model.predict(Xval, verbose=0)\n\n# Print error\ntrain_rmse, test_rmse = print_error(Ytrain, Yval, train_predict, test_predict)\nrmse_table['model'].append('LSTM Neural Network')\nrmse_table['training_rmse'].append(train_rmse)\nrmse_table['testing_rmse'].append(test_rmse)\n\n\nTrain RMSE: 0.198 RMSE\nTest RMSE: 2.008 RMSE\n\n\n\n\n\n\n\n\n\nCode\n# Create an LSTM neural network with regularization\nmodel = create_LSTM(hidden_units=5, dense_units=1, input_shape=(lag, Xtrain.shape[-1]),\n                    activation=['tanh', 'linear'], dropout_rate = 0.2, kernel_regularizer=regularizers.L1L2(l1=1e-5, l2=1e-4))\nhistory = model.fit(Xtrain, Ytrain, epochs=20, batch_size=1, verbose=0)\nplot_model(history, 'LSTM Model (with L1L2 Regularization)')\n\n\nyhat_lstm_reg = [x[0] for x in model.predict(Xval, verbose=0)]\n\ntrain_predict = model.predict(Xtrain, verbose=0)\ntest_predict = model.predict(Xval, verbose=0)\n\n# Print error\ntrain_rmse, test_rmse = print_error(Ytrain, Yval, train_predict, test_predict)\nrmse_table['model'].append('LSTM Neural Network (with L1L2 Regularization)')\nrmse_table['training_rmse'].append(train_rmse)\nrmse_table['testing_rmse'].append(test_rmse)\n\n\nTrain RMSE: 0.208 RMSE\nTest RMSE: 1.959 RMSE\n\n\n\n\n\n\n\n\nThe two plots show the training loss for the LSTM model, one with regularization and one without. The plot with regularization shows less fluctuation, indicating that overfitting has been reduced. The model without regularization had a train RMSE of 0.204 and a test RMSE of 1.688, while the model with regularization had a slightly higher train RMSE of 0.226 and test RMSE of 2.095\n\n\nForecast\n\n\nCode\n# Creating the frame to store both predictions\ndays = df['Date'].values[-len(y):]\nframe = pd.concat([\n    pd.DataFrame({'day': days, 'price': y, 'type': 'original'}),\n    pd.DataFrame({'day': days, 'price': yhat_d, 'type': 'rnn_forecast'}),\n    pd.DataFrame({'day': days, 'price': yhat_gru, 'type': 'gru_forecast'}),\n    pd.DataFrame({'day': days, 'price': yhat_lstm, 'type': 'lstm_forecast'})\n])\n# Creating the unscaled values column\nframe['price_absolute'] = [(x * train_std) + train_mean for x in frame['price']]\n# Pivoting\npivoted = frame.pivot_table(index='day', columns='type')\npivoted.columns = ['_'.join(x).strip() for x in pivoted.columns.values]\n\nplt.figure(figsize=(12, 10))\nplt.plot(pivoted.index, pivoted.price_absolute_original,\n         color='#8B1874', label='original')\nplt.plot(pivoted.index, pivoted.price_absolute_rnn_forecast,\n         color='#F79540', label='RNN Forecast', alpha=0.6)\nplt.plot(pivoted.index, pivoted.price_absolute_gru_forecast,\n         color='#B71375', label='GRU Forecast', alpha=0.6)\nplt.plot(pivoted.index, pivoted.price_absolute_lstm_forecast,\n         color='#FC4F00', label='LSTM Forecast', alpha=0.6)\nplt.title('NASDAQ Composite Index Stock Price Forecasts')\nplt.legend()\nplt.show()\n\n\n\n\n\nWhen comparing the predictions of the deep learning models with the original time series plot, we can observe that the LSTM model is the closest to the actual values. The GRU model is almost the same as the LSTM model, but both the models have some kind of accuracy with the actual plot, but there is still difference. RNN model are pretty far from the actual plot, which indicates that LSTM model is better when compared to others, but we need to note that the model is not accurate, i.e., it is not the same as the actual plot.\n\n\nModel Comparison\n\n\nCode\nrmse_df = pd.DataFrame(rmse_table)\nrmse_df\n\n\n\n\n\n\n  \n    \n      \n      model\n      training_rmse\n      testing_rmse\n    \n  \n  \n    \n      0\n      Recurrent Neural Network\n      0.323824\n      3.251397\n    \n    \n      1\n      Recurrent Neural Network (with L1L2 Regulariza...\n      0.311293\n      3.251499\n    \n    \n      2\n      GRU Neural Network\n      0.664798\n      2.074547\n    \n    \n      3\n      GRU Neural Network (with L1L2 Regularization)\n      0.998977\n      4.199545\n    \n    \n      4\n      LSTM Neural Network\n      0.197880\n      2.007842\n    \n    \n      5\n      LSTM Neural Network (with L1L2 Regularization)\n      0.207665\n      1.958586\n    \n  \n\n\n\n\nFrom the table, we can observe that the LSTM Neural Network with L1L2 Regularization has the lowest testing RMSE value, indicating that it performs the best among all the models. The next best performing model is the GRU Neural Network with L1L2 Regularization, with a testing, followed by the original GRU Neural Network with a testing RMSE.\nComparing the two best performing models, we see that the LSTM Neural Network with L1L2 Regularization outperforms the GNU with L1L2 Regularization by a significant margin. This indicates that the LSTM model is better suited for the task at hand, and the regularization technique has helped to improve its performance. The other models, including the RNN, have higher testing RMSE values and are therefore less effective in predicting the target variable.\n\n\nComparison of deep learning models with traditional single variable time series\nComparing the ARIMA model with the deep learning models from the given table, we can see that the ARIMA model has a much higher RMSE value of 162.141 compared to the best-performing deep learning model, the LSTM Neural Network with L1L2 Regularization.\nThis indicates that the deep learning models are significantly better at predicting the target variable compared to the ARIMA model for the given dataset. However, it’s worth noting that the ARIMA model is a classical time series model, while the deep learning models are more complex and require more data and computational resources. Additionally, the ARIMA model is more interpretable than the deep learning models, which can be an important factor in some applications. Nonetheless, based on the given RMSE values, the deep learning models outperform the ARIMA model in this case."
  },
  {
    "objectID": "deep_learning_index_dji.html",
    "href": "deep_learning_index_dji.html",
    "title": "",
    "section": "",
    "text": "Deep Learning for Dow Jones Index Stock Price\nThe Dow Jones Index (DJI) is a widely followed stock market index that tracks the performance of 30 large publicly traded companies in the United States. Predicting the movement of the DJI is a challenging task due to its complexity and sensitivity to a wide range of factors. Deep Learning has emerged as a powerful technique for analyzing complex data and making accurate predictions. We will explore the use of three deep learning models, Recurrent Neural Network (RNN), Long Short-Term Memory (LSTM), and Gated Recurrent Unit (GRU), to predict the DJI index. Additionally, we investigate the impact of regularization on the performance of the models which is used to prevent overfitting. By using these models, we aim to create a model that can accurately predict the future movements of the DJI index, which can be valuable for investors and traders.\nIn order to apply the deep learning methods, the time series data is first split into training and testing sets, taking into account the time component. The training data constitutes 75% of the total data while the rest is the testing data. Furthermore, to ensure that the values are on the same scale, the data is scaled, which is crucial for deep learning models. The mean and standard deviation of the training data are computed and then utilized to normalize both the training and testing sets. This approach ensures that the models are trained on a standardized dataset, enabling them to make accurate predictions.\n\n\nCode\n# libraries\nimport numpy as np\nimport pandas as pd\nimport math\nfrom sklearn.metrics import mean_squared_error\nfrom matplotlib import pyplot as plt\nfrom keras.models import Sequential\nfrom keras.layers import Dropout\nfrom keras.layers import Dense, SimpleRNN, LSTM, GRU\nfrom keras import regularizers\nimport warnings\nwarnings.filterwarnings('ignore')\n\n# import data\ndf = pd.read_csv('./DATA/CLEANED DATA/dji_raw_data.csv')\n# clean data and get final data for anlaysis\ndf = df[['Date', 'DJI.Adjusted']]\ndf['Date'] = pd.to_datetime(df['Date'])\ndf['DJI.Adjusted'] = pd.to_numeric(df['DJI.Adjusted'], errors='coerce')\ndf = df.dropna(subset=['DJI.Adjusted'])\ndf.sort_values('Date', inplace=True, ascending=True)\ndf = df.reset_index(drop=True)\n\ndef create_X_Y(ts: np.array, lag=1, n_ahead=1, target_index=0) -> tuple:\n    \"\"\"\n    A method to create X and Y matrix from a time series array for the training of \n    deep learning models \n    \"\"\"\n    # Extracting the number of features that are passed from the array\n    n_features = ts.shape[1]\n\n    # Creating placeholder lists\n    X, Y = [], []\n\n    if len(ts) - lag <= 0:\n        X.append(ts)\n    else:\n        for i in range(len(ts) - lag - n_ahead):\n            Y.append(ts[(i + lag):(i + lag + n_ahead), target_index])\n            X.append(ts[i:(i + lag)])\n\n    X, Y = np.array(X), np.array(Y)\n\n    # Reshaping the X array to an RNN input shape\n    X = np.reshape(X, (X.shape[0], lag, n_features))\n\n    return X, Y\n\nts = df[['DJI.Adjusted']].values\nnp.random.seed(123)\nnrows = ts.shape[0]\ntest_share = 0.25\n# Spliting into train and test sets\ntrain = ts[0:int(nrows * (1 - test_share))]\ntest = ts[int(nrows * (1 - test_share)):]\n\n# Scaling the data\ntrain_mean = train.mean()\ntrain_std = train.std()\ntrain = (train - train_mean) / train_std\ntest = (test - train_mean) / train_std\n\n# Creating the final scaled frame\nts_s = np.concatenate([train, test])\n\nlag = 12\nahead = 3\n\n# Creating the X and Y for training\nX, Y = create_X_Y(ts_s, lag=lag, n_ahead=ahead)\n\nXtrain = X[0:int(X.shape[0] * (1 - test_share))]\nYtrain = Y[0:int(X.shape[0] * (1 - test_share))]\n\nXval = X[int(X.shape[0] * (1 - test_share)):]\nYval = Y[int(X.shape[0] * (1 - test_share)):]\n\n\n\n\ndef plot_model(history, model_title):\n    loss = history.history['loss']\n    epochs = range(1, len(loss) + 1)\n    plt.figure()\n    plt.plot(epochs, loss, 'b', label='Training loss', color = \"gray\")\n    plt.title(f'{model_title} Training loss ')\n    plt.legend()\n    \n\n\ndef print_error(trainY, testY, train_predict, test_predict):\n    # Error of predictions\n    train_rmse = math.sqrt(mean_squared_error(\n        trainY[:, 0], train_predict[:, 0]))\n    test_rmse = math.sqrt(mean_squared_error(testY[:, 0], test_predict[:, 0]))\n    # Print RMSE\n    print('Train RMSE: %.3f RMSE' % (train_rmse))\n    print('Test RMSE: %.3f RMSE' % (test_rmse))\n    return train_rmse, test_rmse\n\n\n\n\nRecurrent Neural Network\nThe RNN models that are constructed and trained using TensorFlow and Keras. This model is trained for five hidden layers, one dense layer, and the activation function hyperbolic tangent. A dropout rate of 0.2 is used to reduce the risk of overfitting. The model are trained for 20 epochs and generated validation loss plots to compare the performance of the regularized and non-regularized models. The second RNN model was identical to the first, except for the addition of kernel regularization.\n\nRNNRNN with Regularization\n\n\n\n\nCode\n# Create a RNN\ndef create_RNN(hidden_units, dense_units, input_shape, activation,dropout_rate=0.2,kernel_regularizer=None):\n    model = Sequential()\n    # Create a simple neural network layer\n    model.add(SimpleRNN(hidden_units, input_shape=input_shape,\n              activation=activation[0]))\n    # Add a dense layer (only one, more layers would make it a deep neural net)\n    model.add(Dense(units=dense_units,\n              activation=activation[1],\n              kernel_regularizer=kernel_regularizer))\n    # Add layer dropout\n    model.add(Dropout(dropout_rate))\n    \n    # Compile the model and optimize on mean squared error\n    model.compile(loss='mean_squared_error', optimizer='adam')\n    return model\n  \n# Create a recurrent neural network\nmodel = create_RNN(hidden_units=5, dense_units=1, input_shape=(lag, Xtrain.shape[-1]),\n                   activation=['tanh', 'tanh'], dropout_rate = 0.2)\nhistory = model.fit(Xtrain, Ytrain, epochs=20, batch_size=1, verbose=0,validation_data=(Xval, Yval))\nplot_model(history, 'Recurrent Neural Network Model')\n\nyhat_d = [x[0] for x in model.predict(Xval, verbose=0)]\ny = [y[0] for y in Yval]\n\ntrain_predict = model.predict(Xtrain, verbose=0)\ntest_predict = model.predict(Xval, verbose=0)\n\n# Print error\ntrain_rmse, test_rmse = print_error(Ytrain, Yval, train_predict, test_predict)\nrmse_table = {\n    'model': ['Recurrent Neural Network'],\n    'training_rmse': [train_rmse],\n    'testing_rmse': [test_rmse]\n}\n\n\nTrain RMSE: 0.323 RMSE\nTest RMSE: 1.884 RMSE\n\n\n\n\n\n\n\n\n\nCode\n# Create a recurrent neural network with regularization\n\nmodel = create_RNN(hidden_units=5, dense_units=1, input_shape=(lag, Xtrain.shape[-1]),\n                   activation=['tanh', 'tanh'], kernel_regularizer=regularizers.L1L2(l1=1e-5, l2=1e-4), dropout_rate = 0.2)\nhistory = model.fit(Xtrain, Ytrain, epochs=20, batch_size=1, verbose=0, validation_data=(Xval, Yval))\nplot_model(history, 'Recurrent Neural Network Model (with L1L2 Regularization)')\n\nyhat_d_reg = [x[0] for x in model.predict(Xval, verbose=0)]\n\ntrain_predict = model.predict(Xtrain, verbose=0)\ntest_predict = model.predict(Xval, verbose=0)\n\n# Print error\ntrain_rmse, test_rmse = print_error(Ytrain, Yval, train_predict, test_predict)\nrmse_table['model'].append(\n    'Recurrent Neural Network (with L1L2 Regularization)')\nrmse_table['training_rmse'].append(train_rmse)\nrmse_table['testing_rmse'].append(test_rmse)\n\n\nTrain RMSE: 0.323 RMSE\nTest RMSE: 1.884 RMSE\n\n\n\n\n\n\n\n\nThe second plot shows the training loss with regularization; compared to the original (unregularized) training loss plot, we see a lot of differences. The regularization technique had a noticeable effect on the testing error of the model. The model without regularization have a training error of 0.324 RMSE but a higher testing error of 1.889 RMSE. On the other hand, the model with regularization had a marginally lower training error of 0.322 RMSE but a lower testing error of 1.885 RMSE.\n\n\nGRU Neural Network\nA GRU neural network is a powerful machine learning tool widely adopted in various applications. Its key feature is the recurrent gated unit, a type of memory cell that can store information over a data sequence. The GRU neural network is five hidden layers, one dense layer, and the hyperbolic tangent activation function. The model also includes kernel regularization to help address the issue of overfitting. The models are trained for 20 epochs and validation loss plots were generated to compare the performance of the regularized and non-regularized models. Overall, the GRU neural network has shown great promise in accurately predicting future movements in the DJI index.\n\nGRUGRU with Regularization\n\n\n\n\nCode\n# Create a GRU Neural Network\ndef create_GRU(hidden_units, dense_units, input_shape, activation,dropout_rate=0.2, kernel_regularizer=None):\n    model = Sequential()\n    # Create a simple GRU neural network layer\n    model.add(GRU(hidden_units, input_shape=input_shape,\n              activation=activation[0]))\n    # Add a dense layer (only one, more layers would make it a deep neural net)\n    model.add(Dense(units=dense_units,\n              activation=activation[1], kernel_regularizer=kernel_regularizer))\n    # Add layer dropout\n    model.add(Dropout(dropout_rate))\n    # Compile the model and optimize on mean squared error\n    model.compile(loss='mean_squared_error', optimizer='sgd')\n    return model\n\n# Training and evaluating a GRU-based model\nmodel = create_GRU(hidden_units=5, dense_units=1, input_shape=(lag, Xtrain.shape[-1]),\n                   activation=['tanh', 'relu'], dropout_rate = 0.2)\nhistory = model.fit(Xtrain, Ytrain, epochs=20, batch_size=1, verbose=0)\nplot_model(history, 'GRU Model')\n\nyhat_gru = [x[0] for x in model.predict(Xval, verbose=0)]\n\ntrain_predict = model.predict(Xtrain, verbose=0)\ntest_predict = model.predict(Xval, verbose=0)\n\n# Print error\ntrain_rmse, test_rmse = print_error(Ytrain, Yval, train_predict, test_predict)\nrmse_table['model'].append('GRU Neural Network')\nrmse_table['training_rmse'].append(train_rmse)\nrmse_table['testing_rmse'].append(test_rmse)\n\n\nTrain RMSE: 0.661 RMSE\nTest RMSE: 0.908 RMSE\n\n\n\n\n\n\n\n\n\nCode\n# Training and evaluating a GRU-based model with regularization\nmodel = create_GRU(hidden_units=5, dense_units=1, input_shape=(lag, Xtrain.shape[-1]),\n                   activation=['tanh', 'relu'],dropout_rate = 0.2,  kernel_regularizer=regularizers.L1L2(l1=1e-5, l2=1e-4))\nhistory = model.fit(Xtrain, Ytrain, epochs=20, batch_size=1, verbose=0)\nplot_model(history, 'GRU Model (with L1L2 Regularization)')\n\nyhat_gru_reg = [x[0] for x in model.predict(Xval, verbose=0)]\n\ntrain_predict = model.predict(Xtrain, verbose=0)\ntest_predict = model.predict(Xval, verbose=0)\n\n# Print error\ntrain_rmse, test_rmse = print_error(Ytrain, Yval, train_predict, test_predict)\nrmse_table['model'].append('GRU Neural Network (with L1L2 Regularization)')\nrmse_table['training_rmse'].append(train_rmse)\nrmse_table['testing_rmse'].append(test_rmse)\n\n\nTrain RMSE: 0.999 RMSE\nTest RMSE: 2.833 RMSE\n\n\n\n\n\n\n\n\nThe two plots show the training loss for the GRU model, one with regularization and one without. The plot with regularization shows less fluctuation, indicating that overfitting has been reduced. The model without regularization had a train RMSE of 0.659 and a test RMSE of 0.892, while the training and testing RMSE are slightly higher for regularization.\n\n\nLSTM Neural Network\nThe LSTM (Long Short-Term Memory) neural network is another powerful machine learning tool that has been widely adopted in various applications, particularly in tasks involving sequential data.The LSTM neural network architecture typically consists of multiple LSTM layers, followed by one or more dense layers and an activation function such as the sigmoid or hyperbolic tangent. Similar to the GRU neural network, the LSTM model may also include kernel regularization to address the issue of overfitting. In order to evaluate the performance of the LSTM model, the model can be trained on a dataset for a specified number of epochs, and validation loss plots can be generated to compare the performance of the regularized and non-regularized models.\n\nLSTMLSTM with Regularization\n\n\n\n\nCode\n# Create a LSTM Neural Network\ndef create_LSTM(hidden_units, dense_units, input_shape, activation,dropout_rate = 0.2, kernel_regularizer=None):\n    model = Sequential()\n    # Create a simple long short term memory neural network\n    model.add(LSTM(hidden_units,\n              activation=activation[0], input_shape=input_shape))\n    # Add a dense layer (only one, more layers would make it a deep neural net)\n    model.add(Dense(units=dense_units,\n              activation=activation[1], kernel_regularizer=kernel_regularizer))\n    # Add layer dropout\n    model.add(Dropout(dropout_rate))\n    # Compile the model and optimize on mean squared error\n    model.compile(optimizer=\"RMSprop\", loss='mae')\n    return model\n\n# Create an LSTM neural network\nmodel = create_LSTM(hidden_units=5, dense_units=1, input_shape=(lag, Xtrain.shape[-1]),\n                    activation=['tanh', 'linear'], dropout_rate = 0.2)\nhistory = model.fit(Xtrain, Ytrain, epochs=20, batch_size=1, verbose=0)\nplot_model(history, 'LSTM Model')\n\nyhat_lstm = [x[0] for x in model.predict(Xval, verbose=0)]\n\ntrain_predict = model.predict(Xtrain, verbose=0)\ntest_predict = model.predict(Xval, verbose=0)\n\n# Print error\ntrain_rmse, test_rmse = print_error(Ytrain, Yval, train_predict, test_predict)\nrmse_table['model'].append('LSTM Neural Network')\nrmse_table['training_rmse'].append(train_rmse)\nrmse_table['testing_rmse'].append(test_rmse)\n\n\nTrain RMSE: 0.208 RMSE\nTest RMSE: 0.796 RMSE\n\n\n\n\n\n\n\n\n\nCode\n# Create an LSTM neural network with regularization\nmodel = create_LSTM(hidden_units=5, dense_units=1, input_shape=(lag, Xtrain.shape[-1]),\n                    activation=['tanh', 'linear'], dropout_rate = 0.2, kernel_regularizer=regularizers.L1L2(l1=1e-5, l2=1e-4))\nhistory = model.fit(Xtrain, Ytrain, epochs=20, batch_size=1, verbose=0)\nplot_model(history, 'LSTM Model (with L1L2 Regularization)')\n\n\nyhat_lstm_reg = [x[0] for x in model.predict(Xval, verbose=0)]\n\ntrain_predict = model.predict(Xtrain, verbose=0)\ntest_predict = model.predict(Xval, verbose=0)\n\n# Print error\ntrain_rmse, test_rmse = print_error(Ytrain, Yval, train_predict, test_predict)\nrmse_table['model'].append('LSTM Neural Network (with L1L2 Regularization)')\nrmse_table['training_rmse'].append(train_rmse)\nrmse_table['testing_rmse'].append(test_rmse)\n\n\nTrain RMSE: 0.207 RMSE\nTest RMSE: 0.835 RMSE\n\n\n\n\n\n\n\n\nThe two plots show the training loss for the LSTM model, one with regularization and one without. The plot with regularization shows less fluctuation, indicating that overfitting has been reduced. The model without regularization had a train RMSE of 0.196 and a test RMSE of 0.845, while the model with regularization had a slightly higher train RMSE of 0.199 and a lower test RMSE of 0.884\n\n\nForecast\n\n\nCode\n# Creating the frame to store both predictions\ndays = df['Date'].values[-len(y):]\nframe = pd.concat([\n    pd.DataFrame({'day': days, 'price': y, 'type': 'original'}),\n    pd.DataFrame({'day': days, 'price': yhat_d, 'type': 'rnn_forecast'}),\n    pd.DataFrame({'day': days, 'price': yhat_gru, 'type': 'gru_forecast'}),\n    pd.DataFrame({'day': days, 'price': yhat_lstm, 'type': 'lstm_forecast'})\n])\n# Creating the unscaled values column\nframe['price_absolute'] = [(x * train_std) + train_mean for x in frame['price']]\n# Pivoting\npivoted = frame.pivot_table(index='day', columns='type')\npivoted.columns = ['_'.join(x).strip() for x in pivoted.columns.values]\n\nplt.figure(figsize=(12, 10))\nplt.plot(pivoted.index, pivoted.price_absolute_original,\n         color='#8B1874', label='original')\nplt.plot(pivoted.index, pivoted.price_absolute_rnn_forecast,\n         color='#F79540', label='RNN Forecast', alpha=0.6)\nplt.plot(pivoted.index, pivoted.price_absolute_gru_forecast,\n         color='#B71375', label='GRU Forecast', alpha=0.6)\nplt.plot(pivoted.index, pivoted.price_absolute_lstm_forecast,\n         color='#FC4F00', label='LSTM Forecast', alpha=0.6)\nplt.title('Dow Jones Index Stock Price Forecasts')\nplt.legend()\nplt.show()\n\n\n\n\n\nWhen comparing the predictions of the deep learning models with the original time series plot, we can observe that the LSTM model is the closest to the actual values. The GRU model is almost the same as the LSTM model, but both the models have some kind of accuracy with the actual plot, but there is still difference. RNN model are pretty far from the actual plot, which indicates that LSTM model is better when compared to others, but we need to note that the model is not accurate, i.e., it is not the same as the actual plot.\n\n\nModel Comparison\n\n\nCode\nrmse_df = pd.DataFrame(rmse_table)\nrmse_df\n\n\n\n\n\n\n  \n    \n      \n      model\n      training_rmse\n      testing_rmse\n    \n  \n  \n    \n      0\n      Recurrent Neural Network\n      0.322649\n      1.884412\n    \n    \n      1\n      Recurrent Neural Network (with L1L2 Regulariza...\n      0.323322\n      1.884474\n    \n    \n      2\n      GRU Neural Network\n      0.660934\n      0.908463\n    \n    \n      3\n      GRU Neural Network (with L1L2 Regularization)\n      0.998599\n      2.833358\n    \n    \n      4\n      LSTM Neural Network\n      0.208098\n      0.796179\n    \n    \n      5\n      LSTM Neural Network (with L1L2 Regularization)\n      0.207462\n      0.835074\n    \n  \n\n\n\n\nIn terms of performance on the testing dataset, the LSTM Neural Network without regularization appears to be the best performing model, with a testing_rmse of 0.845307. This is followed by the regularized LSTM with a testing_rmse of 0.883554, the regularized GRU with a testing_rmse of 0.909624, the non-regularized RNN with a testing_rmse of 1.888741, the regularized RNN with a testing_rmse of 1.885051, and finally, the non-regularized GRU with a testing_rmse of 0.892442.\nIt is important to note that the results can be influenced by the specific dataset and the problem at hand. However, in general, LSTMs are known to be effective in tasks that require capturing long-term dependencies in sequential data. GRUs, on the other hand, are a simpler variant of LSTMs and can be used as an alternative if the dataset is relatively small or if computational resources are limited. Regularization can help to prevent overfitting, which is a common problem in deep learning models, and can improve generalization performance.\nOverall, the results suggest that for this particular dataset, the LSTM Neural Network without regularization appears to be the best-performing model, followed closely by the regularized LSTM and the regularized GRU. The non-regularized models, both RNN and GRU, appear to perform worse than the regularized models, which highlights the importance of regularization in preventing overfitting and improving generalization performance.\n\n\nComparison of deep learning models with traditional single-variable time-series\nThe ARIMA model has an RMSE value of 385.9824. Comparing this value to the RMSE values of the deep learning models, we can see that the LSTM model with L1L2 regularization has the lowest testing RMSE, which is significantly lower than the ARIMA model. The other deep learning models also have lower RMSE values than the ARIMA model, with the highest testing RMSE value for the GRU model with L1L2 regularization.\nOverall, this suggests that the deep learning models outperform the ARIMA model in terms of predictive accuracy, with the LSTM model with L1L2 regularization being the most accurate among the deep learning models."
  },
  {
    "objectID": "ds.html",
    "href": "ds.html",
    "title": "",
    "section": "",
    "text": "The World Bank is a comprehensive platform that provides access to a wide range of economic, social, and environmental indicators from around the world. With data covering topics such as poverty, health, education, infrastructure, and trade, the World Bank data page is a valuable resource for researchers, policymakers, and anyone interested in global development. The platform features interactive tools and visualizations that allow users to explore and analyze data in meaningful ways, making it a powerful tool for understanding the complex challenges facing the world today.\nThe global stock market has been highly volatile and unpredictable in recent years due to various factors, including political tensions, trade disputes, and the ongoing COVID-19 pandemic. While some sectors, such as technology and healthcare, have seen significant growth and increased investor interest, others have struggled to keep up. Additionally, inflation concerns and shifts in monetary policy by central banks have added further uncertainty to the market. Despite these challenges, many investors continue to see potential for growth and value in the global stock market, and the availability of online trading platforms has made it easier than ever for individuals to participate in this dynamic and complex market.\n\n\n \n\n\n\n   \n\n\n\n\n\n\n\nDownload CSV"
  },
  {
    "objectID": "ds.html#sector-market-data",
    "href": "ds.html#sector-market-data",
    "title": "",
    "section": "Sector Market Data",
    "text": "Sector Market Data\nA stock market sector is a group of stocks that have a lot in common with each other, usually because they are in similar industries. There are 11 different stock market sectors, according to the most commonly used classification system: the Global Industry Classification Standard (GICS).\n\n\n\n\n\nCode\nlibrary(wesanderson)\noptions(\"getSymbols.warning4.0\"=FALSE)\noptions(\"getSymbols.yahoo.warning\"=FALSE)\n\ntickers = c(\"XLC\",\"XLY\",\"XLP\", \"XLE\", \"XLF\", \"XLV\",\"XLI\",\"XLB\", \"XLRE\", \"XLK\",\"XLU\")\nfor (i in tickers){\n  getSymbols(i,\n             from = \"2018-07-01\",\n             to = \"2023-01-01\")}\n\nx <- list(\n  title = \"date\"\n)\ny <- list(\n  title = \"value\"\n)\n\nstock <- data.frame(XLC$XLC.Adjusted,\n                    XLY$XLY.Adjusted,\n                    XLP$XLP.Adjusted,\n                    XLE$XLE.Adjusted,\n                    XLF$XLF.Adjusted,\n                    XLV$XLV.Adjusted,\n                    XLI$XLI.Adjusted,\n                    XLB$XLB.Adjusted,\n                    XLRE$XLRE.Adjusted,\n                    XLK$XLK.Adjusted,\n                    XLU$XLU.Adjusted)\n\n#create dataframe\nsector_stock_data = cbind(XLC,XLY,XLP, XLE, XLF, XLV,XLI,XLB, XLRE, XLK,XLU)\nsector_stock_data = as.data.frame(sector_stock_data)\n#export it to csv file\nwrite_csv(sector_stock_data, \"DATA/RAW DATA/sector_stock_data.csv\")\n\n\nstock <- data.frame(stock,rownames(stock))\ncolnames(stock) <- append(tickers,'Dates')\n\nstock$date<-as.Date(stock$Dates,\"%Y-%m-%d\")\n\nplot<- ggplot(stock, aes(x=date)) +\n  geom_line(aes(y=XLC, colour=\"Communication Services\"))+\n  geom_line(aes(y=XLY, colour=\"Consumer Discretionary\"))+\n  geom_line(aes(y=XLP, colour=\"Consumer Staples\"))+\n  geom_line(aes(y=XLE, colour=\"Energy\"))+\n  geom_line(aes(y=XLF, colour=\"Financials\"))+\n  geom_line(aes(y=XLV, colour=\"Health Care\"))+\n  geom_line(aes(y=XLI, colour=\"Industrials\"))+\n  geom_line(aes(y=XLB, colour=\"Materials\"))+\n  geom_line(aes(y=XLRE, colour=\"Real Estate\"))+\n  geom_line(aes(y=XLK, colour=\"Technology\"))+\n  geom_line(aes(y=XLU, colour=\"Utilities\"))+\n  scale_color_brewer(palette=\"BuPu\")+\n  theme_bw()+\n   labs(\n    title = \"Stock Market Sector History\",\n    subtitle = \"From July 2018 - January 2023\",\n    x = \"Date\",\n    y = \"Adjusted Closing Prices\")+\n    guides(colour=guide_legend(title=\"Sectors\")) \n\n  \n\n\n\nggplotly(plot)%>%\n  layout(title = list(text = paste0('Stock Market Sector History',\n                                    '<br>',\n                                    '<sup>',\n                                    'From July 2018 - January 2023',\n                                    '</sup>')))\n\n\n\n\n\n\n\nDownload CSV\n\n\nHistorical Asset Data\nAnalyzing historical returns on various investments is one of the most important tasks in financial markets. We need historical data for the assets to perform this analysis. There are numerous data providers, some of which are free while the majority are not. R Package designed to assist the quantitative trader in the development, testing, and deployment of statistically based trading models. The Quantmod – “Quantitative Financial Modeling and Trading Framework for R”! package can load data, chart data, and generate relevant technical signals. This package is compatible with a variety of sources, including Yahoo Finance and FRED. By default, getSymbols() downloads data from Yahoo at the daily frequency and the object has 6 columns representing the open, high, low, close, volume and adjusted closing price for the stock.\n\n\nYahoo Finance\nThe Yahoo Finance gives users immediate access to hours of live market coverage each day, complete with in-depth analysis and data. Investors, financial experts, and corporate executives who are serious about their money belong here. You can access this data at https://finance.yahoo.com/ ."
  },
  {
    "objectID": "ds.html#global-stock-indices-historical-data",
    "href": "ds.html#global-stock-indices-historical-data",
    "title": "",
    "section": "Global Stock Indices Historical Data",
    "text": "Global Stock Indices Historical Data\nGlobal indices are a benchmark to evaluate the strength or weakness in the overall market. Normally, a sample of highly liquid and valuable stocks from the universe of listed stocks is selected and made into an index. The weighted movement of these set of stocks or portfolio of stocks constitutes the movement of global indices. So, if global indices are moving up that means the markets are strong and if global indices are moving lower that means global markets are weak. You can understand global indices as a hypothetical portfolio of investment holdings that represents a segment of the financial market or the global indices market. The calculation of the index value is derived from the prices of the underlying stocks or assets in the index.\nThe data is importing continent-wise, to check the top stock market index around the world.\n\nNorth America and South AmericaEurope and AfricaAsia and Australia\n\n\n\n\nCode\noptions(\"getSymbols.warning4.0\"=FALSE)\noptions(\"getSymbols.yahoo.warning\"=FALSE)\n\n#America's top stock market index\ntickers = c(\"^GSPC\",\"^DJI\",\"^IXIC\", \"^RUT\")\n\nfor (i in tickers){\n  getSymbols(i,\n             from = \"2000-01-01\",\n             to = \"2023-01-01\")}\n\nx <- list(\n  title = \"date\"\n)\ny <- list(\n  title = \"value\"\n)\n\n#create dataframe\namerica_stock_index_data = cbind(GSPC,DJI,IXIC,RUT)\namerica_stock_index_data = as.data.frame(america_stock_index_data)\n#export it to csv file\nwrite_csv(america_stock_index_data, \"DATA/RAW DATA/america_stock_index_data.csv\")\n\nstock <- data.frame(GSPC$GSPC.Adjusted,\n                    DJI$DJI.Adjusted,\n                    IXIC$IXIC.Adjusted,\n                    RUT$RUT.Adjusted)\n\n\n\nstock <- data.frame(stock,rownames(stock))\ncolnames(stock) <- append(tickers,'Dates')\n\nstock$date<-as.Date(stock$Dates,\"%Y-%m-%d\")\ncolnames(stock)=c(\"GSPC\",\"DJI\",\"IXIC\",\"RUT\",\"Dates\",\"date\")\n\n\n#remove columns\nstock <- stock[,-c(5)]\nplot<- ggplot(stock, aes(x=date)) +\n  geom_line(aes(y=GSPC, colour=\"S&P 500 Index\"))+\n  geom_line(aes(y=DJI, colour=\"Dow Jones Industrial Average\"))+\n  geom_line(aes(y=IXIC, colour=\"NASDAQ Composite\"))+\n  geom_line(aes(y=RUT, colour=\"Russell 2000\"))+\n  scale_color_brewer(palette=\"RdPu\")+\n  theme_bw()+\n   labs(\n    title = \"America's Top Stock Market Index History\",\n    subtitle = \"From January 2000 - January 2023\",\n    x = \"Date\",\n    y = \"Adjusted Closing Prices\")+\n    guides(colour=guide_legend(title=\"Indices\")) \n\n  \n\n\nplot = ggplotly(plot)%>%\n  layout(title = list(text = paste0(\"America's Top Stock Market Index History\",\n                                    '<br>',\n                                    '<sup>',\n                                    'From January 2000 - January 2023',\n                                    '</sup>')))\nggplotly(plot)%>%layout(hovermode = \"x\")\n\n\n\n\n\n\n\nDownload CSV\n\n\n\n\n\nCode\n#Europe and Africa Top Indices\noptions(\"getSymbols.warning4.0\"=FALSE)\noptions(\"getSymbols.yahoo.warning\"=FALSE)\ntickers = c(\"^GDAXI\",\"^FTSE\",\"^FCHI\", \"^IBEX\",\"^STOXX50E\")\nfor (i in tickers){\n  getSymbols(i,\n             from = \"2018-01-01\",\n             to = \"2023-01-01\", periodicity=\"monthly\")}\n\nx <- list(\n  title = \"date\"\n)\ny <- list(\n  title = \"value\"\n)\n\n#create dataframe\neurope_africa_stock_index_data = cbind(GDAXI,FTSE,FCHI,IBEX,STOXX50E)\neurope_africa_stock_index_data = as.data.frame(europe_africa_stock_index_data)\n#export it to csv file\nwrite_csv(europe_africa_stock_index_data, \"DATA/RAW DATA/europe_africa_stock_index_data.csv\")\n\n\nstock <- data.frame(GDAXI$GDAXI.Adjusted,\n                    FTSE$FTSE.Adjusted,\n                    FCHI$FCHI.Adjusted,\n                    IBEX$IBEX.Adjusted,\n                    STOXX50E$STOXX50E.Adjusted)\n\n\nstock <- data.frame(stock,rownames(stock))\ncolnames(stock) <- append(tickers,'Dates')\n\nstock$date<-as.Date(stock$Dates,\"%Y-%m-%d\")\ncolnames(stock)=c(\"GDAXI\",\"FTSE\",\"FCHI\",\"IBEX\",\"STOXX50E\",\"Dates\",\"date\")\n#remove columns\nstock <- stock[,-c(6)] \nplot<- ggplot(stock, aes(x=date)) +\n  geom_line(aes(y=GDAXI, colour=\"DAX PERFORMANCE-INDEX\"))+\n  geom_line(aes(y=FTSE, colour=\"FTSE 100 Index\"))+\n  geom_line(aes(y=FCHI, colour=\"CAC 40\"))+\n  geom_line(aes(y=IBEX, colour=\"IBEX 35\"))+\n  geom_line(aes(y=STOXX50E, colour=\"EURO STOXX 50\"))+\n  scale_color_brewer(palette=\"ВuPu\")+\n  theme_bw()+\n   labs(\n    title = \"Europes and Africa's Top Stock Market Index History\",\n    subtitle = \"From January 2018 - January 2023\",\n    x = \"Date\",\n    y = \"Adjusted Closing Prices\")+\n    guides(colour=guide_legend(title=\"Indices\")) \n\n  \n\n\nggplotly(plot)%>%\n  layout(title = list(text = paste0(\"Europes and Africa's Top Stock Market Index History\",\n                                    '<br>',\n                                    '<sup>',\n                                    'From January 2018 - January 2023',\n                                    '</sup>')))\n\n\n\n\n\n\n\nDownload CSV\n\n\n\n\n\nCode\n#Asia and Australia Top Indices\noptions(\"getSymbols.warning4.0\"=FALSE)\noptions(\"getSymbols.yahoo.warning\"=FALSE)\ntickers = c(\"^N225\",\"^HSI\", \"^BSESN\",\"^NSEI\",\"^KS11\", \"^AORD\")\nfor (i in tickers){\n  getSymbols(i,\n             from = \"2012-01-01\",\n             to = \"2023-01-01\", periodicity=\"monthly\")}\n\nx <- list(\n  title = \"date\"\n)\ny <- list(\n  title = \"value\"\n)\n\n#create dataframe\nasia_australia_stock_index_data = cbind(N225,HSI,BSESN,NSEI,AORD)\nasia_australia_stock_index_data = as.data.frame(asia_australia_stock_index_data)\n#export it to csv file\nwrite_csv(asia_australia_stock_index_data, \"DATA/RAW DATA/asia_australia_stock_index_data.csv\")\n\nstock <- data.frame(N225$N225.Adjusted,\n                    HSI$HSI.Adjusted,\n                    BSESN$BSESN.Adjusted,\n                    NSEI$NSEI.Adjusted,\n                    KS11$KS11.Adjusted,\n                    AORD$AORD.Adjusted)\n\n\nstock <- data.frame(stock,rownames(stock))\ncolnames(stock) <- append(tickers,'Dates')\n\nstock$date<-as.Date(stock$Dates,\"%Y-%m-%d\")\ncolnames(stock)=c(\"N225\",\"HSI\",\"BSESN\",\"NSEI\",\"KS11\",\"AORD\",\"Dates\",\"date\")\n#remove columns\nstock <- stock[,-c(7)] \nplot<- ggplot(stock, aes(x=date)) +\n  geom_line(aes(y=N225, colour=\"Nikkei 225\"))+\n  geom_line(aes(y=HSI, colour=\"Hang Seng Index\"))+\n  geom_line(aes(y=BSESN, colour=\"BSE SENSEX\"))+\n  geom_line(aes(y=NSEI, colour=\"NIFTY 50\"))+\n  geom_line(aes(y=KS11, colour=\"KOSPI Composite Index\"))+\n  geom_line(aes(y=AORD, colour=\"ALL ORDINARIES\"))+\n  scale_color_brewer(palette=\"PuBu\")+\n  theme_bw()+\n   labs(\n    title = \"Asia's Top Stock Market Index History\",\n    subtitle = \"From January 2012 - January 2023\",\n    x = \"Date\",\n    y = \"Adjusted Closing Prices\")+\n    guides(colour=guide_legend(title=\"Indices\")) \n\n  \n\n\nggplotly(plot)%>%\n  layout(title = list(text = paste0(\"Asia and Australia's Top Stock Market Index History\",\n                                    '<br>',\n                                    '<sup>',\n                                    'From January 2012 - January 2023',\n                                    '</sup>')))\n\n\n\n\n\n\n\nDownload CSV"
  },
  {
    "objectID": "ds.html#macroeconomic-factors-data",
    "href": "ds.html#macroeconomic-factors-data",
    "title": "",
    "section": "Macroeconomic Factors Data",
    "text": "Macroeconomic Factors Data\nMacroeconomic factors such as inflation rates, gross domestic product (GDP), unemployment rates, and interest rates have a significant impact on financial markets. Historical data on these factors is crucial for analyzing and predicting market trends.\n\nThe Federal Reserve Economic Data\nThe Federal Reserve Economic Data (FRED) website, maintained by the Federal Reserve Bank of St. Louis, is a reliable source for GDP growth rate data. It offers a range of data frequencies and time periods, with a user-friendly interface featuring interactive charts and graphs to visualize trends and patterns. The website also provides tools and resources to manipulate and analyze the data, making it a valuable resource for researchers, analysts, and policymakers alike. You can access this data at https://fred.stlouisfed.org/.\n\n\nU.S. Bureau of Labor Statistics\nThe U.S. Bureau of Labor Statistics (BLS) is a government agency that collects and disseminates data related to labor economics and statistics. One of the key data sets provided by the BLS is information on employment, unemployment, and wages. This data is widely used by economists, policymakers, and businesses to analyze labor market trends and make informed decisions. Additionally, the BLS provides data on other economic indicators such as inflation and productivity. The agency’s commitment to providing accurate, reliable, and timely data has made it a trusted source for labor market information in the United States. You can access this data at https://www.bls.gov/.\n\nAbout the Macroeconomic Factor Data\nThe economic data related to inflation, unemployment, GDP growth rate and interest rate are crucial indicators of the health of an economy. Inflation rate measures the general increase in prices of goods and services over time, which is an essential measure of price stability. The unemployment rate measures the percentage of people who are seeking employment but are unable to find it, indicating the level of economic activity and labor market health. GDP growth rate measures the change in the value of goods and services produced in an economy over time and is an indicator of economic performance. Interest rates measure the cost of borrowing money, and changes in interest rates can have significant impacts on consumer spending, investment, and borrowing decisions.\n\nGDP Growth RateInterest RateInflation RateUnemployment Rate\n\n\n\n\nCode\n#import the data\ngdp <- read.csv(\"DATA/RAW DATA/gdp-growth.csv\")\n\n#change date format\ngdp$DATE <- as.Date(gdp$DATE , \"%m/%d/%Y\")\n\n#plot gdp growth rate\nfig <- plot_ly(gdp, x = ~DATE, y = ~value, type = 'scatter', mode = 'lines',line = list(color = 'rgb(158,202,225)'))\nfig <- fig %>% layout(title = \"U.S GDP Growth Rate: 2010 - 2022\",xaxis = list(title = \"Year\"),yaxis = list(title =\"GDP Growth Rate\"))\nfig\n\n\n\n\n\n\n\nDownload CSV\n\n\n\n\n\nCode\n#import the data\ninterest_data <- read.csv(\"DATA/RAW DATA/interest-rate.csv\")\n\n#change date format\ninterest_data$Date <- as.Date(interest_data$Date , \"%m/%d/%Y\")\n\n#plot interest rate \nfig <- plot_ly(interest_data, x = ~Date, y = ~value, type = 'scatter', mode = 'lines',line = list(color = 'rgb(222, 92, 92)'))\nfig <- fig %>% layout(title = \"U.S Interest Rate: January 2010 - March 2023\",xaxis = list(title = \"Time\"),yaxis = list(title =\"Interest Rate\"))\nfig\n\n\n\n\n\n\n\nDownload CSV\n\n\n\n\n\nCode\n#import the data\ninflation_rate <- read.csv(\"DATA/RAW DATA/inflation-rate.csv\")\n\n\n#cleaning the data\n#remove unwanted columns\ninflation_rate_clean <- subset(inflation_rate, select = -c(1,HALF1,HALF2))\n\n#convert the data to time series data\ninflation_data_ts <- ts(as.vector(t(as.matrix(inflation_rate_clean))), start=c(2010,1), end=c(2023,2), frequency=12)\n\n#plot inflation rate \nfig <- autoplot(inflation_data_ts, ylab = \"Inflation Rate\", color=\"#45818E\")+ggtitle(\"U.S Inflation Rate: January 2010 - February 2023\")+theme_minimal()\nggplotly(fig)\n\n\n\n\n\n\n\nDownload CSV\n\n\n\n\n\nCode\n#import the data\nunemployment_rate <- read.csv(\"DATA/RAW DATA/unemployment-rate.csv\")\n\n#change date format\nunemployment_rate$Date <- as.Date(unemployment_rate$Date , \"%m/%d/%Y\")\n\n#plot unemployment rate \n#plot interest rate \nfig <- plot_ly(unemployment_rate, x = ~Date, y = ~Value, type = 'scatter', mode = 'lines',line = list(color = 'rgb(135, 153, 164)'))\nfig <- fig %>% layout(title = \"U.S Unemployment Rate: January 2010 - March 2023\",xaxis = list(title = \"Time\"),yaxis = list(title =\"Unemployment Rate\"))\nfig\n\n\n\n\n\n\n\nDownload CSV"
  },
  {
    "objectID": "dl.html",
    "href": "dl.html",
    "title": "",
    "section": "",
    "text": "UNIVARIATE DEEP LEARNING PREDICTION\nUnivariate deep learning models are a type of machine learning algorithm that uses a single input variable to predict future values of that variable. In the context of stock market analysis, univariate models can be used to make predictions based on historical data for a particular stock or stock index.\nBy using univariate deep learning models, such as recurrent neural networks (RNNs), long short-term memory (LSTM) networks, and gated recurrent units (GRUs), analysts can make predictions on the future values of these stock indices based on their historical data. These models are designed to capture complex patterns and trends in the data, which can be used to inform investment strategies and help traders make informed decisions.\n\n\nUNIVARIATE DEEP LEARNING PREDICTION FOR STOCK INDEX\nIn this context, we will be looking at the top 3 stock price indexes: S&P 500, Dow Jones Industrial Average (DJIA), and Nasdaq Composite. These indexes are widely used as benchmarks for the overall performance of the US stock market, and predicting their future movements is of great interest to investors, traders, and financial analysts. By using deep learning models, we can leverage the power of neural networks to capture complex patterns and relationships in the historical data, and use them to make more accurate predictions about future movements of these stock price indexes.\nClick to view Dow Jones Index Stock Price\nClick to view NASDAQ Composite Index Stock Price\nClick to view S&P 500 Index Stock Price\n\n\nUNIVARIATE DEEP LEARNING PREDICTION FOR SECTOR MARKET\nIn this context, we will be looking at the sector market, which is made up of indexes that track the performance of specific industry sectors within the stock market, such as technology, healthcare, and energy. These sector indexes are widely used as indicators of the performance of their respective industries and can provide insights into the overall health of the stock market. By using deep learning models, we can leverage the power of neural networks to capture complex patterns and relationships in the historical data of a specific sector index and use them to make more accurate predictions about future movements of that index. This can be particularly useful for investors, traders, and financial analysts who specialize in a specific industry sector and need to make informed decisions based on its market trends.\nClick to view Consumer Staples Sector Fund\nClick to view Utilities Sector Fund\nClick to view Health Care Sector Fund\nClick to view Industrial Sector Fund\nClick to view Financial Sector Fund\nClick to view Consumer Discretionary Sector Fund\nClick to view Communication Services Sector Fund\nClick to view Real Estate Sector Fund\nClick to view Materials Sector Fund\nClick to view Technology Sector Fund\nClick to view Energy Sector Fund"
  }
]